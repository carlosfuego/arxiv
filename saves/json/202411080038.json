[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v1",
                "updated": "2024-11-05T15:22:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v1",
                "updated": "2024-11-04T18:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v1",
                "updated": "2024-11-04T04:15:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v1",
                "updated": "2024-11-02T15:45:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "$\\texttt{HEXA-MoE}$: Efficient and Heterogeneous-aware MoE Acceleration\n  with ZERO Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{HEXA-MoE}$: Efficient and Heterogeneous-aware MoE Acceleration\n  with ZERO Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, \\textit{i.e.}, reducing\n$10\\%\\sim48\\%$ memory consumption and achieving $0.5\\sim4.3\\times$ speed up\ncompared to current state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at\n\\href{https://github.com/UNITES-Lab/HEXA-MoE}{\\underline{here}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, \\textit{i.e.}, reducing\n$10\\%\\sim48\\%$ memory consumption and achieving $0.5\\sim4.3\\times$ speed up\ncompared to current state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at\n\\href{https://github.com/UNITES-Lab/HEXA-MoE}{\\underline{here}}."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v2",
                "updated": "2024-10-28T19:32:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    32,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18126v1",
                "updated": "2024-10-17T04:37:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:37:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers"
                },
                "summary": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shubham"
                    },
                    {
                        "name": "Keichi Takahashi"
                    },
                    {
                        "name": "Hiroyuki Takizawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Takizawa"
                },
                "author": "Hiroyuki Takizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13212v1",
                "updated": "2024-10-17T04:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations"
                },
                "summary": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.04118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04118v1",
                "updated": "2024-11-06T18:51:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T18:51:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "Medical Adaptation of Large Language and Vision-Language Models: Are We\n  Making Progress?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Adaptation of Large Language and Vision-Language Models: Are We\n  Making Progress?"
                },
                "summary": "Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare seven\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting regime for medical question-answering (QA) tasks. For instance,\nacross the tasks and model pairs we consider in the 3-shot setting, medical\nLLMs only outperform their base models in 12.1% of cases, reach a (statistical)\ntie in 49.8% of cases, and are significantly worse than their base models in\nthe remaining 38.2% of cases. Our conclusions are based on (i) comparing each\nmedical model head-to-head, directly against the corresponding base model; (ii)\noptimizing the prompts for each model separately; and (iii) accounting for\nstatistical uncertainty in comparisons. While these basic practices are not\nconsistently adopted in the literature, our ablations show that they\nsubstantially impact conclusions. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare seven\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting regime for medical question-answering (QA) tasks. For instance,\nacross the tasks and model pairs we consider in the 3-shot setting, medical\nLLMs only outperform their base models in 12.1% of cases, reach a (statistical)\ntie in 49.8% of cases, and are significantly worse than their base models in\nthe remaining 38.2% of cases. Our conclusions are based on (i) comparing each\nmedical model head-to-head, directly against the corresponding base model; (ii)\noptimizing the prompts for each model separately; and (iii) accounting for\nstatistical uncertainty in comparisons. While these basic practices are not\nconsistently adopted in the literature, our ablations show that they\nsubstantially impact conclusions. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies."
                },
                "authors": [
                    {
                        "name": "Daniel P. Jeong"
                    },
                    {
                        "name": "Saurabh Garg"
                    },
                    {
                        "name": "Zachary C. Lipton"
                    },
                    {
                        "name": "Michael Oberst"
                    }
                ],
                "author_detail": {
                    "name": "Michael Oberst"
                },
                "author": "Michael Oberst",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference as Long Paper (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04109v1",
                "updated": "2024-11-06T18:36:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    36,
                    22,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T18:36:22Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    36,
                    22,
                    2,
                    311,
                    0
                ],
                "title": "Self-Consistency Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Consistency Preference Optimization"
                },
                "summary": "Self-alignment, whereby models learn to improve themselves without human\nannotation, is a rapidly growing research area. However, existing techniques\noften fail to improve complex reasoning tasks due to the difficulty of\nassigning correct rewards. An orthogonal approach that is known to improve\ncorrectness is self-consistency, a method applied at inference time based on\nmultiple sampling in order to find the most consistent answer. In this work, we\nextend the self-consistency concept to help train models. We thus introduce\nself-consistency preference optimization (ScPO), which iteratively trains\nconsistent answers to be preferred over inconsistent ones on unsupervised new\nproblems. We show ScPO leads to large improvements over conventional reward\nmodel training on reasoning tasks such as GSM8K and MATH, closing the gap with\nsupervised training with gold answers or preferences, and that combining ScPO\nwith standard supervised learning improves results even further. On ZebraLogic,\nScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and\nClaude-3 Haiku.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-alignment, whereby models learn to improve themselves without human\nannotation, is a rapidly growing research area. However, existing techniques\noften fail to improve complex reasoning tasks due to the difficulty of\nassigning correct rewards. An orthogonal approach that is known to improve\ncorrectness is self-consistency, a method applied at inference time based on\nmultiple sampling in order to find the most consistent answer. In this work, we\nextend the self-consistency concept to help train models. We thus introduce\nself-consistency preference optimization (ScPO), which iteratively trains\nconsistent answers to be preferred over inconsistent ones on unsupervised new\nproblems. We show ScPO leads to large improvements over conventional reward\nmodel training on reasoning tasks such as GSM8K and MATH, closing the gap with\nsupervised training with gold answers or preferences, and that combining ScPO\nwith standard supervised learning improves results even further. On ZebraLogic,\nScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and\nClaude-3 Haiku."
                },
                "authors": [
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Weizhe Yuan"
                    },
                    {
                        "name": "Richard Yuanzhe Pang"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Maryam Fazel-Zarandi"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Jane Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jane Yu"
                },
                "author": "Jane Yu",
                "arxiv_comment": "16 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04105v1",
                "updated": "2024-11-06T18:35:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    35,
                    32,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T18:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    35,
                    32,
                    2,
                    311,
                    0
                ],
                "title": "How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis"
                },
                "summary": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve, but we can train a small\ntransformer to achieve perfect accuracy. Building on our set-up, we then pursue\nan understanding of precisely how a three-layer transformer, trained from\nscratch, solves this problem. We are able to identify certain \"planning\" and\n\"reasoning\" circuits in the network that necessitate cooperation between the\nattention blocks to implement the desired logic. To expand our findings, we\nthen study a larger model, Mistral 7B. Using activation patching, we\ncharacterize internal components that are critical in solving our logic\nproblem. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve, but we can train a small\ntransformer to achieve perfect accuracy. Building on our set-up, we then pursue\nan understanding of precisely how a three-layer transformer, trained from\nscratch, solves this problem. We are able to identify certain \"planning\" and\n\"reasoning\" circuits in the network that necessitate cooperation between the\nattention blocks to implement the desired logic. To expand our findings, we\nthen study a larger model, Mistral 7B. Using activation patching, we\ncharacterize internal components that are critical in solving our logic\nproblem. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason."
                },
                "authors": [
                    {
                        "name": "Guan Zhe Hong"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Enming Luo"
                    },
                    {
                        "name": "Cyrus Rashtchian"
                    },
                    {
                        "name": "Rina Panigrahy"
                    }
                ],
                "author_detail": {
                    "name": "Rina Panigrahy"
                },
                "author": "Rina Panigrahy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11832v2",
                "updated": "2024-11-06T18:07:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    7,
                    3,
                    2,
                    311,
                    0
                ],
                "published": "2024-08-06T15:49:58Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    15,
                    49,
                    58,
                    1,
                    219,
                    0
                ],
                "title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs"
                },
                "summary": "The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for automatic tools to check the factual accuracy\nof their outputs, as LLMs often hallucinate. This is difficult as it requires\nassessing the factuality of free-form open-domain responses. While there has\nbeen a lot of research on this topic, different papers use different evaluation\nbenchmarks and measures, which makes them hard to compare and hampers future\nprogress. To mitigate these issues, we developed OpenFactCheck, a unified\nframework, with three modules: (i) RESPONSEEVAL, which allows users to easily\ncustomize an automatic fact-checking system and to assess the factuality of all\nclaims in an input document using that system, (ii) LLMEVAL, which assesses the\noverall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate\nautomatic fact-checking systems. OpenFactCheck is open-sourced\n(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python\nlibrary (https://pypi.org/project/openfactcheck/) and also as a web service\n(http://app.openfactcheck.com). A video describing the system is available at\nhttps://youtu.be/-i9VKL0HleI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for automatic tools to check the factual accuracy\nof their outputs, as LLMs often hallucinate. This is difficult as it requires\nassessing the factuality of free-form open-domain responses. While there has\nbeen a lot of research on this topic, different papers use different evaluation\nbenchmarks and measures, which makes them hard to compare and hampers future\nprogress. To mitigate these issues, we developed OpenFactCheck, a unified\nframework, with three modules: (i) RESPONSEEVAL, which allows users to easily\ncustomize an automatic fact-checking system and to assess the factuality of all\nclaims in an input document using that system, (ii) LLMEVAL, which assesses the\noverall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate\nautomatic fact-checking systems. OpenFactCheck is open-sourced\n(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python\nlibrary (https://pypi.org/project/openfactcheck/) and also as a web service\n(http://app.openfactcheck.com). A video describing the system is available at\nhttps://youtu.be/-i9VKL0HleI."
                },
                "authors": [
                    {
                        "name": "Hasan Iqbal"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Georgi Georgiev"
                    },
                    {
                        "name": "Jiahui Geng"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "arxiv_comment": "11 pages, 4 Figures, 3 Tables, Accepted at EMNLP 2024 System\n  Demonstration. arXiv admin note: substantial text overlap with\n  arXiv:2405.05583",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01632v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01632v4",
                "updated": "2024-11-06T18:04:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    4,
                    35,
                    2,
                    311,
                    0
                ],
                "published": "2024-03-03T22:38:35Z",
                "published_parsed": [
                    2024,
                    3,
                    3,
                    22,
                    38,
                    35,
                    6,
                    63,
                    0
                ],
                "title": "SynCode: LLM Generation with Grammar Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynCode: LLM Generation with Grammar Augmentation"
                },
                "summary": "LLMs are widely used in complex AI applications. These applications\nunderscore the need for LLM outputs to adhere to a specific format, for their\nintegration with other components in the systems. Typically the format rules\ne.g., for data serialization formats such as JSON, YAML, or Code in Programming\nLanguage are expressed as context-free grammar (CFG). Due to the hallucinations\nand unreliability of LLMs, instructing LLMs to adhere to specified syntax\nbecomes an increasingly important challenge.\n  We present SynCode, a novel framework for efficient and general syntactical\ndecoding with LLMs, to address this challenge. SynCode ensures soundness and\ncompleteness with respect to the CFG of a formal language, effectively\nretaining valid tokens while filtering out invalid ones. SynCode uses an\noffline-constructed, efficient lookup table, the DFA mask store, derived from\nthe DFA of the language's grammar for efficient generation. SynCode seamlessly\nintegrates with any language defined by CFG, as evidenced by experiments\nfocusing on generating JSON, Python, and Go outputs. Our experiments evaluating\nthe effectiveness of SynCode for JSON generation demonstrate that SynCode\neliminates all syntax errors and significantly outperforms state-of-the-art\nbaselines. Furthermore, our results underscore how SynCode significantly\nreduces 96.07% of syntax errors in generated Python and Go code, showcasing its\nsubstantial impact on enhancing syntactical precision in LLM generation. Our\ncode is available at https://github.com/uiuc-focal-lab/syncode",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are widely used in complex AI applications. These applications\nunderscore the need for LLM outputs to adhere to a specific format, for their\nintegration with other components in the systems. Typically the format rules\ne.g., for data serialization formats such as JSON, YAML, or Code in Programming\nLanguage are expressed as context-free grammar (CFG). Due to the hallucinations\nand unreliability of LLMs, instructing LLMs to adhere to specified syntax\nbecomes an increasingly important challenge.\n  We present SynCode, a novel framework for efficient and general syntactical\ndecoding with LLMs, to address this challenge. SynCode ensures soundness and\ncompleteness with respect to the CFG of a formal language, effectively\nretaining valid tokens while filtering out invalid ones. SynCode uses an\noffline-constructed, efficient lookup table, the DFA mask store, derived from\nthe DFA of the language's grammar for efficient generation. SynCode seamlessly\nintegrates with any language defined by CFG, as evidenced by experiments\nfocusing on generating JSON, Python, and Go outputs. Our experiments evaluating\nthe effectiveness of SynCode for JSON generation demonstrate that SynCode\neliminates all syntax errors and significantly outperforms state-of-the-art\nbaselines. Furthermore, our results underscore how SynCode significantly\nreduces 96.07% of syntax errors in generated Python and Go code, showcasing its\nsubstantial impact on enhancing syntactical precision in LLM generation. Our\ncode is available at https://github.com/uiuc-focal-lab/syncode"
                },
                "authors": [
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Hangoo Kang"
                    },
                    {
                        "name": "Sasa Misailovic"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01632v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01632v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03294v2",
                "updated": "2024-11-06T17:53:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    17,
                    53,
                    26,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-05T17:41:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    41,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy\n  For Visuomotor Imitation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy\n  For Visuomotor Imitation Learning"
                },
                "summary": "We propose an object-centric recovery policy framework to address the\nchallenges of out-of-distribution (OOD) scenarios in visuomotor policy\nlearning. Previous behavior cloning (BC) methods rely heavily on a large amount\nof labeled data coverage, failing in unfamiliar spatial states. Without relying\non extra data collection, our approach learns a recovery policy constructed by\nan inverse policy inferred from object keypoint manifold gradient in the\noriginal training data. The recovery policy serves as a simple add-on to any\nbase visuomotor BC policy, agnostic to a specific method, guiding the system\nback towards the training distribution to ensure task success even in OOD\nsituations. We demonstrate the effectiveness of our object-centric framework in\nboth simulation and real robot experiments, achieving an improvement of 77.7%\nover the base policy in OOD. Project Website:\nhttps://sites.google.com/view/ocr-penn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an object-centric recovery policy framework to address the\nchallenges of out-of-distribution (OOD) scenarios in visuomotor policy\nlearning. Previous behavior cloning (BC) methods rely heavily on a large amount\nof labeled data coverage, failing in unfamiliar spatial states. Without relying\non extra data collection, our approach learns a recovery policy constructed by\nan inverse policy inferred from object keypoint manifold gradient in the\noriginal training data. The recovery policy serves as a simple add-on to any\nbase visuomotor BC policy, agnostic to a specific method, guiding the system\nback towards the training distribution to ensure task success even in OOD\nsituations. We demonstrate the effectiveness of our object-centric framework in\nboth simulation and real robot experiments, achieving an improvement of 77.7%\nover the base policy in OOD. Project Website:\nhttps://sites.google.com/view/ocr-penn"
                },
                "authors": [
                    {
                        "name": "George Jiayuan Gao"
                    },
                    {
                        "name": "Tianyu Li"
                    },
                    {
                        "name": "Nadia Figueroa"
                    }
                ],
                "author_detail": {
                    "name": "Nadia Figueroa"
                },
                "author": "Nadia Figueroa",
                "arxiv_comment": "Accepted for Spotlight (5 out of 21 papers) at CoRL 2024 Workshop on\n  Lifelong Learning for Home Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12567v2",
                "updated": "2024-11-06T17:44:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    17,
                    44,
                    53,
                    2,
                    311,
                    0
                ],
                "published": "2024-08-22T17:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    33,
                    50,
                    3,
                    235,
                    0
                ],
                "title": "FRB 20121102A monitoring: updated periodicity at L-band",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRB 20121102A monitoring: updated periodicity at L-band"
                },
                "summary": "FRB 20121102A was the first fast radio burst to be observed to repeat. Since\nthen, thousands of bursts have been detected by multiple radio telescopes\naround the world. Previous work has shown an indication of a cyclic activity\nlevel with a periodicity around 160 days. Knowing when the source repeats is\nessential for planning multi-wavelength monitoring to constrain their emission\nextend and progenitor source. We report the monitoring of FRB 20121102A using\nthe 100-m Effelsberg radio telescope at L-band and update the periodicity of\nthe cyclic activity-level. We use the Lomb-Scargle periodogram on a sample of\n284 observing epochs where 42% correspond to detections and 58% to\nnon-detections. Our dataset is composed of the 7 epochs of our monitoring plus\npublicly available data. We investigate two methods, i) binary model,\ndescribing the observing epochs with 1 if there are detections and with 0 for\nnon-detections. ii) normalised rates model: which considers the inferred\ndetections rates. We report no detections in 12.5-hour observations down to a\nfluence of 0.29 Jy ms. The best period found for the cyclic activity window is\n$159.3 \\pm 0.8$ days for the binary model and $159.3 \\pm 0.3$ days for the\nnormalised rates model. The activity phase is shown to be 53%. The normalised\nrates shows a clear Gaussian-like behaviour for the activity level, where the\nnumber of detections peak at the centre of the activity window. The periodicity\nfound through both methods is consistent for the L and S-band datasets implying\nit is intrinsic to the source. The activity phase in S-band however shows an\nindication of it ending before the L-band activity phase, supporting the idea\nof chromatic dependence of the activity window. The sample at C-band however is\nnot large enough to further confirm this result.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRB 20121102A was the first fast radio burst to be observed to repeat. Since\nthen, thousands of bursts have been detected by multiple radio telescopes\naround the world. Previous work has shown an indication of a cyclic activity\nlevel with a periodicity around 160 days. Knowing when the source repeats is\nessential for planning multi-wavelength monitoring to constrain their emission\nextend and progenitor source. We report the monitoring of FRB 20121102A using\nthe 100-m Effelsberg radio telescope at L-band and update the periodicity of\nthe cyclic activity-level. We use the Lomb-Scargle periodogram on a sample of\n284 observing epochs where 42% correspond to detections and 58% to\nnon-detections. Our dataset is composed of the 7 epochs of our monitoring plus\npublicly available data. We investigate two methods, i) binary model,\ndescribing the observing epochs with 1 if there are detections and with 0 for\nnon-detections. ii) normalised rates model: which considers the inferred\ndetections rates. We report no detections in 12.5-hour observations down to a\nfluence of 0.29 Jy ms. The best period found for the cyclic activity window is\n$159.3 \\pm 0.8$ days for the binary model and $159.3 \\pm 0.3$ days for the\nnormalised rates model. The activity phase is shown to be 53%. The normalised\nrates shows a clear Gaussian-like behaviour for the activity level, where the\nnumber of detections peak at the centre of the activity window. The periodicity\nfound through both methods is consistent for the L and S-band datasets implying\nit is intrinsic to the source. The activity phase in S-band however shows an\nindication of it ending before the L-band activity phase, supporting the idea\nof chromatic dependence of the activity window. The sample at C-band however is\nnot large enough to further confirm this result."
                },
                "authors": [
                    {
                        "name": "C. A. Braga"
                    },
                    {
                        "name": "M. Cruces"
                    },
                    {
                        "name": "T. Cassanelli"
                    },
                    {
                        "name": "M. C. Espinoza-Dupouy"
                    },
                    {
                        "name": "L. Rodriguez"
                    },
                    {
                        "name": "L. G. Spitler"
                    },
                    {
                        "name": "J. Vera-Casanova"
                    },
                    {
                        "name": "P. Limaye"
                    }
                ],
                "author_detail": {
                    "name": "P. Limaye"
                },
                "author": "P. Limaye",
                "arxiv_comment": "Revised version. Accepted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03523v3",
                "updated": "2024-11-06T17:19:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    17,
                    19,
                    39,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-04T15:44:23Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    44,
                    23,
                    4,
                    278,
                    0
                ],
                "title": "A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models"
                },
                "summary": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs. Code\navailable at https://github.com/yascho/probabilistic-unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs. Code\navailable at https://github.com/yascho/probabilistic-unlearning."
                },
                "authors": [
                    {
                        "name": "Yan Scholten"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01483v3",
                "updated": "2024-11-06T17:04:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    17,
                    4,
                    36,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T08:49:55Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    8,
                    49,
                    55,
                    6,
                    308,
                    0
                ],
                "title": "Teaching Models to Improve on Tape",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Models to Improve on Tape"
                },
                "summary": "Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such \"corrective feedback\". Here we claim that this\nskill of LLMs can be significantly enhanced via training. We introduce an RL\nframework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such \"corrective feedback\". Here we claim that this\nskill of LLMs can be significantly enhanced via training. We introduce an RL\nframework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts."
                },
                "authors": [
                    {
                        "name": "Liat Bezalel"
                    },
                    {
                        "name": "Eyal Orgad"
                    },
                    {
                        "name": "Amir Globerson"
                    }
                ],
                "author_detail": {
                    "name": "Amir Globerson"
                },
                "author": "Amir Globerson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22997v2",
                "updated": "2024-11-06T16:57:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    57,
                    3,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-30T13:22:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    22,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "A Comparison of Prompt Engineering Techniques for Task Planning and\n  Execution in Service Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparison of Prompt Engineering Techniques for Task Planning and\n  Execution in Service Robotics"
                },
                "summary": "Recent advances in LLM have been instrumental in autonomous robot control and\nhuman-robot interaction by leveraging their vast general knowledge and\ncapabilities to understand and reason across a wide range of tasks and\nscenarios. Previous works have investigated various prompt engineering\ntechniques for improving the performance of LLM to accomplish tasks, while\nothers have proposed methods that utilize LLMs to plan and execute tasks based\non the available functionalities of a given robot platform. In this work, we\nconsider both lines of research by comparing prompt engineering techniques and\ncombinations thereof within the application of high-level task planning and\nexecution in service robotics. We define a diverse set of tasks and a simple\nset of functionalities in simulation, and measure task completion accuracy and\nexecution time for several state-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLM have been instrumental in autonomous robot control and\nhuman-robot interaction by leveraging their vast general knowledge and\ncapabilities to understand and reason across a wide range of tasks and\nscenarios. Previous works have investigated various prompt engineering\ntechniques for improving the performance of LLM to accomplish tasks, while\nothers have proposed methods that utilize LLMs to plan and execute tasks based\non the available functionalities of a given robot platform. In this work, we\nconsider both lines of research by comparing prompt engineering techniques and\ncombinations thereof within the application of high-level task planning and\nexecution in service robotics. We define a diverse set of tasks and a simple\nset of functionalities in simulation, and measure task completion accuracy and\nexecution time for several state-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Jonas Bode"
                    },
                    {
                        "name": "Bastian Pätzold"
                    },
                    {
                        "name": "Raphael Memmesheimer"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "arxiv_comment": "6 pages, 3 figures, 2 tables, to be published in the 2024 IEEE-RAS\n  International Conference on Humanoid Robots, We make our code, including all\n  prompts, available at https://github.com/AIS-Bonn/Prompt_Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14632v2",
                "updated": "2024-11-06T16:54:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    54,
                    48,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-18T17:32:22Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    32,
                    22,
                    4,
                    292,
                    0
                ],
                "title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverging Preferences: When do Annotators Disagree and do Models Know?"
                },
                "summary": "We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training."
                },
                "authors": [
                    {
                        "name": "Michael JQ Zhang"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Jena D. Hwang"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Eunsol Choi"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Pyatkin"
                },
                "author": "Valentina Pyatkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14093v2",
                "updated": "2024-11-06T16:45:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    45,
                    17,
                    2,
                    311,
                    0
                ],
                "published": "2024-07-19T07:57:48Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    7,
                    57,
                    48,
                    4,
                    201,
                    0
                ],
                "title": "Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large\n  Language Models"
                },
                "summary": "Recently, mixture of experts (MoE) has become a popular paradigm for\nachieving the trade-off between modal capacity and efficiency of multi-modal\nlarge language models (MLLMs). Different from previous efforts, we are\ndedicated to exploring the dynamic expert path in an already exist MLLM and\nshow that a standard MLLM can be also a mixture of experts. To approach this\ntarget, we propose a novel dynamic expert scheme for MLLMs, termed Routing\nExperts (RoE), which can achieve example-dependent optimal path routing without\nobvious structure tweaks. Meanwhile, a new regularization of structure sparsity\nis also introduced to enforce MLLMs to learn more short-cut inference, ensuring\nthe efficiency. In addition, we also realize the first attempt of aligning the\ntraining and inference schemes of MLLMs in terms of network routing. To\nvalidate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,\nLLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL\nbenchmarks. The experiment results not only show the great advantages of our\nRoE in improving MLLMs' efficiency, but also yield obvious advantages than\nMoE-LLaVA in both performance and speed, e.g., an average performance gain of\n3.3% on 5 benchmarks while being faster.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, mixture of experts (MoE) has become a popular paradigm for\nachieving the trade-off between modal capacity and efficiency of multi-modal\nlarge language models (MLLMs). Different from previous efforts, we are\ndedicated to exploring the dynamic expert path in an already exist MLLM and\nshow that a standard MLLM can be also a mixture of experts. To approach this\ntarget, we propose a novel dynamic expert scheme for MLLMs, termed Routing\nExperts (RoE), which can achieve example-dependent optimal path routing without\nobvious structure tweaks. Meanwhile, a new regularization of structure sparsity\nis also introduced to enforce MLLMs to learn more short-cut inference, ensuring\nthe efficiency. In addition, we also realize the first attempt of aligning the\ntraining and inference schemes of MLLMs in terms of network routing. To\nvalidate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,\nLLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL\nbenchmarks. The experiment results not only show the great advantages of our\nRoE in improving MLLMs' efficiency, but also yield obvious advantages than\nMoE-LLaVA in both performance and speed, e.g., an average performance gain of\n3.3% on 5 benchmarks while being faster."
                },
                "authors": [
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Zhaoxi Ke"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04036v1",
                "updated": "2024-11-06T16:33:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    33,
                    21,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T16:33:21Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    33,
                    21,
                    2,
                    311,
                    0
                ],
                "title": "Stepping Forward on the Last Mile",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stepping Forward on the Last Mile"
                },
                "summary": "Continuously adapting pre-trained models to local data on resource\nconstrained edge devices is the $\\emph{last mile}$ for model deployment.\nHowever, as models increase in size and depth, backpropagation requires a large\namount of memory, which becomes prohibitive for edge devices. In addition, most\nexisting low power neural processing engines (e.g., NPUs, DSPs, MCUs, etc.) are\ndesigned as fixed-point inference accelerators, without training capabilities.\nForward gradients, solely based on directional derivatives computed from two\nforward calls, have been recently used for model training, with substantial\nsavings in computation and memory. However, the performance of quantized\ntraining with fixed-point forward gradients remains unclear. In this paper, we\ninvestigate the feasibility of on-device training using fixed-point forward\ngradients, by conducting comprehensive experiments across a variety of deep\nlearning benchmark tasks in both vision and audio domains. We propose a series\nof algorithm enhancements that further reduce the memory footprint, and the\naccuracy gap compared to backpropagation. An empirical study on how training\nwith forward gradients navigates in the loss landscape is further explored. Our\nresults demonstrate that on the last mile of model customization on edge\ndevices, training with fixed-point forward gradients is a feasible and\npractical approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuously adapting pre-trained models to local data on resource\nconstrained edge devices is the $\\emph{last mile}$ for model deployment.\nHowever, as models increase in size and depth, backpropagation requires a large\namount of memory, which becomes prohibitive for edge devices. In addition, most\nexisting low power neural processing engines (e.g., NPUs, DSPs, MCUs, etc.) are\ndesigned as fixed-point inference accelerators, without training capabilities.\nForward gradients, solely based on directional derivatives computed from two\nforward calls, have been recently used for model training, with substantial\nsavings in computation and memory. However, the performance of quantized\ntraining with fixed-point forward gradients remains unclear. In this paper, we\ninvestigate the feasibility of on-device training using fixed-point forward\ngradients, by conducting comprehensive experiments across a variety of deep\nlearning benchmark tasks in both vision and audio domains. We propose a series\nof algorithm enhancements that further reduce the memory footprint, and the\naccuracy gap compared to backpropagation. An empirical study on how training\nwith forward gradients navigates in the loss landscape is further explored. Our\nresults demonstrate that on the last mile of model customization on edge\ndevices, training with fixed-point forward gradients is a feasible and\npractical approach."
                },
                "authors": [
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Andrew Zou Li"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zou Li"
                },
                "author": "Andrew Zou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.03358v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.03358v3",
                "updated": "2024-11-06T16:32:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    32,
                    29,
                    2,
                    311,
                    0
                ],
                "published": "2023-03-06T18:46:05Z",
                "published_parsed": [
                    2023,
                    3,
                    6,
                    18,
                    46,
                    5,
                    0,
                    65,
                    0
                ],
                "title": "Nearly Optimal Approximation of Matrix Functions by the Lanczos Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearly Optimal Approximation of Matrix Functions by the Lanczos Method"
                },
                "summary": "Approximating the action of a matrix function $f(\\mathbf{A})$ on a vector\n$\\mathbf{b}$ is an increasingly important primitive in machine learning, data\nscience, and statistics, with applications such as sampling high dimensional\nGaussians, Gaussian process regression and Bayesian inference, principle\ncomponent analysis, and approximating Hessian spectral densities. Over the past\ndecade, a number of algorithms enjoying strong theoretical guarantees have been\nproposed for this task. Many of the most successful belong to a family of\nalgorithms called Krylov subspace methods. Remarkably, a classic Krylov\nsubspace method, called the Lanczos method for matrix functions (Lanczos-FA),\nfrequently outperforms newer methods in practice. Our main result is a\ntheoretical justification for this finding: we show that, for a natural class\nof rational functions, Lanczos-FA matches the error of the best possible Krylov\nsubspace method up to a multiplicative approximation factor. The approximation\nfactor depends on the degree of $f(x)$'s denominator and the condition number\nof $\\mathbf{A}$, but not on the number of iterations $k$. Our result provides a\nstrong justification for the excellent performance of Lanczos-FA, especially on\nfunctions that are well approximated by rationals, such as the matrix square\nroot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximating the action of a matrix function $f(\\mathbf{A})$ on a vector\n$\\mathbf{b}$ is an increasingly important primitive in machine learning, data\nscience, and statistics, with applications such as sampling high dimensional\nGaussians, Gaussian process regression and Bayesian inference, principle\ncomponent analysis, and approximating Hessian spectral densities. Over the past\ndecade, a number of algorithms enjoying strong theoretical guarantees have been\nproposed for this task. Many of the most successful belong to a family of\nalgorithms called Krylov subspace methods. Remarkably, a classic Krylov\nsubspace method, called the Lanczos method for matrix functions (Lanczos-FA),\nfrequently outperforms newer methods in practice. Our main result is a\ntheoretical justification for this finding: we show that, for a natural class\nof rational functions, Lanczos-FA matches the error of the best possible Krylov\nsubspace method up to a multiplicative approximation factor. The approximation\nfactor depends on the degree of $f(x)$'s denominator and the condition number\nof $\\mathbf{A}$, but not on the number of iterations $k$. Our result provides a\nstrong justification for the excellent performance of Lanczos-FA, especially on\nfunctions that are well approximated by rationals, such as the matrix square\nroot."
                },
                "authors": [
                    {
                        "name": "Noah Amsel"
                    },
                    {
                        "name": "Tyler Chen"
                    },
                    {
                        "name": "Anne Greenbaum"
                    },
                    {
                        "name": "Cameron Musco"
                    },
                    {
                        "name": "Chris Musco"
                    }
                ],
                "author_detail": {
                    "name": "Chris Musco"
                },
                "author": "Chris Musco",
                "arxiv_journal_ref": "Conference on Neural Information Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.03358v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.03358v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F60, 65F50, 68Q25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04032v1",
                "updated": "2024-11-06T16:31:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    31,
                    28,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T16:31:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    31,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "Beemo: Benchmark of Expert-edited Machine-generated Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beemo: Benchmark of Expert-edited Machine-generated Outputs"
                },
                "summary": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available."
                },
                "authors": [
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Jason Lucas"
                    },
                    {
                        "name": "Saranya Venkatraman"
                    },
                    {
                        "name": "Jooyoung Lee"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Adaku Uchendu"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    }
                ],
                "author_detail": {
                    "name": "Vladislav Mikhailov"
                },
                "author": "Vladislav Mikhailov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08262v3",
                "updated": "2024-11-06T16:19:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    19,
                    24,
                    2,
                    311,
                    0
                ],
                "published": "2024-04-12T06:21:48Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    6,
                    21,
                    48,
                    4,
                    103,
                    0
                ],
                "title": "Pretraining and Updates of Domain-Specific LLM: A Case Study in the\n  Japanese Business Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining and Updates of Domain-Specific LLM: A Case Study in the\n  Japanese Business Domain"
                },
                "summary": "The development of Large Language Models (LLMs) in various languages has been\nadvancing, but the combination of non-English languages with domain-specific\ncontexts remains underexplored. This paper presents our findings from training\nand evaluating a Japanese business domain-specific LLM designed to better\nunderstand business-related documents, such as the news on current affairs,\ntechnical reports, and patents. Additionally, LLMs in this domain require\nregular updates to incorporate the most recent knowledge. Therefore, we also\nreport our findings from the first experiments and evaluations involving\nupdates to this LLM using the latest article data, which is an important\nproblem setting that has not been addressed in previous research. From our\nexperiments on a newly created benchmark dataset for question answering in the\ntarget domain, we found that (1) our pretrained model improves QA accuracy\nwithout losing general knowledge, and (2) a proper mixture of the latest and\nolder texts in the training data for the update is necessary. Our pretrained\nmodel and business domain benchmark are publicly available to support further\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) in various languages has been\nadvancing, but the combination of non-English languages with domain-specific\ncontexts remains underexplored. This paper presents our findings from training\nand evaluating a Japanese business domain-specific LLM designed to better\nunderstand business-related documents, such as the news on current affairs,\ntechnical reports, and patents. Additionally, LLMs in this domain require\nregular updates to incorporate the most recent knowledge. Therefore, we also\nreport our findings from the first experiments and evaluations involving\nupdates to this LLM using the latest article data, which is an important\nproblem setting that has not been addressed in previous research. From our\nexperiments on a newly created benchmark dataset for question answering in the\ntarget domain, we found that (1) our pretrained model improves QA accuracy\nwithout losing general knowledge, and (2) a proper mixture of the latest and\nolder texts in the training data for the update is necessary. Our pretrained\nmodel and business domain benchmark are publicly available to support further\nstudies."
                },
                "authors": [
                    {
                        "name": "Kosuke Takahashi"
                    },
                    {
                        "name": "Takahiro Omi"
                    },
                    {
                        "name": "Kosuke Arima"
                    },
                    {
                        "name": "Tatsuya Ishigaki"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Ishigaki"
                },
                "author": "Tatsuya Ishigaki",
                "arxiv_comment": "Accepted at PACLIC 38",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07520v2",
                "updated": "2024-11-06T16:17:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    17,
                    21,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-10T01:21:48Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    1,
                    21,
                    48,
                    3,
                    284,
                    0
                ],
                "title": "News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News"
                },
                "summary": "Large Language Models (LLMs) have fast become an essential tools to many\nconversational chatbots due to their ability to provide coherent answers for\nvaried queries. Datasets used to train these LLMs are often a mix of generic\nand synthetic samples, thus lacking the verification needed to provide correct\nand verifiable answers for T.V. News.\n  We collect and share a large collection of QA pairs extracted from\ntranscripts of news recordings from various news-channels across the United\nStates. Resultant QA pairs are then used to fine-tune an off-the-shelf LLM\nmodel. Our model surpasses base models of similar size on several open LLM\nbenchmarks. We further integrate and propose a RAG method to improve\ncontextualization of our answers and also point it to a verifiable news\nrecording.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have fast become an essential tools to many\nconversational chatbots due to their ability to provide coherent answers for\nvaried queries. Datasets used to train these LLMs are often a mix of generic\nand synthetic samples, thus lacking the verification needed to provide correct\nand verifiable answers for T.V. News.\n  We collect and share a large collection of QA pairs extracted from\ntranscripts of news recordings from various news-channels across the United\nStates. Resultant QA pairs are then used to fine-tune an off-the-shelf LLM\nmodel. Our model surpasses base models of similar size on several open LLM\nbenchmarks. We further integrate and propose a RAG method to improve\ncontextualization of our answers and also point it to a verifiable news\nrecording."
                },
                "authors": [
                    {
                        "name": "Tarun Jain"
                    },
                    {
                        "name": "Yufei Gao"
                    },
                    {
                        "name": "Sridhar Vanga"
                    },
                    {
                        "name": "Karan Singla"
                    }
                ],
                "author_detail": {
                    "name": "Karan Singla"
                },
                "author": "Karan Singla",
                "arxiv_comment": "5 pages, under review at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04013v1",
                "updated": "2024-11-06T15:50:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    50,
                    19,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T15:50:19Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    50,
                    19,
                    2,
                    311,
                    0
                ],
                "title": "$k$NN Attention Demystified: A Theoretical Exploration for Scalable\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$k$NN Attention Demystified: A Theoretical Exploration for Scalable\n  Transformers"
                },
                "summary": "Despite their power, Transformers face challenges with long sequences due to\nthe quadratic complexity of self-attention. To address this limitation, methods\nlike $k$-Nearest-Neighbor ($k$NN) attention have been introduced [Roy, Saffar,\nVaswani, Grangier, 2021] enabling each token to attend to only its $k$ closest\ntokens. While $k$NN attention has shown empirical success in making\nTransformers more efficient, its exact approximation guarantees have not been\ntheoretically analyzed. In this work, we establish a theoretical framework for\n$k$NN attention, reformulating self-attention as expectations over softmax\ndistributions and leveraging lazy Gumbel sampling [Mussmann, Levy, Ermon, 2017]\nwith $k$NN indices for efficient approximation. Building on this framework, we\nalso propose novel sub-quadratic algorithms that approximate self-attention\ngradients by leveraging efficient sampling techniques, such as Markov\nChain-based estimation. Finally, we demonstrate the practical effectiveness of\nthese algorithms through empirical experiments, showcasing their benefits in\nboth training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their power, Transformers face challenges with long sequences due to\nthe quadratic complexity of self-attention. To address this limitation, methods\nlike $k$-Nearest-Neighbor ($k$NN) attention have been introduced [Roy, Saffar,\nVaswani, Grangier, 2021] enabling each token to attend to only its $k$ closest\ntokens. While $k$NN attention has shown empirical success in making\nTransformers more efficient, its exact approximation guarantees have not been\ntheoretically analyzed. In this work, we establish a theoretical framework for\n$k$NN attention, reformulating self-attention as expectations over softmax\ndistributions and leveraging lazy Gumbel sampling [Mussmann, Levy, Ermon, 2017]\nwith $k$NN indices for efficient approximation. Building on this framework, we\nalso propose novel sub-quadratic algorithms that approximate self-attention\ngradients by leveraging efficient sampling techniques, such as Markov\nChain-based estimation. Finally, we demonstrate the practical effectiveness of\nthese algorithms through empirical experiments, showcasing their benefits in\nboth training and inference."
                },
                "authors": [
                    {
                        "name": "Themistoklis Haris"
                    }
                ],
                "author_detail": {
                    "name": "Themistoklis Haris"
                },
                "author": "Themistoklis Haris",
                "arxiv_comment": "30 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16676v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16676v3",
                "updated": "2024-11-06T15:49:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    49,
                    30,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-22T04:18:19Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    4,
                    18,
                    19,
                    1,
                    296,
                    0
                ],
                "title": "Improving Causal Reasoning in Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Causal Reasoning in Large Language Models: A Survey"
                },
                "summary": "Causal reasoning (CR) is a crucial aspect of intelligence, essential for\nproblem-solving, decision-making, and understanding the world. While large\nlanguage models (LLMs) can generate rationales for their outputs, their ability\nto reliably perform causal reasoning remains uncertain, often falling short in\ntasks requiring a deep understanding of causality. In this survey, we provide a\ncomprehensive review of research aimed at enhancing LLMs for causal reasoning.\nWe categorize existing methods based on the role of LLMs: either as reasoning\nengines or as helpers providing knowledge or data to traditional CR methods,\nfollowed by a detailed discussion of the methodologies in each category. We\nthen evaluate the performance of LLMs on various causal reasoning tasks,\nproviding key findings and in-depth analysis. Finally, we provide insights from\ncurrent studies and highlight promising directions for future research. We aim\nfor this work to serve as a comprehensive resource, fostering further\nadvancements in causal reasoning with LLMs. Resources are available at\nhttps://github.com/chendl02/Awesome-LLM-causal-reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning (CR) is a crucial aspect of intelligence, essential for\nproblem-solving, decision-making, and understanding the world. While large\nlanguage models (LLMs) can generate rationales for their outputs, their ability\nto reliably perform causal reasoning remains uncertain, often falling short in\ntasks requiring a deep understanding of causality. In this survey, we provide a\ncomprehensive review of research aimed at enhancing LLMs for causal reasoning.\nWe categorize existing methods based on the role of LLMs: either as reasoning\nengines or as helpers providing knowledge or data to traditional CR methods,\nfollowed by a detailed discussion of the methodologies in each category. We\nthen evaluate the performance of LLMs on various causal reasoning tasks,\nproviding key findings and in-depth analysis. Finally, we provide insights from\ncurrent studies and highlight promising directions for future research. We aim\nfor this work to serve as a comprehensive resource, fostering further\nadvancements in causal reasoning with LLMs. Resources are available at\nhttps://github.com/chendl02/Awesome-LLM-causal-reasoning."
                },
                "authors": [
                    {
                        "name": "Longxuan Yu"
                    },
                    {
                        "name": "Delin Chen"
                    },
                    {
                        "name": "Siheng Xiong"
                    },
                    {
                        "name": "Qingyang Wu"
                    },
                    {
                        "name": "Qingzhen Liu"
                    },
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Zhikai Chen"
                    },
                    {
                        "name": "Xiaoze Liu"
                    },
                    {
                        "name": "Liangming Pan"
                    }
                ],
                "author_detail": {
                    "name": "Liangming Pan"
                },
                "author": "Liangming Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16676v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16676v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02059v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02059v2",
                "updated": "2024-11-06T15:38:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    38,
                    37,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T13:03:13Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    3,
                    13,
                    0,
                    309,
                    0
                ],
                "title": "TableGPT2: A Large Multimodal Model with Tabular Data Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableGPT2: A Large Multimodal Model with Tabular Data Integration"
                },
                "summary": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact."
                },
                "authors": [
                    {
                        "name": "Aofeng Su"
                    },
                    {
                        "name": "Aowen Wang"
                    },
                    {
                        "name": "Chao Ye"
                    },
                    {
                        "name": "Chen Zhou"
                    },
                    {
                        "name": "Ga Zhang"
                    },
                    {
                        "name": "Guangcheng Zhu"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Haoze Li"
                    },
                    {
                        "name": "Haoxuan Lan"
                    },
                    {
                        "name": "Jiaming Tian"
                    },
                    {
                        "name": "Jing Yuan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Junlin Zhou"
                    },
                    {
                        "name": "Kaizhe Shou"
                    },
                    {
                        "name": "Liangyu Zha"
                    },
                    {
                        "name": "Lin Long"
                    },
                    {
                        "name": "Liyao Li"
                    },
                    {
                        "name": "Pengzuo Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Qingyi Huang"
                    },
                    {
                        "name": "Saisai Yang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Wentao Ye"
                    },
                    {
                        "name": "Wufang Zhu"
                    },
                    {
                        "name": "Xiaomeng Hu"
                    },
                    {
                        "name": "Xijun Gu"
                    },
                    {
                        "name": "Xinjie Sun"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yuhang Yang"
                    },
                    {
                        "name": "Zhiqing Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqing Xiao"
                },
                "author": "Zhiqing Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02059v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03976v1",
                "updated": "2024-11-06T15:13:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    13,
                    31,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T15:13:31Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    13,
                    31,
                    2,
                    311,
                    0
                ],
                "title": "HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion\n  Segmentation"
                },
                "summary": "High resolution is crucial for precise segmentation in fundus images, yet\nhandling high-resolution inputs incurs considerable GPU memory costs, with\ndiminishing performance gains as overhead increases. To address this issue\nwhile tackling the challenge of segmenting tiny objects, recent studies have\nexplored local-global fusion methods. These methods preserve fine details using\nlocal regions and capture long-range context information from downscaled global\nimages. However, the necessity of multiple forward passes inevitably incurs\nsignificant computational overhead, adversely affecting inference speed. In\nthis paper, we propose HRDecoder, a simple High-Resolution Decoder network for\nfundus lesion segmentation. It integrates a high-resolution representation\nlearning module to capture fine-grained local features and a high-resolution\nfusion module to fuse multi-scale predictions. Our method effectively improves\nthe overall segmentation accuracy of fundus lesions while consuming reasonable\nmemory and computational overhead, and maintaining satisfying inference speed.\nExperimental results on the IDRID and DDR datasets demonstrate the\neffectiveness of our method. Code is available at\nhttps://github.com/CVIU-CSU/HRDecoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High resolution is crucial for precise segmentation in fundus images, yet\nhandling high-resolution inputs incurs considerable GPU memory costs, with\ndiminishing performance gains as overhead increases. To address this issue\nwhile tackling the challenge of segmenting tiny objects, recent studies have\nexplored local-global fusion methods. These methods preserve fine details using\nlocal regions and capture long-range context information from downscaled global\nimages. However, the necessity of multiple forward passes inevitably incurs\nsignificant computational overhead, adversely affecting inference speed. In\nthis paper, we propose HRDecoder, a simple High-Resolution Decoder network for\nfundus lesion segmentation. It integrates a high-resolution representation\nlearning module to capture fine-grained local features and a high-resolution\nfusion module to fuse multi-scale predictions. Our method effectively improves\nthe overall segmentation accuracy of fundus lesions while consuming reasonable\nmemory and computational overhead, and maintaining satisfying inference speed.\nExperimental results on the IDRID and DDR datasets demonstrate the\neffectiveness of our method. Code is available at\nhttps://github.com/CVIU-CSU/HRDecoder."
                },
                "authors": [
                    {
                        "name": "Ziyuan Ding"
                    },
                    {
                        "name": "Yixiong Liang"
                    },
                    {
                        "name": "Shichao Kan"
                    },
                    {
                        "name": "Qing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qing Liu"
                },
                "author": "Qing Liu",
                "arxiv_doi": "10.1007/978-3-031-72114-4_32",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-72114-4_32",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.03976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 3 figures, accepted by MICCAI 2024, the revised version",
                "arxiv_journal_ref": "MICCAI 2024, LNCS 15009, pp. 1-11, 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01818v2",
                "updated": "2024-11-06T14:58:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    58,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-06-03T22:15:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    22,
                    15,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Long-term foehn reconstruction combining unsupervised and supervised\n  learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-term foehn reconstruction combining unsupervised and supervised\n  learning"
                },
                "summary": "Foehn winds, characterized by abrupt temperature increases and wind speed\nchanges, significantly impact regions on the leeward side of mountain ranges,\ne.g., by spreading wildfires. Understanding how foehn occurrences change under\nclimate change is crucial. Unfortunately, foehn cannot be measured directly but\nhas to be inferred from meteorological measurements employing suitable\nclassification schemes. Hence, this approach is typically limited to specific\nperiods for which the necessary data are available. We present a novel approach\nfor reconstructing historical foehn occurrences using a combination of\nunsupervised and supervised probabilistic statistical learning methods. We\nutilize in-situ measurements (available for recent decades) to train an\nunsupervised learner (finite mixture model) for automatic foehn classification.\nThese labeled data are then linked to reanalysis data (covering longer periods)\nusing a supervised learner (lasso or boosting). This allows to reconstruct past\nfoehn probabilities based solely on reanalysis data. Applying this method to\nERA5 reanalysis data for six stations across Switzerland and Austria achieves\naccurate hourly reconstructions of north and south foehn occurrence,\nrespectively, dating back to 1940. This paves the way for investigating how\nseasonal foehn patterns have evolved over the past 83 years, providing valuable\ninsights into climate change impacts on these critical wind events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foehn winds, characterized by abrupt temperature increases and wind speed\nchanges, significantly impact regions on the leeward side of mountain ranges,\ne.g., by spreading wildfires. Understanding how foehn occurrences change under\nclimate change is crucial. Unfortunately, foehn cannot be measured directly but\nhas to be inferred from meteorological measurements employing suitable\nclassification schemes. Hence, this approach is typically limited to specific\nperiods for which the necessary data are available. We present a novel approach\nfor reconstructing historical foehn occurrences using a combination of\nunsupervised and supervised probabilistic statistical learning methods. We\nutilize in-situ measurements (available for recent decades) to train an\nunsupervised learner (finite mixture model) for automatic foehn classification.\nThese labeled data are then linked to reanalysis data (covering longer periods)\nusing a supervised learner (lasso or boosting). This allows to reconstruct past\nfoehn probabilities based solely on reanalysis data. Applying this method to\nERA5 reanalysis data for six stations across Switzerland and Austria achieves\naccurate hourly reconstructions of north and south foehn occurrence,\nrespectively, dating back to 1940. This paves the way for investigating how\nseasonal foehn patterns have evolved over the past 83 years, providing valuable\ninsights into climate change impacts on these critical wind events."
                },
                "authors": [
                    {
                        "name": "Reto Stauffer"
                    },
                    {
                        "name": "Achim Zeileis"
                    },
                    {
                        "name": "Georg J. Mayr"
                    }
                ],
                "author_detail": {
                    "name": "Georg J. Mayr"
                },
                "author": "Georg J. Mayr",
                "arxiv_doi": "10.1002/joc.8673",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/joc.8673",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.01818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Revised version published November 6, 2024: Stauffer, R., Zeileis, A.\n  and Mayr, G. J. (2024), Long-Term Foehn Reconstruction Combining Unsupervised\n  and Supervised Learning. International Journal of Climatology, published\n  November 6 2024, https://doi.org/10.1002/joc.8673",
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03964v1",
                "updated": "2024-11-06T14:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    54,
                    19,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T14:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    54,
                    19,
                    2,
                    311,
                    0
                ],
                "title": "What Really is Commonsense Knowledge?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Really is Commonsense Knowledge?"
                },
                "summary": "Commonsense datasets have been well developed in Natural Language Processing,\nmainly through crowdsource human annotation. However, there are debates on the\ngenuineness of commonsense reasoning benchmarks. In specific, a significant\nportion of instances in some commonsense benchmarks do not concern commonsense\nknowledge. That problem would undermine the measurement of the true commonsense\nreasoning ability of evaluated models. It is also suggested that the problem\noriginated from a blurry concept of commonsense knowledge, as distinguished\nfrom other types of knowledge. To demystify all of the above claims, in this\nstudy, we survey existing definitions of commonsense knowledge, ground into the\nthree frameworks for defining concepts, and consolidate them into a\nmulti-framework unified definition of commonsense knowledge (so-called\nconsolidated definition). We then use the consolidated definition for\nannotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets\nto examine the above claims. Our study shows that there exists a large portion\nof non-commonsense-knowledge instances in the two datasets, and a large\nperformance gap on these two subsets where Large Language Models (LLMs) perform\nworse on commonsense-knowledge instances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense datasets have been well developed in Natural Language Processing,\nmainly through crowdsource human annotation. However, there are debates on the\ngenuineness of commonsense reasoning benchmarks. In specific, a significant\nportion of instances in some commonsense benchmarks do not concern commonsense\nknowledge. That problem would undermine the measurement of the true commonsense\nreasoning ability of evaluated models. It is also suggested that the problem\noriginated from a blurry concept of commonsense knowledge, as distinguished\nfrom other types of knowledge. To demystify all of the above claims, in this\nstudy, we survey existing definitions of commonsense knowledge, ground into the\nthree frameworks for defining concepts, and consolidate them into a\nmulti-framework unified definition of commonsense knowledge (so-called\nconsolidated definition). We then use the consolidated definition for\nannotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets\nto examine the above claims. Our study shows that there exists a large portion\nof non-commonsense-knowledge instances in the two datasets, and a large\nperformance gap on these two subsets where Large Language Models (LLMs) perform\nworse on commonsense-knowledge instances."
                },
                "authors": [
                    {
                        "name": "Quyet V. Do"
                    },
                    {
                        "name": "Junze Li"
                    },
                    {
                        "name": "Tung-Duong Vuong"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Xiaojuan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojuan Ma"
                },
                "author": "Xiaojuan Ma",
                "arxiv_comment": "Code and data will be released together with the next version of the\n  paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03962v1",
                "updated": "2024-11-06T14:51:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T14:51:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?"
                },
                "summary": "The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many ontology matching (OM) systems. However, the lack of\nstandardisation in text preprocessing creates diversity in mapping results. In\nthis paper, we investigate the effect of the text preprocessing pipeline on OM\ntasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation\nInitiative (OAEI) track repositories with 49 distinct alignments indicate: (1)\nTokenisation and Normalisation are currently more effective than Stop Words\nRemoval and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and\nStemming is task-specific. We recommend standalone Lemmatisation or Stemming\nwith post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer\nperform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)\nTagging does not help Lemmatisation. To repair less effective Stop Words\nRemoval and Stemming/Lemmatisation used in OM tasks, we propose a novel\ncontext-based pipeline repair approach that significantly improves matching\ncorrectness and overall matching performance. We also discuss the use of text\npreprocessing pipeline in the new era of large language models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many ontology matching (OM) systems. However, the lack of\nstandardisation in text preprocessing creates diversity in mapping results. In\nthis paper, we investigate the effect of the text preprocessing pipeline on OM\ntasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation\nInitiative (OAEI) track repositories with 49 distinct alignments indicate: (1)\nTokenisation and Normalisation are currently more effective than Stop Words\nRemoval and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and\nStemming is task-specific. We recommend standalone Lemmatisation or Stemming\nwith post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer\nperform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)\nTagging does not help Lemmatisation. To repair less effective Stop Words\nRemoval and Stemming/Lemmatisation used in OM tasks, we propose a novel\ncontext-based pipeline repair approach that significantly improves matching\ncorrectness and overall matching performance. We also discuss the use of text\npreprocessing pipeline in the new era of large language models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqing Wang"
                },
                "author": "Weiqing Wang",
                "arxiv_comment": "13 pages, 26 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10149v2",
                "updated": "2024-11-06T14:50:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    50,
                    40,
                    2,
                    311,
                    0
                ],
                "published": "2024-06-14T16:00:29Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    16,
                    0,
                    29,
                    4,
                    166,
                    0
                ],
                "title": "BABILong: Testing the Limits of LLMs with Long Context\n  Reasoning-in-a-Haystack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BABILong: Testing the Limits of LLMs with Long Context\n  Reasoning-in-a-Haystack"
                },
                "summary": "In recent years, the input context sizes of large language models (LLMs) have\nincreased dramatically. However, existing evaluation methods have not kept\npace, failing to comprehensively assess the efficiency of models in handling\nlong contexts. To bridge this gap, we introduce the BABILong benchmark,\ndesigned to test language models' ability to reason across facts distributed in\nextremely long documents. BABILong includes a diverse set of 20 reasoning\ntasks, including fact chaining, simple induction, deduction, counting, and\nhandling lists/sets. These tasks are challenging on their own, and even more\ndemanding when the required facts are scattered across long natural text. Our\nevaluations show that popular LLMs effectively utilize only 10-20\\% of the\ncontext and their performance declines sharply with increased reasoning\ncomplexity. Among alternatives to in-context reasoning, Retrieval-Augmented\nGeneration methods achieve a modest 60\\% accuracy on single-fact question\nanswering, independent of context length. Among context extension methods, the\nhighest performance is demonstrated by recurrent memory transformers after\nfine-tuning, enabling the processing of lengths up to 50 million tokens. The\nBABILong benchmark is extendable to any length to support the evaluation of new\nupcoming models with increased capabilities, and we provide splits up to 10\nmillion token lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the input context sizes of large language models (LLMs) have\nincreased dramatically. However, existing evaluation methods have not kept\npace, failing to comprehensively assess the efficiency of models in handling\nlong contexts. To bridge this gap, we introduce the BABILong benchmark,\ndesigned to test language models' ability to reason across facts distributed in\nextremely long documents. BABILong includes a diverse set of 20 reasoning\ntasks, including fact chaining, simple induction, deduction, counting, and\nhandling lists/sets. These tasks are challenging on their own, and even more\ndemanding when the required facts are scattered across long natural text. Our\nevaluations show that popular LLMs effectively utilize only 10-20\\% of the\ncontext and their performance declines sharply with increased reasoning\ncomplexity. Among alternatives to in-context reasoning, Retrieval-Augmented\nGeneration methods achieve a modest 60\\% accuracy on single-fact question\nanswering, independent of context length. Among context extension methods, the\nhighest performance is demonstrated by recurrent memory transformers after\nfine-tuning, enabling the processing of lengths up to 50 million tokens. The\nBABILong benchmark is extendable to any length to support the evaluation of new\nupcoming models with increased capabilities, and we provide splits up to 10\nmillion token lengths."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Petr Anokhin"
                    },
                    {
                        "name": "Ivan Rodkin"
                    },
                    {
                        "name": "Dmitry Sorokin"
                    },
                    {
                        "name": "Artyom Sorokin"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03957v1",
                "updated": "2024-11-06T14:42:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    42,
                    39,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T14:42:39Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    42,
                    39,
                    2,
                    311,
                    0
                ],
                "title": "Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has proven to be an effective method for\nmitigating hallucination issues inherent in large language models (LLMs).\nPrevious approaches typically train retrievers based on semantic similarity,\nlacking optimization for RAG. More recent works have proposed aligning\nretrievers with the preference signals of LLMs. However, these preference\nsignals are often difficult for dense retrievers, which typically have weaker\nlanguage capabilities, to understand and learn effectively. Drawing inspiration\nfrom pedagogical theories like Guided Discovery Learning, we propose a novel\nframework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the\nlanguage capabilities of LLMs to construct examples from a more granular,\ninformation-centric perspective to guide the learning of retrievers.\nSpecifically, our method utilizes LLMs to construct easy-to-understand examples\nfrom samples where the retriever performs poorly, focusing on three learning\nobjectives highly relevant to the RAG scenario: relevance, comprehensiveness,\nand purity. These examples serve as scaffolding to ultimately align the\nretriever with the LLM's preferences. Furthermore, we employ a dual curriculum\nlearning strategy and leverage the reciprocal feedback between LLM and\nretriever to further enhance the performance of the RAG system. A series of\nexperiments demonstrate that our proposed framework enhances the performance of\nRAG systems equipped with different retrievers and is applicable to various\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has proven to be an effective method for\nmitigating hallucination issues inherent in large language models (LLMs).\nPrevious approaches typically train retrievers based on semantic similarity,\nlacking optimization for RAG. More recent works have proposed aligning\nretrievers with the preference signals of LLMs. However, these preference\nsignals are often difficult for dense retrievers, which typically have weaker\nlanguage capabilities, to understand and learn effectively. Drawing inspiration\nfrom pedagogical theories like Guided Discovery Learning, we propose a novel\nframework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the\nlanguage capabilities of LLMs to construct examples from a more granular,\ninformation-centric perspective to guide the learning of retrievers.\nSpecifically, our method utilizes LLMs to construct easy-to-understand examples\nfrom samples where the retriever performs poorly, focusing on three learning\nobjectives highly relevant to the RAG scenario: relevance, comprehensiveness,\nand purity. These examples serve as scaffolding to ultimately align the\nretriever with the LLM's preferences. Furthermore, we employ a dual curriculum\nlearning strategy and leverage the reciprocal feedback between LLM and\nretriever to further enhance the performance of the RAG system. A series of\nexperiments demonstrate that our proposed framework enhances the performance of\nRAG systems equipped with different retrievers and is applicable to various\nLLMs."
                },
                "authors": [
                    {
                        "name": "Yuhang Liu"
                    },
                    {
                        "name": "Xueyu Hu"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Jingyuan Chen"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03948v1",
                "updated": "2024-11-06T14:29:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    29,
                    49,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T14:29:49Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    29,
                    49,
                    2,
                    311,
                    0
                ],
                "title": "Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of\n  Study in Tabletop Role-Playing Games Soundtracks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of\n  Study in Tabletop Role-Playing Games Soundtracks"
                },
                "summary": "This paper investigates the capabilities of text-to-audio music generation\nmodels in producing long-form music with prompts that change over time,\nfocusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We\nintroduce Babel Bardo, a system that uses Large Language Models (LLMs) to\ntransform speech transcriptions into music descriptions for controlling a\ntext-to-music model. Four versions of Babel Bardo were compared in two TRPG\ncampaigns: a baseline using direct speech transcriptions, and three LLM-based\nversions with varying approaches to music description generation. Evaluations\nconsidered audio quality, story alignment, and transition smoothness. Results\nindicate that detailed music descriptions improve audio quality while\nmaintaining consistency across consecutive descriptions enhances story\nalignment and transition smoothness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the capabilities of text-to-audio music generation\nmodels in producing long-form music with prompts that change over time,\nfocusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We\nintroduce Babel Bardo, a system that uses Large Language Models (LLMs) to\ntransform speech transcriptions into music descriptions for controlling a\ntext-to-music model. Four versions of Babel Bardo were compared in two TRPG\ncampaigns: a baseline using direct speech transcriptions, and three LLM-based\nversions with varying approaches to music description generation. Evaluations\nconsidered audio quality, story alignment, and transition smoothness. Results\nindicate that detailed music descriptions improve audio quality while\nmaintaining consistency across consecutive descriptions enhances story\nalignment and transition smoothness."
                },
                "authors": [
                    {
                        "name": "Felipe Marra"
                    },
                    {
                        "name": "Lucas N. Ferreira"
                    }
                ],
                "author_detail": {
                    "name": "Lucas N. Ferreira"
                },
                "author": "Lucas N. Ferreira",
                "arxiv_comment": "Paper accepted at the LAMIR 2024 workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07724v2",
                "updated": "2024-11-06T14:29:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    29,
                    36,
                    2,
                    311,
                    0
                ],
                "published": "2024-04-11T13:16:47Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    13,
                    16,
                    47,
                    3,
                    102,
                    0
                ],
                "title": "Applying Guidance in a Limited Interval Improves Sample and Distribution\n  Quality in Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applying Guidance in a Limited Interval Improves Sample and Distribution\n  Quality in Diffusion Models"
                },
                "summary": "Guidance is a crucial technique for extracting the best performance out of\nimage-generating diffusion models. Traditionally, a constant guidance weight\nhas been applied throughout the sampling chain of an image. We show that\nguidance is clearly harmful toward the beginning of the chain (high noise\nlevels), largely unnecessary toward the end (low noise levels), and only\nbeneficial in the middle. We thus restrict it to a specific range of noise\nlevels, improving both the inference speed and result quality. This limited\nguidance interval improves the record FID in ImageNet-512 significantly, from\n1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial\nacross different sampler parameters, network architectures, and datasets,\nincluding the large-scale setting of Stable Diffusion XL. We thus suggest\nexposing the guidance interval as a hyperparameter in all diffusion models that\nuse guidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guidance is a crucial technique for extracting the best performance out of\nimage-generating diffusion models. Traditionally, a constant guidance weight\nhas been applied throughout the sampling chain of an image. We show that\nguidance is clearly harmful toward the beginning of the chain (high noise\nlevels), largely unnecessary toward the end (low noise levels), and only\nbeneficial in the middle. We thus restrict it to a specific range of noise\nlevels, improving both the inference speed and result quality. This limited\nguidance interval improves the record FID in ImageNet-512 significantly, from\n1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial\nacross different sampler parameters, network architectures, and datasets,\nincluding the large-scale setting of Stable Diffusion XL. We thus suggest\nexposing the guidance interval as a hyperparameter in all diffusion models that\nuse guidance."
                },
                "authors": [
                    {
                        "name": "Tuomas Kynkäänniemi"
                    },
                    {
                        "name": "Miika Aittala"
                    },
                    {
                        "name": "Tero Karras"
                    },
                    {
                        "name": "Samuli Laine"
                    },
                    {
                        "name": "Timo Aila"
                    },
                    {
                        "name": "Jaakko Lehtinen"
                    }
                ],
                "author_detail": {
                    "name": "Jaakko Lehtinen"
                },
                "author": "Jaakko Lehtinen",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12656v2",
                "updated": "2024-11-06T14:14:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    14,
                    58,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-16T15:17:20Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    17,
                    20,
                    2,
                    290,
                    0
                ],
                "title": "Evaluating Morphological Compositional Generalization in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Morphological Compositional Generalization in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans."
                },
                "authors": [
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Defne Circi"
                    },
                    {
                        "name": "Jonne Sälevä"
                    },
                    {
                        "name": "Hale Sirin"
                    },
                    {
                        "name": "Abdullatif Köksal"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    },
                    {
                        "name": "Duygu Ataman"
                    }
                ],
                "author_detail": {
                    "name": "Duygu Ataman"
                },
                "author": "Duygu Ataman",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.05328v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.05328v7",
                "updated": "2024-11-06T13:58:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    58,
                    1,
                    2,
                    311,
                    0
                ],
                "published": "2023-03-09T15:19:31Z",
                "published_parsed": [
                    2023,
                    3,
                    9,
                    15,
                    19,
                    31,
                    3,
                    68,
                    0
                ],
                "title": "Simulation-based, Finite-sample Inference for Privatized Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based, Finite-sample Inference for Privatized Data"
                },
                "summary": "Privacy protection methods, such as differentially private mechanisms,\nintroduce noise into resulting statistics which often produces complex and\nintractable sampling distributions. In this paper, we propose a\nsimulation-based \"repro sample\" approach to produce statistically valid\nconfidence intervals and hypothesis tests, which builds on the work of Xie and\nWang (2022). We show that this methodology is applicable to a wide variety of\nprivate inference problems, appropriately accounts for biases introduced by\nprivacy mechanisms (such as by clamping), and improves over other\nstate-of-the-art inference methods such as the parametric bootstrap in terms of\nthe coverage and type I error of the private inference. We also develop\nsignificant improvements and extensions for the repro sample methodology for\ngeneral models (not necessarily related to privacy), including 1) modifying the\nprocedure to ensure guaranteed coverage and type I errors, even accounting for\nMonte Carlo error, and 2) proposing efficient numerical algorithms to implement\nthe confidence intervals and $p$-values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy protection methods, such as differentially private mechanisms,\nintroduce noise into resulting statistics which often produces complex and\nintractable sampling distributions. In this paper, we propose a\nsimulation-based \"repro sample\" approach to produce statistically valid\nconfidence intervals and hypothesis tests, which builds on the work of Xie and\nWang (2022). We show that this methodology is applicable to a wide variety of\nprivate inference problems, appropriately accounts for biases introduced by\nprivacy mechanisms (such as by clamping), and improves over other\nstate-of-the-art inference methods such as the parametric bootstrap in terms of\nthe coverage and type I error of the private inference. We also develop\nsignificant improvements and extensions for the repro sample methodology for\ngeneral models (not necessarily related to privacy), including 1) modifying the\nprocedure to ensure guaranteed coverage and type I errors, even accounting for\nMonte Carlo error, and 2) proposing efficient numerical algorithms to implement\nthe confidence intervals and $p$-values."
                },
                "authors": [
                    {
                        "name": "Jordan Awan"
                    },
                    {
                        "name": "Zhanyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanyu Wang"
                },
                "author": "Zhanyu Wang",
                "arxiv_comment": "25 pages before references and appendices, 42 pages total, 10\n  figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.05328v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.05328v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03923v1",
                "updated": "2024-11-06T13:54:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    54,
                    8,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T13:54:08Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    54,
                    8,
                    2,
                    311,
                    0
                ],
                "title": "Evaluation data contamination in LLMs: how do we measure it and (when)\n  does it matter?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation data contamination in LLMs: how do we measure it and (when)\n  does it matter?"
                },
                "summary": "Hampering the interpretation of benchmark scores, evaluation data\ncontamination has become a growing concern in the evaluation of LLMs, and an\nactive area of research studies its effects. While evaluation data\ncontamination is easily understood intuitively, it is surprisingly difficult to\ndefine precisely which samples should be considered contaminated and,\nconsequently, how it impacts benchmark scores. We propose that these questions\nshould be addressed together and that contamination metrics can be assessed\nbased on whether models benefit from the examples they mark contaminated. We\npropose a novel analysis method called ConTAM, and show with a large scale\nsurvey of existing and novel n-gram based contamination metrics across 13\nbenchmarks and 7 models from 2 different families that ConTAM can be used to\nbetter understand evaluation data contamination and its effects. We find that\ncontamination may have a much larger effect than reported in recent LLM\nreleases and benefits models differently at different scales. We also find that\nconsidering only the longest contaminated substring provides a better signal\nthan considering a union of all contaminated substrings, and that doing model\nand benchmark specific threshold analysis greatly increases the specificity of\nthe results. Lastly, we investigate the impact of hyperparameter choices,\nfinding that, among other things, both using larger values of n and\ndisregarding matches that are infrequent in the pre-training data lead to many\nfalse negatives. With ConTAM, we provide a method to empirically ground\nevaluation data contamination metrics in downstream effects. With our\nexploration, we shed light on how evaluation data contamination can impact LLMs\nand provide insight into the considerations important when doing contamination\nanalysis. We end our paper by discussing these in more detail and providing\nconcrete suggestions for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hampering the interpretation of benchmark scores, evaluation data\ncontamination has become a growing concern in the evaluation of LLMs, and an\nactive area of research studies its effects. While evaluation data\ncontamination is easily understood intuitively, it is surprisingly difficult to\ndefine precisely which samples should be considered contaminated and,\nconsequently, how it impacts benchmark scores. We propose that these questions\nshould be addressed together and that contamination metrics can be assessed\nbased on whether models benefit from the examples they mark contaminated. We\npropose a novel analysis method called ConTAM, and show with a large scale\nsurvey of existing and novel n-gram based contamination metrics across 13\nbenchmarks and 7 models from 2 different families that ConTAM can be used to\nbetter understand evaluation data contamination and its effects. We find that\ncontamination may have a much larger effect than reported in recent LLM\nreleases and benefits models differently at different scales. We also find that\nconsidering only the longest contaminated substring provides a better signal\nthan considering a union of all contaminated substrings, and that doing model\nand benchmark specific threshold analysis greatly increases the specificity of\nthe results. Lastly, we investigate the impact of hyperparameter choices,\nfinding that, among other things, both using larger values of n and\ndisregarding matches that are infrequent in the pre-training data lead to many\nfalse negatives. With ConTAM, we provide a method to empirically ground\nevaluation data contamination metrics in downstream effects. With our\nexploration, we shed light on how evaluation data contamination can impact LLMs\nand provide insight into the considerations important when doing contamination\nanalysis. We end our paper by discussing these in more detail and providing\nconcrete suggestions for future work."
                },
                "authors": [
                    {
                        "name": "Aaditya K. Singh"
                    },
                    {
                        "name": "Muhammed Yusuf Kocyigit"
                    },
                    {
                        "name": "Andrew Poulton"
                    },
                    {
                        "name": "David Esiobu"
                    },
                    {
                        "name": "Maria Lomeli"
                    },
                    {
                        "name": "Gergely Szilvasy"
                    },
                    {
                        "name": "Dieuwke Hupkes"
                    }
                ],
                "author_detail": {
                    "name": "Dieuwke Hupkes"
                },
                "author": "Dieuwke Hupkes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03920v1",
                "updated": "2024-11-06T13:51:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    51,
                    42,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T13:51:42Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    51,
                    42,
                    2,
                    311,
                    0
                ],
                "title": "RAGulator: Lightweight Out-of-Context Detectors for Grounded Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGulator: Lightweight Out-of-Context Detectors for Grounded Text\n  Generation"
                },
                "summary": "Real-time detection of out-of-context LLM outputs is crucial for enterprises\nlooking to safely adopt RAG applications. In this work, we train lightweight\nmodels to discriminate LLM-generated text that is semantically out-of-context\nfrom retrieved text documents. We preprocess a combination of summarisation and\nsemantic textual similarity datasets to construct training data using minimal\nresources. We find that DeBERTa is not only the best-performing model under\nthis pipeline, but it is also fast and does not require additional text\npreprocessing or feature engineering. While emerging work demonstrates that\ngenerative LLMs can also be fine-tuned and used in complex data pipelines to\nachieve state-of-the-art performance, we note that speed and resource limits\nare important considerations for on-premise deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time detection of out-of-context LLM outputs is crucial for enterprises\nlooking to safely adopt RAG applications. In this work, we train lightweight\nmodels to discriminate LLM-generated text that is semantically out-of-context\nfrom retrieved text documents. We preprocess a combination of summarisation and\nsemantic textual similarity datasets to construct training data using minimal\nresources. We find that DeBERTa is not only the best-performing model under\nthis pipeline, but it is also fast and does not require additional text\npreprocessing or feature engineering. While emerging work demonstrates that\ngenerative LLMs can also be fine-tuned and used in complex data pipelines to\nachieve state-of-the-art performance, we note that speed and resource limits\nare important considerations for on-premise deployment."
                },
                "authors": [
                    {
                        "name": "Ian Poey"
                    },
                    {
                        "name": "Jiajun Liu"
                    },
                    {
                        "name": "Qishuai Zhong"
                    },
                    {
                        "name": "Adrien Chenailler"
                    }
                ],
                "author_detail": {
                    "name": "Adrien Chenailler"
                },
                "author": "Adrien Chenailler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03919v1",
                "updated": "2024-11-06T13:51:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    51,
                    6,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T13:51:06Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    51,
                    6,
                    2,
                    311,
                    0
                ],
                "title": "A Causal Framework for Precision Rehabilitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Causal Framework for Precision Rehabilitation"
                },
                "summary": "Precision rehabilitation offers the promise of an evidence-based approach for\noptimizing individual rehabilitation to improve long-term functional outcomes.\nEmerging techniques, including those driven by artificial intelligence, are\nrapidly expanding our ability to quantify the different domains of function\nduring rehabilitation, other encounters with healthcare, and in the community.\nWhile this seems poised to usher rehabilitation into the era of big data and\nshould be a powerful driver of precision rehabilitation, our field lacks a\ncoherent framework to utilize these data and deliver on this promise. We\npropose a framework that builds upon multiple existing pillars to fill this\ngap. Our framework aims to identify the Optimal Dynamic Treatment Regimens\n(ODTR), or the decision-making strategy that takes in the range of available\nmeasurements and biomarkers to identify interventions likely to maximize\nlong-term function. This is achieved by designing and fitting causal models,\nwhich extend the Computational Neurorehabilitation framework using tools from\ncausal inference. These causal models can learn from heterogeneous data from\ndifferent silos, which must include detailed documentation of interventions,\nsuch as using the Rehabilitation Treatment Specification System. The models\nthen serve as digital twins of patient recovery trajectories, which can be used\nto learn the ODTR. Our causal modeling framework also emphasizes quantitatively\nlinking changes across levels of the functioning to ensure that interventions\ncan be precisely selected based on careful measurement of impairments while\nalso being selected to maximize outcomes that are meaningful to patients and\nstakeholders. We believe this approach can provide a unifying framework to\nleverage growing big rehabilitation data and AI-powered measurements to produce\nprecision rehabilitation treatments that can improve clinical outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precision rehabilitation offers the promise of an evidence-based approach for\noptimizing individual rehabilitation to improve long-term functional outcomes.\nEmerging techniques, including those driven by artificial intelligence, are\nrapidly expanding our ability to quantify the different domains of function\nduring rehabilitation, other encounters with healthcare, and in the community.\nWhile this seems poised to usher rehabilitation into the era of big data and\nshould be a powerful driver of precision rehabilitation, our field lacks a\ncoherent framework to utilize these data and deliver on this promise. We\npropose a framework that builds upon multiple existing pillars to fill this\ngap. Our framework aims to identify the Optimal Dynamic Treatment Regimens\n(ODTR), or the decision-making strategy that takes in the range of available\nmeasurements and biomarkers to identify interventions likely to maximize\nlong-term function. This is achieved by designing and fitting causal models,\nwhich extend the Computational Neurorehabilitation framework using tools from\ncausal inference. These causal models can learn from heterogeneous data from\ndifferent silos, which must include detailed documentation of interventions,\nsuch as using the Rehabilitation Treatment Specification System. The models\nthen serve as digital twins of patient recovery trajectories, which can be used\nto learn the ODTR. Our causal modeling framework also emphasizes quantitatively\nlinking changes across levels of the functioning to ensure that interventions\ncan be precisely selected based on careful measurement of impairments while\nalso being selected to maximize outcomes that are meaningful to patients and\nstakeholders. We believe this approach can provide a unifying framework to\nleverage growing big rehabilitation data and AI-powered measurements to produce\nprecision rehabilitation treatments that can improve clinical outcomes."
                },
                "authors": [
                    {
                        "name": "R. James Cotton"
                    },
                    {
                        "name": "Bryant A. Seamon"
                    },
                    {
                        "name": "Richard L. Segal"
                    },
                    {
                        "name": "Randal D. Davis"
                    },
                    {
                        "name": "Amrita Sahu"
                    },
                    {
                        "name": "Michelle M. McLeod"
                    },
                    {
                        "name": "Pablo Celnik"
                    },
                    {
                        "name": "Sharon L. Ramey"
                    }
                ],
                "author_detail": {
                    "name": "Sharon L. Ramey"
                },
                "author": "Sharon L. Ramey",
                "arxiv_comment": "keywords: rehabilitation; precision rehabilitation; causal inference;\n  international classification of functioning; rehabilitation treatment\n  specification system; computational neurorehabilitation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03914v1",
                "updated": "2024-11-06T13:47:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    47,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T13:47:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    47,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Game-Theoretic Machine Unlearning: Mitigating Extra Privacy Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Game-Theoretic Machine Unlearning: Mitigating Extra Privacy Leakage"
                },
                "summary": "With the extensive use of machine learning technologies, data providers\nencounter increasing privacy risks. Recent legislation, such as GDPR, obligates\norganizations to remove requested data and its influence from a trained model.\nMachine unlearning is an emerging technique designed to enable machine learning\nmodels to erase users' private information. Although several efficient machine\nunlearning schemes have been proposed, these methods still have limitations.\nFirst, removing the contributions of partial data may lead to model performance\ndegradation. Second, discrepancies between the original and generated unlearned\nmodels can be exploited by attackers to obtain target sample's information,\nresulting in additional privacy leakage risks. To address above challenges, we\nproposed a game-theoretic machine unlearning algorithm that simulates the\ncompetitive relationship between unlearning performance and privacy protection.\nThis algorithm comprises unlearning and privacy modules. The unlearning module\npossesses a loss function composed of model distance and classification error,\nwhich is used to derive the optimal strategy. The privacy module aims to make\nit difficult for an attacker to infer membership information from the unlearned\ndata, thereby reducing the privacy leakage risk during the unlearning process.\nAdditionally, the experimental results on real-world datasets demonstrate that\nthis game-theoretic unlearning algorithm's effectiveness and its ability to\ngenerate an unlearned model with a performance similar to that of the retrained\none while mitigating extra privacy leakage risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the extensive use of machine learning technologies, data providers\nencounter increasing privacy risks. Recent legislation, such as GDPR, obligates\norganizations to remove requested data and its influence from a trained model.\nMachine unlearning is an emerging technique designed to enable machine learning\nmodels to erase users' private information. Although several efficient machine\nunlearning schemes have been proposed, these methods still have limitations.\nFirst, removing the contributions of partial data may lead to model performance\ndegradation. Second, discrepancies between the original and generated unlearned\nmodels can be exploited by attackers to obtain target sample's information,\nresulting in additional privacy leakage risks. To address above challenges, we\nproposed a game-theoretic machine unlearning algorithm that simulates the\ncompetitive relationship between unlearning performance and privacy protection.\nThis algorithm comprises unlearning and privacy modules. The unlearning module\npossesses a loss function composed of model distance and classification error,\nwhich is used to derive the optimal strategy. The privacy module aims to make\nit difficult for an attacker to infer membership information from the unlearned\ndata, thereby reducing the privacy leakage risk during the unlearning process.\nAdditionally, the experimental results on real-world datasets demonstrate that\nthis game-theoretic unlearning algorithm's effectiveness and its ability to\ngenerate an unlearned model with a performance similar to that of the retrained\none while mitigating extra privacy leakage risks."
                },
                "authors": [
                    {
                        "name": "Hengzhu Liu"
                    },
                    {
                        "name": "Tianqing Zhu"
                    },
                    {
                        "name": "Lefeng Zhang"
                    },
                    {
                        "name": "Ping Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Ping Xiong"
                },
                "author": "Ping Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03906v1",
                "updated": "2024-11-06T13:37:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    37,
                    28,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T13:37:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    37,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "Lexicalization Is All You Need: Examining the Impact of Lexical\n  Knowledge in a Compositional QALD System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexicalization Is All You Need: Examining the Impact of Lexical\n  Knowledge in a Compositional QALD System"
                },
                "summary": "In this paper, we examine the impact of lexicalization on Question Answering\nover Linked Data (QALD). It is well known that one of the key challenges in\ninterpreting natural language questions with respect to SPARQL lies in bridging\nthe lexical gap, that is mapping the words in the query to the correct\nvocabulary elements. We argue in this paper that lexicalization, that is\nexplicit knowledge about the potential interpretations of a word with respect\nto the given vocabulary, significantly eases the task and increases the\nperformance of QA systems. Towards this goal, we present a compositional QA\nsystem that can leverage explicit lexical knowledge in a compositional manner\nto infer the meaning of a question in terms of a SPARQL query. We show that\nsuch a system, given lexical knowledge, has a performance well beyond current\nQA systems, achieving up to a $35.8\\%$ increase in the micro $F_1$ score\ncompared to the best QA system on QALD-9. This shows the importance and\npotential of including explicit lexical knowledge. In contrast, we show that\nLLMs have limited abilities to exploit lexical knowledge, with only marginal\nimprovements compared to a version without lexical knowledge. This shows that\nLLMs have no ability to compositionally interpret a question on the basis of\nthe meaning of its parts, a key feature of compositional approaches. Taken\ntogether, our work shows new avenues for QALD research, emphasizing the\nimportance of lexicalization and compositionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we examine the impact of lexicalization on Question Answering\nover Linked Data (QALD). It is well known that one of the key challenges in\ninterpreting natural language questions with respect to SPARQL lies in bridging\nthe lexical gap, that is mapping the words in the query to the correct\nvocabulary elements. We argue in this paper that lexicalization, that is\nexplicit knowledge about the potential interpretations of a word with respect\nto the given vocabulary, significantly eases the task and increases the\nperformance of QA systems. Towards this goal, we present a compositional QA\nsystem that can leverage explicit lexical knowledge in a compositional manner\nto infer the meaning of a question in terms of a SPARQL query. We show that\nsuch a system, given lexical knowledge, has a performance well beyond current\nQA systems, achieving up to a $35.8\\%$ increase in the micro $F_1$ score\ncompared to the best QA system on QALD-9. This shows the importance and\npotential of including explicit lexical knowledge. In contrast, we show that\nLLMs have limited abilities to exploit lexical knowledge, with only marginal\nimprovements compared to a version without lexical knowledge. This shows that\nLLMs have no ability to compositionally interpret a question on the basis of\nthe meaning of its parts, a key feature of compositional approaches. Taken\ntogether, our work shows new avenues for QALD research, emphasizing the\nimportance of lexicalization and compositionality."
                },
                "authors": [
                    {
                        "name": "David Maria Schmidt"
                    },
                    {
                        "name": "Mohammad Fazleh Elahi"
                    },
                    {
                        "name": "Philipp Cimiano"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Cimiano"
                },
                "author": "Philipp Cimiano",
                "arxiv_comment": "24th International Conference on Knowledge Engineering and Knowledge\n  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18379v2",
                "updated": "2024-11-06T13:26:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    26,
                    50,
                    2,
                    311,
                    0
                ],
                "published": "2024-06-26T14:21:09Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    14,
                    21,
                    9,
                    2,
                    178,
                    0
                ],
                "title": "MALSIGHT: Exploring Malicious Source Code and Benign Pseudocode for\n  Iterative Binary Malware Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALSIGHT: Exploring Malicious Source Code and Benign Pseudocode for\n  Iterative Binary Malware Summarization"
                },
                "summary": "Binary malware summarization aims to automatically generate human-readable\ndescriptions of malware behaviors from executable files, facilitating tasks\nlike malware cracking and detection. Previous methods based on Large Language\nModels (LLMs) have shown great promise. However, they still face significant\nissues, including poor usability, inaccurate explanations,and incomplete\nsummaries, primarily due to the obscure pseudocode structure and the lack of\nmalware training summaries. Further, calling relationships between functions,\nwhich involve the rich interactions within a binary malware, remain largely\nunderexplored. To this end, we propose MALSIGHT, a novel code summarization\nframework that can iteratively generate descriptions of binary malware by\nexploring malicious source code and benign pseudocode. Specifically, we\nconstruct the first malware summary dataset, MalS and MalP, using an LLM and\nmanually refine this dataset with human effort. At the training stage, we tune\nour proposed MalT5, a novel LLM-based code model, on the MalS and benign\npseudocode datasets. Then, at the test stage, we iteratively feed the\npseudocode functions into MalT5 to obtain the summary. Such a procedure\nfacilitates the understanding of pseudocode structure and captures the\nintricate interactions between functions, thereby benefiting summaries'\nusability, accuracy, and completeness. Additionally, we propose a novel\nevaluation benchmark, BLEURT-sum, to measure the quality of summaries.\nExperiments on three datasets show the effectiveness of the proposed MALSIGHT.\nNotably, our proposed MalT5, with only 0.77B parameters, delivers comparable\nperformance to much larger Code-Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary malware summarization aims to automatically generate human-readable\ndescriptions of malware behaviors from executable files, facilitating tasks\nlike malware cracking and detection. Previous methods based on Large Language\nModels (LLMs) have shown great promise. However, they still face significant\nissues, including poor usability, inaccurate explanations,and incomplete\nsummaries, primarily due to the obscure pseudocode structure and the lack of\nmalware training summaries. Further, calling relationships between functions,\nwhich involve the rich interactions within a binary malware, remain largely\nunderexplored. To this end, we propose MALSIGHT, a novel code summarization\nframework that can iteratively generate descriptions of binary malware by\nexploring malicious source code and benign pseudocode. Specifically, we\nconstruct the first malware summary dataset, MalS and MalP, using an LLM and\nmanually refine this dataset with human effort. At the training stage, we tune\nour proposed MalT5, a novel LLM-based code model, on the MalS and benign\npseudocode datasets. Then, at the test stage, we iteratively feed the\npseudocode functions into MalT5 to obtain the summary. Such a procedure\nfacilitates the understanding of pseudocode structure and captures the\nintricate interactions between functions, thereby benefiting summaries'\nusability, accuracy, and completeness. Additionally, we propose a novel\nevaluation benchmark, BLEURT-sum, to measure the quality of summaries.\nExperiments on three datasets show the effectiveness of the proposed MALSIGHT.\nNotably, our proposed MalT5, with only 0.77B parameters, delivers comparable\nperformance to much larger Code-Llama."
                },
                "authors": [
                    {
                        "name": "Haolang Lu"
                    },
                    {
                        "name": "Hongrui Peng"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Jiaoyang Cui"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Weifei Jin"
                    },
                    {
                        "name": "Songtao Wang"
                    },
                    {
                        "name": "Shengli Pan"
                    },
                    {
                        "name": "Xiaofeng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Tao"
                },
                "author": "Xiaofeng Tao",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03900v1",
                "updated": "2024-11-06T13:24:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    24,
                    34,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T13:24:34Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    24,
                    34,
                    2,
                    311,
                    0
                ],
                "title": "Retentive Neural Quantum States: Efficient Ansätze for Ab Initio\n  Quantum Chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retentive Neural Quantum States: Efficient Ansätze for Ab Initio\n  Quantum Chemistry"
                },
                "summary": "Neural-network quantum states (NQS) has emerged as a powerful application of\nquantum-inspired deep learning for variational Monte Carlo methods, offering a\ncompetitive alternative to existing techniques for identifying ground states of\nquantum problems. A significant advancement toward improving the practical\nscalability of NQS has been the incorporation of autoregressive models, most\nrecently transformers, as variational ansatze. Transformers learn sequence\ninformation with greater expressiveness than recurrent models, but at the cost\nof increased time complexity with respect to sequence length. We explore the\nuse of the retentive network (RetNet), a recurrent alternative to transformers,\nas an ansatz for solving electronic ground state problems in $\\textit{ab\ninitio}$ quantum chemistry. Unlike transformers, RetNets overcome this time\ncomplexity bottleneck by processing data in parallel during training, and\nrecurrently during inference. We give a simple computational cost estimate of\nthe RetNet and directly compare it with similar estimates for transformers,\nestablishing a clear threshold ratio of problem-to-model size past which the\nRetNet's time complexity outperforms that of the transformer. Though this\nefficiency can comes at the expense of decreased expressiveness relative to the\ntransformer, we overcome this gap through training strategies that leverage the\nautoregressive structure of the model -- namely, variational neural annealing.\nOur findings support the RetNet as a means of improving the time complexity of\nNQS without sacrificing accuracy. We provide further evidence that the ablative\nimprovements of neural annealing extend beyond the RetNet architecture,\nsuggesting it would serve as an effective general training strategy for\nautoregressive NQS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural-network quantum states (NQS) has emerged as a powerful application of\nquantum-inspired deep learning for variational Monte Carlo methods, offering a\ncompetitive alternative to existing techniques for identifying ground states of\nquantum problems. A significant advancement toward improving the practical\nscalability of NQS has been the incorporation of autoregressive models, most\nrecently transformers, as variational ansatze. Transformers learn sequence\ninformation with greater expressiveness than recurrent models, but at the cost\nof increased time complexity with respect to sequence length. We explore the\nuse of the retentive network (RetNet), a recurrent alternative to transformers,\nas an ansatz for solving electronic ground state problems in $\\textit{ab\ninitio}$ quantum chemistry. Unlike transformers, RetNets overcome this time\ncomplexity bottleneck by processing data in parallel during training, and\nrecurrently during inference. We give a simple computational cost estimate of\nthe RetNet and directly compare it with similar estimates for transformers,\nestablishing a clear threshold ratio of problem-to-model size past which the\nRetNet's time complexity outperforms that of the transformer. Though this\nefficiency can comes at the expense of decreased expressiveness relative to the\ntransformer, we overcome this gap through training strategies that leverage the\nautoregressive structure of the model -- namely, variational neural annealing.\nOur findings support the RetNet as a means of improving the time complexity of\nNQS without sacrificing accuracy. We provide further evidence that the ablative\nimprovements of neural annealing extend beyond the RetNet architecture,\nsuggesting it would serve as an effective general training strategy for\nautoregressive NQS."
                },
                "authors": [
                    {
                        "name": "Oliver Knitter"
                    },
                    {
                        "name": "Dan Zhao"
                    },
                    {
                        "name": "James Stokes"
                    },
                    {
                        "name": "Martin Ganahl"
                    },
                    {
                        "name": "Stefan Leichenauer"
                    },
                    {
                        "name": "Shravan Veerapaneni"
                    }
                ],
                "author_detail": {
                    "name": "Shravan Veerapaneni"
                },
                "author": "Shravan Veerapaneni",
                "arxiv_comment": "16 pages, 1 figure, to be submitted for peer-reviewed publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03884v1",
                "updated": "2024-11-06T13:00:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    0,
                    34,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T13:00:34Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    0,
                    34,
                    2,
                    311,
                    0
                ],
                "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polynomial Composition Activations: Unleashing the Dynamics of Large\n  Language Models"
                },
                "summary": "Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom."
                },
                "authors": [
                    {
                        "name": "Zhijian Zhuo"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Xiaoqing Li"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Jinwen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jinwen Ma"
                },
                "author": "Jinwen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03883v1",
                "updated": "2024-11-06T12:57:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    57,
                    58,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:57:58Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    57,
                    58,
                    2,
                    311,
                    0
                ],
                "title": "MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering"
                },
                "summary": "Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder."
                },
                "authors": [
                    {
                        "name": "Laura Cabello"
                    },
                    {
                        "name": "Carmen Martin-Turrero"
                    },
                    {
                        "name": "Uchenna Akujuobi"
                    },
                    {
                        "name": "Anders Søgaard"
                    },
                    {
                        "name": "Carlos Bobed"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Bobed"
                },
                "author": "Carlos Bobed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03881v1",
                "updated": "2024-11-06T12:54:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    54,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:54:27Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    54,
                    27,
                    2,
                    311,
                    0
                ],
                "title": "Data Fusion of Synthetic Query Variants With Generative Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Fusion of Synthetic Query Variants With Generative Large Language\n  Models"
                },
                "summary": "Considering query variance in information retrieval (IR) experiments is\nbeneficial for retrieval effectiveness. Especially ranking ensembles based on\ndifferent topically related queries retrieve better results than rankings based\non a single query alone. Recently, generative instruction-tuned Large Language\nModels (LLMs) improved on a variety of different tasks in capturing human\nlanguage. To this end, this work explores the feasibility of using synthetic\nquery variants generated by instruction-tuned LLMs in data fusion experiments.\nMore specifically, we introduce a lightweight, unsupervised, and cost-efficient\napproach that exploits principled prompting and data fusion techniques. In our\nexperiments, LLMs produce more effective queries when provided with additional\ncontext information on the topic. Furthermore, our analysis based on four TREC\nnewswire benchmarks shows that data fusion based on synthetic query variants is\nsignificantly better than baselines with single queries and also outperforms\npseudo-relevance feedback methods. We publicly share the code and query\ndatasets with the community as resources for follow-up studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considering query variance in information retrieval (IR) experiments is\nbeneficial for retrieval effectiveness. Especially ranking ensembles based on\ndifferent topically related queries retrieve better results than rankings based\non a single query alone. Recently, generative instruction-tuned Large Language\nModels (LLMs) improved on a variety of different tasks in capturing human\nlanguage. To this end, this work explores the feasibility of using synthetic\nquery variants generated by instruction-tuned LLMs in data fusion experiments.\nMore specifically, we introduce a lightweight, unsupervised, and cost-efficient\napproach that exploits principled prompting and data fusion techniques. In our\nexperiments, LLMs produce more effective queries when provided with additional\ncontext information on the topic. Furthermore, our analysis based on four TREC\nnewswire benchmarks shows that data fusion based on synthetic query variants is\nsignificantly better than baselines with single queries and also outperforms\npseudo-relevance feedback methods. We publicly share the code and query\ndatasets with the community as resources for follow-up studies."
                },
                "authors": [
                    {
                        "name": "Timo Breuer"
                    }
                ],
                "author_detail": {
                    "name": "Timo Breuer"
                },
                "author": "Timo Breuer",
                "arxiv_doi": "10.1145/3673791.3698423",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3673791.3698423",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.03881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive version of record was published in SIGIR-AP '24",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03877v1",
                "updated": "2024-11-06T12:48:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    48,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:48:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    48,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning"
                },
                "summary": "Answering reasoning-based complex questions over text and hybrid sources,\nincluding tables, is a challenging task. Recent advances in large language\nmodels (LLMs) have enabled in-context learning (ICL), allowing LLMs to acquire\nproficiency in a specific task using only a few demonstration samples\n(exemplars). A critical challenge in ICL is the selection of optimal exemplars,\nwhich can be either task-specific (static) or test-example-specific (dynamic).\nStatic exemplars provide faster inference times and increased robustness across\na distribution of test examples. In this paper, we propose an algorithm for\nstatic exemplar subset selection for complex reasoning tasks. We introduce\nEXPLORA, a novel exploration method designed to estimate the parameters of the\nscoring function, which evaluates exemplar subsets without incorporating\nconfidence information. EXPLORA significantly reduces the number of LLM calls\nto ~11% of those required by state-of-the-art methods and achieves a\nsubstantial performance improvement of 12.24%. We open-source our code and data\n(https://github.com/kiranpurohit/EXPLORA).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering reasoning-based complex questions over text and hybrid sources,\nincluding tables, is a challenging task. Recent advances in large language\nmodels (LLMs) have enabled in-context learning (ICL), allowing LLMs to acquire\nproficiency in a specific task using only a few demonstration samples\n(exemplars). A critical challenge in ICL is the selection of optimal exemplars,\nwhich can be either task-specific (static) or test-example-specific (dynamic).\nStatic exemplars provide faster inference times and increased robustness across\na distribution of test examples. In this paper, we propose an algorithm for\nstatic exemplar subset selection for complex reasoning tasks. We introduce\nEXPLORA, a novel exploration method designed to estimate the parameters of the\nscoring function, which evaluates exemplar subsets without incorporating\nconfidence information. EXPLORA significantly reduces the number of LLM calls\nto ~11% of those required by state-of-the-art methods and achieves a\nsubstantial performance improvement of 12.24%. We open-source our code and data\n(https://github.com/kiranpurohit/EXPLORA)."
                },
                "authors": [
                    {
                        "name": "Kiran Purohit"
                    },
                    {
                        "name": "Venktesh V"
                    },
                    {
                        "name": "Raghuram Devalla"
                    },
                    {
                        "name": "Krishna Mohan Yerragorla"
                    },
                    {
                        "name": "Sourangshu Bhattacharya"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03876v1",
                "updated": "2024-11-06T12:45:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    45,
                    46,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:45:46Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    45,
                    46,
                    2,
                    311,
                    0
                ],
                "title": "Large Generative Model-assisted Talking-face Semantic Communication\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Generative Model-assisted Talking-face Semantic Communication\n  System"
                },
                "summary": "The rapid development of generative Artificial Intelligence (AI) continually\nunveils the potential of Semantic Communication (SemCom). However, current\ntalking-face SemCom systems still encounter challenges such as low bandwidth\nutilization, semantic ambiguity, and diminished Quality of Experience (QoE).\nThis study introduces a Large Generative Model-assisted Talking-face Semantic\nCommunication (LGM-TSC) System tailored for the talking-face video\ncommunication. Firstly, we introduce a Generative Semantic Extractor (GSE) at\nthe transmitter based on the FunASR model to convert semantically sparse\ntalking-face videos into texts with high information density. Secondly, we\nestablish a private Knowledge Base (KB) based on the Large Language Model (LLM)\nfor semantic disambiguation and correction, complemented by a joint knowledge\nbase-semantic-channel coding scheme. Finally, at the receiver, we propose a\nGenerative Semantic Reconstructor (GSR) that utilizes BERT-VITS2 and SadTalker\nmodels to transform text back into a high-QoE talking-face video matching the\nuser's timbre. Simulation results demonstrate the feasibility and effectiveness\nof the proposed LGM-TSC system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of generative Artificial Intelligence (AI) continually\nunveils the potential of Semantic Communication (SemCom). However, current\ntalking-face SemCom systems still encounter challenges such as low bandwidth\nutilization, semantic ambiguity, and diminished Quality of Experience (QoE).\nThis study introduces a Large Generative Model-assisted Talking-face Semantic\nCommunication (LGM-TSC) System tailored for the talking-face video\ncommunication. Firstly, we introduce a Generative Semantic Extractor (GSE) at\nthe transmitter based on the FunASR model to convert semantically sparse\ntalking-face videos into texts with high information density. Secondly, we\nestablish a private Knowledge Base (KB) based on the Large Language Model (LLM)\nfor semantic disambiguation and correction, complemented by a joint knowledge\nbase-semantic-channel coding scheme. Finally, at the receiver, we propose a\nGenerative Semantic Reconstructor (GSR) that utilizes BERT-VITS2 and SadTalker\nmodels to transform text back into a high-QoE talking-face video matching the\nuser's timbre. Simulation results demonstrate the feasibility and effectiveness\nof the proposed LGM-TSC system."
                },
                "authors": [
                    {
                        "name": "Feibo Jiang"
                    },
                    {
                        "name": "Siwei Tu"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Cunhua Pan"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Xiaohu You"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu You"
                },
                "author": "Xiaohu You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03205v2",
                "updated": "2024-11-06T12:42:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    42,
                    5,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-05T15:53:59Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    53,
                    59,
                    1,
                    310,
                    0
                ],
                "title": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis"
                },
                "summary": "Recent advancements in Generative AI offer promising capabilities for spatial\nanalysis. Despite their potential, the integration of generative AI with\nestablished GIS platforms remains underexplored. In this study, we propose a\nframework for integrating LLMs directly into existing GIS platforms, using QGIS\nas an example. Our approach leverages the reasoning and programming\ncapabilities of LLMs to autonomously generate spatial analysis workflows and\ncode through an informed agent that has comprehensive documentation of key GIS\ntools and parameters. The implementation of this framework resulted in the\ndevelopment of a \"GIS Copilot\" that allows GIS users to interact with QGIS\nusing natural language commands for spatial analysis. The GIS Copilot was\nevaluated based on three complexity levels: basic tasks that require one GIS\ntool and typically involve one data layer to perform simple operations;\nintermediate tasks involving multi-step processes with multiple tools, guided\nby user instructions; and advanced tasks which involve multi-step processes\nthat require multiple tools but not guided by user instructions, necessitating\nthe agent to independently decide on and executes the necessary steps. The\nevaluation reveals that the GIS Copilot demonstrates strong potential in\nautomating foundational GIS operations, with a high success rate in tool\nselection and code generation for basic and intermediate tasks, while\nchallenges remain in achieving full autonomy for more complex tasks. This study\ncontributes to the emerging vision of Autonomous GIS, providing a pathway for\nnon-experts to engage with geospatial analysis with minimal prior expertise.\nWhile full autonomy is yet to be achieved, the GIS Copilot demonstrates\nsignificant potential for simplifying GIS workflows and enhancing\ndecision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Generative AI offer promising capabilities for spatial\nanalysis. Despite their potential, the integration of generative AI with\nestablished GIS platforms remains underexplored. In this study, we propose a\nframework for integrating LLMs directly into existing GIS platforms, using QGIS\nas an example. Our approach leverages the reasoning and programming\ncapabilities of LLMs to autonomously generate spatial analysis workflows and\ncode through an informed agent that has comprehensive documentation of key GIS\ntools and parameters. The implementation of this framework resulted in the\ndevelopment of a \"GIS Copilot\" that allows GIS users to interact with QGIS\nusing natural language commands for spatial analysis. The GIS Copilot was\nevaluated based on three complexity levels: basic tasks that require one GIS\ntool and typically involve one data layer to perform simple operations;\nintermediate tasks involving multi-step processes with multiple tools, guided\nby user instructions; and advanced tasks which involve multi-step processes\nthat require multiple tools but not guided by user instructions, necessitating\nthe agent to independently decide on and executes the necessary steps. The\nevaluation reveals that the GIS Copilot demonstrates strong potential in\nautomating foundational GIS operations, with a high success rate in tool\nselection and code generation for basic and intermediate tasks, while\nchallenges remain in achieving full autonomy for more complex tasks. This study\ncontributes to the emerging vision of Autonomous GIS, providing a pathway for\nnon-experts to engage with geospatial analysis with minimal prior expertise.\nWhile full autonomy is yet to be achieved, the GIS Copilot demonstrates\nsignificant potential for simplifying GIS workflows and enhancing\ndecision-making processes."
                },
                "authors": [
                    {
                        "name": "Temitope Akinboyewa"
                    },
                    {
                        "name": "Zhenlong Li"
                    },
                    {
                        "name": "Huan Ning"
                    },
                    {
                        "name": "M. Naser Lessani"
                    }
                ],
                "author_detail": {
                    "name": "M. Naser Lessani"
                },
                "author": "M. Naser Lessani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10499v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10499v3",
                "updated": "2024-11-06T12:35:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    35,
                    37,
                    2,
                    311,
                    0
                ],
                "published": "2024-07-15T07:43:55Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    7,
                    43,
                    55,
                    0,
                    197,
                    0
                ],
                "title": "CIBench: Evaluating Your LLMs with a Code Interpreter Plugin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CIBench: Evaluating Your LLMs with a Code Interpreter Plugin"
                },
                "summary": "While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization."
                },
                "authors": [
                    {
                        "name": "Chuyu Zhang"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Yingfan Hu"
                    },
                    {
                        "name": "Haowen Shen"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Zerun Ma"
                    },
                    {
                        "name": "Fengzhe Zhou"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Xuming He"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Under review. The first three authors contribute equally, and\n  Songyang Zhang is the project leader",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10499v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10499v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03866v1",
                "updated": "2024-11-06T12:22:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    22,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:22:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    22,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the\n  Way Forward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the\n  Way Forward"
                },
                "summary": "Recent research has demonstrated that training a linear connector between\nspeech foundation encoders and large language models (LLMs) enables this\narchitecture to achieve strong ASR capabilities. Despite the impressive\nresults, it remains unclear whether these simple approaches are robust enough\nacross different scenarios and speech conditions, such as domain shifts and\ndifferent speech perturbations. In this paper, we address these questions by\nconducting various ablation experiments using a recent and widely adopted\napproach called SLAM-ASR. We present novel empirical findings that offer\ninsights on how to effectively utilize the SLAM-ASR architecture across a wide\nrange of settings. Our main findings indicate that the SLAM-ASR exhibits poor\nperformance in cross-domain evaluation settings. Additionally, speech\nperturbations within in-domain data, such as changes in speed or the presence\nof additive noise, can significantly impact performance. Our findings offer\ncritical insights for fine-tuning and configuring robust LLM-based ASR models,\ntailored to different data characteristics and computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has demonstrated that training a linear connector between\nspeech foundation encoders and large language models (LLMs) enables this\narchitecture to achieve strong ASR capabilities. Despite the impressive\nresults, it remains unclear whether these simple approaches are robust enough\nacross different scenarios and speech conditions, such as domain shifts and\ndifferent speech perturbations. In this paper, we address these questions by\nconducting various ablation experiments using a recent and widely adopted\napproach called SLAM-ASR. We present novel empirical findings that offer\ninsights on how to effectively utilize the SLAM-ASR architecture across a wide\nrange of settings. Our main findings indicate that the SLAM-ASR exhibits poor\nperformance in cross-domain evaluation settings. Additionally, speech\nperturbations within in-domain data, such as changes in speed or the presence\nof additive noise, can significantly impact performance. Our findings offer\ncritical insights for fine-tuning and configuring robust LLM-based ASR models,\ntailored to different data characteristics and computational resources."
                },
                "authors": [
                    {
                        "name": "Shashi Kumar"
                    },
                    {
                        "name": "Iuliia Thorbecke"
                    },
                    {
                        "name": "Sergio Burdisso"
                    },
                    {
                        "name": "Esaú Villatoro-Tello"
                    },
                    {
                        "name": "Manjunath K E"
                    },
                    {
                        "name": "Kadri Hacioğlu"
                    },
                    {
                        "name": "Pradeep Rangappa"
                    },
                    {
                        "name": "Petr Motlicek"
                    },
                    {
                        "name": "Aravind Ganapathiraju"
                    },
                    {
                        "name": "Andreas Stolcke"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Stolcke"
                },
                "author": "Andreas Stolcke",
                "arxiv_comment": "Submitted to ICASSP 2025 SALMA Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03865v1",
                "updated": "2024-11-06T12:19:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    19,
                    1,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    19,
                    1,
                    2,
                    311,
                    0
                ],
                "title": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making"
                },
                "summary": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety."
                },
                "authors": [
                    {
                        "name": "Yizhe Huang"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Aoyang Qin"
                    },
                    {
                        "name": "Min Tang"
                    },
                    {
                        "name": "Xiaoxi Wang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Mingjie Bi"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "arxiv_comment": "Accepted at NeurIPS D&B 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03864v1",
                "updated": "2024-11-06T12:17:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    17,
                    21,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:17:21Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    17,
                    21,
                    2,
                    311,
                    0
                ],
                "title": "Hidden Cooling Flows in Elliptical Galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Cooling Flows in Elliptical Galaxies"
                },
                "summary": "The radiative cooling time of hot gas in the cool cores of many galaxy\nclusters and massive elliptical galaxies drops in the centre to below 100\nmillion years. The mass cooling rates inferred from simple modelling of X-ray\nobservations of these objects are very low, indicating that either AGN feedback\nis tightly balanced or that soft X-rays from cooling gas are somehow hidden\nfrom view. An intrinsic absorption model developed for application to galaxy\nclusters is used here to search for hidden cooling flows (HCFs) in seven nearby\nelliptical galaxies. Mass cooling rates of 0.5-8 solar masses per year are\nfound in each galaxy. The absorbed cooling flow luminosity is in agreement with\nthe observed Far Infrared (FIR) luminosity in each case, indicating absorbed\nemission is energetically capable of emerging in the FIR band. An observed lack\nof agreement between HCF rates and normal star formation rates suggests the\ncooled material must have an alternative fate, with low-mass star formation\nconsidered as the primary outcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The radiative cooling time of hot gas in the cool cores of many galaxy\nclusters and massive elliptical galaxies drops in the centre to below 100\nmillion years. The mass cooling rates inferred from simple modelling of X-ray\nobservations of these objects are very low, indicating that either AGN feedback\nis tightly balanced or that soft X-rays from cooling gas are somehow hidden\nfrom view. An intrinsic absorption model developed for application to galaxy\nclusters is used here to search for hidden cooling flows (HCFs) in seven nearby\nelliptical galaxies. Mass cooling rates of 0.5-8 solar masses per year are\nfound in each galaxy. The absorbed cooling flow luminosity is in agreement with\nthe observed Far Infrared (FIR) luminosity in each case, indicating absorbed\nemission is energetically capable of emerging in the FIR band. An observed lack\nof agreement between HCF rates and normal star formation rates suggests the\ncooled material must have an alternative fate, with low-mass star formation\nconsidered as the primary outcome."
                },
                "authors": [
                    {
                        "name": "L. R. Ivey"
                    },
                    {
                        "name": "A. C. Fabian"
                    },
                    {
                        "name": "J. S. Sanders"
                    },
                    {
                        "name": "C. Pinto"
                    },
                    {
                        "name": "G. J. Ferland"
                    },
                    {
                        "name": "S. Walker"
                    },
                    {
                        "name": "J. Jiang"
                    }
                ],
                "author_detail": {
                    "name": "J. Jiang"
                },
                "author": "J. Jiang",
                "arxiv_comment": "17 pages, 14 figures, 7 tables. Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01204v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01204v3",
                "updated": "2024-11-06T12:02:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    2,
                    52,
                    2,
                    311,
                    0
                ],
                "published": "2024-04-01T16:00:01Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    16,
                    0,
                    1,
                    0,
                    92,
                    0
                ],
                "title": "The Fine Line: Navigating Large Language Model Pretraining with\n  Down-streaming Capability Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fine Line: Navigating Large Language Model Pretraining with\n  Down-streaming Capability Analysis"
                },
                "summary": "Uncovering early-stage metrics that reflect final model performance is one\ncore principle for large-scale pretraining. The existing scaling law\ndemonstrates the power-law correlation between pretraining loss and training\nflops, which serves as an important indicator of the current training state for\nlarge language models. However, this principle only focuses on the model's\ncompression properties on the training data, resulting in an inconsistency with\nthe ability improvements on the downstream tasks. Some follow-up works\nattempted to extend the scaling-law to more complex metrics (such as\nhyperparameters), but still lacked a comprehensive analysis of the dynamic\ndifferences among various capabilities during pretraining. To address the\naforementioned limitations, this paper undertakes a comprehensive comparison of\nmodel capabilities at various pretraining intermediate checkpoints. Through\nthis analysis, we confirm that specific downstream metrics exhibit similar\ntraining dynamics across models of different sizes, up to 67 billion\nparameters. In addition to our core findings, we've reproduced Amber and\nOpenLLaMA, releasing their intermediate checkpoints. This initiative offers\nvaluable resources to the research community and facilitates the verification\nand exploration of LLM pretraining by open-source researchers. Besides, we\nprovide empirical summaries, including performance comparisons of different\nmodels and capabilities, and tuition of key metrics for different training\nphases. Based on these findings, we provide a more user-friendly strategy for\nevaluating the optimization state, offering guidance for establishing a stable\npretraining process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering early-stage metrics that reflect final model performance is one\ncore principle for large-scale pretraining. The existing scaling law\ndemonstrates the power-law correlation between pretraining loss and training\nflops, which serves as an important indicator of the current training state for\nlarge language models. However, this principle only focuses on the model's\ncompression properties on the training data, resulting in an inconsistency with\nthe ability improvements on the downstream tasks. Some follow-up works\nattempted to extend the scaling-law to more complex metrics (such as\nhyperparameters), but still lacked a comprehensive analysis of the dynamic\ndifferences among various capabilities during pretraining. To address the\naforementioned limitations, this paper undertakes a comprehensive comparison of\nmodel capabilities at various pretraining intermediate checkpoints. Through\nthis analysis, we confirm that specific downstream metrics exhibit similar\ntraining dynamics across models of different sizes, up to 67 billion\nparameters. In addition to our core findings, we've reproduced Amber and\nOpenLLaMA, releasing their intermediate checkpoints. This initiative offers\nvaluable resources to the research community and facilitates the verification\nand exploration of LLM pretraining by open-source researchers. Besides, we\nprovide empirical summaries, including performance comparisons of different\nmodels and capabilities, and tuition of key metrics for different training\nphases. Based on these findings, we provide a more user-friendly strategy for\nevaluating the optimization state, offering guidance for establishing a stable\npretraining process."
                },
                "authors": [
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Junzhuo Li"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zhaoliang Chen"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Stephen W. Huang"
                    },
                    {
                        "name": "Shawn Yue"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01204v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01204v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19453v2",
                "updated": "2024-11-06T11:49:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    11,
                    49,
                    10,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-25T10:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    28,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework"
                },
                "summary": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research"
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Chenming Shang"
                    },
                    {
                        "name": "Sizhe Wang"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Renliang Sun"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03847v1",
                "updated": "2024-11-06T11:37:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    11,
                    37,
                    30,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T11:37:30Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    11,
                    37,
                    30,
                    2,
                    311,
                    0
                ],
                "title": "A Novel Access Control and Privacy-Enhancing Approach for Models in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Access Control and Privacy-Enhancing Approach for Models in Edge\n  Computing"
                },
                "summary": "With the widespread adoption of edge computing technologies and the\nincreasing prevalence of deep learning models in these environments, the\nsecurity risks and privacy threats to models and data have grown more acute.\nAttackers can exploit various techniques to illegally obtain models or misuse\ndata, leading to serious issues such as intellectual property infringement and\nprivacy breaches. Existing model access control technologies primarily rely on\ntraditional encryption and authentication methods; however, these approaches\nexhibit significant limitations in terms of flexibility and adaptability in\ndynamic environments. Although there have been advancements in model\nwatermarking techniques for marking model ownership, they remain limited in\ntheir ability to proactively protect intellectual property and prevent\nunauthorized access. To address these challenges, we propose a novel model\naccess control method tailored for edge computing environments. This method\nleverages image style as a licensing mechanism, embedding style recognition\ninto the model's operational framework to enable intrinsic access control.\nConsequently, models deployed on edge platforms are designed to correctly infer\nonly on license data with specific style, rendering them ineffective on any\nother data. By restricting the input data to the edge model, this approach not\nonly prevents attackers from gaining unauthorized access to the model but also\nenhances the privacy of data on terminal devices. We conducted extensive\nexperiments on benchmark datasets, including MNIST, CIFAR-10, and FACESCRUB,\nand the results demonstrate that our method effectively prevents unauthorized\naccess to the model while maintaining accuracy. Additionally, the model shows\nstrong resistance against attacks such as forged licenses and fine-tuning.\nThese results underscore the method's usability, security, and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of edge computing technologies and the\nincreasing prevalence of deep learning models in these environments, the\nsecurity risks and privacy threats to models and data have grown more acute.\nAttackers can exploit various techniques to illegally obtain models or misuse\ndata, leading to serious issues such as intellectual property infringement and\nprivacy breaches. Existing model access control technologies primarily rely on\ntraditional encryption and authentication methods; however, these approaches\nexhibit significant limitations in terms of flexibility and adaptability in\ndynamic environments. Although there have been advancements in model\nwatermarking techniques for marking model ownership, they remain limited in\ntheir ability to proactively protect intellectual property and prevent\nunauthorized access. To address these challenges, we propose a novel model\naccess control method tailored for edge computing environments. This method\nleverages image style as a licensing mechanism, embedding style recognition\ninto the model's operational framework to enable intrinsic access control.\nConsequently, models deployed on edge platforms are designed to correctly infer\nonly on license data with specific style, rendering them ineffective on any\nother data. By restricting the input data to the edge model, this approach not\nonly prevents attackers from gaining unauthorized access to the model but also\nenhances the privacy of data on terminal devices. We conducted extensive\nexperiments on benchmark datasets, including MNIST, CIFAR-10, and FACESCRUB,\nand the results demonstrate that our method effectively prevents unauthorized\naccess to the model while maintaining accuracy. Additionally, the model shows\nstrong resistance against attacks such as forged licenses and fine-tuning.\nThese results underscore the method's usability, security, and robustness."
                },
                "authors": [
                    {
                        "name": "Peihao Li"
                    }
                ],
                "author_detail": {
                    "name": "Peihao Li"
                },
                "author": "Peihao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03835v1",
                "updated": "2024-11-06T11:14:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    11,
                    14,
                    49,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T11:14:49Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    11,
                    14,
                    49,
                    2,
                    311,
                    0
                ],
                "title": "An Edge Computing-Based Solution for Real-Time Leaf Disease\n  Classification using Thermal Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Edge Computing-Based Solution for Real-Time Leaf Disease\n  Classification using Thermal Imaging"
                },
                "summary": "Deep learning (DL) technologies can transform agriculture by improving crop\nhealth monitoring and management, thus improving food safety. In this paper, we\nexplore the potential of edge computing for real-time classification of leaf\ndiseases using thermal imaging. We present a thermal image dataset for plant\ndisease classification and evaluate deep learning models, including\nInceptionV3, MobileNetV1, MobileNetV2, and VGG-16, on resource-constrained\ndevices like the Raspberry Pi 4B. Using pruning and quantization-aware\ntraining, these models achieve inference times up to 1.48x faster on Edge TPU\nMax for VGG16, and up to 2.13x faster with precision reduction on Intel NCS2\nfor MobileNetV1, compared to high-end GPUs like the RTX 3090, while maintaining\nstate-of-the-art accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) technologies can transform agriculture by improving crop\nhealth monitoring and management, thus improving food safety. In this paper, we\nexplore the potential of edge computing for real-time classification of leaf\ndiseases using thermal imaging. We present a thermal image dataset for plant\ndisease classification and evaluate deep learning models, including\nInceptionV3, MobileNetV1, MobileNetV2, and VGG-16, on resource-constrained\ndevices like the Raspberry Pi 4B. Using pruning and quantization-aware\ntraining, these models achieve inference times up to 1.48x faster on Edge TPU\nMax for VGG16, and up to 2.13x faster with precision reduction on Intel NCS2\nfor MobileNetV1, compared to high-end GPUs like the RTX 3090, while maintaining\nstate-of-the-art accuracy."
                },
                "authors": [
                    {
                        "name": "Públio Elon Correa da Silva"
                    },
                    {
                        "name": "Jurandy Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Jurandy Almeida"
                },
                "author": "Jurandy Almeida",
                "arxiv_doi": "10.1109/LGRS.2024.3456637",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LGRS.2024.3456637",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.03835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Geoscience and Remote Sensing Letters (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03827v1",
                "updated": "2024-11-06T11:00:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    11,
                    0,
                    44,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T11:00:44Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    11,
                    0,
                    44,
                    2,
                    311,
                    0
                ],
                "title": "DesignMinds: Enhancing Video-Based Design Ideation with Vision-Language\n  Model and Context-Injected Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DesignMinds: Enhancing Video-Based Design Ideation with Vision-Language\n  Model and Context-Injected Large Language Model"
                },
                "summary": "Ideation is a critical component of video-based design (VBD), where videos\nserve as the primary medium for design exploration and inspiration. The\nemergence of generative AI offers considerable potential to enhance this\nprocess by streamlining video analysis and facilitating idea generation. In\nthis paper, we present DesignMinds, a prototype that integrates a\nstate-of-the-art Vision-Language Model (VLM) with a context-enhanced Large\nLanguage Model (LLM) to support ideation in VBD. To evaluate DesignMinds, we\nconducted a between-subject study with 35 design practitioners, comparing its\nperformance to a baseline condition. Our results demonstrate that DesignMinds\nsignificantly enhances the flexibility and originality of ideation, while also\nincreasing task engagement. Importantly, the introduction of this technology\ndid not negatively impact user experience, technology acceptance, or usability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ideation is a critical component of video-based design (VBD), where videos\nserve as the primary medium for design exploration and inspiration. The\nemergence of generative AI offers considerable potential to enhance this\nprocess by streamlining video analysis and facilitating idea generation. In\nthis paper, we present DesignMinds, a prototype that integrates a\nstate-of-the-art Vision-Language Model (VLM) with a context-enhanced Large\nLanguage Model (LLM) to support ideation in VBD. To evaluate DesignMinds, we\nconducted a between-subject study with 35 design practitioners, comparing its\nperformance to a baseline condition. Our results demonstrate that DesignMinds\nsignificantly enhances the flexibility and originality of ideation, while also\nincreasing task engagement. Importantly, the introduction of this technology\ndid not negatively impact user experience, technology acceptance, or usability."
                },
                "authors": [
                    {
                        "name": "Tianhao He"
                    },
                    {
                        "name": "Andrija Stankovic"
                    },
                    {
                        "name": "Evangelos Niforatos"
                    },
                    {
                        "name": "Gerd Kortuem"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Kortuem"
                },
                "author": "Gerd Kortuem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03823v1",
                "updated": "2024-11-06T10:44:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    44,
                    15,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T10:44:15Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    44,
                    15,
                    2,
                    311,
                    0
                ],
                "title": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination"
                },
                "summary": "The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting dataset contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is sensitive to varying degrees of contamination and can\nhighlight significant performance improvements due to leakage of the training\nset of multimodal benchmarks. Furthermore, We also explore the possibility of\ncontamination originating from the pre-training phase of LLMs used by MLLMs and\nthe fine-tuning phase of MLLMs, offering new insights into the stages at which\ncontamination may be introduced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting dataset contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is sensitive to varying degrees of contamination and can\nhighlight significant performance improvements due to leakage of the training\nset of multimodal benchmarks. Furthermore, We also explore the possibility of\ncontamination originating from the pre-training phase of LLMs used by MLLMs and\nthe fine-tuning phase of MLLMs, offering new insights into the stages at which\ncontamination may be introduced."
                },
                "authors": [
                    {
                        "name": "Dingjie Song"
                    },
                    {
                        "name": "Sicheng Lai"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03817v1",
                "updated": "2024-11-06T10:35:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    35,
                    11,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T10:35:11Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    35,
                    11,
                    2,
                    311,
                    0
                ],
                "title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning"
                },
                "summary": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Zhirui Deng"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Ruibin Xiong"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03814v1",
                "updated": "2024-11-06T10:32:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    32,
                    9,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T10:32:09Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    32,
                    9,
                    2,
                    311,
                    0
                ],
                "title": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue"
                },
                "summary": "Large Language Models (LLMs) demonstrate outstanding performance in their\nreservoir of knowledge and understanding capabilities, but they have also been\nshown to be prone to illegal or unethical reactions when subjected to jailbreak\nattacks. To ensure their responsible deployment in critical applications, it is\ncrucial to understand the safety capabilities and vulnerabilities of LLMs.\nPrevious works mainly focus on jailbreak in single-round dialogue, overlooking\nthe potential jailbreak risks in multi-round dialogues, which are a vital way\nhumans interact with and extract information from LLMs. Some studies have\nincreasingly concentrated on the risks associated with jailbreak in multi-round\ndialogues. These efforts typically involve the use of manually crafted\ntemplates or prompt engineering techniques. However, due to the inherent\ncomplexity of multi-round dialogues, their jailbreak performance is limited. To\nsolve this problem, we propose a novel multi-round dialogue jailbreaking agent,\nemphasizing the importance of stealthiness in identifying and mitigating\npotential threats to human values posed by LLMs. We propose a risk\ndecomposition strategy that distributes risks across multiple rounds of queries\nand utilizes psychological strategies to enhance attack strength. Extensive\nexperiments show that our proposed method surpasses other attack methods and\nachieves state-of-the-art attack success rate. We will make the corresponding\ncode and dataset available for future research. The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate outstanding performance in their\nreservoir of knowledge and understanding capabilities, but they have also been\nshown to be prone to illegal or unethical reactions when subjected to jailbreak\nattacks. To ensure their responsible deployment in critical applications, it is\ncrucial to understand the safety capabilities and vulnerabilities of LLMs.\nPrevious works mainly focus on jailbreak in single-round dialogue, overlooking\nthe potential jailbreak risks in multi-round dialogues, which are a vital way\nhumans interact with and extract information from LLMs. Some studies have\nincreasingly concentrated on the risks associated with jailbreak in multi-round\ndialogues. These efforts typically involve the use of manually crafted\ntemplates or prompt engineering techniques. However, due to the inherent\ncomplexity of multi-round dialogues, their jailbreak performance is limited. To\nsolve this problem, we propose a novel multi-round dialogue jailbreaking agent,\nemphasizing the importance of stealthiness in identifying and mitigating\npotential threats to human values posed by LLMs. We propose a risk\ndecomposition strategy that distributes risks across multiple rounds of queries\nand utilizes psychological strategies to enhance attack strength. Extensive\nexperiments show that our proposed method surpasses other attack methods and\nachieves state-of-the-art attack success rate. We will make the corresponding\ncode and dataset available for future research. The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Fengxiang Wang"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Peng Xiao"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "YueFeng Chen"
                    },
                    {
                        "name": "Chongwen Wang"
                    },
                    {
                        "name": "Jialing Tao"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Hui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xue"
                },
                "author": "Hui Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18680v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18680v3",
                "updated": "2024-11-06T10:27:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    27,
                    5,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-27T12:06:53Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    6,
                    53,
                    4,
                    271,
                    0
                ],
                "title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models"
                },
                "summary": "Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines."
                },
                "authors": [
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Xianghu Yue"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Luis Fernando D'Haro"
                    },
                    {
                        "name": "Robby T. Tan"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "EMNLP24 Findings. Data available at\n  https://github.com/MatthewCYM/MALLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18680v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18680v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06922v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06922v3",
                "updated": "2024-11-06T10:22:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    22,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-02-10T11:07:24Z",
                "published_parsed": [
                    2024,
                    2,
                    10,
                    11,
                    7,
                    24,
                    5,
                    41,
                    0
                ],
                "title": "Whispers in the Machine: Confidentiality in LLM-integrated Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whispers in the Machine: Confidentiality in LLM-integrated Systems"
                },
                "summary": "Large Language Models (LLMs) are increasingly augmented with external tools\nand commercial services into LLM-integrated systems. While these interfaces can\nsignificantly enhance the capabilities of the models, they also introduce a new\nattack surface. Manipulated integrations, for example, can exploit the model\nand compromise sensitive data accessed through other interfaces. While previous\nwork primarily focused on attacks targeting a model's alignment or the leakage\nof training data, the security of data that is only available during inference\nhas escaped scrutiny so far. In this work, we demonstrate the vulnerabilities\nassociated with external components and introduce a systematic approach to\nevaluate confidentiality risks in LLM-integrated systems. We identify two\nspecific attack scenarios unique to these systems and formalize these into a\ntool-robustness framework designed to measure a model's ability to protect\nsensitive information. Our findings show that all examined models are highly\nvulnerable to confidentiality attacks, with the risk increasing significantly\nwhen models are used together with external tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly augmented with external tools\nand commercial services into LLM-integrated systems. While these interfaces can\nsignificantly enhance the capabilities of the models, they also introduce a new\nattack surface. Manipulated integrations, for example, can exploit the model\nand compromise sensitive data accessed through other interfaces. While previous\nwork primarily focused on attacks targeting a model's alignment or the leakage\nof training data, the security of data that is only available during inference\nhas escaped scrutiny so far. In this work, we demonstrate the vulnerabilities\nassociated with external components and introduce a systematic approach to\nevaluate confidentiality risks in LLM-integrated systems. We identify two\nspecific attack scenarios unique to these systems and formalize these into a\ntool-robustness framework designed to measure a model's ability to protect\nsensitive information. Our findings show that all examined models are highly\nvulnerable to confidentiality attacks, with the risk increasing significantly\nwhen models are used together with external tools."
                },
                "authors": [
                    {
                        "name": "Jonathan Evertz"
                    },
                    {
                        "name": "Merlin Chlosta"
                    },
                    {
                        "name": "Lea Schönherr"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Eisenhofer"
                },
                "author": "Thorsten Eisenhofer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06922v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06922v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03811v1",
                "updated": "2024-11-06T10:14:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    14,
                    58,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T10:14:58Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    14,
                    58,
                    2,
                    311,
                    0
                ],
                "title": "The natural stability of autonomous morphology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The natural stability of autonomous morphology"
                },
                "summary": "Autonomous morphology, such as inflection class systems and paradigmatic\ndistribution patterns, is widespread and diachronically resilient in natural\nlanguage. Why this should be so has remained unclear given that autonomous\nmorphology imposes learning costs, offers no clear benefit relative to its\nabsence and could easily be removed by the analogical forces which are\nconstantly reshaping it. Here we propose an explanation for the resilience of\nautonomous morphology, in terms of a diachronic dynamic of attraction and\nrepulsion between morphomic categories, which emerges spontaneously from a\nsimple paradigm cell filling process. Employing computational evolutionary\nmodels, our key innovation is to bring to light the role of `dissociative\nevidence', i.e., evidence for inflectional distinctiveness which a rational\nreasoner will have access to during analogical inference. Dissociative evidence\ncreates a repulsion dynamic which prevents morphomic classes from collapsing\ntogether entirely, i.e., undergoing complete levelling. As we probe alternative\nmodels, we reveal the limits of conditional entropy as a measure for\npredictability in systems that are undergoing change. Finally, we demonstrate\nthat autonomous morphology, far from being `unnatural' (e.g.\n\\citealt{Aronoff1994}), is rather the natural (emergent) consequence of a\nnatural (rational) process of inference applied to inflectional systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous morphology, such as inflection class systems and paradigmatic\ndistribution patterns, is widespread and diachronically resilient in natural\nlanguage. Why this should be so has remained unclear given that autonomous\nmorphology imposes learning costs, offers no clear benefit relative to its\nabsence and could easily be removed by the analogical forces which are\nconstantly reshaping it. Here we propose an explanation for the resilience of\nautonomous morphology, in terms of a diachronic dynamic of attraction and\nrepulsion between morphomic categories, which emerges spontaneously from a\nsimple paradigm cell filling process. Employing computational evolutionary\nmodels, our key innovation is to bring to light the role of `dissociative\nevidence', i.e., evidence for inflectional distinctiveness which a rational\nreasoner will have access to during analogical inference. Dissociative evidence\ncreates a repulsion dynamic which prevents morphomic classes from collapsing\ntogether entirely, i.e., undergoing complete levelling. As we probe alternative\nmodels, we reveal the limits of conditional entropy as a measure for\npredictability in systems that are undergoing change. Finally, we demonstrate\nthat autonomous morphology, far from being `unnatural' (e.g.\n\\citealt{Aronoff1994}), is rather the natural (emergent) consequence of a\nnatural (rational) process of inference applied to inflectional systems."
                },
                "authors": [
                    {
                        "name": "Erich Round"
                    },
                    {
                        "name": "Louise Esher"
                    },
                    {
                        "name": "Sacha Beniamine"
                    }
                ],
                "author_detail": {
                    "name": "Sacha Beniamine"
                },
                "author": "Sacha Beniamine",
                "arxiv_comment": "Accepted for publication by the journal Morphology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.6.m; J.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03806v1",
                "updated": "2024-11-06T10:06:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    6,
                    21,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T10:06:21Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    6,
                    21,
                    2,
                    311,
                    0
                ],
                "title": "Understanding the Effects of Human-written Paraphrases in LLM-generated\n  Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Effects of Human-written Paraphrases in LLM-generated\n  Text Detection"
                },
                "summary": "Natural Language Generation has been rapidly developing with the advent of\nlarge language models (LLMs). While their usage has sparked significant\nattention from the general public, it is important for readers to be aware when\na piece of text is LLM-generated. This has brought about the need for building\nmodels that enable automated LLM-generated text detection, with the aim of\nmitigating potential negative outcomes of such content. Existing LLM-generated\ndetectors show competitive performances in telling apart LLM-generated and\nhuman-written text, but this performance is likely to deteriorate when\nparaphrased texts are considered. In this study, we devise a new data\ncollection strategy to collect Human & LLM Paraphrase Collection (HLPC), a\nfirst-of-its-kind dataset that incorporates human-written texts and\nparaphrases, as well as LLM-generated texts and paraphrases. With the aim of\nunderstanding the effects of human-written paraphrases on the performance of\nstate-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark\ndetectors, we perform classification experiments that incorporate human-written\nparaphrases, watermarked and non-watermarked LLM-generated documents from GPT\nand OPT, and LLM-generated paraphrases from DIPPER and BART. The results show\nthat the inclusion of human-written paraphrases has a significant impact of\nLLM-generated detector performance, promoting TPR@1%FPR with a possible\ntrade-off of AUROC and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Generation has been rapidly developing with the advent of\nlarge language models (LLMs). While their usage has sparked significant\nattention from the general public, it is important for readers to be aware when\na piece of text is LLM-generated. This has brought about the need for building\nmodels that enable automated LLM-generated text detection, with the aim of\nmitigating potential negative outcomes of such content. Existing LLM-generated\ndetectors show competitive performances in telling apart LLM-generated and\nhuman-written text, but this performance is likely to deteriorate when\nparaphrased texts are considered. In this study, we devise a new data\ncollection strategy to collect Human & LLM Paraphrase Collection (HLPC), a\nfirst-of-its-kind dataset that incorporates human-written texts and\nparaphrases, as well as LLM-generated texts and paraphrases. With the aim of\nunderstanding the effects of human-written paraphrases on the performance of\nstate-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark\ndetectors, we perform classification experiments that incorporate human-written\nparaphrases, watermarked and non-watermarked LLM-generated documents from GPT\nand OPT, and LLM-generated paraphrases from DIPPER and BART. The results show\nthat the inclusion of human-written paraphrases has a significant impact of\nLLM-generated detector performance, promoting TPR@1%FPR with a possible\ntrade-off of AUROC and accuracy."
                },
                "authors": [
                    {
                        "name": "Hiu Ting Lau"
                    },
                    {
                        "name": "Arkaitz Zubiaga"
                    }
                ],
                "author_detail": {
                    "name": "Arkaitz Zubiaga"
                },
                "author": "Arkaitz Zubiaga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03805v1",
                "updated": "2024-11-06T10:02:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    2,
                    50,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T10:02:50Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    2,
                    50,
                    2,
                    311,
                    0
                ],
                "title": "A Comparative Study of Recent Large Language Models on Generating\n  Hospital Discharge Summaries for Lung Cancer Patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study of Recent Large Language Models on Generating\n  Hospital Discharge Summaries for Lung Cancer Patients"
                },
                "summary": "Generating discharge summaries is a crucial yet time-consuming task in\nclinical practice, essential for conveying pertinent patient information and\nfacilitating continuity of care. Recent advancements in large language models\n(LLMs) have significantly enhanced their capability in understanding and\nsummarizing complex medical texts. This research aims to explore how LLMs can\nalleviate the burden of manual summarization, streamline workflow efficiencies,\nand support informed decision-making in healthcare settings. Clinical notes\nfrom a cohort of 1,099 lung cancer patients were utilized, with a subset of 50\npatients for testing purposes, and 102 patients used for model fine-tuning.\nThis study evaluates the performance of multiple LLMs, including GPT-3.5,\nGPT-4, GPT-4o, and LLaMA 3 8b, in generating discharge summaries. Evaluation\nmetrics included token-level analysis (BLEU, ROUGE-1, ROUGE-2, ROUGE-L) and\nsemantic similarity scores between model-generated summaries and\nphysician-written gold standards. LLaMA 3 8b was further tested on clinical\nnotes of varying lengths to examine the stability of its performance. The study\nfound notable variations in summarization capabilities among LLMs. GPT-4o and\nfine-tuned LLaMA 3 demonstrated superior token-level evaluation metrics, while\nLLaMA 3 consistently produced concise summaries across different input lengths.\nSemantic similarity scores indicated GPT-4o and LLaMA 3 as leading models in\ncapturing clinical relevance. This study contributes insights into the efficacy\nof LLMs for generating discharge summaries, highlighting LLaMA 3's robust\nperformance in maintaining clarity and relevance across varying clinical\ncontexts. These findings underscore the potential of automated summarization\ntools to enhance documentation precision and efficiency, ultimately improving\npatient care and operational capability in healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating discharge summaries is a crucial yet time-consuming task in\nclinical practice, essential for conveying pertinent patient information and\nfacilitating continuity of care. Recent advancements in large language models\n(LLMs) have significantly enhanced their capability in understanding and\nsummarizing complex medical texts. This research aims to explore how LLMs can\nalleviate the burden of manual summarization, streamline workflow efficiencies,\nand support informed decision-making in healthcare settings. Clinical notes\nfrom a cohort of 1,099 lung cancer patients were utilized, with a subset of 50\npatients for testing purposes, and 102 patients used for model fine-tuning.\nThis study evaluates the performance of multiple LLMs, including GPT-3.5,\nGPT-4, GPT-4o, and LLaMA 3 8b, in generating discharge summaries. Evaluation\nmetrics included token-level analysis (BLEU, ROUGE-1, ROUGE-2, ROUGE-L) and\nsemantic similarity scores between model-generated summaries and\nphysician-written gold standards. LLaMA 3 8b was further tested on clinical\nnotes of varying lengths to examine the stability of its performance. The study\nfound notable variations in summarization capabilities among LLMs. GPT-4o and\nfine-tuned LLaMA 3 demonstrated superior token-level evaluation metrics, while\nLLaMA 3 consistently produced concise summaries across different input lengths.\nSemantic similarity scores indicated GPT-4o and LLaMA 3 as leading models in\ncapturing clinical relevance. This study contributes insights into the efficacy\nof LLMs for generating discharge summaries, highlighting LLaMA 3's robust\nperformance in maintaining clarity and relevance across varying clinical\ncontexts. These findings underscore the potential of automated summarization\ntools to enhance documentation precision and efficiency, ultimately improving\npatient care and operational capability in healthcare settings."
                },
                "authors": [
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Fang Li"
                    },
                    {
                        "name": "Kirk Roberts"
                    },
                    {
                        "name": "Licong Cui"
                    },
                    {
                        "name": "Cui Tao"
                    },
                    {
                        "name": "Hua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xu"
                },
                "author": "Hua Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03424v2",
                "updated": "2024-11-06T10:00:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    0,
                    42,
                    2,
                    311,
                    0
                ],
                "published": "2024-07-03T18:06:21Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    18,
                    6,
                    21,
                    2,
                    185,
                    0
                ],
                "title": "Supernova Shocks Cannot Explain the Inflated State of Hypervelocity\n  Runaways from White Dwarf Binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supernova Shocks Cannot Explain the Inflated State of Hypervelocity\n  Runaways from White Dwarf Binaries"
                },
                "summary": "Recent observations have found a growing number of hypervelocity stars with\nspeeds of $\\approx 1500-2500\\,$km\\,s$^{-1}$ which could have only been produced\nthrough thermonuclear supernovae in white dwarf binaries. Most of the observed\nhypervelocity runaways in this class display a surprising inflated structure:\ntheir current radii are roughly an order of magnitude greater than they would\nhave been as white dwarfs filling their Roche lobe. While many simulations\nexist studying the dynamical phase leading to supernova detonation in these\nsystems, no detailed calculations of the long-term structure of the runaways\nhave yet been performed. We use an existing \\textsc{Arepo} hydrodynamical\nsimulation of a supernova in a white dwarf binary as a starting point for the\nevolution of these stars with the 1 dimensional stellar evolution code MESA. We\nshow that the supernova shock is not enough to inflate the white dwarf over\ntimescales longer than a few thousand years, significantly shorter than the\n$10^{5-6}$ year lifetimes inferred for observed hypervelocity runaways. Despite\nexperiencing a shock from a supernova less than $\\approx 0.02\\,R_\\odot$ away,\nour models do not experience significant interior heating, and all contract\nback to radii around $0.01\\,R_\\odot$ within about $10^4$\\,years. Explaining the\nobserved inflated states requires either an additional source of significant\nheating or some other physics that is not yet accounted for in the subsequent\nevolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent observations have found a growing number of hypervelocity stars with\nspeeds of $\\approx 1500-2500\\,$km\\,s$^{-1}$ which could have only been produced\nthrough thermonuclear supernovae in white dwarf binaries. Most of the observed\nhypervelocity runaways in this class display a surprising inflated structure:\ntheir current radii are roughly an order of magnitude greater than they would\nhave been as white dwarfs filling their Roche lobe. While many simulations\nexist studying the dynamical phase leading to supernova detonation in these\nsystems, no detailed calculations of the long-term structure of the runaways\nhave yet been performed. We use an existing \\textsc{Arepo} hydrodynamical\nsimulation of a supernova in a white dwarf binary as a starting point for the\nevolution of these stars with the 1 dimensional stellar evolution code MESA. We\nshow that the supernova shock is not enough to inflate the white dwarf over\ntimescales longer than a few thousand years, significantly shorter than the\n$10^{5-6}$ year lifetimes inferred for observed hypervelocity runaways. Despite\nexperiencing a shock from a supernova less than $\\approx 0.02\\,R_\\odot$ away,\nour models do not experience significant interior heating, and all contract\nback to radii around $0.01\\,R_\\odot$ within about $10^4$\\,years. Explaining the\nobserved inflated states requires either an additional source of significant\nheating or some other physics that is not yet accounted for in the subsequent\nevolution."
                },
                "authors": [
                    {
                        "name": "Aakash Bhat"
                    },
                    {
                        "name": "Evan B. Bauer"
                    },
                    {
                        "name": "Rüdiger Pakmor"
                    },
                    {
                        "name": "Ken J. Shen"
                    },
                    {
                        "name": "Ilaria Caiazzo"
                    },
                    {
                        "name": "Abinaya Swaruba Rajamuthukumar"
                    },
                    {
                        "name": "Kareem El-Badry"
                    },
                    {
                        "name": "Wolfgang E. Kerzendorf"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang E. Kerzendorf"
                },
                "author": "Wolfgang E. Kerzendorf",
                "arxiv_comment": "Accepted for publication by A\\&A. 15 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15306v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15306v3",
                "updated": "2024-11-06T09:49:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    49,
                    31,
                    2,
                    311,
                    0
                ],
                "published": "2024-05-24T07:48:35Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    7,
                    48,
                    35,
                    4,
                    145,
                    0
                ],
                "title": "DeTikZify: Synthesizing Graphics Programs for Scientific Figures and\n  Sketches with TikZ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeTikZify: Synthesizing Graphics Programs for Scientific Figures and\n  Sketches with TikZ"
                },
                "summary": "Creating high-quality scientific figures can be time-consuming and\nchallenging, even though sketching ideas on paper is relatively easy.\nFurthermore, recreating existing figures that are not stored in formats\npreserving semantic information is equally complex. To tackle this problem, we\nintroduce DeTikZify, a novel multimodal language model that automatically\nsynthesizes scientific figures as semantics-preserving TikZ graphics programs\nbased on sketches and existing figures. To achieve this, we create three new\ndatasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k\nhuman-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn\nsketches with their corresponding scientific figures; and MetaFig, a collection\nof diverse scientific figures and associated metadata. We train DeTikZify on\nMetaFig and DaTikZv2, along with synthetically generated sketches learned from\nSketchFig. We also introduce an MCTS-based inference algorithm that enables\nDeTikZify to iteratively refine its outputs without the need for additional\ntraining. Through both automatic and human evaluation, we demonstrate that\nDeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ\nprograms, with the MCTS algorithm effectively boosting its performance. We make\nour code, models, and datasets publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating high-quality scientific figures can be time-consuming and\nchallenging, even though sketching ideas on paper is relatively easy.\nFurthermore, recreating existing figures that are not stored in formats\npreserving semantic information is equally complex. To tackle this problem, we\nintroduce DeTikZify, a novel multimodal language model that automatically\nsynthesizes scientific figures as semantics-preserving TikZ graphics programs\nbased on sketches and existing figures. To achieve this, we create three new\ndatasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k\nhuman-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn\nsketches with their corresponding scientific figures; and MetaFig, a collection\nof diverse scientific figures and associated metadata. We train DeTikZify on\nMetaFig and DaTikZv2, along with synthetically generated sketches learned from\nSketchFig. We also introduce an MCTS-based inference algorithm that enables\nDeTikZify to iteratively refine its outputs without the need for additional\ntraining. Through both automatic and human evaluation, we demonstrate that\nDeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ\nprograms, with the MCTS algorithm effectively boosting its performance. We make\nour code, models, and datasets publicly available."
                },
                "authors": [
                    {
                        "name": "Jonas Belouadi"
                    },
                    {
                        "name": "Simone Paolo Ponzetto"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "arxiv_comment": "Accepted at NeurIPS 2024 (spotlight); Project page:\n  https://github.com/potamides/DeTikZify",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15306v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15306v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03795v1",
                "updated": "2024-11-06T09:39:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    39,
                    52,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T09:39:52Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    39,
                    52,
                    2,
                    311,
                    0
                ],
                "title": "VQA$^2$:Visual Question Answering for Video Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQA$^2$:Visual Question Answering for Video Quality Assessment"
                },
                "summary": "The advent and proliferation of large multi-modal models (LMMs) have\nintroduced a new paradigm to video-related computer vision fields, including\ntraining and inference methods based on visual question answering (VQA). These\nmethods enable models to handle multiple downstream tasks robustly. Video\nQuality Assessment (VQA), a classic field in low-level visual quality\nevaluation, originally focused on quantitative video quality scoring. However,\ndriven by advances in LMMs, it is now evolving towards more comprehensive\nvisual quality understanding tasks. Visual question answering has significantly\nimproved low-level visual evaluation within the image domain recently. However,\nrelated work is almost nonexistent in the video domain, leaving substantial\nroom for improvement. To address this gap, we introduce the VQA2 Instruction\nDataset the first visual question answering instruction dataset entirely\nfocuses on video quality assessment, and based on it, we propose the VQA2\nseries models The VQA2 Instruction Dataset consists of three stages and covers\nvarious video types, containing 157,735 instruction question-answer pairs,\nincluding both manually annotated and synthetic data. We conduct extensive\nexperiments on both video quality scoring and video quality understanding\ntasks. Results demonstrate that the VQA2 series models achieve state-of-the-art\n(SOTA) performance in quality scoring tasks, and their performance in visual\nquality question answering surpasses the renowned GPT-4o. Additionally, our\nfinal model, the VQA2-Assistant, performs well across both scoring and\nquestion-answering tasks, validating its versatility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent and proliferation of large multi-modal models (LMMs) have\nintroduced a new paradigm to video-related computer vision fields, including\ntraining and inference methods based on visual question answering (VQA). These\nmethods enable models to handle multiple downstream tasks robustly. Video\nQuality Assessment (VQA), a classic field in low-level visual quality\nevaluation, originally focused on quantitative video quality scoring. However,\ndriven by advances in LMMs, it is now evolving towards more comprehensive\nvisual quality understanding tasks. Visual question answering has significantly\nimproved low-level visual evaluation within the image domain recently. However,\nrelated work is almost nonexistent in the video domain, leaving substantial\nroom for improvement. To address this gap, we introduce the VQA2 Instruction\nDataset the first visual question answering instruction dataset entirely\nfocuses on video quality assessment, and based on it, we propose the VQA2\nseries models The VQA2 Instruction Dataset consists of three stages and covers\nvarious video types, containing 157,735 instruction question-answer pairs,\nincluding both manually annotated and synthetic data. We conduct extensive\nexperiments on both video quality scoring and video quality understanding\ntasks. Results demonstrate that the VQA2 series models achieve state-of-the-art\n(SOTA) performance in quality scoring tasks, and their performance in visual\nquality question answering surpasses the renowned GPT-4o. Additionally, our\nfinal model, the VQA2-Assistant, performs well across both scoring and\nquestion-answering tasks, validating its versatility."
                },
                "authors": [
                    {
                        "name": "Ziheng Jia"
                    },
                    {
                        "name": "Zicheng Zhang"
                    },
                    {
                        "name": "Jiaying Qian"
                    },
                    {
                        "name": "Haoning Wu"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Chunyi Li"
                    },
                    {
                        "name": "Xiaohong Liu"
                    },
                    {
                        "name": "Weisi Lin"
                    },
                    {
                        "name": "Guangtao Zhai"
                    },
                    {
                        "name": "Xiongkuo Min"
                    }
                ],
                "author_detail": {
                    "name": "Xiongkuo Min"
                },
                "author": "Xiongkuo Min",
                "arxiv_comment": "10 pages 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03786v1",
                "updated": "2024-11-06T09:23:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    23,
                    50,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T09:23:50Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    23,
                    50,
                    2,
                    311,
                    0
                ],
                "title": "The N-Grammys: Accelerating Autoregressive Inference with Learning-Free\n  Batched Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The N-Grammys: Accelerating Autoregressive Inference with Learning-Free\n  Batched Speculation"
                },
                "summary": "Speculative decoding aims to speed up autoregressive generation of a language\nmodel by verifying in parallel the tokens generated by a smaller draft model.In\nthis work, we explore the effectiveness of learning-free, negligible-cost draft\nstrategies, namely $N$-grams obtained from the model weights and the context.\nWhile the predicted next token of the base model is rarely the top prediction\nof these simple strategies, we observe that it is often within their top-$k$\npredictions for small $k$. Based on this, we show that combinations of simple\nstrategies can achieve significant inference speedups over different tasks. The\noverall performance is comparable to more complex methods, yet does not require\nexpensive preprocessing or modification of the base model, and allows for\nseamless `plug-and-play' integration into pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding aims to speed up autoregressive generation of a language\nmodel by verifying in parallel the tokens generated by a smaller draft model.In\nthis work, we explore the effectiveness of learning-free, negligible-cost draft\nstrategies, namely $N$-grams obtained from the model weights and the context.\nWhile the predicted next token of the base model is rarely the top prediction\nof these simple strategies, we observe that it is often within their top-$k$\npredictions for small $k$. Based on this, we show that combinations of simple\nstrategies can achieve significant inference speedups over different tasks. The\noverall performance is comparable to more complex methods, yet does not require\nexpensive preprocessing or modification of the base model, and allows for\nseamless `plug-and-play' integration into pipelines."
                },
                "authors": [
                    {
                        "name": "Lawrence Stewart"
                    },
                    {
                        "name": "Matthew Trager"
                    },
                    {
                        "name": "Sujan Kumar Gonugondla"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "arxiv_affiliation": "UCLA-CS",
                "author": "Stefano Soatto",
                "arxiv_journal_ref": "ENLSP-IV 2024 - 4th NeurIPS Efficient Natural Language and Speech\n  Processing Workshop, Dec 2024, Vancouver, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03262v2",
                "updated": "2024-11-06T09:11:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    11,
                    14,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-05T17:01:16Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    17,
                    1,
                    16,
                    1,
                    310,
                    0
                ],
                "title": "Spatio-temporal topology of plasmonic spin meron pairs revealed by\n  polarimetric photo-emission microscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal topology of plasmonic spin meron pairs revealed by\n  polarimetric photo-emission microscopy"
                },
                "summary": "Topology is the study of geometrical properties and spatial relations\nunaffected by continuous changes, and has become an important tool for\nunderstanding complex physical systems. Although recent optical experiments\nhave inferred the existence of vector fields with the topologies of merons, the\ninability to extract the full three dimensional vectors misses a richer set of\ntopologies that have not yet been fully explored. In our work, we extend the\nstudy of the topology of electromagnetic fields on surfaces to a spin\nquasi-particle with the topology of a meron pair, formed by interfering surface\nplasmon polaritons, and show that the in-plane vectors are constrained by the\nembedding topology of the space as dictated by the Poincare-Hopf theorem. In\naddition we explore the time evolution of the three dimensional topology of the\nspin field formed by femtosecond laser pulses. These experiments are possible\nusing our here developed method called polarimetric photoemission electron\nmicroscopy (polarimetric PEEM) that combines an optical pump-probe technique\nand polarimetry with photo-emission electron microscopy. This method allows for\nthe accurate generation of surface plasmon polariton fields and their\nsubsequent measurement, revealing both the spatial distribution of the full\nthree-dimensional electromagnetic fields at deep sub-wavelength resolution and\ntheir time evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology is the study of geometrical properties and spatial relations\nunaffected by continuous changes, and has become an important tool for\nunderstanding complex physical systems. Although recent optical experiments\nhave inferred the existence of vector fields with the topologies of merons, the\ninability to extract the full three dimensional vectors misses a richer set of\ntopologies that have not yet been fully explored. In our work, we extend the\nstudy of the topology of electromagnetic fields on surfaces to a spin\nquasi-particle with the topology of a meron pair, formed by interfering surface\nplasmon polaritons, and show that the in-plane vectors are constrained by the\nembedding topology of the space as dictated by the Poincare-Hopf theorem. In\naddition we explore the time evolution of the three dimensional topology of the\nspin field formed by femtosecond laser pulses. These experiments are possible\nusing our here developed method called polarimetric photoemission electron\nmicroscopy (polarimetric PEEM) that combines an optical pump-probe technique\nand polarimetry with photo-emission electron microscopy. This method allows for\nthe accurate generation of surface plasmon polariton fields and their\nsubsequent measurement, revealing both the spatial distribution of the full\nthree-dimensional electromagnetic fields at deep sub-wavelength resolution and\ntheir time evolution."
                },
                "authors": [
                    {
                        "name": "Pascal Dreher"
                    },
                    {
                        "name": "Alexander Neuhaus"
                    },
                    {
                        "name": "David Janoschka"
                    },
                    {
                        "name": "Alexandra Roedl"
                    },
                    {
                        "name": "Tim Meiler"
                    },
                    {
                        "name": "Bettina Frank"
                    },
                    {
                        "name": "Timothy J. Davis"
                    },
                    {
                        "name": "Harald Giessen"
                    },
                    {
                        "name": "Frank Meyer zu Heringdorf"
                    }
                ],
                "author_detail": {
                    "name": "Frank Meyer zu Heringdorf"
                },
                "arxiv_affiliation": "University of Stuttgart",
                "author": "Frank Meyer zu Heringdorf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03773v1",
                "updated": "2024-11-06T09:10:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    10,
                    25,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T09:10:25Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    10,
                    25,
                    2,
                    311,
                    0
                ],
                "title": "Model-independent calibration of Gamma-Ray Bursts with neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-independent calibration of Gamma-Ray Bursts with neural networks"
                },
                "summary": "The $\\Lambda$ Cold Dark Matter ($\\Lambda$CDM) cosmological model has been\nhighly successful in predicting cosmic structure and evolution, yet recent\nprecision measurements have highlighted discrepancies, especially in the Hubble\nconstant inferred from local and early-Universe data. Gamma-ray bursts (GRBs)\npresent a promising alternative for cosmological measurements, capable of\nreaching higher redshifts than traditional distance indicators. This work\nleverages GRBs to refine cosmological parameters independently of the\n$\\Lambda$CDM framework. Using the Platinum compilation of long GRBs, we\ncalibrate the Dainotti relations-empirical correlations among GRB luminosity\nproperties-as standard candles through artificial neural networks (ANNs). We\nanalyze both the 2D and 3D Dainotti calibration relations, leveraging an\nANN-driven Markov Chain Monte Carlo approach to minimize scatter in the\ncalibration parameters, thereby achieving a stable Hubble diagram. This\nANN-based calibration approach offers advantages over Gaussian processes,\navoiding issues such as kernel function dependence and overfitting. Our results\nemphasize the need for model-independent calibration approaches to address\nsystematic challenges in GRB luminosity variability, ultimately extending the\ncosmic distance ladder in a robust way. By addressing redshift evolution and\nreducing systematic uncertainties, GRBs can serve as reliable high-redshift\ndistance indicators, offering critical insights into current cosmological\ntensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $\\Lambda$ Cold Dark Matter ($\\Lambda$CDM) cosmological model has been\nhighly successful in predicting cosmic structure and evolution, yet recent\nprecision measurements have highlighted discrepancies, especially in the Hubble\nconstant inferred from local and early-Universe data. Gamma-ray bursts (GRBs)\npresent a promising alternative for cosmological measurements, capable of\nreaching higher redshifts than traditional distance indicators. This work\nleverages GRBs to refine cosmological parameters independently of the\n$\\Lambda$CDM framework. Using the Platinum compilation of long GRBs, we\ncalibrate the Dainotti relations-empirical correlations among GRB luminosity\nproperties-as standard candles through artificial neural networks (ANNs). We\nanalyze both the 2D and 3D Dainotti calibration relations, leveraging an\nANN-driven Markov Chain Monte Carlo approach to minimize scatter in the\ncalibration parameters, thereby achieving a stable Hubble diagram. This\nANN-based calibration approach offers advantages over Gaussian processes,\navoiding issues such as kernel function dependence and overfitting. Our results\nemphasize the need for model-independent calibration approaches to address\nsystematic challenges in GRB luminosity variability, ultimately extending the\ncosmic distance ladder in a robust way. By addressing redshift evolution and\nreducing systematic uncertainties, GRBs can serve as reliable high-redshift\ndistance indicators, offering critical insights into current cosmological\ntensions."
                },
                "authors": [
                    {
                        "name": "Purba Mukherjee"
                    },
                    {
                        "name": "Maria Giovanna Dainotti"
                    },
                    {
                        "name": "Konstantinos F. Dialektopoulos"
                    },
                    {
                        "name": "Jackson Levi Said"
                    },
                    {
                        "name": "Jurgen Mifsud"
                    }
                ],
                "author_detail": {
                    "name": "Jurgen Mifsud"
                },
                "author": "Jurgen Mifsud",
                "arxiv_comment": "21 pages; 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03768v1",
                "updated": "2024-11-06T09:04:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    4,
                    13,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T09:04:13Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    4,
                    13,
                    2,
                    311,
                    0
                ],
                "title": "A Bayesian Approach to Data Point Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Approach to Data Point Selection"
                },
                "summary": "Data point selection (DPS) is becoming a critical topic in deep learning due\nto the ease of acquiring uncurated training data compared to the difficulty of\nobtaining curated or processed data. Existing approaches to DPS are\npredominantly based on a bi-level optimisation (BLO) formulation, which is\ndemanding in terms of memory and computation, and exhibits some theoretical\ndefects regarding minibatches. Thus, we propose a novel Bayesian approach to\nDPS. We view the DPS problem as posterior inference in a novel Bayesian model\nwhere the posterior distributions of the instance-wise weights and the main\nneural network parameters are inferred under a reasonable prior and likelihood\nmodel. We employ stochastic gradient Langevin MCMC sampling to learn the main\nnetwork and instance-wise weights jointly, ensuring convergence even with\nminibatches. Our update equation is comparable to the widely used SGD and much\nmore efficient than existing BLO-based methods. Through controlled experiments\nin both the vision and language domains, we present the proof-of-concept.\nAdditionally, we demonstrate that our method scales effectively to large\nlanguage models and facilitates automated per-task optimization for instruction\nfine-tuning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data point selection (DPS) is becoming a critical topic in deep learning due\nto the ease of acquiring uncurated training data compared to the difficulty of\nobtaining curated or processed data. Existing approaches to DPS are\npredominantly based on a bi-level optimisation (BLO) formulation, which is\ndemanding in terms of memory and computation, and exhibits some theoretical\ndefects regarding minibatches. Thus, we propose a novel Bayesian approach to\nDPS. We view the DPS problem as posterior inference in a novel Bayesian model\nwhere the posterior distributions of the instance-wise weights and the main\nneural network parameters are inferred under a reasonable prior and likelihood\nmodel. We employ stochastic gradient Langevin MCMC sampling to learn the main\nnetwork and instance-wise weights jointly, ensuring convergence even with\nminibatches. Our update equation is comparable to the widely used SGD and much\nmore efficient than existing BLO-based methods. Through controlled experiments\nin both the vision and language domains, we present the proof-of-concept.\nAdditionally, we demonstrate that our method scales effectively to large\nlanguage models and facilitates automated per-task optimization for instruction\nfine-tuning datasets."
                },
                "authors": [
                    {
                        "name": "Xinnuo Xu"
                    },
                    {
                        "name": "Minyoung Kim"
                    },
                    {
                        "name": "Royson Lee"
                    },
                    {
                        "name": "Brais Martinez"
                    },
                    {
                        "name": "Timothy Hospedales"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Hospedales"
                },
                "author": "Timothy Hospedales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03766v1",
                "updated": "2024-11-06T08:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    59,
                    44,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T08:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    59,
                    44,
                    2,
                    311,
                    0
                ],
                "title": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It"
                },
                "summary": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as special tokenizers, PEs, and\nnumber formats), comprehensively evaluating their effectiveness using our\ntestbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and\nfind that 1) naive finetuning can improve NUPA a lot on many but not all tasks,\nand 2) surprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work takes a preliminary step towards understanding and\nimproving NUPA of LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as special tokenizers, PEs, and\nnumber formats), comprehensively evaluating their effectiveness using our\ntestbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and\nfind that 1) naive finetuning can improve NUPA a lot on many but not all tasks,\nand 2) surprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work takes a preliminary step towards understanding and\nimproving NUPA of LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook."
                },
                "authors": [
                    {
                        "name": "Haotong Yang"
                    },
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Shijia Kang"
                    },
                    {
                        "name": "Zhouchen Lin"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02603v2",
                "updated": "2024-11-06T08:51:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    51,
                    52,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T20:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    20,
                    53,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "FactTest: Factuality Testing in Large Language Models with Finite-Sample\n  and Distribution-Free Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FactTest: Factuality Testing in Large Language Models with Finite-Sample\n  and Distribution-Free Guarantees"
                },
                "summary": "The propensity of Large Language Models (LLMs) to generate hallucinations and\nnon-factual content undermines their reliability in high-stakes domains, where\nrigorous control over Type I errors (the conditional probability of incorrectly\nclassifying hallucinations as truthful content) is essential. Despite its\nimportance, formal verification of LLM factuality with such guarantees remains\nlargely unexplored. In this paper, we introduce FactTest, a novel framework\nthat statistically assesses whether an LLM can confidently provide correct\nanswers to given questions with high-probability correctness guarantees. We\nformulate factuality testing as hypothesis testing problem to enforce an upper\nbound of Type I errors at user-specified significance levels. Notably, we prove\nthat our framework also ensures strong Type II error control under mild\nconditions and can be extended to maintain its effectiveness when covariate\nshifts exist. %These analyses are amenable to the principled NP framework. Our\napproach is distribution-free and works for any number of human-annotated\nsamples. It is model-agnostic and applies to any black-box or white-box LM.\nExtensive experiments on question-answering (QA) and multiple-choice benchmarks\ndemonstrate that \\approach effectively detects hallucinations and improves the\nmodel's ability to abstain from answering unknown questions, leading to an over\n40% accuracy improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The propensity of Large Language Models (LLMs) to generate hallucinations and\nnon-factual content undermines their reliability in high-stakes domains, where\nrigorous control over Type I errors (the conditional probability of incorrectly\nclassifying hallucinations as truthful content) is essential. Despite its\nimportance, formal verification of LLM factuality with such guarantees remains\nlargely unexplored. In this paper, we introduce FactTest, a novel framework\nthat statistically assesses whether an LLM can confidently provide correct\nanswers to given questions with high-probability correctness guarantees. We\nformulate factuality testing as hypothesis testing problem to enforce an upper\nbound of Type I errors at user-specified significance levels. Notably, we prove\nthat our framework also ensures strong Type II error control under mild\nconditions and can be extended to maintain its effectiveness when covariate\nshifts exist. %These analyses are amenable to the principled NP framework. Our\napproach is distribution-free and works for any number of human-annotated\nsamples. It is model-agnostic and applies to any black-box or white-box LM.\nExtensive experiments on question-answering (QA) and multiple-choice benchmarks\ndemonstrate that \\approach effectively detects hallucinations and improves the\nmodel's ability to abstain from answering unknown questions, leading to an over\n40% accuracy improvement."
                },
                "authors": [
                    {
                        "name": "Fan Nie"
                    },
                    {
                        "name": "Xiaotian Hou"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Linjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linjun Zhang"
                },
                "author": "Linjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03759v1",
                "updated": "2024-11-06T08:42:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    42,
                    53,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T08:42:53Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    42,
                    53,
                    2,
                    311,
                    0
                ],
                "title": "Variational Inference on the Boolean Hypercube with the Quantum Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Inference on the Boolean Hypercube with the Quantum Entropy"
                },
                "summary": "In this paper, we derive variational inference upper-bounds on the\nlog-partition function of pairwise Markov random fields on the Boolean\nhypercube, based on quantum relaxations of the Kullback-Leibler divergence. We\nthen propose an efficient algorithm to compute these bounds based on\nprimal-dual optimization. An improvement of these bounds through the use of\n''hierarchies,'' similar to sum-of-squares (SoS) hierarchies is proposed, and\nwe present a greedy algorithm to select among these relaxations. We carry\nextensive numerical experiments and compare with state-of-the-art methods for\nthis inference problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we derive variational inference upper-bounds on the\nlog-partition function of pairwise Markov random fields on the Boolean\nhypercube, based on quantum relaxations of the Kullback-Leibler divergence. We\nthen propose an efficient algorithm to compute these bounds based on\nprimal-dual optimization. An improvement of these bounds through the use of\n''hierarchies,'' similar to sum-of-squares (SoS) hierarchies is proposed, and\nwe present a greedy algorithm to select among these relaxations. We carry\nextensive numerical experiments and compare with state-of-the-art methods for\nthis inference problem."
                },
                "authors": [
                    {
                        "name": "Eliot Beyler"
                    },
                    {
                        "name": "Francis Bach"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bach"
                },
                "arxiv_affiliation": "SIERRA",
                "author": "Francis Bach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03758v1",
                "updated": "2024-11-06T08:33:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    33,
                    7,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T08:33:07Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    33,
                    7,
                    2,
                    311,
                    0
                ],
                "title": "Sub-DM:Subspace Diffusion Model with Orthogonal Decomposition for MRI\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-DM:Subspace Diffusion Model with Orthogonal Decomposition for MRI\n  Reconstruction"
                },
                "summary": "Diffusion model-based approaches recently achieved re-markable success in MRI\nreconstruction, but integration into clinical routine remains challenging due\nto its time-consuming convergence. This phenomenon is partic-ularly notable\nwhen directly apply conventional diffusion process to k-space data without\nconsidering the inherent properties of k-space sampling, limiting k-space\nlearning efficiency and image reconstruction quality. To tackle these\nchallenges, we introduce subspace diffusion model with orthogonal\ndecomposition, a method (referred to as Sub-DM) that restrict the diffusion\nprocess via projections onto subspace as the k-space data distribution evolves\ntoward noise. Particularly, the subspace diffusion model circumvents the\ninference challenges posed by the com-plex and high-dimensional characteristics\nof k-space data, so the highly compact subspace ensures that diffusion process\nrequires only a few simple iterations to produce accurate prior information.\nFurthermore, the orthogonal decomposition strategy based on wavelet transform\nhin-ders the information loss during the migration of the vanilla diffusion\nprocess to the subspace. Considering the strate-gy is approximately reversible,\nsuch that the entire pro-cess can be reversed. As a result, it allows the\ndiffusion processes in different spaces to refine models through a mutual\nfeedback mechanism, enabling the learning of ac-curate prior even when dealing\nwith complex k-space data. Comprehensive experiments on different datasets\nclearly demonstrate that the superiority of Sub-DM against state of-the-art\nmethods in terms of reconstruction speed and quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion model-based approaches recently achieved re-markable success in MRI\nreconstruction, but integration into clinical routine remains challenging due\nto its time-consuming convergence. This phenomenon is partic-ularly notable\nwhen directly apply conventional diffusion process to k-space data without\nconsidering the inherent properties of k-space sampling, limiting k-space\nlearning efficiency and image reconstruction quality. To tackle these\nchallenges, we introduce subspace diffusion model with orthogonal\ndecomposition, a method (referred to as Sub-DM) that restrict the diffusion\nprocess via projections onto subspace as the k-space data distribution evolves\ntoward noise. Particularly, the subspace diffusion model circumvents the\ninference challenges posed by the com-plex and high-dimensional characteristics\nof k-space data, so the highly compact subspace ensures that diffusion process\nrequires only a few simple iterations to produce accurate prior information.\nFurthermore, the orthogonal decomposition strategy based on wavelet transform\nhin-ders the information loss during the migration of the vanilla diffusion\nprocess to the subspace. Considering the strate-gy is approximately reversible,\nsuch that the entire pro-cess can be reversed. As a result, it allows the\ndiffusion processes in different spaces to refine models through a mutual\nfeedback mechanism, enabling the learning of ac-curate prior even when dealing\nwith complex k-space data. Comprehensive experiments on different datasets\nclearly demonstrate that the superiority of Sub-DM against state of-the-art\nmethods in terms of reconstruction speed and quality."
                },
                "authors": [
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Qinrong Cai"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Qiuyun Fan"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Qiegen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qiegen Liu"
                },
                "author": "Qiegen Liu",
                "arxiv_comment": "10 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13824v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13824v3",
                "updated": "2024-11-06T08:29:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    29,
                    22,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-17T17:48:54Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    48,
                    54,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Webpage UIs for Text-Rich Visual Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Webpage UIs for Text-Rich Visual Understanding"
                },
                "summary": "Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios."
                },
                "authors": [
                    {
                        "name": "Junpeng Liu"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Yuxiao Qu"
                    },
                    {
                        "name": "Wai Lam"
                    },
                    {
                        "name": "Chenyan Xiong"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Xiang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yue"
                },
                "author": "Xiang Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13824v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13824v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03743v1",
                "updated": "2024-11-06T08:16:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    16,
                    56,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T08:16:56Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    16,
                    56,
                    2,
                    311,
                    0
                ],
                "title": "Automating Exploratory Proteomics Research via Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Exploratory Proteomics Research via Language Models"
                },
                "summary": "With the development of artificial intelligence, its contribution to science\nis evolving from simulating a complex problem to automating entire research\nprocesses and producing novel discoveries. Achieving this advancement requires\nboth specialized general models grounded in real-world scientific data and\niterative, exploratory frameworks that mirror human scientific methodologies.\nIn this paper, we present PROTEUS, a fully automated system for scientific\ndiscovery from raw proteomics data. PROTEUS uses large language models (LLMs)\nto perform hierarchical planning, execute specialized bioinformatics tools, and\niteratively refine analysis workflows to generate high-quality scientific\nhypotheses. The system takes proteomics datasets as input and produces a\ncomprehensive set of research objectives, analysis results, and novel\nbiological hypotheses without human intervention. We evaluated PROTEUS on 12\nproteomics datasets collected from various biological samples (e.g. immune\ncells, tumors) and different sample types (single-cell and bulk), generating\n191 scientific hypotheses. These were assessed using both automatic LLM-based\nscoring on 5 metrics and detailed reviews from human experts. Results\ndemonstrate that PROTEUS consistently produces reliable, logically coherent\nresults that align well with existing literature while also proposing novel,\nevaluable hypotheses. The system's flexible architecture facilitates seamless\nintegration of diverse analysis tools and adaptation to different proteomics\ndata types. By automating complex proteomics analysis workflows and hypothesis\ngeneration, PROTEUS has the potential to considerably accelerate the pace of\nscientific discovery in proteomics research, enabling researchers to\nefficiently explore large-scale datasets and uncover biological insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of artificial intelligence, its contribution to science\nis evolving from simulating a complex problem to automating entire research\nprocesses and producing novel discoveries. Achieving this advancement requires\nboth specialized general models grounded in real-world scientific data and\niterative, exploratory frameworks that mirror human scientific methodologies.\nIn this paper, we present PROTEUS, a fully automated system for scientific\ndiscovery from raw proteomics data. PROTEUS uses large language models (LLMs)\nto perform hierarchical planning, execute specialized bioinformatics tools, and\niteratively refine analysis workflows to generate high-quality scientific\nhypotheses. The system takes proteomics datasets as input and produces a\ncomprehensive set of research objectives, analysis results, and novel\nbiological hypotheses without human intervention. We evaluated PROTEUS on 12\nproteomics datasets collected from various biological samples (e.g. immune\ncells, tumors) and different sample types (single-cell and bulk), generating\n191 scientific hypotheses. These were assessed using both automatic LLM-based\nscoring on 5 metrics and detailed reviews from human experts. Results\ndemonstrate that PROTEUS consistently produces reliable, logically coherent\nresults that align well with existing literature while also proposing novel,\nevaluable hypotheses. The system's flexible architecture facilitates seamless\nintegration of diverse analysis tools and adaptation to different proteomics\ndata types. By automating complex proteomics analysis workflows and hypothesis\ngeneration, PROTEUS has the potential to considerably accelerate the pace of\nscientific discovery in proteomics research, enabling researchers to\nefficiently explore large-scale datasets and uncover biological insights."
                },
                "authors": [
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Linhai Xie"
                    },
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Zaoqu Liu"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Yibai Xiong"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Zhangren Chen"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Fuchu He"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.02420v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.02420v5",
                "updated": "2024-11-06T07:58:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    58,
                    39,
                    2,
                    311,
                    0
                ],
                "published": "2023-02-05T16:19:01Z",
                "published_parsed": [
                    2023,
                    2,
                    5,
                    16,
                    19,
                    1,
                    6,
                    36,
                    0
                ],
                "title": "Variational Inference on the Final-Layer Output of Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Inference on the Final-Layer Output of Neural Networks"
                },
                "summary": "Traditional neural networks are simple to train but they typically produce\noverconfident predictions. In contrast, Bayesian neural networks provide good\nuncertainty quantification but optimizing them is time consuming due to the\nlarge parameter space. This paper proposes to combine the advantages of both\napproaches by performing Variational Inference in the Final layer Output space\n(VIFO), because the output space is much smaller than the parameter space. We\nuse neural networks to learn the mean and the variance of the probabilistic\noutput. Using the Bayesian formulation we incorporate collapsed variational\ninference with VIFO which significantly improves the performance in practice.\nOn the other hand, like standard, non-Bayesian models, VIFO enjoys simple\ntraining and one can use Rademacher complexity to provide risk bounds for the\nmodel. Experiments show that VIFO provides a good tradeoff in terms of run time\nand uncertainty quantification, especially for out of distribution data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional neural networks are simple to train but they typically produce\noverconfident predictions. In contrast, Bayesian neural networks provide good\nuncertainty quantification but optimizing them is time consuming due to the\nlarge parameter space. This paper proposes to combine the advantages of both\napproaches by performing Variational Inference in the Final layer Output space\n(VIFO), because the output space is much smaller than the parameter space. We\nuse neural networks to learn the mean and the variance of the probabilistic\noutput. Using the Bayesian formulation we incorporate collapsed variational\ninference with VIFO which significantly improves the performance in practice.\nOn the other hand, like standard, non-Bayesian models, VIFO enjoys simple\ntraining and one can use Rademacher complexity to provide risk bounds for the\nmodel. Experiments show that VIFO provides a good tradeoff in terms of run time\nand uncertainty quantification, especially for out of distribution data."
                },
                "authors": [
                    {
                        "name": "Yadi Wei"
                    },
                    {
                        "name": "Roni Khardon"
                    }
                ],
                "author_detail": {
                    "name": "Roni Khardon"
                },
                "author": "Roni Khardon",
                "arxiv_comment": "Published to TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.02420v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.02420v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03728v1",
                "updated": "2024-11-06T07:46:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    46,
                    34,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:46:34Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    46,
                    34,
                    2,
                    311,
                    0
                ],
                "title": "Efficient Fourier Filtering Network with Contrastive Learning for\n  UAV-based Unaligned Bi-modal Salient Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Fourier Filtering Network with Contrastive Learning for\n  UAV-based Unaligned Bi-modal Salient Object Detection"
                },
                "summary": "Unmanned aerial vehicle (UAV)-based bi-modal salient object detection (BSOD)\naims to segment salient objects in a scene utilizing complementary cues in\nunaligned RGB and thermal image pairs. However, the high computational expense\nof existing UAV-based BSOD models limits their applicability to real-world UAV\ndevices. To address this problem, we propose an efficient Fourier filter\nnetwork with contrastive learning that achieves both real-time and accurate\nperformance. Specifically, we first design a semantic contrastive alignment\nloss to align the two modalities at the semantic level, which facilitates\nmutual refinement in a parameter-free way. Second, inspired by the fast Fourier\ntransform that obtains global relevance in linear complexity, we propose\nsynchronized alignment fusion, which aligns and fuses bi-modal features in the\nchannel and spatial dimensions by a hierarchical filtering mechanism. Our\nproposed model, AlignSal, reduces the number of parameters by 70.0%, decreases\nthe floating point operations by 49.4%, and increases the inference speed by\n152.5% compared to the cutting-edge BSOD model (i.e., MROS). Extensive\nexperiments on the UAV RGB-T 2400 and three weakly aligned datasets demonstrate\nthat AlignSal achieves both real-time inference speed and better performance\nand generalizability compared to sixteen state-of-the-art BSOD models across\nmost evaluation metrics. In addition, our ablation studies further verify\nAlignSal's potential in boosting the performance of existing aligned BSOD\nmodels on UAV-based unaligned data. The code is available at:\nhttps://github.com/JoshuaLPF/AlignSal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicle (UAV)-based bi-modal salient object detection (BSOD)\naims to segment salient objects in a scene utilizing complementary cues in\nunaligned RGB and thermal image pairs. However, the high computational expense\nof existing UAV-based BSOD models limits their applicability to real-world UAV\ndevices. To address this problem, we propose an efficient Fourier filter\nnetwork with contrastive learning that achieves both real-time and accurate\nperformance. Specifically, we first design a semantic contrastive alignment\nloss to align the two modalities at the semantic level, which facilitates\nmutual refinement in a parameter-free way. Second, inspired by the fast Fourier\ntransform that obtains global relevance in linear complexity, we propose\nsynchronized alignment fusion, which aligns and fuses bi-modal features in the\nchannel and spatial dimensions by a hierarchical filtering mechanism. Our\nproposed model, AlignSal, reduces the number of parameters by 70.0%, decreases\nthe floating point operations by 49.4%, and increases the inference speed by\n152.5% compared to the cutting-edge BSOD model (i.e., MROS). Extensive\nexperiments on the UAV RGB-T 2400 and three weakly aligned datasets demonstrate\nthat AlignSal achieves both real-time inference speed and better performance\nand generalizability compared to sixteen state-of-the-art BSOD models across\nmost evaluation metrics. In addition, our ablation studies further verify\nAlignSal's potential in boosting the performance of existing aligned BSOD\nmodels on UAV-based unaligned data. The code is available at:\nhttps://github.com/JoshuaLPF/AlignSal."
                },
                "authors": [
                    {
                        "name": "Pengfei Lyu"
                    },
                    {
                        "name": "Pak-Hei Yeung"
                    },
                    {
                        "name": "Xiufei Cheng"
                    },
                    {
                        "name": "Xiaosheng Yu"
                    },
                    {
                        "name": "Chengdong Wu"
                    },
                    {
                        "name": "Jagath C. Rajapakse"
                    }
                ],
                "author_detail": {
                    "name": "Jagath C. Rajapakse"
                },
                "author": "Jagath C. Rajapakse",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03700v1",
                "updated": "2024-11-06T06:50:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    50,
                    50,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T06:50:50Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    50,
                    50,
                    2,
                    311,
                    0
                ],
                "title": "The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms\n  in Aligned Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms\n  in Aligned Language Models"
                },
                "summary": "Natural-language assistants are designed to provide users with helpful\nresponses while avoiding harmful outputs, largely achieved through alignment to\nhuman preferences. Yet there is limited understanding of whether alignment\ntechniques may inadvertently perpetuate or even amplify harmful biases\ninherited from their pre-aligned base models. This issue is compounded by the\nchoice of bias evaluation benchmarks in popular preference-finetuned models,\nwhich predominantly focus on dominant social categories, such as binary gender,\nthereby limiting insights into biases affecting underrepresented groups.\nTowards addressing this gap, we center transgender, nonbinary, and other\ngender-diverse identities to investigate how alignment procedures interact with\npre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a\ncomprehensive survey of bias evaluation modalities across leading\npreference-finetuned LLMs, highlighting critical gaps in gender-diverse\nrepresentation, 2) systematic evaluation of gender-diverse biases across 12\nmodels spanning Direct Preference Optimization (DPO) stages, uncovering harms\npopular bias benchmarks fail to detect, and 3) a flexible framework for\nmeasuring harmful biases in implicit reward signals applicable to other social\ncontexts. Our findings reveal that DPO-aligned models are particularly\nsensitive to supervised finetuning (SFT), and can amplify two forms of\nreal-world gender-diverse harms from their base models: stigmatization and\ngender non-affirmative language. We conclude with recommendations tailored to\nDPO and broader alignment practices, advocating for the adoption of\ncommunity-informed bias evaluation frameworks to more effectively identify and\naddress underrepresented harms in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural-language assistants are designed to provide users with helpful\nresponses while avoiding harmful outputs, largely achieved through alignment to\nhuman preferences. Yet there is limited understanding of whether alignment\ntechniques may inadvertently perpetuate or even amplify harmful biases\ninherited from their pre-aligned base models. This issue is compounded by the\nchoice of bias evaluation benchmarks in popular preference-finetuned models,\nwhich predominantly focus on dominant social categories, such as binary gender,\nthereby limiting insights into biases affecting underrepresented groups.\nTowards addressing this gap, we center transgender, nonbinary, and other\ngender-diverse identities to investigate how alignment procedures interact with\npre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a\ncomprehensive survey of bias evaluation modalities across leading\npreference-finetuned LLMs, highlighting critical gaps in gender-diverse\nrepresentation, 2) systematic evaluation of gender-diverse biases across 12\nmodels spanning Direct Preference Optimization (DPO) stages, uncovering harms\npopular bias benchmarks fail to detect, and 3) a flexible framework for\nmeasuring harmful biases in implicit reward signals applicable to other social\ncontexts. Our findings reveal that DPO-aligned models are particularly\nsensitive to supervised finetuning (SFT), and can amplify two forms of\nreal-world gender-diverse harms from their base models: stigmatization and\ngender non-affirmative language. We conclude with recommendations tailored to\nDPO and broader alignment practices, advocating for the adoption of\ncommunity-informed bias evaluation frameworks to more effectively identify and\naddress underrepresented harms in LLMs."
                },
                "authors": [
                    {
                        "name": "Anaelia Ovalle"
                    },
                    {
                        "name": "Krunoslav Lehman Pavasovic"
                    },
                    {
                        "name": "Louis Martin"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Eric Michael Smith"
                    },
                    {
                        "name": "Adina Williams"
                    },
                    {
                        "name": "Levent Sagun"
                    }
                ],
                "author_detail": {
                    "name": "Levent Sagun"
                },
                "author": "Levent Sagun",
                "arxiv_comment": "Accepted to 2024 Neurips Queer in AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01147v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01147v4",
                "updated": "2024-11-06T06:29:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    29,
                    0,
                    2,
                    311,
                    0
                ],
                "published": "2023-11-02T11:04:02Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    11,
                    4,
                    2,
                    3,
                    306,
                    0
                ],
                "title": "Variational Inference for Sparse Poisson Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Inference for Sparse Poisson Regression"
                },
                "summary": "We have utilized the non-conjugate VB method for the problem of the sparse\nPoisson regression model. To provide an approximated conjugacy in the model,\nthe likelihood is approximated by a quadratic function, which provides the\nconjugacy of the approximation component with the Gaussian prior to the\nregression coefficient. Three sparsity-enforcing priors are used for this\nproblem. The proposed models are compared with each other and two frequentist\nsparse Poisson methods (LASSO and SCAD) to evaluate the estimation, prediction\nand the sparsing performance of the proposed methods. Throughout a simulated\ndata example, the accuracy of the VB methods is computed compared to the\ncorresponding benchmark MCMC methods. It can be observed that the proposed VB\nmethods have provided a good approximation to the posterior distribution of the\nparameters, while the VB methods are much faster than the MCMC ones. Using\nseveral benchmark count response data sets, the prediction performance of the\nproposed methods is evaluated in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have utilized the non-conjugate VB method for the problem of the sparse\nPoisson regression model. To provide an approximated conjugacy in the model,\nthe likelihood is approximated by a quadratic function, which provides the\nconjugacy of the approximation component with the Gaussian prior to the\nregression coefficient. Three sparsity-enforcing priors are used for this\nproblem. The proposed models are compared with each other and two frequentist\nsparse Poisson methods (LASSO and SCAD) to evaluate the estimation, prediction\nand the sparsing performance of the proposed methods. Throughout a simulated\ndata example, the accuracy of the VB methods is computed compared to the\ncorresponding benchmark MCMC methods. It can be observed that the proposed VB\nmethods have provided a good approximation to the posterior distribution of the\nparameters, while the VB methods are much faster than the MCMC ones. Using\nseveral benchmark count response data sets, the prediction performance of the\nproposed methods is evaluated in real-world applications."
                },
                "authors": [
                    {
                        "name": "Mitra Kharabati"
                    },
                    {
                        "name": "Morteza Amini"
                    }
                ],
                "author_detail": {
                    "name": "Morteza Amini"
                },
                "author": "Morteza Amini",
                "arxiv_comment": "A part of the PhD thesis of Miss Mitra Kharabati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01147v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01147v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01345v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01345v3",
                "updated": "2024-11-06T06:28:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    28,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-05-02T14:49:50Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    14,
                    49,
                    50,
                    3,
                    123,
                    0
                ],
                "title": "The Power of Question Translation Training in Multilingual Reasoning:\n  Broadened Scope and Deepened Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Question Translation Training in Multilingual Reasoning:\n  Broadened Scope and Deepened Insights"
                },
                "summary": "Bridging the significant gap between large language model's English and\nnon-English performance presents a great challenge. While some previous studies\nattempt to mitigate this gap with translated training data, the recently\nproposed question alignment framework leverages the model's English expertise\nto improve multilingual performance with minimum usage of expensive,\nerror-prone translation. In this paper, we explore how broadly this method can\nbe applied by examining its effects in reasoning with and without\nchain-of-thought, as well as with program-of-thought. We also explore applying\nthis framework to extremely large language models in an efficient manner, such\nas through proxy-tuning. Experiment results on multilingual reasoning\nbenchmarks mGSM, mSVAMP, xCSQA and xNLI demonstrate that we can extend question\nalignment framework to boost multilingual performance across diverse reasoning\nscenarios, model families, and sizes. For instance, when applied to the LLaMA2\nmodels, it brings an average accuracy improvements of 12.2% on mGSM even with\nthe 70B model. To understand the mechanism of its success, we analyze\nrepresentation space, generated response and data scales, and reveal how\nquestion translation training strengthens language alignment within LLMs and\nshapes their working patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the significant gap between large language model's English and\nnon-English performance presents a great challenge. While some previous studies\nattempt to mitigate this gap with translated training data, the recently\nproposed question alignment framework leverages the model's English expertise\nto improve multilingual performance with minimum usage of expensive,\nerror-prone translation. In this paper, we explore how broadly this method can\nbe applied by examining its effects in reasoning with and without\nchain-of-thought, as well as with program-of-thought. We also explore applying\nthis framework to extremely large language models in an efficient manner, such\nas through proxy-tuning. Experiment results on multilingual reasoning\nbenchmarks mGSM, mSVAMP, xCSQA and xNLI demonstrate that we can extend question\nalignment framework to boost multilingual performance across diverse reasoning\nscenarios, model families, and sizes. For instance, when applied to the LLaMA2\nmodels, it brings an average accuracy improvements of 12.2% on mGSM even with\nthe 70B model. To understand the mechanism of its success, we analyze\nrepresentation space, generated response and data scales, and reveal how\nquestion translation training strengthens language alignment within LLMs and\nshapes their working patterns."
                },
                "authors": [
                    {
                        "name": "Wenhao Zhu"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Fei Yuan"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01345v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01345v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11650v2",
                "updated": "2024-11-06T06:25:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    25,
                    49,
                    2,
                    311,
                    0
                ],
                "published": "2024-08-21T14:24:04Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    24,
                    4,
                    2,
                    234,
                    0
                ],
                "title": "CIPHER: Cybersecurity Intelligent Penetration-testing Helper for Ethical\n  Researcher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CIPHER: Cybersecurity Intelligent Penetration-testing Helper for Ethical\n  Researcher"
                },
                "summary": "Penetration testing, a critical component of cybersecurity, typically\nrequires extensive time and effort to find vulnerabilities. Beginners in this\nfield often benefit from collaborative approaches with the community or\nexperts. To address this, we develop CIPHER (Cybersecurity Intelligent\nPenetration-testing Helper for Ethical Researchers), a large language model\nspecifically trained to assist in penetration testing tasks. We trained CIPHER\nusing over 300 high-quality write-ups of vulnerable machines, hacking\ntechniques, and documentation of open-source penetration testing tools.\nAdditionally, we introduced the Findings, Action, Reasoning, and Results (FARR)\nFlow augmentation, a novel method to augment penetration testing write-ups to\nestablish a fully automated pentesting simulation benchmark tailored for large\nlanguage models. This approach fills a significant gap in traditional\ncybersecurity Q\\&A benchmarks and provides a realistic and rigorous standard\nfor evaluating AI's technical knowledge, reasoning capabilities, and practical\nutility in dynamic penetration testing scenarios. In our assessments, CIPHER\nachieved the best overall performance in providing accurate suggestion\nresponses compared to other open-source penetration testing models of similar\nsize and even larger state-of-the-art models like Llama 3 70B and Qwen1.5 72B\nChat, particularly on insane difficulty machine setups. This demonstrates that\nthe current capabilities of general LLMs are insufficient for effectively\nguiding users through the penetration testing process. We also discuss the\npotential for improvement through scaling and the development of better\nbenchmarks using FARR Flow augmentation results. Our benchmark will be released\npublicly at https://github.com/ibndias/CIPHER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration testing, a critical component of cybersecurity, typically\nrequires extensive time and effort to find vulnerabilities. Beginners in this\nfield often benefit from collaborative approaches with the community or\nexperts. To address this, we develop CIPHER (Cybersecurity Intelligent\nPenetration-testing Helper for Ethical Researchers), a large language model\nspecifically trained to assist in penetration testing tasks. We trained CIPHER\nusing over 300 high-quality write-ups of vulnerable machines, hacking\ntechniques, and documentation of open-source penetration testing tools.\nAdditionally, we introduced the Findings, Action, Reasoning, and Results (FARR)\nFlow augmentation, a novel method to augment penetration testing write-ups to\nestablish a fully automated pentesting simulation benchmark tailored for large\nlanguage models. This approach fills a significant gap in traditional\ncybersecurity Q\\&A benchmarks and provides a realistic and rigorous standard\nfor evaluating AI's technical knowledge, reasoning capabilities, and practical\nutility in dynamic penetration testing scenarios. In our assessments, CIPHER\nachieved the best overall performance in providing accurate suggestion\nresponses compared to other open-source penetration testing models of similar\nsize and even larger state-of-the-art models like Llama 3 70B and Qwen1.5 72B\nChat, particularly on insane difficulty machine setups. This demonstrates that\nthe current capabilities of general LLMs are insufficient for effectively\nguiding users through the penetration testing process. We also discuss the\npotential for improvement through scaling and the development of better\nbenchmarks using FARR Flow augmentation results. Our benchmark will be released\npublicly at https://github.com/ibndias/CIPHER."
                },
                "authors": [
                    {
                        "name": "Derry Pratama"
                    },
                    {
                        "name": "Naufal Suryanto"
                    },
                    {
                        "name": "Andro Aprila Adiputra"
                    },
                    {
                        "name": "Thi-Thu-Huong Le"
                    },
                    {
                        "name": "Ahmada Yusril Kadiptya"
                    },
                    {
                        "name": "Muhammad Iqbal"
                    },
                    {
                        "name": "Howon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Howon Kim"
                },
                "author": "Howon Kim",
                "arxiv_doi": "10.3390/s24216878",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/s24216878",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.11650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "28 pages, github available",
                "arxiv_journal_ref": "Sensors 2024, 24(21), 6878;",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03687v1",
                "updated": "2024-11-06T06:13:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    13,
                    57,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T06:13:57Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    13,
                    57,
                    2,
                    311,
                    0
                ],
                "title": "Beyond Model Adaptation at Test Time: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Model Adaptation at Test Time: A Survey"
                },
                "summary": "Machine learning algorithms have achieved remarkable success across various\ndisciplines, use cases and applications, under the prevailing assumption that\ntraining and test samples are drawn from the same distribution. Consequently,\nthese algorithms struggle and become brittle even when samples in the test\ndistribution start to deviate from the ones observed during training. Domain\nadaptation and domain generalization have been studied extensively as\napproaches to address distribution shifts across test and train domains, but\neach has its limitations. Test-time adaptation, a recently emerging learning\nparadigm, combines the benefits of domain adaptation and domain generalization\nby training models only on source data and adapting them to target data during\ntest-time inference. In this survey, we provide a comprehensive and systematic\nreview on test-time adaptation, covering more than 400 recent papers. We\nstructure our review by categorizing existing methods into five distinct\ncategories based on what component of the method is adjusted for test-time\nadaptation: the model, the inference, the normalization, the sample, or the\nprompt, providing detailed analysis of each. We further discuss the various\npreparation and adaptation settings for methods within these categories,\noffering deeper insights into the effective deployment for the evaluation of\ndistribution shifts and their real-world application in understanding images,\nvideo and 3D, as well as modalities beyond vision. We close the survey with an\noutlook on emerging research opportunities for test-time adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning algorithms have achieved remarkable success across various\ndisciplines, use cases and applications, under the prevailing assumption that\ntraining and test samples are drawn from the same distribution. Consequently,\nthese algorithms struggle and become brittle even when samples in the test\ndistribution start to deviate from the ones observed during training. Domain\nadaptation and domain generalization have been studied extensively as\napproaches to address distribution shifts across test and train domains, but\neach has its limitations. Test-time adaptation, a recently emerging learning\nparadigm, combines the benefits of domain adaptation and domain generalization\nby training models only on source data and adapting them to target data during\ntest-time inference. In this survey, we provide a comprehensive and systematic\nreview on test-time adaptation, covering more than 400 recent papers. We\nstructure our review by categorizing existing methods into five distinct\ncategories based on what component of the method is adjusted for test-time\nadaptation: the model, the inference, the normalization, the sample, or the\nprompt, providing detailed analysis of each. We further discuss the various\npreparation and adaptation settings for methods within these categories,\noffering deeper insights into the effective deployment for the evaluation of\ndistribution shifts and their real-world application in understanding images,\nvideo and 3D, as well as modalities beyond vision. We close the survey with an\noutlook on emerging research opportunities for test-time adaptation."
                },
                "authors": [
                    {
                        "name": "Zehao Xiao"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    }
                ],
                "author_detail": {
                    "name": "Cees G. M. Snoek"
                },
                "author": "Cees G. M. Snoek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17812v2",
                "updated": "2024-11-06T05:33:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    33,
                    16,
                    2,
                    311,
                    0
                ],
                "published": "2024-02-27T14:51:11Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    14,
                    51,
                    11,
                    1,
                    58,
                    0
                ],
                "title": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping\n  Backward Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping\n  Backward Propagation"
                },
                "summary": "Large language models (LLMs) have achieved significant success across various\ndomains. However, training these LLMs typically involves substantial memory and\ncomputational costs during both forward and backward propagation. While\nparameter-efficient fine-tuning (PEFT) considerably reduces the training memory\nassociated with parameters, it does not address the significant computational\ncosts and activation memory. In this paper, we propose Dropping Backward\nPropagation (DropBP), a novel approach designed to reduce computational costs\nand activation memory while maintaining accuracy. DropBP randomly drops layers\nduring backward propagation, which is essentially equivalent to training\nshallow submodules generated by undropped layers and residual connections.\nAdditionally, DropBP calculates the sensitivity of each layer to assign an\nappropriate drop rate, thereby stabilizing the training process. DropBP is not\nonly applicable to full fine-tuning but can also be orthogonally integrated\nwith all types of PEFT by dropping layers during backward propagation.\nSpecifically, DropBP can reduce training time by 44% with comparable accuracy\nto the baseline, accelerate convergence to the same perplexity by 1.5x, and\nenable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.\nFurthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100\nGPU and 117% on an Intel Gaudi2 HPU. The code is available at\nhttps://github.com/WooSunghyeon/dropbp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant success across various\ndomains. However, training these LLMs typically involves substantial memory and\ncomputational costs during both forward and backward propagation. While\nparameter-efficient fine-tuning (PEFT) considerably reduces the training memory\nassociated with parameters, it does not address the significant computational\ncosts and activation memory. In this paper, we propose Dropping Backward\nPropagation (DropBP), a novel approach designed to reduce computational costs\nand activation memory while maintaining accuracy. DropBP randomly drops layers\nduring backward propagation, which is essentially equivalent to training\nshallow submodules generated by undropped layers and residual connections.\nAdditionally, DropBP calculates the sensitivity of each layer to assign an\nappropriate drop rate, thereby stabilizing the training process. DropBP is not\nonly applicable to full fine-tuning but can also be orthogonally integrated\nwith all types of PEFT by dropping layers during backward propagation.\nSpecifically, DropBP can reduce training time by 44% with comparable accuracy\nto the baseline, accelerate convergence to the same perplexity by 1.5x, and\nenable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.\nFurthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100\nGPU and 117% on an Intel Gaudi2 HPU. The code is available at\nhttps://github.com/WooSunghyeon/dropbp."
                },
                "authors": [
                    {
                        "name": "Sunghyeon Woo"
                    },
                    {
                        "name": "Baeseong Park"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Minjung Jo"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsuk Jeon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongsoo Lee"
                },
                "author": "Dongsoo Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03675v1",
                "updated": "2024-11-06T05:24:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    24,
                    9,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T05:24:09Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    24,
                    9,
                    2,
                    311,
                    0
                ],
                "title": "QUILL: Quotation Generation Enhancement of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUILL: Quotation Generation Enhancement of Large Language Models"
                },
                "summary": "While Large language models (LLMs) have become excellent writing assistants,\nthey still struggle with quotation generation. This is because they either\nhallucinate when providing factual quotations or fail to provide quotes that\nexceed human expectations. To bridge the gap, we systematically study how to\nevaluate and improve LLMs' performance in quotation generation tasks. We first\nestablish a holistic and automatic evaluation system for quotation generation\ntask, which consists of five criteria each with corresponding automatic metric.\nTo improve the LLMs' quotation generation abilities, we construct a bilingual\nknowledge base that is broad in scope and rich in dimensions, containing up to\n32,022 quotes. Moreover, guided by our critiria, we further design a\nquotation-specific metric to rerank the retrieved quotations from the knowledge\nbase. Extensive experiments show that our metrics strongly correlate with human\npreferences. Existing LLMs struggle to generate desired quotes, but our\nquotation knowledge base and reranking metric help narrow this gap. Our dataset\nand code are publicly available at https://github.com/GraceXiaoo/QUILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large language models (LLMs) have become excellent writing assistants,\nthey still struggle with quotation generation. This is because they either\nhallucinate when providing factual quotations or fail to provide quotes that\nexceed human expectations. To bridge the gap, we systematically study how to\nevaluate and improve LLMs' performance in quotation generation tasks. We first\nestablish a holistic and automatic evaluation system for quotation generation\ntask, which consists of five criteria each with corresponding automatic metric.\nTo improve the LLMs' quotation generation abilities, we construct a bilingual\nknowledge base that is broad in scope and rich in dimensions, containing up to\n32,022 quotes. Moreover, guided by our critiria, we further design a\nquotation-specific metric to rerank the retrieved quotations from the knowledge\nbase. Extensive experiments show that our metrics strongly correlate with human\npreferences. Existing LLMs struggle to generate desired quotes, but our\nquotation knowledge base and reranking metric help narrow this gap. Our dataset\nand code are publicly available at https://github.com/GraceXiaoo/QUILL."
                },
                "authors": [
                    {
                        "name": "Jin Xiao"
                    },
                    {
                        "name": "Bowei Zhang"
                    },
                    {
                        "name": "Qianyu He"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Feng Wei"
                    },
                    {
                        "name": "Jinglei Chen"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13147v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13147v5",
                "updated": "2024-11-06T05:18:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    18,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-17T02:04:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    4,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "Utilizing Large Language Models in an iterative paradigm with Domain\n  feedback for Zero-shot Molecule optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models in an iterative paradigm with Domain\n  feedback for Zero-shot Molecule optimization"
                },
                "summary": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing shows limited performance. In this work, we facilitate utilizing LLMs\nin an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to explicitly guide the LLM to refine the modified\nmolecule. We conduct experiments across both single- and multi-property\nobjectives with 2 thresholds, where $\\text{Re}^3$DF shows significant\nimprovements. Particularly, for 20 single-property objectives, $\\text{Re}^3$DF\nenhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing shows limited performance. In this work, we facilitate utilizing LLMs\nin an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to explicitly guide the LLM to refine the modified\nmolecule. We conduct experiments across both single- and multi-property\nobjectives with 2 thresholds, where $\\text{Re}^3$DF shows significant\nimprovements. Particularly, for 20 single-property objectives, $\\text{Re}^3$DF\nenhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%."
                },
                "authors": [
                    {
                        "name": "Khiem Le"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13147v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13147v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09774v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09774v3",
                "updated": "2024-11-06T05:16:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    16,
                    59,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-15T15:46:03Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    15,
                    46,
                    3,
                    6,
                    259,
                    0
                ],
                "title": "Generalizing Alignment Paradigm of Text-to-Image Generation with\n  Preferences through $f$-divergence Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing Alignment Paradigm of Text-to-Image Generation with\n  Preferences through $f$-divergence Minimization"
                },
                "summary": "Direct Preference Optimization (DPO) has recently expanded its successful\napplication from aligning large language models (LLMs) to aligning\ntext-to-image models with human preferences, which has generated considerable\ninterest within the community. However, we have observed that these approaches\nrely solely on minimizing the reverse Kullback-Leibler divergence during\nalignment process between the fine-tuned model and the reference model,\nneglecting the incorporation of other divergence constraints. In this study, we\nfocus on extending reverse Kullback-Leibler divergence in the alignment\nparadigm of text-to-image models to $f$-divergence, which aims to garner better\nalignment performance as well as good generation diversity. We provide the\ngeneralized formula of the alignment paradigm under the $f$-divergence\ncondition and thoroughly analyze the impact of different divergence constraints\non alignment process from the perspective of gradient fields. We conduct\ncomprehensive evaluation on image-text alignment performance, human value\nalignment performance and generation diversity performance under different\ndivergence constraints, and the results indicate that alignment based on\nJensen-Shannon divergence achieves the best trade-off among them. The option of\ndivergence employed for aligning text-to-image models significantly impacts the\ntrade-off between alignment performance (especially human value alignment) and\ngeneration diversity, which highlights the necessity of selecting an\nappropriate divergence for practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has recently expanded its successful\napplication from aligning large language models (LLMs) to aligning\ntext-to-image models with human preferences, which has generated considerable\ninterest within the community. However, we have observed that these approaches\nrely solely on minimizing the reverse Kullback-Leibler divergence during\nalignment process between the fine-tuned model and the reference model,\nneglecting the incorporation of other divergence constraints. In this study, we\nfocus on extending reverse Kullback-Leibler divergence in the alignment\nparadigm of text-to-image models to $f$-divergence, which aims to garner better\nalignment performance as well as good generation diversity. We provide the\ngeneralized formula of the alignment paradigm under the $f$-divergence\ncondition and thoroughly analyze the impact of different divergence constraints\non alignment process from the perspective of gradient fields. We conduct\ncomprehensive evaluation on image-text alignment performance, human value\nalignment performance and generation diversity performance under different\ndivergence constraints, and the results indicate that alignment based on\nJensen-Shannon divergence achieves the best trade-off among them. The option of\ndivergence employed for aligning text-to-image models significantly impacts the\ntrade-off between alignment performance (especially human value alignment) and\ngeneration diversity, which highlights the necessity of selecting an\nappropriate divergence for practical applications."
                },
                "authors": [
                    {
                        "name": "Haoyuan Sun"
                    },
                    {
                        "name": "Bo Xia"
                    },
                    {
                        "name": "Yongzhe Chang"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09774v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09774v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13941v2",
                "updated": "2024-11-06T05:05:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    5,
                    12,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-20T23:04:21Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    23,
                    4,
                    21,
                    4,
                    264,
                    0
                ],
                "title": "TalkMosaic: Interactive PhotoMosaic with Multi-modal LLM Q&A\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TalkMosaic: Interactive PhotoMosaic with Multi-modal LLM Q&A\n  Interactions"
                },
                "summary": "We use images of cars of a wide range of varieties to compose an image of an\nanimal such as a bird or a lion for the theme of environmental protection to\nmaximize the information about cars in a single composed image and to raise the\nawareness about environmental challenges. We present a novel way of image\ninteraction with an artistically-composed photomosaic image, in which a simple\noperation of \"click and display\" is used to demonstrate the interactive switch\nbetween a tile image in a photomosaic image and the corresponding original car\nimage, which will be automatically saved on the Desktop. We build a multimodal\ncustom GPT named TalkMosaic by incorporating car images information and the\nrelated knowledge to ChatGPT. By uploading the original car image to\nTalkMosaic, we can ask questions about the given car image and get the\ncorresponding answers efficiently and effectively such as where to buy the tire\nin the car image that satisfies high environmental standards. We give an\nin-depth analysis on how to speed up the inference of multimodal LLM using\nsparse attention and quantization techniques with presented probabilistic\nFlashAttention (PrFlashAttention) and Staircase Adaptive Quantization (SAQ)\nmethods. The implemented prototype demonstrates the feasibility and\neffectiveness of the presented approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use images of cars of a wide range of varieties to compose an image of an\nanimal such as a bird or a lion for the theme of environmental protection to\nmaximize the information about cars in a single composed image and to raise the\nawareness about environmental challenges. We present a novel way of image\ninteraction with an artistically-composed photomosaic image, in which a simple\noperation of \"click and display\" is used to demonstrate the interactive switch\nbetween a tile image in a photomosaic image and the corresponding original car\nimage, which will be automatically saved on the Desktop. We build a multimodal\ncustom GPT named TalkMosaic by incorporating car images information and the\nrelated knowledge to ChatGPT. By uploading the original car image to\nTalkMosaic, we can ask questions about the given car image and get the\ncorresponding answers efficiently and effectively such as where to buy the tire\nin the car image that satisfies high environmental standards. We give an\nin-depth analysis on how to speed up the inference of multimodal LLM using\nsparse attention and quantization techniques with presented probabilistic\nFlashAttention (PrFlashAttention) and Staircase Adaptive Quantization (SAQ)\nmethods. The implemented prototype demonstrates the feasibility and\neffectiveness of the presented approach."
                },
                "authors": [
                    {
                        "name": "Kevin Li"
                    },
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03665v1",
                "updated": "2024-11-06T04:52:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    4,
                    52,
                    38,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T04:52:38Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    4,
                    52,
                    38,
                    2,
                    311,
                    0
                ],
                "title": "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework"
                },
                "summary": "Proper moral beliefs are fundamental for language models, yet assessing these\nbeliefs poses a significant challenge. This study introduces a novel\nthree-module framework to evaluate the moral beliefs of four prominent large\nlanguage models. Initially, we constructed a dataset containing 472 moral\nchoice scenarios in Chinese, derived from moral words. The decision-making\nprocess of the models in these scenarios reveals their moral principle\npreferences. By ranking these moral choices, we discern the varying moral\nbeliefs held by different language models. Additionally, through moral debates,\nwe investigate the firmness of these models to their moral choices. Our\nfindings indicate that English language models, namely ChatGPT and Gemini,\nclosely mirror moral decisions of the sample of Chinese university students,\ndemonstrating strong adherence to their choices and a preference for\nindividualistic moral beliefs. In contrast, Chinese models such as Ernie and\nChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their\nmoral choices and debates. This study also uncovers gender bias embedded within\nthe moral beliefs of all examined language models. Our methodology offers an\ninnovative means to assess moral beliefs in both artificial and human\nintelligence, facilitating a comparison of moral values across different\ncultures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proper moral beliefs are fundamental for language models, yet assessing these\nbeliefs poses a significant challenge. This study introduces a novel\nthree-module framework to evaluate the moral beliefs of four prominent large\nlanguage models. Initially, we constructed a dataset containing 472 moral\nchoice scenarios in Chinese, derived from moral words. The decision-making\nprocess of the models in these scenarios reveals their moral principle\npreferences. By ranking these moral choices, we discern the varying moral\nbeliefs held by different language models. Additionally, through moral debates,\nwe investigate the firmness of these models to their moral choices. Our\nfindings indicate that English language models, namely ChatGPT and Gemini,\nclosely mirror moral decisions of the sample of Chinese university students,\ndemonstrating strong adherence to their choices and a preference for\nindividualistic moral beliefs. In contrast, Chinese models such as Ernie and\nChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their\nmoral choices and debates. This study also uncovers gender bias embedded within\nthe moral beliefs of all examined language models. Our methodology offers an\ninnovative means to assess moral beliefs in both artificial and human\nintelligence, facilitating a comparison of moral values across different\ncultures."
                },
                "authors": [
                    {
                        "name": "Xuelin Liu"
                    },
                    {
                        "name": "Yanfei Zhu"
                    },
                    {
                        "name": "Shucheng Zhu"
                    },
                    {
                        "name": "Pengyuan Liu"
                    },
                    {
                        "name": "Ying Liu"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03663v1",
                "updated": "2024-11-06T04:44:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    4,
                    44,
                    51,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T04:44:51Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    4,
                    44,
                    51,
                    2,
                    311,
                    0
                ],
                "title": "Can Graph Neural Networks Expose Training Data Properties? An Efficient\n  Risk Assessment Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Graph Neural Networks Expose Training Data Properties? An Efficient\n  Risk Assessment Approach"
                },
                "summary": "Graph neural networks (GNNs) have attracted considerable attention due to\ntheir diverse applications. However, the scarcity and quality limitations of\ngraph data present challenges to their training process in practical settings.\nTo facilitate the development of effective GNNs, companies and researchers\noften seek external collaboration. Yet, directly sharing data raises privacy\nconcerns, motivating data owners to train GNNs on their private graphs and\nshare the trained models. Unfortunately, these models may still inadvertently\ndisclose sensitive properties of their training graphs (e.g., average default\nrate in a transaction network), leading to severe consequences for data owners.\nIn this work, we study graph property inference attack to identify the risk of\nsensitive property information leakage from shared models. Existing approaches\ntypically train numerous shadow models for developing such attack, which is\ncomputationally intensive and impractical. To address this issue, we propose an\nefficient graph property inference attack by leveraging model approximation\ntechniques. Our method only requires training a small set of models on graphs,\nwhile generating a sufficient number of approximated shadow models for attacks.\nTo enhance diversity while reducing errors in the approximated models, we apply\nedit distance to quantify the diversity within a group of approximated models\nand introduce a theoretically guaranteed criterion to evaluate each model's\nerror. Subsequently, we propose a novel selection mechanism to ensure that the\nretained approximated models achieve high diversity and low error. Extensive\nexperiments across six real-world scenarios demonstrate our method's\nsubstantial improvement, with average increases of 2.7% in attack accuracy and\n4.1% in ROC-AUC, while being 6.5$\\times$ faster compared to the best baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have attracted considerable attention due to\ntheir diverse applications. However, the scarcity and quality limitations of\ngraph data present challenges to their training process in practical settings.\nTo facilitate the development of effective GNNs, companies and researchers\noften seek external collaboration. Yet, directly sharing data raises privacy\nconcerns, motivating data owners to train GNNs on their private graphs and\nshare the trained models. Unfortunately, these models may still inadvertently\ndisclose sensitive properties of their training graphs (e.g., average default\nrate in a transaction network), leading to severe consequences for data owners.\nIn this work, we study graph property inference attack to identify the risk of\nsensitive property information leakage from shared models. Existing approaches\ntypically train numerous shadow models for developing such attack, which is\ncomputationally intensive and impractical. To address this issue, we propose an\nefficient graph property inference attack by leveraging model approximation\ntechniques. Our method only requires training a small set of models on graphs,\nwhile generating a sufficient number of approximated shadow models for attacks.\nTo enhance diversity while reducing errors in the approximated models, we apply\nedit distance to quantify the diversity within a group of approximated models\nand introduce a theoretically guaranteed criterion to evaluate each model's\nerror. Subsequently, we propose a novel selection mechanism to ensure that the\nretained approximated models achieve high diversity and low error. Extensive\nexperiments across six real-world scenarios demonstrate our method's\nsubstantial improvement, with average increases of 2.7% in attack accuracy and\n4.1% in ROC-AUC, while being 6.5$\\times$ faster compared to the best baseline."
                },
                "authors": [
                    {
                        "name": "Hanyang Yuan"
                    },
                    {
                        "name": "Jiarong Xu"
                    },
                    {
                        "name": "Renhong Huang"
                    },
                    {
                        "name": "Mingli Song"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Yang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yang"
                },
                "author": "Yang Yang",
                "arxiv_comment": "In NeurIPS'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09324v3",
                "updated": "2024-11-06T04:43:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    4,
                    43,
                    57,
                    2,
                    311,
                    0
                ],
                "published": "2024-06-13T17:01:40Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    17,
                    1,
                    40,
                    3,
                    165,
                    0
                ],
                "title": "Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated significant\ncapabilities in executing complex tasks in a zero-shot manner, they are\nsusceptible to jailbreak attacks and can be manipulated to produce harmful\noutputs. Recently, a growing body of research has categorized jailbreak attacks\ninto token-level and prompt-level attacks. However, previous work primarily\noverlooks the diverse key factors of jailbreak attacks, with most studies\nconcentrating on LLM vulnerabilities and lacking exploration of\ndefense-enhanced LLMs. To address these issues, we introduced\n$\\textbf{JailTrickBench}$ to evaluate the impact of various attack settings on\nLLM performance and provide a baseline for jailbreak attacks, encouraging the\nadoption of a standardized evaluation framework. Specifically, we evaluate the\neight key factors of implementing jailbreak attacks on LLMs from both\ntarget-level and attack-level perspectives. We further conduct seven\nrepresentative jailbreak attacks on six defense methods across two widely used\ndatasets, encompassing approximately 354 experiments with about 55,000 GPU\nhours on A800-80G. Our experimental results highlight the need for standardized\nbenchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is\navailable at https://github.com/usail-hkust/JailTrickBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated significant\ncapabilities in executing complex tasks in a zero-shot manner, they are\nsusceptible to jailbreak attacks and can be manipulated to produce harmful\noutputs. Recently, a growing body of research has categorized jailbreak attacks\ninto token-level and prompt-level attacks. However, previous work primarily\noverlooks the diverse key factors of jailbreak attacks, with most studies\nconcentrating on LLM vulnerabilities and lacking exploration of\ndefense-enhanced LLMs. To address these issues, we introduced\n$\\textbf{JailTrickBench}$ to evaluate the impact of various attack settings on\nLLM performance and provide a baseline for jailbreak attacks, encouraging the\nadoption of a standardized evaluation framework. Specifically, we evaluate the\neight key factors of implementing jailbreak attacks on LLMs from both\ntarget-level and attack-level perspectives. We further conduct seven\nrepresentative jailbreak attacks on six defense methods across two widely used\ndatasets, encompassing approximately 354 experiments with about 55,000 GPU\nhours on A800-80G. Our experimental results highlight the need for standardized\nbenchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is\navailable at https://github.com/usail-hkust/JailTrickBench."
                },
                "authors": [
                    {
                        "name": "Zhao Xu"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03659v1",
                "updated": "2024-11-06T04:41:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    4,
                    41,
                    13,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T04:41:13Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    4,
                    41,
                    13,
                    2,
                    311,
                    0
                ],
                "title": "Towards Scalable Automated Grading: Leveraging Large Language Models for\n  Conceptual Question Evaluation in Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable Automated Grading: Leveraging Large Language Models for\n  Conceptual Question Evaluation in Engineering"
                },
                "summary": "This study explores the feasibility of using large language models (LLMs),\nspecifically GPT-4o (ChatGPT), for automated grading of conceptual questions in\nan undergraduate Mechanical Engineering course. We compared the grading\nperformance of GPT-4o with that of human teaching assistants (TAs) on ten quiz\nproblems from the MEEN 361 course at Texas A&M University, each answered by\napproximately 225 students. Both the LLM and TAs followed the same\ninstructor-provided rubric to ensure grading consistency. We evaluated\nperformance using Spearman's rank correlation coefficient and Root Mean Square\nError (RMSE) to assess the alignment between rankings and the accuracy of\nscores assigned by GPT-4o and TAs under zero- and few-shot grading settings. In\nthe zero-shot setting, GPT-4o demonstrated a strong correlation with TA\ngrading, with Spearman's rank correlation coefficient exceeding 0.6 in seven\nout of ten datasets and reaching a high of 0.9387. Our analysis reveals that\nGPT-4o performs well when grading criteria are straightforward but struggles\nwith nuanced answers, particularly those involving synonyms not present in the\nrubric. The model also tends to grade more stringently in ambiguous cases\ncompared to human TAs. Overall, ChatGPT shows promise as a tool for grading\nconceptual questions, offering scalability and consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the feasibility of using large language models (LLMs),\nspecifically GPT-4o (ChatGPT), for automated grading of conceptual questions in\nan undergraduate Mechanical Engineering course. We compared the grading\nperformance of GPT-4o with that of human teaching assistants (TAs) on ten quiz\nproblems from the MEEN 361 course at Texas A&M University, each answered by\napproximately 225 students. Both the LLM and TAs followed the same\ninstructor-provided rubric to ensure grading consistency. We evaluated\nperformance using Spearman's rank correlation coefficient and Root Mean Square\nError (RMSE) to assess the alignment between rankings and the accuracy of\nscores assigned by GPT-4o and TAs under zero- and few-shot grading settings. In\nthe zero-shot setting, GPT-4o demonstrated a strong correlation with TA\ngrading, with Spearman's rank correlation coefficient exceeding 0.6 in seven\nout of ten datasets and reaching a high of 0.9387. Our analysis reveals that\nGPT-4o performs well when grading criteria are straightforward but struggles\nwith nuanced answers, particularly those involving synonyms not present in the\nrubric. The model also tends to grade more stringently in ambiguous cases\ncompared to human TAs. Overall, ChatGPT shows promise as a tool for grading\nconceptual questions, offering scalability and consistency."
                },
                "authors": [
                    {
                        "name": "Rujun Gao"
                    },
                    {
                        "name": "Xiaosu Guo"
                    },
                    {
                        "name": "Xiaodi Li"
                    },
                    {
                        "name": "Arun Balajiee Lekshmi Narayanan"
                    },
                    {
                        "name": "Naveen Thomas"
                    },
                    {
                        "name": "Arun R. Srinivasa"
                    }
                ],
                "author_detail": {
                    "name": "Arun R. Srinivasa"
                },
                "author": "Arun R. Srinivasa",
                "arxiv_comment": "21 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15735v3",
                "updated": "2024-11-06T03:53:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    3,
                    53,
                    13,
                    2,
                    311,
                    0
                ],
                "published": "2024-06-22T04:56:16Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    4,
                    56,
                    16,
                    5,
                    174,
                    0
                ],
                "title": "Identifying and Solving Conditional Image Leakage in Image-to-Video\n  Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying and Solving Conditional Image Leakage in Image-to-Video\n  Diffusion Model"
                },
                "summary": "Diffusion models have obtained substantial progress in image-to-video\ngeneration. However, in this paper, we find that these models tend to generate\nvideos with less motion than expected. We attribute this to the issue called\nconditional image leakage, where the image-to-video diffusion models (I2V-DMs)\ntend to over-rely on the conditional image at large time steps. We further\naddress this challenge from both inference and training aspects. First, we\npropose to start the generation process from an earlier time step to avoid the\nunreliable large-time steps of I2V-DMs, as well as an initial noise\ndistribution with optimal analytic expressions (Analytic-Init) by minimizing\nthe KL divergence between it and the actual marginal distribution to bridge the\ntraining-inference gap. Second, we design a time-dependent noise distribution\n(TimeNoise) for the conditional image during training, applying higher noise\nlevels at larger time steps to disrupt it and reduce the model's dependency on\nit. We validate these general strategies on various I2V-DMs on our collected\nopen-domain image benchmark and the UCF101 dataset. Extensive results show that\nour methods outperform baselines by producing higher motion scores with lower\nerrors while maintaining image alignment and temporal consistency, thereby\nyielding superior overall performance and enabling more accurate motion\ncontrol. The project page: \\url{https://cond-image-leak.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have obtained substantial progress in image-to-video\ngeneration. However, in this paper, we find that these models tend to generate\nvideos with less motion than expected. We attribute this to the issue called\nconditional image leakage, where the image-to-video diffusion models (I2V-DMs)\ntend to over-rely on the conditional image at large time steps. We further\naddress this challenge from both inference and training aspects. First, we\npropose to start the generation process from an earlier time step to avoid the\nunreliable large-time steps of I2V-DMs, as well as an initial noise\ndistribution with optimal analytic expressions (Analytic-Init) by minimizing\nthe KL divergence between it and the actual marginal distribution to bridge the\ntraining-inference gap. Second, we design a time-dependent noise distribution\n(TimeNoise) for the conditional image during training, applying higher noise\nlevels at larger time steps to disrupt it and reduce the model's dependency on\nit. We validate these general strategies on various I2V-DMs on our collected\nopen-domain image benchmark and the UCF101 dataset. Extensive results show that\nour methods outperform baselines by producing higher motion scores with lower\nerrors while maintaining image alignment and temporal consistency, thereby\nyielding superior overall performance and enabling more accurate motion\ncontrol. The project page: \\url{https://cond-image-leak.github.io/}."
                },
                "authors": [
                    {
                        "name": "Min Zhao"
                    },
                    {
                        "name": "Hongzhou Zhu"
                    },
                    {
                        "name": "Chendong Xiang"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Chongxuan Li"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_comment": "NeurIPS 2024. Project page: https://cond-image-leak.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2202.06420v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2202.06420v4",
                "updated": "2024-11-06T03:50:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    3,
                    50,
                    40,
                    2,
                    311,
                    0
                ],
                "published": "2022-02-13T21:57:18Z",
                "published_parsed": [
                    2022,
                    2,
                    13,
                    21,
                    57,
                    18,
                    6,
                    44,
                    0
                ],
                "title": "Statistical Inference for Cell Type Deconvolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Cell Type Deconvolution"
                },
                "summary": "Integrating data from different platforms, such as bulk and single-cell RNA\nsequencing, is crucial for improving the accuracy and interpretability of\ncomplex biological analyses like cell type deconvolution. However, this task is\ncomplicated by measurement and biological heterogeneity between target and\nreference datasets. For the problem of cell type deconvolution, existing\nmethods often neglect the correlation and uncertainty in cell type proportion\nestimates, possibly leading to an additional concern of false positives in\ndownstream comparisons across multiple individuals. We introduce MEAD, a\ncomprehensive statistical framework that not only estimates cell type\nproportions but also provides asymptotically valid statistical inference on the\nestimates. One of our key contributions is the identifiability result, which\nrigorously establishes the conditions under which cell type proportions are\nidentifiable despite arbitrary heterogeneity of measurement biases between\nplatforms. MEAD also supports the comparison of cell type proportions across\nindividuals after deconvolution, accounting for gene-gene correlations and\nbiological variability. Through simulations and real-data analysis, MEAD\ndemonstrates superior reliability for inferring cell type compositions in\ncomplex biological systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating data from different platforms, such as bulk and single-cell RNA\nsequencing, is crucial for improving the accuracy and interpretability of\ncomplex biological analyses like cell type deconvolution. However, this task is\ncomplicated by measurement and biological heterogeneity between target and\nreference datasets. For the problem of cell type deconvolution, existing\nmethods often neglect the correlation and uncertainty in cell type proportion\nestimates, possibly leading to an additional concern of false positives in\ndownstream comparisons across multiple individuals. We introduce MEAD, a\ncomprehensive statistical framework that not only estimates cell type\nproportions but also provides asymptotically valid statistical inference on the\nestimates. One of our key contributions is the identifiability result, which\nrigorously establishes the conditions under which cell type proportions are\nidentifiable despite arbitrary heterogeneity of measurement biases between\nplatforms. MEAD also supports the comparison of cell type proportions across\nindividuals after deconvolution, accounting for gene-gene correlations and\nbiological variability. Through simulations and real-data analysis, MEAD\ndemonstrates superior reliability for inferring cell type compositions in\ncomplex biological systems."
                },
                "authors": [
                    {
                        "name": "Dongyue Xie"
                    },
                    {
                        "name": "Jingshu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingshu Wang"
                },
                "author": "Jingshu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2202.06420v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2202.06420v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03231v2",
                "updated": "2024-11-06T02:56:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    56,
                    57,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-05T16:23:19Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    23,
                    19,
                    1,
                    310,
                    0
                ],
                "title": "Formal Logic-guided Robust Federated Learning against Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Logic-guided Robust Federated Learning against Poisoning Attacks"
                },
                "summary": "Federated Learning (FL) offers a promising solution to the privacy concerns\nassociated with centralized Machine Learning (ML) by enabling decentralized,\ncollaborative learning. However, FL is vulnerable to various security threats,\nincluding poisoning attacks, where adversarial clients manipulate the training\ndata or model updates to degrade overall model performance. Recognizing this\nthreat, researchers have focused on developing defense mechanisms to counteract\npoisoning attacks in FL systems. However, existing robust FL methods\npredominantly focus on computer vision tasks, leaving a gap in addressing the\nunique challenges of FL with time series data. In this paper, we present\nFLORAL, a defense mechanism designed to mitigate poisoning attacks in federated\nlearning for time-series tasks, even in scenarios with heterogeneous client\ndata and a large number of adversarial participants. Unlike traditional\nmodel-centric defenses, FLORAL leverages logical reasoning to evaluate client\ntrustworthiness by aligning their predictions with global time-series patterns,\nrather than relying solely on the similarity of client updates. Our approach\nextracts logical reasoning properties from clients, then hierarchically infers\nglobal properties, and uses these to verify client updates. Through formal\nlogic verification, we assess the robustness of each client contribution,\nidentifying deviations indicative of adversarial behavior. Experimental results\non two datasets demonstrate the superior performance of our approach compared\nto existing baseline methods, highlighting its potential to enhance the\nrobustness of FL to time series applications. Notably, FLORAL reduced the\nprediction error by 93.27% in the best-case scenario compared to the\nsecond-best baseline. Our code is available at\nhttps://anonymous.4open.science/r/FLORAL-Robust-FTS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) offers a promising solution to the privacy concerns\nassociated with centralized Machine Learning (ML) by enabling decentralized,\ncollaborative learning. However, FL is vulnerable to various security threats,\nincluding poisoning attacks, where adversarial clients manipulate the training\ndata or model updates to degrade overall model performance. Recognizing this\nthreat, researchers have focused on developing defense mechanisms to counteract\npoisoning attacks in FL systems. However, existing robust FL methods\npredominantly focus on computer vision tasks, leaving a gap in addressing the\nunique challenges of FL with time series data. In this paper, we present\nFLORAL, a defense mechanism designed to mitigate poisoning attacks in federated\nlearning for time-series tasks, even in scenarios with heterogeneous client\ndata and a large number of adversarial participants. Unlike traditional\nmodel-centric defenses, FLORAL leverages logical reasoning to evaluate client\ntrustworthiness by aligning their predictions with global time-series patterns,\nrather than relying solely on the similarity of client updates. Our approach\nextracts logical reasoning properties from clients, then hierarchically infers\nglobal properties, and uses these to verify client updates. Through formal\nlogic verification, we assess the robustness of each client contribution,\nidentifying deviations indicative of adversarial behavior. Experimental results\non two datasets demonstrate the superior performance of our approach compared\nto existing baseline methods, highlighting its potential to enhance the\nrobustness of FL to time series applications. Notably, FLORAL reduced the\nprediction error by 93.27% in the best-case scenario compared to the\nsecond-best baseline. Our code is available at\nhttps://anonymous.4open.science/r/FLORAL-Robust-FTS."
                },
                "authors": [
                    {
                        "name": "Dung Thuy Nguyen"
                    },
                    {
                        "name": "Ziyan An"
                    },
                    {
                        "name": "Taylor T. Johnson"
                    },
                    {
                        "name": "Meiyi Ma"
                    },
                    {
                        "name": "Kevin Leach"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Leach"
                },
                "author": "Kevin Leach",
                "arxiv_comment": "12 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00870v4",
                "updated": "2024-11-06T02:55:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    55,
                    1,
                    2,
                    311,
                    0
                ],
                "published": "2023-12-30T01:26:42Z",
                "published_parsed": [
                    2023,
                    12,
                    30,
                    1,
                    26,
                    42,
                    5,
                    364,
                    0
                ],
                "title": "ConfusionPrompt: Practical Private Inference for Online Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusionPrompt: Practical Private Inference for Online Large Language\n  Models"
                },
                "summary": "State-of-the-art large language models (LLMs) are typically deployed as\nonline services, requiring users to transmit detailed prompts to cloud servers.\nThis raises significant privacy concerns. In response, we introduce\nConfusionPrompt, a novel framework for private LLM inference that protects user\nprivacy by: (i) decomposing the original prompt into smaller sub-prompts, and\n(ii) generating pseudo-prompts alongside the genuine sub-prompts, which are\nthen sent to the LLM. The server responses are later recomposed by the user to\nreconstruct the final output. This approach offers key advantages over previous\nLLM privacy protection methods: (i) it integrates seamlessly with existing\nblack-box LLMs, and (ii) it delivers a significantly improved privacy-utility\ntrade-off compared to existing text perturbation methods. We also develop a\n$(\\lambda, \\mu, \\rho)$-privacy model to formulate the requirements for a\nprivacy-preserving group of prompts and provide a complexity analysis to\njustify the role of prompt decomposition. Our empirical evaluation shows that\nConfusionPrompt achieves significantly higher utility than local inference\nmethods using open-source models and perturbation-based techniques, while also\nreducing memory consumption compared to open-source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art large language models (LLMs) are typically deployed as\nonline services, requiring users to transmit detailed prompts to cloud servers.\nThis raises significant privacy concerns. In response, we introduce\nConfusionPrompt, a novel framework for private LLM inference that protects user\nprivacy by: (i) decomposing the original prompt into smaller sub-prompts, and\n(ii) generating pseudo-prompts alongside the genuine sub-prompts, which are\nthen sent to the LLM. The server responses are later recomposed by the user to\nreconstruct the final output. This approach offers key advantages over previous\nLLM privacy protection methods: (i) it integrates seamlessly with existing\nblack-box LLMs, and (ii) it delivers a significantly improved privacy-utility\ntrade-off compared to existing text perturbation methods. We also develop a\n$(\\lambda, \\mu, \\rho)$-privacy model to formulate the requirements for a\nprivacy-preserving group of prompts and provide a complexity analysis to\njustify the role of prompt decomposition. Our empirical evaluation shows that\nConfusionPrompt achieves significantly higher utility than local inference\nmethods using open-source models and perturbation-based techniques, while also\nreducing memory consumption compared to open-source LLMs."
                },
                "authors": [
                    {
                        "name": "Peihua Mai"
                    },
                    {
                        "name": "Youjia Yang"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Yan Pang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Pang"
                },
                "author": "Yan Pang",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02199v2",
                "updated": "2024-11-06T02:51:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    51,
                    16,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T15:54:32Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    54,
                    32,
                    0,
                    309,
                    0
                ],
                "title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning"
                },
                "summary": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings."
                },
                "authors": [
                    {
                        "name": "Dake Bu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Andi Han"
                    },
                    {
                        "name": "Atsushi Nitanda"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Hau-San Wong"
                    }
                ],
                "author_detail": {
                    "name": "Hau-San Wong"
                },
                "author": "Hau-San Wong",
                "arxiv_comment": "Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03625v1",
                "updated": "2024-11-06T02:46:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    46,
                    2,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T02:46:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    46,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "Identification and Inference in General Bunching Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification and Inference in General Bunching Designs"
                },
                "summary": "This paper develops a formal econometric framework and tools for the\nidentification and inference of a structural parameter in general bunching\ndesigns. We present both point and partial identification results, which\ngeneralize previous approaches in the literature. The key assumption for point\nidentification is the analyticity of the counterfactual density, which defines\na broader class of distributions than many well-known parametric families. In\nthe partial identification approach, the analyticity condition is relaxed and\nvarious shape restrictions can be incorporated, including those found in the\nliterature. Both of our identification results account for observable\nheterogeneity in the model, which has previously been permitted only in limited\nways. We provide a suite of counterfactual estimation and inference methods,\ntermed the generalized polynomial strategy. Our method restores the merits of\nthe original polynomial strategy proposed by Chetty et al. (2011) while\naddressing several weaknesses in the widespread practice. The efficacy of the\nproposed method is demonstrated compared to a version of the polynomial\nestimator in a series of Monte Carlo studies within the augmented isoelastic\nmodel. We revisit the data used in Saez (2010) and find substantially different\nresults relative to those from the polynomial strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a formal econometric framework and tools for the\nidentification and inference of a structural parameter in general bunching\ndesigns. We present both point and partial identification results, which\ngeneralize previous approaches in the literature. The key assumption for point\nidentification is the analyticity of the counterfactual density, which defines\na broader class of distributions than many well-known parametric families. In\nthe partial identification approach, the analyticity condition is relaxed and\nvarious shape restrictions can be incorporated, including those found in the\nliterature. Both of our identification results account for observable\nheterogeneity in the model, which has previously been permitted only in limited\nways. We provide a suite of counterfactual estimation and inference methods,\ntermed the generalized polynomial strategy. Our method restores the merits of\nthe original polynomial strategy proposed by Chetty et al. (2011) while\naddressing several weaknesses in the widespread practice. The efficacy of the\nproposed method is demonstrated compared to a version of the polynomial\nestimator in a series of Monte Carlo studies within the augmented isoelastic\nmodel. We revisit the data used in Saez (2010) and find substantially different\nresults relative to those from the polynomial strategy."
                },
                "authors": [
                    {
                        "name": "Myunghyun Song"
                    }
                ],
                "author_detail": {
                    "name": "Myunghyun Song"
                },
                "author": "Myunghyun Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02937v2",
                "updated": "2024-11-06T02:36:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    36,
                    2,
                    2,
                    311,
                    0
                ],
                "published": "2024-08-06T03:44:06Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    3,
                    44,
                    6,
                    1,
                    219,
                    0
                ],
                "title": "A Real-Time Adaptive Multi-Stream GPU System for Online Approximate\n  Nearest Neighborhood Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Real-Time Adaptive Multi-Stream GPU System for Online Approximate\n  Nearest Neighborhood Search"
                },
                "summary": "In recent years, Approximate Nearest Neighbor Search (ANNS) has played a\npivotal role in modern search and recommendation systems, especially in\nemerging LLM applications like Retrieval-Augmented Generation. There is a\ngrowing exploration into harnessing the parallel computing capabilities of GPUs\nto meet the substantial demands of ANNS. However, existing systems primarily\nfocus on offline scenarios, overlooking the distinct requirements of online\napplications that necessitate real-time insertion of new vectors. This\nlimitation renders such systems inefficient for real-world scenarios. Moreover,\nprevious architectures struggled to effectively support real-time insertion due\nto their reliance on serial execution streams. In this paper, we introduce a\nnovel Real-Time Adaptive Multi-Stream GPU ANNS System (RTAMS-GANNS). Our\narchitecture achieves its objectives through three key advancements: 1) We\ninitially examined the real-time insertion mechanisms in existing GPU ANNS\nsystems and discovered their reliance on repetitive copying and memory\nallocation, which significantly hinders real-time effectiveness on GPUs. As a\nsolution, we introduce a dynamic vector insertion algorithm based on memory\nblocks, which includes in-place rearrangement. 2) To enable real-time vector\ninsertion in parallel, we introduce a multi-stream parallel execution mode,\nwhich differs from existing systems that operate serially within a single\nstream. Our system utilizes a dynamic resource pool, allowing multiple streams\nto execute concurrently without additional execution blocking. 3) Through\nextensive experiments and comparisons, our approach effectively handles varying\nQPS levels across different datasets, reducing latency by up to 40%-80%. The\nproposed system has also been deployed in real-world industrial search and\nrecommendation systems, serving hundreds of millions of users daily, and has\nachieved good results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Approximate Nearest Neighbor Search (ANNS) has played a\npivotal role in modern search and recommendation systems, especially in\nemerging LLM applications like Retrieval-Augmented Generation. There is a\ngrowing exploration into harnessing the parallel computing capabilities of GPUs\nto meet the substantial demands of ANNS. However, existing systems primarily\nfocus on offline scenarios, overlooking the distinct requirements of online\napplications that necessitate real-time insertion of new vectors. This\nlimitation renders such systems inefficient for real-world scenarios. Moreover,\nprevious architectures struggled to effectively support real-time insertion due\nto their reliance on serial execution streams. In this paper, we introduce a\nnovel Real-Time Adaptive Multi-Stream GPU ANNS System (RTAMS-GANNS). Our\narchitecture achieves its objectives through three key advancements: 1) We\ninitially examined the real-time insertion mechanisms in existing GPU ANNS\nsystems and discovered their reliance on repetitive copying and memory\nallocation, which significantly hinders real-time effectiveness on GPUs. As a\nsolution, we introduce a dynamic vector insertion algorithm based on memory\nblocks, which includes in-place rearrangement. 2) To enable real-time vector\ninsertion in parallel, we introduce a multi-stream parallel execution mode,\nwhich differs from existing systems that operate serially within a single\nstream. Our system utilizes a dynamic resource pool, allowing multiple streams\nto execute concurrently without additional execution blocking. 3) Through\nextensive experiments and comparisons, our approach effectively handles varying\nQPS levels across different datasets, reducing latency by up to 40%-80%. The\nproposed system has also been deployed in real-world industrial search and\nrecommendation systems, serving hundreds of millions of users daily, and has\nachieved good results."
                },
                "authors": [
                    {
                        "name": "Yiping Sun"
                    },
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Jiaolong Du"
                    }
                ],
                "author_detail": {
                    "name": "Jiaolong Du"
                },
                "author": "Jiaolong Du",
                "arxiv_comment": "Accepted by CIKM'24, V2 fixes some typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06209v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06209v6",
                "updated": "2024-11-06T02:35:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    35,
                    30,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-08T17:11:24Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    11,
                    24,
                    1,
                    282,
                    0
                ],
                "title": "LeanAgent: Lifelong Learning for Formal Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanAgent: Lifelong Learning for Formal Theorem Proving"
                },
                "summary": "Large Language Models (LLMs) have been successful in mathematical reasoning\ntasks such as formal theorem proving when integrated with interactive proof\nassistants like Lean. Existing approaches involve training or fine-tuning an\nLLM on a specific dataset to perform well on particular domains, such as\nundergraduate-level mathematics. These methods struggle with generalizability\nto advanced mathematics. A fundamental limitation is that these approaches\noperate on static domains, failing to capture how mathematicians often work\nacross multiple domains and projects simultaneously or cyclically. We present\nLeanAgent, a novel lifelong learning framework for theorem proving that\ncontinuously generalizes to and improves on ever-expanding mathematical\nknowledge without forgetting previously learned knowledge. LeanAgent introduces\nseveral key innovations, including a curriculum learning strategy that\noptimizes the learning trajectory in terms of mathematical difficulty, a\ndynamic database for efficient management of evolving mathematical knowledge,\nand progressive training to balance stability and plasticity. LeanAgent\nsuccessfully proves 162 theorems previously unproved by humans across 23\ndiverse Lean repositories, many from advanced mathematics. It performs\nsignificantly better than the static LLM baseline, proving challenging theorems\nin domains like abstract algebra and algebraic topology while showcasing a\nclear progression of learning from basic concepts to advanced topics. In\naddition, we analyze LeanAgent's superior performance on key lifelong learning\nmetrics. LeanAgent achieves exceptional scores in stability and backward\ntransfer, where learning new tasks improves performance on previously learned\ntasks. This emphasizes LeanAgent's continuous generalizability and improvement,\nexplaining its superior theorem-proving performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been successful in mathematical reasoning\ntasks such as formal theorem proving when integrated with interactive proof\nassistants like Lean. Existing approaches involve training or fine-tuning an\nLLM on a specific dataset to perform well on particular domains, such as\nundergraduate-level mathematics. These methods struggle with generalizability\nto advanced mathematics. A fundamental limitation is that these approaches\noperate on static domains, failing to capture how mathematicians often work\nacross multiple domains and projects simultaneously or cyclically. We present\nLeanAgent, a novel lifelong learning framework for theorem proving that\ncontinuously generalizes to and improves on ever-expanding mathematical\nknowledge without forgetting previously learned knowledge. LeanAgent introduces\nseveral key innovations, including a curriculum learning strategy that\noptimizes the learning trajectory in terms of mathematical difficulty, a\ndynamic database for efficient management of evolving mathematical knowledge,\nand progressive training to balance stability and plasticity. LeanAgent\nsuccessfully proves 162 theorems previously unproved by humans across 23\ndiverse Lean repositories, many from advanced mathematics. It performs\nsignificantly better than the static LLM baseline, proving challenging theorems\nin domains like abstract algebra and algebraic topology while showcasing a\nclear progression of learning from basic concepts to advanced topics. In\naddition, we analyze LeanAgent's superior performance on key lifelong learning\nmetrics. LeanAgent achieves exceptional scores in stability and backward\ntransfer, where learning new tasks improves performance on previously learned\ntasks. This emphasizes LeanAgent's continuous generalizability and improvement,\nexplaining its superior theorem-proving performance."
                },
                "authors": [
                    {
                        "name": "Adarsh Kumarappan"
                    },
                    {
                        "name": "Mo Tiwari"
                    },
                    {
                        "name": "Peiyang Song"
                    },
                    {
                        "name": "Robert Joseph George"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06209v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06209v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09380v3",
                "updated": "2024-11-06T02:26:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    26,
                    7,
                    2,
                    311,
                    0
                ],
                "published": "2024-08-18T06:41:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    6,
                    41,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "ELASTIC: Efficient Linear Attention for Sequential Interest Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELASTIC: Efficient Linear Attention for Sequential Interest Compression"
                },
                "summary": "State-of-the-art sequential recommendation models heavily rely on\ntransformer's attention mechanism. However, the quadratic computational and\nmemory complexities of self attention have limited its scalability for modeling\nusers' long range behaviour sequences. To address this problem, we propose\nELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,\nrequiring only linear time complexity and decoupling model capacity from\ncomputational cost. Specifically, ELASTIC introduces a fixed length interest\nexperts with linear dispatcher attention mechanism which compresses the\nlong-term behaviour sequences to a significantly more compact representation\nwhich reduces up to 90% GPU memory usage with x2.7 inference speed up. The\nproposed linear dispatcher attention mechanism significantly reduces the\nquadratic complexity and makes the model feasible for adequately modeling\nextremely long sequences. Moreover, in order to retain the capacity for\nmodeling various user interests, ELASTIC initializes a vast learnable interest\nmemory bank and sparsely retrieves compressed user's interests from the memory\nwith a negligible computational overhead. The proposed interest memory\nretrieval technique significantly expands the cardinality of available interest\nspace while keeping the same computational cost, thereby striking a trade-off\nbetween recommendation accuracy and efficiency. To validate the effectiveness\nof our proposed ELASTIC, we conduct extensive experiments on various public\ndatasets and compare it with several strong sequential recommenders.\nExperimental results demonstrate that ELASTIC consistently outperforms\nbaselines by a significant margin and also highlight the computational\nefficiency of ELASTIC when modeling long sequences. We will make our\nimplementation code publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art sequential recommendation models heavily rely on\ntransformer's attention mechanism. However, the quadratic computational and\nmemory complexities of self attention have limited its scalability for modeling\nusers' long range behaviour sequences. To address this problem, we propose\nELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,\nrequiring only linear time complexity and decoupling model capacity from\ncomputational cost. Specifically, ELASTIC introduces a fixed length interest\nexperts with linear dispatcher attention mechanism which compresses the\nlong-term behaviour sequences to a significantly more compact representation\nwhich reduces up to 90% GPU memory usage with x2.7 inference speed up. The\nproposed linear dispatcher attention mechanism significantly reduces the\nquadratic complexity and makes the model feasible for adequately modeling\nextremely long sequences. Moreover, in order to retain the capacity for\nmodeling various user interests, ELASTIC initializes a vast learnable interest\nmemory bank and sparsely retrieves compressed user's interests from the memory\nwith a negligible computational overhead. The proposed interest memory\nretrieval technique significantly expands the cardinality of available interest\nspace while keeping the same computational cost, thereby striking a trade-off\nbetween recommendation accuracy and efficiency. To validate the effectiveness\nof our proposed ELASTIC, we conduct extensive experiments on various public\ndatasets and compare it with several strong sequential recommenders.\nExperimental results demonstrate that ELASTIC consistently outperforms\nbaselines by a significant margin and also highlight the computational\nefficiency of ELASTIC when modeling long sequences. We will make our\nimplementation code publicly available."
                },
                "authors": [
                    {
                        "name": "Jiaxin Deng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Song Lu"
                    },
                    {
                        "name": "Yinfeng Li"
                    },
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Yuanjun Liu"
                    },
                    {
                        "name": "Peixing Xu"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "We hereby withdraw this paper from arXiv due to incomplete\n  experiments. Upon further review, we have determined that additional\n  experimental work is necessary to fully validate our findings and conclusions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19272v2",
                "updated": "2024-11-06T01:58:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    58,
                    20,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-28T07:13:33Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    7,
                    13,
                    33,
                    5,
                    272,
                    0
                ],
                "title": "Perception Compressor:A training-free prompt compression method in long\n  context scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception Compressor:A training-free prompt compression method in long\n  context scenarios"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities in various\nscenarios. However, they suffer from much redundant information and are\nsensitive to the position of key information (relevant to the input question)\nin long context scenarios, leading to inferior performance. To address these\nchallenges, we present Perception Compressor, a training-free prompt\ncompression method. It includes a perception retriever that leverages guiding\nquestions and instruction to retrieve the most relevant demonstrations, a\ndual-slope ratio allocator to dynamically allocate compression ratios and\nopen-book ratios, and a semi-guided iterative compression that retains key\ninformation at the token level while removing tokens that distract the LLM. We\nconduct extensive experiments on long context benchmarks, i.e.,\nNaturalQuestions, LongBench, and MuSiQue. Experiment results show that\nPerception Compressor outperforms existing methods by a large margin, achieving\nstate-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities in various\nscenarios. However, they suffer from much redundant information and are\nsensitive to the position of key information (relevant to the input question)\nin long context scenarios, leading to inferior performance. To address these\nchallenges, we present Perception Compressor, a training-free prompt\ncompression method. It includes a perception retriever that leverages guiding\nquestions and instruction to retrieve the most relevant demonstrations, a\ndual-slope ratio allocator to dynamically allocate compression ratios and\nopen-book ratios, and a semi-guided iterative compression that retains key\ninformation at the token level while removing tokens that distract the LLM. We\nconduct extensive experiments on long context benchmarks, i.e.,\nNaturalQuestions, LongBench, and MuSiQue. Experiment results show that\nPerception Compressor outperforms existing methods by a large margin, achieving\nstate-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Tingwei Lu"
                    },
                    {
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "name": "Yiming Zhao"
                    },
                    {
                        "name": "Lin Hai"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Tao Zheng"
                },
                "author": "Hai-Tao Zheng",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.04118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04118v1",
                "updated": "2024-11-06T18:51:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T18:51:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "Medical Adaptation of Large Language and Vision-Language Models: Are We\n  Making Progress?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Adaptation of Large Language and Vision-Language Models: Are We\n  Making Progress?"
                },
                "summary": "Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare seven\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting regime for medical question-answering (QA) tasks. For instance,\nacross the tasks and model pairs we consider in the 3-shot setting, medical\nLLMs only outperform their base models in 12.1% of cases, reach a (statistical)\ntie in 49.8% of cases, and are significantly worse than their base models in\nthe remaining 38.2% of cases. Our conclusions are based on (i) comparing each\nmedical model head-to-head, directly against the corresponding base model; (ii)\noptimizing the prompts for each model separately; and (iii) accounting for\nstatistical uncertainty in comparisons. While these basic practices are not\nconsistently adopted in the literature, our ablations show that they\nsubstantially impact conclusions. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare seven\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting regime for medical question-answering (QA) tasks. For instance,\nacross the tasks and model pairs we consider in the 3-shot setting, medical\nLLMs only outperform their base models in 12.1% of cases, reach a (statistical)\ntie in 49.8% of cases, and are significantly worse than their base models in\nthe remaining 38.2% of cases. Our conclusions are based on (i) comparing each\nmedical model head-to-head, directly against the corresponding base model; (ii)\noptimizing the prompts for each model separately; and (iii) accounting for\nstatistical uncertainty in comparisons. While these basic practices are not\nconsistently adopted in the literature, our ablations show that they\nsubstantially impact conclusions. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies."
                },
                "authors": [
                    {
                        "name": "Daniel P. Jeong"
                    },
                    {
                        "name": "Saurabh Garg"
                    },
                    {
                        "name": "Zachary C. Lipton"
                    },
                    {
                        "name": "Michael Oberst"
                    }
                ],
                "author_detail": {
                    "name": "Michael Oberst"
                },
                "author": "Michael Oberst",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference as Long Paper (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04105v1",
                "updated": "2024-11-06T18:35:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    35,
                    32,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T18:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    35,
                    32,
                    2,
                    311,
                    0
                ],
                "title": "How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Transformers Solve Propositional Logic Problems: A Mechanistic\n  Analysis"
                },
                "summary": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve, but we can train a small\ntransformer to achieve perfect accuracy. Building on our set-up, we then pursue\nan understanding of precisely how a three-layer transformer, trained from\nscratch, solves this problem. We are able to identify certain \"planning\" and\n\"reasoning\" circuits in the network that necessitate cooperation between the\nattention blocks to implement the desired logic. To expand our findings, we\nthen study a larger model, Mistral 7B. Using activation patching, we\ncharacterize internal components that are critical in solving our logic\nproblem. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown amazing performance on tasks that\nrequire planning and reasoning. Motivated by this, we investigate the internal\nmechanisms that underpin a network's ability to perform complex logical\nreasoning. We first construct a synthetic propositional logic problem that\nserves as a concrete test-bed for network training and evaluation. Crucially,\nthis problem demands nontrivial planning to solve, but we can train a small\ntransformer to achieve perfect accuracy. Building on our set-up, we then pursue\nan understanding of precisely how a three-layer transformer, trained from\nscratch, solves this problem. We are able to identify certain \"planning\" and\n\"reasoning\" circuits in the network that necessitate cooperation between the\nattention blocks to implement the desired logic. To expand our findings, we\nthen study a larger model, Mistral 7B. Using activation patching, we\ncharacterize internal components that are critical in solving our logic\nproblem. Overall, our work systemically uncovers novel aspects of small and\nlarge transformers, and continues the study of how they plan and reason."
                },
                "authors": [
                    {
                        "name": "Guan Zhe Hong"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Enming Luo"
                    },
                    {
                        "name": "Cyrus Rashtchian"
                    },
                    {
                        "name": "Rina Panigrahy"
                    }
                ],
                "author_detail": {
                    "name": "Rina Panigrahy"
                },
                "author": "Rina Panigrahy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20393v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20393v2",
                "updated": "2024-11-06T18:26:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    26,
                    21,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-27T09:41:30Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    9,
                    41,
                    30,
                    6,
                    301,
                    0
                ],
                "title": "Fundamental performance bounds on time-series generation using reservoir\n  computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental performance bounds on time-series generation using reservoir\n  computing"
                },
                "summary": "Reservoir computing (RC) harnesses the intrinsic dynamics of a chaotic\nsystem, called the reservoir, to perform various time-varying functions. An\nimportant use-case of RC is the generation of target temporal sequences via a\ntrainable output-to-reservoir feedback loop. Despite the promise of RC in\nvarious domains, we lack a theory of performance bounds on RC systems. Here, we\nformulate an existence condition for a feedback loop that produces the target\nsequence. We next demonstrate that, given a sufficiently chaotic neural network\nreservoir, two separate factors are needed for successful training: global\nnetwork stability of the target orbit, and the ability of the training\nalgorithm to drive the system close enough to the target, which we term\n`reach'. By computing the training phase diagram over a range of target output\namplitudes and periods, we verify that reach-limited failures depend on the\ntraining algorithm while stability-limited failures are invariant across\ndifferent algorithms. We leverage dynamical mean field theory (DMFT) to provide\nan analytical amplitude-period bound on achievable outputs by RC networks and\npropose a way of enhancing algorithm reach via forgetting. The resulting\nmechanistic understanding of RC performance can guide the future design and\ndeployment of reservoir networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reservoir computing (RC) harnesses the intrinsic dynamics of a chaotic\nsystem, called the reservoir, to perform various time-varying functions. An\nimportant use-case of RC is the generation of target temporal sequences via a\ntrainable output-to-reservoir feedback loop. Despite the promise of RC in\nvarious domains, we lack a theory of performance bounds on RC systems. Here, we\nformulate an existence condition for a feedback loop that produces the target\nsequence. We next demonstrate that, given a sufficiently chaotic neural network\nreservoir, two separate factors are needed for successful training: global\nnetwork stability of the target orbit, and the ability of the training\nalgorithm to drive the system close enough to the target, which we term\n`reach'. By computing the training phase diagram over a range of target output\namplitudes and periods, we verify that reach-limited failures depend on the\ntraining algorithm while stability-limited failures are invariant across\ndifferent algorithms. We leverage dynamical mean field theory (DMFT) to provide\nan analytical amplitude-period bound on achievable outputs by RC networks and\npropose a way of enhancing algorithm reach via forgetting. The resulting\nmechanistic understanding of RC performance can guide the future design and\ndeployment of reservoir networks."
                },
                "authors": [
                    {
                        "name": "Daoyuan Qian"
                    },
                    {
                        "name": "Ila Fiete"
                    }
                ],
                "author_detail": {
                    "name": "Ila Fiete"
                },
                "author": "Ila Fiete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20393v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20393v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nlin.CD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11832v2",
                "updated": "2024-11-06T18:07:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    7,
                    3,
                    2,
                    311,
                    0
                ],
                "published": "2024-08-06T15:49:58Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    15,
                    49,
                    58,
                    1,
                    219,
                    0
                ],
                "title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs"
                },
                "summary": "The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for automatic tools to check the factual accuracy\nof their outputs, as LLMs often hallucinate. This is difficult as it requires\nassessing the factuality of free-form open-domain responses. While there has\nbeen a lot of research on this topic, different papers use different evaluation\nbenchmarks and measures, which makes them hard to compare and hampers future\nprogress. To mitigate these issues, we developed OpenFactCheck, a unified\nframework, with three modules: (i) RESPONSEEVAL, which allows users to easily\ncustomize an automatic fact-checking system and to assess the factuality of all\nclaims in an input document using that system, (ii) LLMEVAL, which assesses the\noverall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate\nautomatic fact-checking systems. OpenFactCheck is open-sourced\n(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python\nlibrary (https://pypi.org/project/openfactcheck/) and also as a web service\n(http://app.openfactcheck.com). A video describing the system is available at\nhttps://youtu.be/-i9VKL0HleI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased use of large language models (LLMs) across a variety of\nreal-world applications calls for automatic tools to check the factual accuracy\nof their outputs, as LLMs often hallucinate. This is difficult as it requires\nassessing the factuality of free-form open-domain responses. While there has\nbeen a lot of research on this topic, different papers use different evaluation\nbenchmarks and measures, which makes them hard to compare and hampers future\nprogress. To mitigate these issues, we developed OpenFactCheck, a unified\nframework, with three modules: (i) RESPONSEEVAL, which allows users to easily\ncustomize an automatic fact-checking system and to assess the factuality of all\nclaims in an input document using that system, (ii) LLMEVAL, which assesses the\noverall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate\nautomatic fact-checking systems. OpenFactCheck is open-sourced\n(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python\nlibrary (https://pypi.org/project/openfactcheck/) and also as a web service\n(http://app.openfactcheck.com). A video describing the system is available at\nhttps://youtu.be/-i9VKL0HleI."
                },
                "authors": [
                    {
                        "name": "Hasan Iqbal"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Georgi Georgiev"
                    },
                    {
                        "name": "Jiahui Geng"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "arxiv_comment": "11 pages, 4 Figures, 3 Tables, Accepted at EMNLP 2024 System\n  Demonstration. arXiv admin note: substantial text overlap with\n  arXiv:2405.05583",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01632v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01632v4",
                "updated": "2024-11-06T18:04:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    4,
                    35,
                    2,
                    311,
                    0
                ],
                "published": "2024-03-03T22:38:35Z",
                "published_parsed": [
                    2024,
                    3,
                    3,
                    22,
                    38,
                    35,
                    6,
                    63,
                    0
                ],
                "title": "SynCode: LLM Generation with Grammar Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynCode: LLM Generation with Grammar Augmentation"
                },
                "summary": "LLMs are widely used in complex AI applications. These applications\nunderscore the need for LLM outputs to adhere to a specific format, for their\nintegration with other components in the systems. Typically the format rules\ne.g., for data serialization formats such as JSON, YAML, or Code in Programming\nLanguage are expressed as context-free grammar (CFG). Due to the hallucinations\nand unreliability of LLMs, instructing LLMs to adhere to specified syntax\nbecomes an increasingly important challenge.\n  We present SynCode, a novel framework for efficient and general syntactical\ndecoding with LLMs, to address this challenge. SynCode ensures soundness and\ncompleteness with respect to the CFG of a formal language, effectively\nretaining valid tokens while filtering out invalid ones. SynCode uses an\noffline-constructed, efficient lookup table, the DFA mask store, derived from\nthe DFA of the language's grammar for efficient generation. SynCode seamlessly\nintegrates with any language defined by CFG, as evidenced by experiments\nfocusing on generating JSON, Python, and Go outputs. Our experiments evaluating\nthe effectiveness of SynCode for JSON generation demonstrate that SynCode\neliminates all syntax errors and significantly outperforms state-of-the-art\nbaselines. Furthermore, our results underscore how SynCode significantly\nreduces 96.07% of syntax errors in generated Python and Go code, showcasing its\nsubstantial impact on enhancing syntactical precision in LLM generation. Our\ncode is available at https://github.com/uiuc-focal-lab/syncode",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are widely used in complex AI applications. These applications\nunderscore the need for LLM outputs to adhere to a specific format, for their\nintegration with other components in the systems. Typically the format rules\ne.g., for data serialization formats such as JSON, YAML, or Code in Programming\nLanguage are expressed as context-free grammar (CFG). Due to the hallucinations\nand unreliability of LLMs, instructing LLMs to adhere to specified syntax\nbecomes an increasingly important challenge.\n  We present SynCode, a novel framework for efficient and general syntactical\ndecoding with LLMs, to address this challenge. SynCode ensures soundness and\ncompleteness with respect to the CFG of a formal language, effectively\nretaining valid tokens while filtering out invalid ones. SynCode uses an\noffline-constructed, efficient lookup table, the DFA mask store, derived from\nthe DFA of the language's grammar for efficient generation. SynCode seamlessly\nintegrates with any language defined by CFG, as evidenced by experiments\nfocusing on generating JSON, Python, and Go outputs. Our experiments evaluating\nthe effectiveness of SynCode for JSON generation demonstrate that SynCode\neliminates all syntax errors and significantly outperforms state-of-the-art\nbaselines. Furthermore, our results underscore how SynCode significantly\nreduces 96.07% of syntax errors in generated Python and Go code, showcasing its\nsubstantial impact on enhancing syntactical precision in LLM generation. Our\ncode is available at https://github.com/uiuc-focal-lab/syncode"
                },
                "authors": [
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Hangoo Kang"
                    },
                    {
                        "name": "Sasa Misailovic"
                    },
                    {
                        "name": "Gagandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Gagandeep Singh"
                },
                "author": "Gagandeep Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01632v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01632v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.19673v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.19673v5",
                "updated": "2024-11-06T17:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    17,
                    33,
                    59,
                    2,
                    311,
                    0
                ],
                "published": "2023-10-30T15:51:39Z",
                "published_parsed": [
                    2023,
                    10,
                    30,
                    15,
                    51,
                    39,
                    0,
                    303,
                    0
                ],
                "title": "Non-Pyrotechnic Radial Deployment Mechanism for Payloads in Sounding\n  Rockets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Pyrotechnic Radial Deployment Mechanism for Payloads in Sounding\n  Rockets"
                },
                "summary": "A novel, non-pyrotechnic payload deployment mechanism tailored for sounding\nrockets is introduced in this research paper. The mechanism addresses the\nchallenge of efficiently and compactly deploying payloads radially during a\nsingle launch, featuring a cylindrical carrier structure actuated by a\nrack-pinion mechanism. Powered by a servo motor, the carrier structure\ntranslates to enable radial ejection of payloads. The paper presents the\nmechanism's design and conducts a comprehensive performance analysis, including\nstructural stability, system dynamics and power requirements. A simulation\nmodel is developed to assess payload deployment behavior under various\nconditions, demonstrating the mechanism's viability and efficiency for\ndeploying multiple payloads within a single sounding rocket launch. The\nmechanism's adaptability to accommodate diverse payload types, sizes and\nweights enhances its versatility, while its radial deployment capability allows\npayloads to be released at different altitudes, offering greater flexibility\nfor scientific experiments. The paper concludes that this innovative payload\nradial deployment mechanism represents a significant advancement in sounding\nrocket technology and holds promise for a wide array of applications in both\nscientific and commercial missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel, non-pyrotechnic payload deployment mechanism tailored for sounding\nrockets is introduced in this research paper. The mechanism addresses the\nchallenge of efficiently and compactly deploying payloads radially during a\nsingle launch, featuring a cylindrical carrier structure actuated by a\nrack-pinion mechanism. Powered by a servo motor, the carrier structure\ntranslates to enable radial ejection of payloads. The paper presents the\nmechanism's design and conducts a comprehensive performance analysis, including\nstructural stability, system dynamics and power requirements. A simulation\nmodel is developed to assess payload deployment behavior under various\nconditions, demonstrating the mechanism's viability and efficiency for\ndeploying multiple payloads within a single sounding rocket launch. The\nmechanism's adaptability to accommodate diverse payload types, sizes and\nweights enhances its versatility, while its radial deployment capability allows\npayloads to be released at different altitudes, offering greater flexibility\nfor scientific experiments. The paper concludes that this innovative payload\nradial deployment mechanism represents a significant advancement in sounding\nrocket technology and holds promise for a wide array of applications in both\nscientific and commercial missions."
                },
                "authors": [
                    {
                        "name": "Thakur Pranav G. Singh"
                    },
                    {
                        "name": "Utkarsh Anand"
                    },
                    {
                        "name": "Tanvi Agrawal"
                    },
                    {
                        "name": "Srinivas G"
                    }
                ],
                "author_detail": {
                    "name": "Srinivas G"
                },
                "author": "Srinivas G",
                "arxiv_comment": "The paper has been converted into a new format with same content",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.19673v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.19673v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03523v3",
                "updated": "2024-11-06T17:19:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    17,
                    19,
                    39,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-04T15:44:23Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    44,
                    23,
                    4,
                    278,
                    0
                ],
                "title": "A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models"
                },
                "summary": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs. Code\navailable at https://github.com/yascho/probabilistic-unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs. Code\navailable at https://github.com/yascho/probabilistic-unlearning."
                },
                "authors": [
                    {
                        "name": "Yan Scholten"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01483v3",
                "updated": "2024-11-06T17:04:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    17,
                    4,
                    36,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T08:49:55Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    8,
                    49,
                    55,
                    6,
                    308,
                    0
                ],
                "title": "Teaching Models to Improve on Tape",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Models to Improve on Tape"
                },
                "summary": "Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such \"corrective feedback\". Here we claim that this\nskill of LLMs can be significantly enhanced via training. We introduce an RL\nframework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle when prompted to generate content\nunder specific constraints. However, in such cases it is often easy to check\nwhether these constraints are satisfied or violated. Recent works have shown\nthat LLMs can benefit from such \"corrective feedback\". Here we claim that this\nskill of LLMs can be significantly enhanced via training. We introduce an RL\nframework for teaching models to use such rewards, by simulating interaction\nsessions, and rewarding the model according to its ability to satisfy the\nconstraints. We refer to our method as CORGI (Controlled Generation with RL for\nGuided Interaction), and evaluate it on a variety of controlled generation\ntasks using unlabeled training data. We find that CORGI consistently\noutperforms the baseline reinforcement learning method that does not\nincorporate conversational feedback. Furthermore, CORGI's interactive framework\nenables meta-learning, allowing the LLM to generalize better to guided\ninteraction in new tasks. Our results clearly show that conversational\noptimization, when combined with reinforcement learning, significantly improves\nthe effectiveness of LLMs in controlled generation contexts."
                },
                "authors": [
                    {
                        "name": "Liat Bezalel"
                    },
                    {
                        "name": "Eyal Orgad"
                    },
                    {
                        "name": "Amir Globerson"
                    }
                ],
                "author_detail": {
                    "name": "Amir Globerson"
                },
                "author": "Amir Globerson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22997v2",
                "updated": "2024-11-06T16:57:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    57,
                    3,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-30T13:22:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    13,
                    22,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "A Comparison of Prompt Engineering Techniques for Task Planning and\n  Execution in Service Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparison of Prompt Engineering Techniques for Task Planning and\n  Execution in Service Robotics"
                },
                "summary": "Recent advances in LLM have been instrumental in autonomous robot control and\nhuman-robot interaction by leveraging their vast general knowledge and\ncapabilities to understand and reason across a wide range of tasks and\nscenarios. Previous works have investigated various prompt engineering\ntechniques for improving the performance of LLM to accomplish tasks, while\nothers have proposed methods that utilize LLMs to plan and execute tasks based\non the available functionalities of a given robot platform. In this work, we\nconsider both lines of research by comparing prompt engineering techniques and\ncombinations thereof within the application of high-level task planning and\nexecution in service robotics. We define a diverse set of tasks and a simple\nset of functionalities in simulation, and measure task completion accuracy and\nexecution time for several state-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLM have been instrumental in autonomous robot control and\nhuman-robot interaction by leveraging their vast general knowledge and\ncapabilities to understand and reason across a wide range of tasks and\nscenarios. Previous works have investigated various prompt engineering\ntechniques for improving the performance of LLM to accomplish tasks, while\nothers have proposed methods that utilize LLMs to plan and execute tasks based\non the available functionalities of a given robot platform. In this work, we\nconsider both lines of research by comparing prompt engineering techniques and\ncombinations thereof within the application of high-level task planning and\nexecution in service robotics. We define a diverse set of tasks and a simple\nset of functionalities in simulation, and measure task completion accuracy and\nexecution time for several state-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Jonas Bode"
                    },
                    {
                        "name": "Bastian Pätzold"
                    },
                    {
                        "name": "Raphael Memmesheimer"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "arxiv_comment": "6 pages, 3 figures, 2 tables, to be published in the 2024 IEEE-RAS\n  International Conference on Humanoid Robots, We make our code, including all\n  prompts, available at https://github.com/AIS-Bonn/Prompt_Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14632v2",
                "updated": "2024-11-06T16:54:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    54,
                    48,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-18T17:32:22Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    32,
                    22,
                    4,
                    292,
                    0
                ],
                "title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverging Preferences: When do Annotators Disagree and do Models Know?"
                },
                "summary": "We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training."
                },
                "authors": [
                    {
                        "name": "Michael JQ Zhang"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Jena D. Hwang"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Eunsol Choi"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Pyatkin"
                },
                "author": "Valentina Pyatkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04036v1",
                "updated": "2024-11-06T16:33:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    33,
                    21,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T16:33:21Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    33,
                    21,
                    2,
                    311,
                    0
                ],
                "title": "Stepping Forward on the Last Mile",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stepping Forward on the Last Mile"
                },
                "summary": "Continuously adapting pre-trained models to local data on resource\nconstrained edge devices is the $\\emph{last mile}$ for model deployment.\nHowever, as models increase in size and depth, backpropagation requires a large\namount of memory, which becomes prohibitive for edge devices. In addition, most\nexisting low power neural processing engines (e.g., NPUs, DSPs, MCUs, etc.) are\ndesigned as fixed-point inference accelerators, without training capabilities.\nForward gradients, solely based on directional derivatives computed from two\nforward calls, have been recently used for model training, with substantial\nsavings in computation and memory. However, the performance of quantized\ntraining with fixed-point forward gradients remains unclear. In this paper, we\ninvestigate the feasibility of on-device training using fixed-point forward\ngradients, by conducting comprehensive experiments across a variety of deep\nlearning benchmark tasks in both vision and audio domains. We propose a series\nof algorithm enhancements that further reduce the memory footprint, and the\naccuracy gap compared to backpropagation. An empirical study on how training\nwith forward gradients navigates in the loss landscape is further explored. Our\nresults demonstrate that on the last mile of model customization on edge\ndevices, training with fixed-point forward gradients is a feasible and\npractical approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuously adapting pre-trained models to local data on resource\nconstrained edge devices is the $\\emph{last mile}$ for model deployment.\nHowever, as models increase in size and depth, backpropagation requires a large\namount of memory, which becomes prohibitive for edge devices. In addition, most\nexisting low power neural processing engines (e.g., NPUs, DSPs, MCUs, etc.) are\ndesigned as fixed-point inference accelerators, without training capabilities.\nForward gradients, solely based on directional derivatives computed from two\nforward calls, have been recently used for model training, with substantial\nsavings in computation and memory. However, the performance of quantized\ntraining with fixed-point forward gradients remains unclear. In this paper, we\ninvestigate the feasibility of on-device training using fixed-point forward\ngradients, by conducting comprehensive experiments across a variety of deep\nlearning benchmark tasks in both vision and audio domains. We propose a series\nof algorithm enhancements that further reduce the memory footprint, and the\naccuracy gap compared to backpropagation. An empirical study on how training\nwith forward gradients navigates in the loss landscape is further explored. Our\nresults demonstrate that on the last mile of model customization on edge\ndevices, training with fixed-point forward gradients is a feasible and\npractical approach."
                },
                "authors": [
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Andrew Zou Li"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zou Li"
                },
                "author": "Andrew Zou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04032v1",
                "updated": "2024-11-06T16:31:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    31,
                    28,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T16:31:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    31,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "Beemo: Benchmark of Expert-edited Machine-generated Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beemo: Benchmark of Expert-edited Machine-generated Outputs"
                },
                "summary": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available."
                },
                "authors": [
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Jason Lucas"
                    },
                    {
                        "name": "Saranya Venkatraman"
                    },
                    {
                        "name": "Jooyoung Lee"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Adaku Uchendu"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    }
                ],
                "author_detail": {
                    "name": "Vladislav Mikhailov"
                },
                "author": "Vladislav Mikhailov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08262v3",
                "updated": "2024-11-06T16:19:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    19,
                    24,
                    2,
                    311,
                    0
                ],
                "published": "2024-04-12T06:21:48Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    6,
                    21,
                    48,
                    4,
                    103,
                    0
                ],
                "title": "Pretraining and Updates of Domain-Specific LLM: A Case Study in the\n  Japanese Business Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining and Updates of Domain-Specific LLM: A Case Study in the\n  Japanese Business Domain"
                },
                "summary": "The development of Large Language Models (LLMs) in various languages has been\nadvancing, but the combination of non-English languages with domain-specific\ncontexts remains underexplored. This paper presents our findings from training\nand evaluating a Japanese business domain-specific LLM designed to better\nunderstand business-related documents, such as the news on current affairs,\ntechnical reports, and patents. Additionally, LLMs in this domain require\nregular updates to incorporate the most recent knowledge. Therefore, we also\nreport our findings from the first experiments and evaluations involving\nupdates to this LLM using the latest article data, which is an important\nproblem setting that has not been addressed in previous research. From our\nexperiments on a newly created benchmark dataset for question answering in the\ntarget domain, we found that (1) our pretrained model improves QA accuracy\nwithout losing general knowledge, and (2) a proper mixture of the latest and\nolder texts in the training data for the update is necessary. Our pretrained\nmodel and business domain benchmark are publicly available to support further\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) in various languages has been\nadvancing, but the combination of non-English languages with domain-specific\ncontexts remains underexplored. This paper presents our findings from training\nand evaluating a Japanese business domain-specific LLM designed to better\nunderstand business-related documents, such as the news on current affairs,\ntechnical reports, and patents. Additionally, LLMs in this domain require\nregular updates to incorporate the most recent knowledge. Therefore, we also\nreport our findings from the first experiments and evaluations involving\nupdates to this LLM using the latest article data, which is an important\nproblem setting that has not been addressed in previous research. From our\nexperiments on a newly created benchmark dataset for question answering in the\ntarget domain, we found that (1) our pretrained model improves QA accuracy\nwithout losing general knowledge, and (2) a proper mixture of the latest and\nolder texts in the training data for the update is necessary. Our pretrained\nmodel and business domain benchmark are publicly available to support further\nstudies."
                },
                "authors": [
                    {
                        "name": "Kosuke Takahashi"
                    },
                    {
                        "name": "Takahiro Omi"
                    },
                    {
                        "name": "Kosuke Arima"
                    },
                    {
                        "name": "Tatsuya Ishigaki"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuya Ishigaki"
                },
                "author": "Tatsuya Ishigaki",
                "arxiv_comment": "Accepted at PACLIC 38",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07520v2",
                "updated": "2024-11-06T16:17:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    17,
                    21,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-10T01:21:48Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    1,
                    21,
                    48,
                    3,
                    284,
                    0
                ],
                "title": "News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News"
                },
                "summary": "Large Language Models (LLMs) have fast become an essential tools to many\nconversational chatbots due to their ability to provide coherent answers for\nvaried queries. Datasets used to train these LLMs are often a mix of generic\nand synthetic samples, thus lacking the verification needed to provide correct\nand verifiable answers for T.V. News.\n  We collect and share a large collection of QA pairs extracted from\ntranscripts of news recordings from various news-channels across the United\nStates. Resultant QA pairs are then used to fine-tune an off-the-shelf LLM\nmodel. Our model surpasses base models of similar size on several open LLM\nbenchmarks. We further integrate and propose a RAG method to improve\ncontextualization of our answers and also point it to a verifiable news\nrecording.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have fast become an essential tools to many\nconversational chatbots due to their ability to provide coherent answers for\nvaried queries. Datasets used to train these LLMs are often a mix of generic\nand synthetic samples, thus lacking the verification needed to provide correct\nand verifiable answers for T.V. News.\n  We collect and share a large collection of QA pairs extracted from\ntranscripts of news recordings from various news-channels across the United\nStates. Resultant QA pairs are then used to fine-tune an off-the-shelf LLM\nmodel. Our model surpasses base models of similar size on several open LLM\nbenchmarks. We further integrate and propose a RAG method to improve\ncontextualization of our answers and also point it to a verifiable news\nrecording."
                },
                "authors": [
                    {
                        "name": "Tarun Jain"
                    },
                    {
                        "name": "Yufei Gao"
                    },
                    {
                        "name": "Sridhar Vanga"
                    },
                    {
                        "name": "Karan Singla"
                    }
                ],
                "author_detail": {
                    "name": "Karan Singla"
                },
                "author": "Karan Singla",
                "arxiv_comment": "5 pages, under review at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16676v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16676v3",
                "updated": "2024-11-06T15:49:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    49,
                    30,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-22T04:18:19Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    4,
                    18,
                    19,
                    1,
                    296,
                    0
                ],
                "title": "Improving Causal Reasoning in Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Causal Reasoning in Large Language Models: A Survey"
                },
                "summary": "Causal reasoning (CR) is a crucial aspect of intelligence, essential for\nproblem-solving, decision-making, and understanding the world. While large\nlanguage models (LLMs) can generate rationales for their outputs, their ability\nto reliably perform causal reasoning remains uncertain, often falling short in\ntasks requiring a deep understanding of causality. In this survey, we provide a\ncomprehensive review of research aimed at enhancing LLMs for causal reasoning.\nWe categorize existing methods based on the role of LLMs: either as reasoning\nengines or as helpers providing knowledge or data to traditional CR methods,\nfollowed by a detailed discussion of the methodologies in each category. We\nthen evaluate the performance of LLMs on various causal reasoning tasks,\nproviding key findings and in-depth analysis. Finally, we provide insights from\ncurrent studies and highlight promising directions for future research. We aim\nfor this work to serve as a comprehensive resource, fostering further\nadvancements in causal reasoning with LLMs. Resources are available at\nhttps://github.com/chendl02/Awesome-LLM-causal-reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning (CR) is a crucial aspect of intelligence, essential for\nproblem-solving, decision-making, and understanding the world. While large\nlanguage models (LLMs) can generate rationales for their outputs, their ability\nto reliably perform causal reasoning remains uncertain, often falling short in\ntasks requiring a deep understanding of causality. In this survey, we provide a\ncomprehensive review of research aimed at enhancing LLMs for causal reasoning.\nWe categorize existing methods based on the role of LLMs: either as reasoning\nengines or as helpers providing knowledge or data to traditional CR methods,\nfollowed by a detailed discussion of the methodologies in each category. We\nthen evaluate the performance of LLMs on various causal reasoning tasks,\nproviding key findings and in-depth analysis. Finally, we provide insights from\ncurrent studies and highlight promising directions for future research. We aim\nfor this work to serve as a comprehensive resource, fostering further\nadvancements in causal reasoning with LLMs. Resources are available at\nhttps://github.com/chendl02/Awesome-LLM-causal-reasoning."
                },
                "authors": [
                    {
                        "name": "Longxuan Yu"
                    },
                    {
                        "name": "Delin Chen"
                    },
                    {
                        "name": "Siheng Xiong"
                    },
                    {
                        "name": "Qingyang Wu"
                    },
                    {
                        "name": "Qingzhen Liu"
                    },
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Zhikai Chen"
                    },
                    {
                        "name": "Xiaoze Liu"
                    },
                    {
                        "name": "Liangming Pan"
                    }
                ],
                "author_detail": {
                    "name": "Liangming Pan"
                },
                "author": "Liangming Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16676v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16676v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02059v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02059v2",
                "updated": "2024-11-06T15:38:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    38,
                    37,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T13:03:13Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    13,
                    3,
                    13,
                    0,
                    309,
                    0
                ],
                "title": "TableGPT2: A Large Multimodal Model with Tabular Data Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableGPT2: A Large Multimodal Model with Tabular Data Integration"
                },
                "summary": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact."
                },
                "authors": [
                    {
                        "name": "Aofeng Su"
                    },
                    {
                        "name": "Aowen Wang"
                    },
                    {
                        "name": "Chao Ye"
                    },
                    {
                        "name": "Chen Zhou"
                    },
                    {
                        "name": "Ga Zhang"
                    },
                    {
                        "name": "Guangcheng Zhu"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Haoze Li"
                    },
                    {
                        "name": "Haoxuan Lan"
                    },
                    {
                        "name": "Jiaming Tian"
                    },
                    {
                        "name": "Jing Yuan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Junlin Zhou"
                    },
                    {
                        "name": "Kaizhe Shou"
                    },
                    {
                        "name": "Liangyu Zha"
                    },
                    {
                        "name": "Lin Long"
                    },
                    {
                        "name": "Liyao Li"
                    },
                    {
                        "name": "Pengzuo Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Qingyi Huang"
                    },
                    {
                        "name": "Saisai Yang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Wentao Ye"
                    },
                    {
                        "name": "Wufang Zhu"
                    },
                    {
                        "name": "Xiaomeng Hu"
                    },
                    {
                        "name": "Xijun Gu"
                    },
                    {
                        "name": "Xinjie Sun"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yuhang Yang"
                    },
                    {
                        "name": "Zhiqing Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqing Xiao"
                },
                "author": "Zhiqing Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02059v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03964v1",
                "updated": "2024-11-06T14:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    54,
                    19,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T14:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    54,
                    19,
                    2,
                    311,
                    0
                ],
                "title": "What Really is Commonsense Knowledge?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Really is Commonsense Knowledge?"
                },
                "summary": "Commonsense datasets have been well developed in Natural Language Processing,\nmainly through crowdsource human annotation. However, there are debates on the\ngenuineness of commonsense reasoning benchmarks. In specific, a significant\nportion of instances in some commonsense benchmarks do not concern commonsense\nknowledge. That problem would undermine the measurement of the true commonsense\nreasoning ability of evaluated models. It is also suggested that the problem\noriginated from a blurry concept of commonsense knowledge, as distinguished\nfrom other types of knowledge. To demystify all of the above claims, in this\nstudy, we survey existing definitions of commonsense knowledge, ground into the\nthree frameworks for defining concepts, and consolidate them into a\nmulti-framework unified definition of commonsense knowledge (so-called\nconsolidated definition). We then use the consolidated definition for\nannotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets\nto examine the above claims. Our study shows that there exists a large portion\nof non-commonsense-knowledge instances in the two datasets, and a large\nperformance gap on these two subsets where Large Language Models (LLMs) perform\nworse on commonsense-knowledge instances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense datasets have been well developed in Natural Language Processing,\nmainly through crowdsource human annotation. However, there are debates on the\ngenuineness of commonsense reasoning benchmarks. In specific, a significant\nportion of instances in some commonsense benchmarks do not concern commonsense\nknowledge. That problem would undermine the measurement of the true commonsense\nreasoning ability of evaluated models. It is also suggested that the problem\noriginated from a blurry concept of commonsense knowledge, as distinguished\nfrom other types of knowledge. To demystify all of the above claims, in this\nstudy, we survey existing definitions of commonsense knowledge, ground into the\nthree frameworks for defining concepts, and consolidate them into a\nmulti-framework unified definition of commonsense knowledge (so-called\nconsolidated definition). We then use the consolidated definition for\nannotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets\nto examine the above claims. Our study shows that there exists a large portion\nof non-commonsense-knowledge instances in the two datasets, and a large\nperformance gap on these two subsets where Large Language Models (LLMs) perform\nworse on commonsense-knowledge instances."
                },
                "authors": [
                    {
                        "name": "Quyet V. Do"
                    },
                    {
                        "name": "Junze Li"
                    },
                    {
                        "name": "Tung-Duong Vuong"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Xiaojuan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojuan Ma"
                },
                "author": "Xiaojuan Ma",
                "arxiv_comment": "Code and data will be released together with the next version of the\n  paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03962v1",
                "updated": "2024-11-06T14:51:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T14:51:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?"
                },
                "summary": "The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many ontology matching (OM) systems. However, the lack of\nstandardisation in text preprocessing creates diversity in mapping results. In\nthis paper, we investigate the effect of the text preprocessing pipeline on OM\ntasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation\nInitiative (OAEI) track repositories with 49 distinct alignments indicate: (1)\nTokenisation and Normalisation are currently more effective than Stop Words\nRemoval and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and\nStemming is task-specific. We recommend standalone Lemmatisation or Stemming\nwith post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer\nperform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)\nTagging does not help Lemmatisation. To repair less effective Stop Words\nRemoval and Stemming/Lemmatisation used in OM tasks, we propose a novel\ncontext-based pipeline repair approach that significantly improves matching\ncorrectness and overall matching performance. We also discuss the use of text\npreprocessing pipeline in the new era of large language models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many ontology matching (OM) systems. However, the lack of\nstandardisation in text preprocessing creates diversity in mapping results. In\nthis paper, we investigate the effect of the text preprocessing pipeline on OM\ntasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation\nInitiative (OAEI) track repositories with 49 distinct alignments indicate: (1)\nTokenisation and Normalisation are currently more effective than Stop Words\nRemoval and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and\nStemming is task-specific. We recommend standalone Lemmatisation or Stemming\nwith post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer\nperform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)\nTagging does not help Lemmatisation. To repair less effective Stop Words\nRemoval and Stemming/Lemmatisation used in OM tasks, we propose a novel\ncontext-based pipeline repair approach that significantly improves matching\ncorrectness and overall matching performance. We also discuss the use of text\npreprocessing pipeline in the new era of large language models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqing Wang"
                },
                "author": "Weiqing Wang",
                "arxiv_comment": "13 pages, 26 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10149v2",
                "updated": "2024-11-06T14:50:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    50,
                    40,
                    2,
                    311,
                    0
                ],
                "published": "2024-06-14T16:00:29Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    16,
                    0,
                    29,
                    4,
                    166,
                    0
                ],
                "title": "BABILong: Testing the Limits of LLMs with Long Context\n  Reasoning-in-a-Haystack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BABILong: Testing the Limits of LLMs with Long Context\n  Reasoning-in-a-Haystack"
                },
                "summary": "In recent years, the input context sizes of large language models (LLMs) have\nincreased dramatically. However, existing evaluation methods have not kept\npace, failing to comprehensively assess the efficiency of models in handling\nlong contexts. To bridge this gap, we introduce the BABILong benchmark,\ndesigned to test language models' ability to reason across facts distributed in\nextremely long documents. BABILong includes a diverse set of 20 reasoning\ntasks, including fact chaining, simple induction, deduction, counting, and\nhandling lists/sets. These tasks are challenging on their own, and even more\ndemanding when the required facts are scattered across long natural text. Our\nevaluations show that popular LLMs effectively utilize only 10-20\\% of the\ncontext and their performance declines sharply with increased reasoning\ncomplexity. Among alternatives to in-context reasoning, Retrieval-Augmented\nGeneration methods achieve a modest 60\\% accuracy on single-fact question\nanswering, independent of context length. Among context extension methods, the\nhighest performance is demonstrated by recurrent memory transformers after\nfine-tuning, enabling the processing of lengths up to 50 million tokens. The\nBABILong benchmark is extendable to any length to support the evaluation of new\nupcoming models with increased capabilities, and we provide splits up to 10\nmillion token lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the input context sizes of large language models (LLMs) have\nincreased dramatically. However, existing evaluation methods have not kept\npace, failing to comprehensively assess the efficiency of models in handling\nlong contexts. To bridge this gap, we introduce the BABILong benchmark,\ndesigned to test language models' ability to reason across facts distributed in\nextremely long documents. BABILong includes a diverse set of 20 reasoning\ntasks, including fact chaining, simple induction, deduction, counting, and\nhandling lists/sets. These tasks are challenging on their own, and even more\ndemanding when the required facts are scattered across long natural text. Our\nevaluations show that popular LLMs effectively utilize only 10-20\\% of the\ncontext and their performance declines sharply with increased reasoning\ncomplexity. Among alternatives to in-context reasoning, Retrieval-Augmented\nGeneration methods achieve a modest 60\\% accuracy on single-fact question\nanswering, independent of context length. Among context extension methods, the\nhighest performance is demonstrated by recurrent memory transformers after\nfine-tuning, enabling the processing of lengths up to 50 million tokens. The\nBABILong benchmark is extendable to any length to support the evaluation of new\nupcoming models with increased capabilities, and we provide splits up to 10\nmillion token lengths."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Petr Anokhin"
                    },
                    {
                        "name": "Ivan Rodkin"
                    },
                    {
                        "name": "Dmitry Sorokin"
                    },
                    {
                        "name": "Artyom Sorokin"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03957v1",
                "updated": "2024-11-06T14:42:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    42,
                    39,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T14:42:39Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    42,
                    39,
                    2,
                    311,
                    0
                ],
                "title": "Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has proven to be an effective method for\nmitigating hallucination issues inherent in large language models (LLMs).\nPrevious approaches typically train retrievers based on semantic similarity,\nlacking optimization for RAG. More recent works have proposed aligning\nretrievers with the preference signals of LLMs. However, these preference\nsignals are often difficult for dense retrievers, which typically have weaker\nlanguage capabilities, to understand and learn effectively. Drawing inspiration\nfrom pedagogical theories like Guided Discovery Learning, we propose a novel\nframework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the\nlanguage capabilities of LLMs to construct examples from a more granular,\ninformation-centric perspective to guide the learning of retrievers.\nSpecifically, our method utilizes LLMs to construct easy-to-understand examples\nfrom samples where the retriever performs poorly, focusing on three learning\nobjectives highly relevant to the RAG scenario: relevance, comprehensiveness,\nand purity. These examples serve as scaffolding to ultimately align the\nretriever with the LLM's preferences. Furthermore, we employ a dual curriculum\nlearning strategy and leverage the reciprocal feedback between LLM and\nretriever to further enhance the performance of the RAG system. A series of\nexperiments demonstrate that our proposed framework enhances the performance of\nRAG systems equipped with different retrievers and is applicable to various\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has proven to be an effective method for\nmitigating hallucination issues inherent in large language models (LLMs).\nPrevious approaches typically train retrievers based on semantic similarity,\nlacking optimization for RAG. More recent works have proposed aligning\nretrievers with the preference signals of LLMs. However, these preference\nsignals are often difficult for dense retrievers, which typically have weaker\nlanguage capabilities, to understand and learn effectively. Drawing inspiration\nfrom pedagogical theories like Guided Discovery Learning, we propose a novel\nframework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the\nlanguage capabilities of LLMs to construct examples from a more granular,\ninformation-centric perspective to guide the learning of retrievers.\nSpecifically, our method utilizes LLMs to construct easy-to-understand examples\nfrom samples where the retriever performs poorly, focusing on three learning\nobjectives highly relevant to the RAG scenario: relevance, comprehensiveness,\nand purity. These examples serve as scaffolding to ultimately align the\nretriever with the LLM's preferences. Furthermore, we employ a dual curriculum\nlearning strategy and leverage the reciprocal feedback between LLM and\nretriever to further enhance the performance of the RAG system. A series of\nexperiments demonstrate that our proposed framework enhances the performance of\nRAG systems equipped with different retrievers and is applicable to various\nLLMs."
                },
                "authors": [
                    {
                        "name": "Yuhang Liu"
                    },
                    {
                        "name": "Xueyu Hu"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Jingyuan Chen"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03948v1",
                "updated": "2024-11-06T14:29:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    29,
                    49,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T14:29:49Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    29,
                    49,
                    2,
                    311,
                    0
                ],
                "title": "Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of\n  Study in Tabletop Role-Playing Games Soundtracks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of\n  Study in Tabletop Role-Playing Games Soundtracks"
                },
                "summary": "This paper investigates the capabilities of text-to-audio music generation\nmodels in producing long-form music with prompts that change over time,\nfocusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We\nintroduce Babel Bardo, a system that uses Large Language Models (LLMs) to\ntransform speech transcriptions into music descriptions for controlling a\ntext-to-music model. Four versions of Babel Bardo were compared in two TRPG\ncampaigns: a baseline using direct speech transcriptions, and three LLM-based\nversions with varying approaches to music description generation. Evaluations\nconsidered audio quality, story alignment, and transition smoothness. Results\nindicate that detailed music descriptions improve audio quality while\nmaintaining consistency across consecutive descriptions enhances story\nalignment and transition smoothness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the capabilities of text-to-audio music generation\nmodels in producing long-form music with prompts that change over time,\nfocusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We\nintroduce Babel Bardo, a system that uses Large Language Models (LLMs) to\ntransform speech transcriptions into music descriptions for controlling a\ntext-to-music model. Four versions of Babel Bardo were compared in two TRPG\ncampaigns: a baseline using direct speech transcriptions, and three LLM-based\nversions with varying approaches to music description generation. Evaluations\nconsidered audio quality, story alignment, and transition smoothness. Results\nindicate that detailed music descriptions improve audio quality while\nmaintaining consistency across consecutive descriptions enhances story\nalignment and transition smoothness."
                },
                "authors": [
                    {
                        "name": "Felipe Marra"
                    },
                    {
                        "name": "Lucas N. Ferreira"
                    }
                ],
                "author_detail": {
                    "name": "Lucas N. Ferreira"
                },
                "author": "Lucas N. Ferreira",
                "arxiv_comment": "Paper accepted at the LAMIR 2024 workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12656v2",
                "updated": "2024-11-06T14:14:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    14,
                    58,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-16T15:17:20Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    15,
                    17,
                    20,
                    2,
                    290,
                    0
                ],
                "title": "Evaluating Morphological Compositional Generalization in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Morphological Compositional Generalization in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant progress in\nvarious natural language generation and understanding tasks. However, their\nlinguistic generalization capabilities remain questionable, raising doubts\nabout whether these models learn language similarly to humans. While humans\nexhibit compositional generalization and linguistic creativity in language use,\nthe extent to which LLMs replicate these abilities, particularly in morphology,\nis under-explored. In this work, we systematically investigate the\nmorphological generalization abilities of LLMs through the lens of\ncompositionality. We define morphemes as compositional primitives and design a\nnovel suite of generative and discriminative tasks to assess morphological\nproductivity and systematicity. Focusing on agglutinative languages such as\nTurkish and Finnish, we evaluate several state-of-the-art instruction-finetuned\nmultilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs\nstruggle with morphological compositional generalization particularly when\napplied to novel word roots, with performance declining sharply as\nmorphological complexity increases. While models can identify individual\nmorphological combinations better than chance, their performance lacks\nsystematicity, leading to significant accuracy gaps compared to humans."
                },
                "authors": [
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Defne Circi"
                    },
                    {
                        "name": "Jonne Sälevä"
                    },
                    {
                        "name": "Hale Sirin"
                    },
                    {
                        "name": "Abdullatif Köksal"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    },
                    {
                        "name": "Duygu Ataman"
                    }
                ],
                "author_detail": {
                    "name": "Duygu Ataman"
                },
                "author": "Duygu Ataman",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03923v1",
                "updated": "2024-11-06T13:54:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    54,
                    8,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T13:54:08Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    54,
                    8,
                    2,
                    311,
                    0
                ],
                "title": "Evaluation data contamination in LLMs: how do we measure it and (when)\n  does it matter?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation data contamination in LLMs: how do we measure it and (when)\n  does it matter?"
                },
                "summary": "Hampering the interpretation of benchmark scores, evaluation data\ncontamination has become a growing concern in the evaluation of LLMs, and an\nactive area of research studies its effects. While evaluation data\ncontamination is easily understood intuitively, it is surprisingly difficult to\ndefine precisely which samples should be considered contaminated and,\nconsequently, how it impacts benchmark scores. We propose that these questions\nshould be addressed together and that contamination metrics can be assessed\nbased on whether models benefit from the examples they mark contaminated. We\npropose a novel analysis method called ConTAM, and show with a large scale\nsurvey of existing and novel n-gram based contamination metrics across 13\nbenchmarks and 7 models from 2 different families that ConTAM can be used to\nbetter understand evaluation data contamination and its effects. We find that\ncontamination may have a much larger effect than reported in recent LLM\nreleases and benefits models differently at different scales. We also find that\nconsidering only the longest contaminated substring provides a better signal\nthan considering a union of all contaminated substrings, and that doing model\nand benchmark specific threshold analysis greatly increases the specificity of\nthe results. Lastly, we investigate the impact of hyperparameter choices,\nfinding that, among other things, both using larger values of n and\ndisregarding matches that are infrequent in the pre-training data lead to many\nfalse negatives. With ConTAM, we provide a method to empirically ground\nevaluation data contamination metrics in downstream effects. With our\nexploration, we shed light on how evaluation data contamination can impact LLMs\nand provide insight into the considerations important when doing contamination\nanalysis. We end our paper by discussing these in more detail and providing\nconcrete suggestions for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hampering the interpretation of benchmark scores, evaluation data\ncontamination has become a growing concern in the evaluation of LLMs, and an\nactive area of research studies its effects. While evaluation data\ncontamination is easily understood intuitively, it is surprisingly difficult to\ndefine precisely which samples should be considered contaminated and,\nconsequently, how it impacts benchmark scores. We propose that these questions\nshould be addressed together and that contamination metrics can be assessed\nbased on whether models benefit from the examples they mark contaminated. We\npropose a novel analysis method called ConTAM, and show with a large scale\nsurvey of existing and novel n-gram based contamination metrics across 13\nbenchmarks and 7 models from 2 different families that ConTAM can be used to\nbetter understand evaluation data contamination and its effects. We find that\ncontamination may have a much larger effect than reported in recent LLM\nreleases and benefits models differently at different scales. We also find that\nconsidering only the longest contaminated substring provides a better signal\nthan considering a union of all contaminated substrings, and that doing model\nand benchmark specific threshold analysis greatly increases the specificity of\nthe results. Lastly, we investigate the impact of hyperparameter choices,\nfinding that, among other things, both using larger values of n and\ndisregarding matches that are infrequent in the pre-training data lead to many\nfalse negatives. With ConTAM, we provide a method to empirically ground\nevaluation data contamination metrics in downstream effects. With our\nexploration, we shed light on how evaluation data contamination can impact LLMs\nand provide insight into the considerations important when doing contamination\nanalysis. We end our paper by discussing these in more detail and providing\nconcrete suggestions for future work."
                },
                "authors": [
                    {
                        "name": "Aaditya K. Singh"
                    },
                    {
                        "name": "Muhammed Yusuf Kocyigit"
                    },
                    {
                        "name": "Andrew Poulton"
                    },
                    {
                        "name": "David Esiobu"
                    },
                    {
                        "name": "Maria Lomeli"
                    },
                    {
                        "name": "Gergely Szilvasy"
                    },
                    {
                        "name": "Dieuwke Hupkes"
                    }
                ],
                "author_detail": {
                    "name": "Dieuwke Hupkes"
                },
                "author": "Dieuwke Hupkes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03920v1",
                "updated": "2024-11-06T13:51:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    51,
                    42,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T13:51:42Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    51,
                    42,
                    2,
                    311,
                    0
                ],
                "title": "RAGulator: Lightweight Out-of-Context Detectors for Grounded Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGulator: Lightweight Out-of-Context Detectors for Grounded Text\n  Generation"
                },
                "summary": "Real-time detection of out-of-context LLM outputs is crucial for enterprises\nlooking to safely adopt RAG applications. In this work, we train lightweight\nmodels to discriminate LLM-generated text that is semantically out-of-context\nfrom retrieved text documents. We preprocess a combination of summarisation and\nsemantic textual similarity datasets to construct training data using minimal\nresources. We find that DeBERTa is not only the best-performing model under\nthis pipeline, but it is also fast and does not require additional text\npreprocessing or feature engineering. While emerging work demonstrates that\ngenerative LLMs can also be fine-tuned and used in complex data pipelines to\nachieve state-of-the-art performance, we note that speed and resource limits\nare important considerations for on-premise deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time detection of out-of-context LLM outputs is crucial for enterprises\nlooking to safely adopt RAG applications. In this work, we train lightweight\nmodels to discriminate LLM-generated text that is semantically out-of-context\nfrom retrieved text documents. We preprocess a combination of summarisation and\nsemantic textual similarity datasets to construct training data using minimal\nresources. We find that DeBERTa is not only the best-performing model under\nthis pipeline, but it is also fast and does not require additional text\npreprocessing or feature engineering. While emerging work demonstrates that\ngenerative LLMs can also be fine-tuned and used in complex data pipelines to\nachieve state-of-the-art performance, we note that speed and resource limits\nare important considerations for on-premise deployment."
                },
                "authors": [
                    {
                        "name": "Ian Poey"
                    },
                    {
                        "name": "Jiajun Liu"
                    },
                    {
                        "name": "Qishuai Zhong"
                    },
                    {
                        "name": "Adrien Chenailler"
                    }
                ],
                "author_detail": {
                    "name": "Adrien Chenailler"
                },
                "author": "Adrien Chenailler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03906v1",
                "updated": "2024-11-06T13:37:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    37,
                    28,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T13:37:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    37,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "Lexicalization Is All You Need: Examining the Impact of Lexical\n  Knowledge in a Compositional QALD System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexicalization Is All You Need: Examining the Impact of Lexical\n  Knowledge in a Compositional QALD System"
                },
                "summary": "In this paper, we examine the impact of lexicalization on Question Answering\nover Linked Data (QALD). It is well known that one of the key challenges in\ninterpreting natural language questions with respect to SPARQL lies in bridging\nthe lexical gap, that is mapping the words in the query to the correct\nvocabulary elements. We argue in this paper that lexicalization, that is\nexplicit knowledge about the potential interpretations of a word with respect\nto the given vocabulary, significantly eases the task and increases the\nperformance of QA systems. Towards this goal, we present a compositional QA\nsystem that can leverage explicit lexical knowledge in a compositional manner\nto infer the meaning of a question in terms of a SPARQL query. We show that\nsuch a system, given lexical knowledge, has a performance well beyond current\nQA systems, achieving up to a $35.8\\%$ increase in the micro $F_1$ score\ncompared to the best QA system on QALD-9. This shows the importance and\npotential of including explicit lexical knowledge. In contrast, we show that\nLLMs have limited abilities to exploit lexical knowledge, with only marginal\nimprovements compared to a version without lexical knowledge. This shows that\nLLMs have no ability to compositionally interpret a question on the basis of\nthe meaning of its parts, a key feature of compositional approaches. Taken\ntogether, our work shows new avenues for QALD research, emphasizing the\nimportance of lexicalization and compositionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we examine the impact of lexicalization on Question Answering\nover Linked Data (QALD). It is well known that one of the key challenges in\ninterpreting natural language questions with respect to SPARQL lies in bridging\nthe lexical gap, that is mapping the words in the query to the correct\nvocabulary elements. We argue in this paper that lexicalization, that is\nexplicit knowledge about the potential interpretations of a word with respect\nto the given vocabulary, significantly eases the task and increases the\nperformance of QA systems. Towards this goal, we present a compositional QA\nsystem that can leverage explicit lexical knowledge in a compositional manner\nto infer the meaning of a question in terms of a SPARQL query. We show that\nsuch a system, given lexical knowledge, has a performance well beyond current\nQA systems, achieving up to a $35.8\\%$ increase in the micro $F_1$ score\ncompared to the best QA system on QALD-9. This shows the importance and\npotential of including explicit lexical knowledge. In contrast, we show that\nLLMs have limited abilities to exploit lexical knowledge, with only marginal\nimprovements compared to a version without lexical knowledge. This shows that\nLLMs have no ability to compositionally interpret a question on the basis of\nthe meaning of its parts, a key feature of compositional approaches. Taken\ntogether, our work shows new avenues for QALD research, emphasizing the\nimportance of lexicalization and compositionality."
                },
                "authors": [
                    {
                        "name": "David Maria Schmidt"
                    },
                    {
                        "name": "Mohammad Fazleh Elahi"
                    },
                    {
                        "name": "Philipp Cimiano"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Cimiano"
                },
                "author": "Philipp Cimiano",
                "arxiv_comment": "24th International Conference on Knowledge Engineering and Knowledge\n  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18379v2",
                "updated": "2024-11-06T13:26:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    26,
                    50,
                    2,
                    311,
                    0
                ],
                "published": "2024-06-26T14:21:09Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    14,
                    21,
                    9,
                    2,
                    178,
                    0
                ],
                "title": "MALSIGHT: Exploring Malicious Source Code and Benign Pseudocode for\n  Iterative Binary Malware Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MALSIGHT: Exploring Malicious Source Code and Benign Pseudocode for\n  Iterative Binary Malware Summarization"
                },
                "summary": "Binary malware summarization aims to automatically generate human-readable\ndescriptions of malware behaviors from executable files, facilitating tasks\nlike malware cracking and detection. Previous methods based on Large Language\nModels (LLMs) have shown great promise. However, they still face significant\nissues, including poor usability, inaccurate explanations,and incomplete\nsummaries, primarily due to the obscure pseudocode structure and the lack of\nmalware training summaries. Further, calling relationships between functions,\nwhich involve the rich interactions within a binary malware, remain largely\nunderexplored. To this end, we propose MALSIGHT, a novel code summarization\nframework that can iteratively generate descriptions of binary malware by\nexploring malicious source code and benign pseudocode. Specifically, we\nconstruct the first malware summary dataset, MalS and MalP, using an LLM and\nmanually refine this dataset with human effort. At the training stage, we tune\nour proposed MalT5, a novel LLM-based code model, on the MalS and benign\npseudocode datasets. Then, at the test stage, we iteratively feed the\npseudocode functions into MalT5 to obtain the summary. Such a procedure\nfacilitates the understanding of pseudocode structure and captures the\nintricate interactions between functions, thereby benefiting summaries'\nusability, accuracy, and completeness. Additionally, we propose a novel\nevaluation benchmark, BLEURT-sum, to measure the quality of summaries.\nExperiments on three datasets show the effectiveness of the proposed MALSIGHT.\nNotably, our proposed MalT5, with only 0.77B parameters, delivers comparable\nperformance to much larger Code-Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary malware summarization aims to automatically generate human-readable\ndescriptions of malware behaviors from executable files, facilitating tasks\nlike malware cracking and detection. Previous methods based on Large Language\nModels (LLMs) have shown great promise. However, they still face significant\nissues, including poor usability, inaccurate explanations,and incomplete\nsummaries, primarily due to the obscure pseudocode structure and the lack of\nmalware training summaries. Further, calling relationships between functions,\nwhich involve the rich interactions within a binary malware, remain largely\nunderexplored. To this end, we propose MALSIGHT, a novel code summarization\nframework that can iteratively generate descriptions of binary malware by\nexploring malicious source code and benign pseudocode. Specifically, we\nconstruct the first malware summary dataset, MalS and MalP, using an LLM and\nmanually refine this dataset with human effort. At the training stage, we tune\nour proposed MalT5, a novel LLM-based code model, on the MalS and benign\npseudocode datasets. Then, at the test stage, we iteratively feed the\npseudocode functions into MalT5 to obtain the summary. Such a procedure\nfacilitates the understanding of pseudocode structure and captures the\nintricate interactions between functions, thereby benefiting summaries'\nusability, accuracy, and completeness. Additionally, we propose a novel\nevaluation benchmark, BLEURT-sum, to measure the quality of summaries.\nExperiments on three datasets show the effectiveness of the proposed MALSIGHT.\nNotably, our proposed MalT5, with only 0.77B parameters, delivers comparable\nperformance to much larger Code-Llama."
                },
                "authors": [
                    {
                        "name": "Haolang Lu"
                    },
                    {
                        "name": "Hongrui Peng"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Jiaoyang Cui"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Weifei Jin"
                    },
                    {
                        "name": "Songtao Wang"
                    },
                    {
                        "name": "Shengli Pan"
                    },
                    {
                        "name": "Xiaofeng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Tao"
                },
                "author": "Xiaofeng Tao",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03884v1",
                "updated": "2024-11-06T13:00:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    0,
                    34,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T13:00:34Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    13,
                    0,
                    34,
                    2,
                    311,
                    0
                ],
                "title": "Polynomial Composition Activations: Unleashing the Dynamics of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polynomial Composition Activations: Unleashing the Dynamics of Large\n  Language Models"
                },
                "summary": "Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom."
                },
                "authors": [
                    {
                        "name": "Zhijian Zhuo"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Xiaoqing Li"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Jinwen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jinwen Ma"
                },
                "author": "Jinwen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03883v1",
                "updated": "2024-11-06T12:57:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    57,
                    58,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:57:58Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    57,
                    58,
                    2,
                    311,
                    0
                ],
                "title": "MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEG: Medical Knowledge-Augmented Large Language Models for Question\n  Answering"
                },
                "summary": "Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering is a natural language understanding task that involves\nreasoning over both explicit context and unstated, relevant domain knowledge.\nLarge language models (LLMs), which underpin most contemporary question\nanswering systems, struggle to induce how concepts relate in specialized\ndomains such as medicine. Existing medical LLMs are also costly to train. In\nthis work, we present MEG, a parameter-efficient approach for medical\nknowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate\ngraph embeddings into the LLM, enabling it to leverage external knowledge in a\ncost-effective way. We evaluate our method on four popular medical\nmultiple-choice datasets and show that LLMs greatly benefit from the factual\ngrounding provided by knowledge graph embeddings. MEG attains an average of\n+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized\nmodels like BioMistral. We also show results based on Llama-3. Finally, we show\nthat MEG's performance remains robust to the choice of graph encoder."
                },
                "authors": [
                    {
                        "name": "Laura Cabello"
                    },
                    {
                        "name": "Carmen Martin-Turrero"
                    },
                    {
                        "name": "Uchenna Akujuobi"
                    },
                    {
                        "name": "Anders Søgaard"
                    },
                    {
                        "name": "Carlos Bobed"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Bobed"
                },
                "author": "Carlos Bobed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03881v1",
                "updated": "2024-11-06T12:54:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    54,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:54:27Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    54,
                    27,
                    2,
                    311,
                    0
                ],
                "title": "Data Fusion of Synthetic Query Variants With Generative Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Fusion of Synthetic Query Variants With Generative Large Language\n  Models"
                },
                "summary": "Considering query variance in information retrieval (IR) experiments is\nbeneficial for retrieval effectiveness. Especially ranking ensembles based on\ndifferent topically related queries retrieve better results than rankings based\non a single query alone. Recently, generative instruction-tuned Large Language\nModels (LLMs) improved on a variety of different tasks in capturing human\nlanguage. To this end, this work explores the feasibility of using synthetic\nquery variants generated by instruction-tuned LLMs in data fusion experiments.\nMore specifically, we introduce a lightweight, unsupervised, and cost-efficient\napproach that exploits principled prompting and data fusion techniques. In our\nexperiments, LLMs produce more effective queries when provided with additional\ncontext information on the topic. Furthermore, our analysis based on four TREC\nnewswire benchmarks shows that data fusion based on synthetic query variants is\nsignificantly better than baselines with single queries and also outperforms\npseudo-relevance feedback methods. We publicly share the code and query\ndatasets with the community as resources for follow-up studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considering query variance in information retrieval (IR) experiments is\nbeneficial for retrieval effectiveness. Especially ranking ensembles based on\ndifferent topically related queries retrieve better results than rankings based\non a single query alone. Recently, generative instruction-tuned Large Language\nModels (LLMs) improved on a variety of different tasks in capturing human\nlanguage. To this end, this work explores the feasibility of using synthetic\nquery variants generated by instruction-tuned LLMs in data fusion experiments.\nMore specifically, we introduce a lightweight, unsupervised, and cost-efficient\napproach that exploits principled prompting and data fusion techniques. In our\nexperiments, LLMs produce more effective queries when provided with additional\ncontext information on the topic. Furthermore, our analysis based on four TREC\nnewswire benchmarks shows that data fusion based on synthetic query variants is\nsignificantly better than baselines with single queries and also outperforms\npseudo-relevance feedback methods. We publicly share the code and query\ndatasets with the community as resources for follow-up studies."
                },
                "authors": [
                    {
                        "name": "Timo Breuer"
                    }
                ],
                "author_detail": {
                    "name": "Timo Breuer"
                },
                "author": "Timo Breuer",
                "arxiv_doi": "10.1145/3673791.3698423",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3673791.3698423",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.03881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive version of record was published in SIGIR-AP '24",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03877v1",
                "updated": "2024-11-06T12:48:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    48,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:48:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    48,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning"
                },
                "summary": "Answering reasoning-based complex questions over text and hybrid sources,\nincluding tables, is a challenging task. Recent advances in large language\nmodels (LLMs) have enabled in-context learning (ICL), allowing LLMs to acquire\nproficiency in a specific task using only a few demonstration samples\n(exemplars). A critical challenge in ICL is the selection of optimal exemplars,\nwhich can be either task-specific (static) or test-example-specific (dynamic).\nStatic exemplars provide faster inference times and increased robustness across\na distribution of test examples. In this paper, we propose an algorithm for\nstatic exemplar subset selection for complex reasoning tasks. We introduce\nEXPLORA, a novel exploration method designed to estimate the parameters of the\nscoring function, which evaluates exemplar subsets without incorporating\nconfidence information. EXPLORA significantly reduces the number of LLM calls\nto ~11% of those required by state-of-the-art methods and achieves a\nsubstantial performance improvement of 12.24%. We open-source our code and data\n(https://github.com/kiranpurohit/EXPLORA).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering reasoning-based complex questions over text and hybrid sources,\nincluding tables, is a challenging task. Recent advances in large language\nmodels (LLMs) have enabled in-context learning (ICL), allowing LLMs to acquire\nproficiency in a specific task using only a few demonstration samples\n(exemplars). A critical challenge in ICL is the selection of optimal exemplars,\nwhich can be either task-specific (static) or test-example-specific (dynamic).\nStatic exemplars provide faster inference times and increased robustness across\na distribution of test examples. In this paper, we propose an algorithm for\nstatic exemplar subset selection for complex reasoning tasks. We introduce\nEXPLORA, a novel exploration method designed to estimate the parameters of the\nscoring function, which evaluates exemplar subsets without incorporating\nconfidence information. EXPLORA significantly reduces the number of LLM calls\nto ~11% of those required by state-of-the-art methods and achieves a\nsubstantial performance improvement of 12.24%. We open-source our code and data\n(https://github.com/kiranpurohit/EXPLORA)."
                },
                "authors": [
                    {
                        "name": "Kiran Purohit"
                    },
                    {
                        "name": "Venktesh V"
                    },
                    {
                        "name": "Raghuram Devalla"
                    },
                    {
                        "name": "Krishna Mohan Yerragorla"
                    },
                    {
                        "name": "Sourangshu Bhattacharya"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03876v1",
                "updated": "2024-11-06T12:45:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    45,
                    46,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:45:46Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    45,
                    46,
                    2,
                    311,
                    0
                ],
                "title": "Large Generative Model-assisted Talking-face Semantic Communication\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Generative Model-assisted Talking-face Semantic Communication\n  System"
                },
                "summary": "The rapid development of generative Artificial Intelligence (AI) continually\nunveils the potential of Semantic Communication (SemCom). However, current\ntalking-face SemCom systems still encounter challenges such as low bandwidth\nutilization, semantic ambiguity, and diminished Quality of Experience (QoE).\nThis study introduces a Large Generative Model-assisted Talking-face Semantic\nCommunication (LGM-TSC) System tailored for the talking-face video\ncommunication. Firstly, we introduce a Generative Semantic Extractor (GSE) at\nthe transmitter based on the FunASR model to convert semantically sparse\ntalking-face videos into texts with high information density. Secondly, we\nestablish a private Knowledge Base (KB) based on the Large Language Model (LLM)\nfor semantic disambiguation and correction, complemented by a joint knowledge\nbase-semantic-channel coding scheme. Finally, at the receiver, we propose a\nGenerative Semantic Reconstructor (GSR) that utilizes BERT-VITS2 and SadTalker\nmodels to transform text back into a high-QoE talking-face video matching the\nuser's timbre. Simulation results demonstrate the feasibility and effectiveness\nof the proposed LGM-TSC system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of generative Artificial Intelligence (AI) continually\nunveils the potential of Semantic Communication (SemCom). However, current\ntalking-face SemCom systems still encounter challenges such as low bandwidth\nutilization, semantic ambiguity, and diminished Quality of Experience (QoE).\nThis study introduces a Large Generative Model-assisted Talking-face Semantic\nCommunication (LGM-TSC) System tailored for the talking-face video\ncommunication. Firstly, we introduce a Generative Semantic Extractor (GSE) at\nthe transmitter based on the FunASR model to convert semantically sparse\ntalking-face videos into texts with high information density. Secondly, we\nestablish a private Knowledge Base (KB) based on the Large Language Model (LLM)\nfor semantic disambiguation and correction, complemented by a joint knowledge\nbase-semantic-channel coding scheme. Finally, at the receiver, we propose a\nGenerative Semantic Reconstructor (GSR) that utilizes BERT-VITS2 and SadTalker\nmodels to transform text back into a high-QoE talking-face video matching the\nuser's timbre. Simulation results demonstrate the feasibility and effectiveness\nof the proposed LGM-TSC system."
                },
                "authors": [
                    {
                        "name": "Feibo Jiang"
                    },
                    {
                        "name": "Siwei Tu"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Cunhua Pan"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Xiaohu You"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu You"
                },
                "author": "Xiaohu You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03205v2",
                "updated": "2024-11-06T12:42:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    42,
                    5,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-05T15:53:59Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    53,
                    59,
                    1,
                    310,
                    0
                ],
                "title": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis"
                },
                "summary": "Recent advancements in Generative AI offer promising capabilities for spatial\nanalysis. Despite their potential, the integration of generative AI with\nestablished GIS platforms remains underexplored. In this study, we propose a\nframework for integrating LLMs directly into existing GIS platforms, using QGIS\nas an example. Our approach leverages the reasoning and programming\ncapabilities of LLMs to autonomously generate spatial analysis workflows and\ncode through an informed agent that has comprehensive documentation of key GIS\ntools and parameters. The implementation of this framework resulted in the\ndevelopment of a \"GIS Copilot\" that allows GIS users to interact with QGIS\nusing natural language commands for spatial analysis. The GIS Copilot was\nevaluated based on three complexity levels: basic tasks that require one GIS\ntool and typically involve one data layer to perform simple operations;\nintermediate tasks involving multi-step processes with multiple tools, guided\nby user instructions; and advanced tasks which involve multi-step processes\nthat require multiple tools but not guided by user instructions, necessitating\nthe agent to independently decide on and executes the necessary steps. The\nevaluation reveals that the GIS Copilot demonstrates strong potential in\nautomating foundational GIS operations, with a high success rate in tool\nselection and code generation for basic and intermediate tasks, while\nchallenges remain in achieving full autonomy for more complex tasks. This study\ncontributes to the emerging vision of Autonomous GIS, providing a pathway for\nnon-experts to engage with geospatial analysis with minimal prior expertise.\nWhile full autonomy is yet to be achieved, the GIS Copilot demonstrates\nsignificant potential for simplifying GIS workflows and enhancing\ndecision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Generative AI offer promising capabilities for spatial\nanalysis. Despite their potential, the integration of generative AI with\nestablished GIS platforms remains underexplored. In this study, we propose a\nframework for integrating LLMs directly into existing GIS platforms, using QGIS\nas an example. Our approach leverages the reasoning and programming\ncapabilities of LLMs to autonomously generate spatial analysis workflows and\ncode through an informed agent that has comprehensive documentation of key GIS\ntools and parameters. The implementation of this framework resulted in the\ndevelopment of a \"GIS Copilot\" that allows GIS users to interact with QGIS\nusing natural language commands for spatial analysis. The GIS Copilot was\nevaluated based on three complexity levels: basic tasks that require one GIS\ntool and typically involve one data layer to perform simple operations;\nintermediate tasks involving multi-step processes with multiple tools, guided\nby user instructions; and advanced tasks which involve multi-step processes\nthat require multiple tools but not guided by user instructions, necessitating\nthe agent to independently decide on and executes the necessary steps. The\nevaluation reveals that the GIS Copilot demonstrates strong potential in\nautomating foundational GIS operations, with a high success rate in tool\nselection and code generation for basic and intermediate tasks, while\nchallenges remain in achieving full autonomy for more complex tasks. This study\ncontributes to the emerging vision of Autonomous GIS, providing a pathway for\nnon-experts to engage with geospatial analysis with minimal prior expertise.\nWhile full autonomy is yet to be achieved, the GIS Copilot demonstrates\nsignificant potential for simplifying GIS workflows and enhancing\ndecision-making processes."
                },
                "authors": [
                    {
                        "name": "Temitope Akinboyewa"
                    },
                    {
                        "name": "Zhenlong Li"
                    },
                    {
                        "name": "Huan Ning"
                    },
                    {
                        "name": "M. Naser Lessani"
                    }
                ],
                "author_detail": {
                    "name": "M. Naser Lessani"
                },
                "author": "M. Naser Lessani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10499v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10499v3",
                "updated": "2024-11-06T12:35:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    35,
                    37,
                    2,
                    311,
                    0
                ],
                "published": "2024-07-15T07:43:55Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    7,
                    43,
                    55,
                    0,
                    197,
                    0
                ],
                "title": "CIBench: Evaluating Your LLMs with a Code Interpreter Plugin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CIBench: Evaluating Your LLMs with a Code Interpreter Plugin"
                },
                "summary": "While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization."
                },
                "authors": [
                    {
                        "name": "Chuyu Zhang"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Yingfan Hu"
                    },
                    {
                        "name": "Haowen Shen"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Zerun Ma"
                    },
                    {
                        "name": "Fengzhe Zhou"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Xuming He"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Under review. The first three authors contribute equally, and\n  Songyang Zhang is the project leader",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10499v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10499v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03866v1",
                "updated": "2024-11-06T12:22:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    22,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:22:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    22,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the\n  Way Forward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the\n  Way Forward"
                },
                "summary": "Recent research has demonstrated that training a linear connector between\nspeech foundation encoders and large language models (LLMs) enables this\narchitecture to achieve strong ASR capabilities. Despite the impressive\nresults, it remains unclear whether these simple approaches are robust enough\nacross different scenarios and speech conditions, such as domain shifts and\ndifferent speech perturbations. In this paper, we address these questions by\nconducting various ablation experiments using a recent and widely adopted\napproach called SLAM-ASR. We present novel empirical findings that offer\ninsights on how to effectively utilize the SLAM-ASR architecture across a wide\nrange of settings. Our main findings indicate that the SLAM-ASR exhibits poor\nperformance in cross-domain evaluation settings. Additionally, speech\nperturbations within in-domain data, such as changes in speed or the presence\nof additive noise, can significantly impact performance. Our findings offer\ncritical insights for fine-tuning and configuring robust LLM-based ASR models,\ntailored to different data characteristics and computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has demonstrated that training a linear connector between\nspeech foundation encoders and large language models (LLMs) enables this\narchitecture to achieve strong ASR capabilities. Despite the impressive\nresults, it remains unclear whether these simple approaches are robust enough\nacross different scenarios and speech conditions, such as domain shifts and\ndifferent speech perturbations. In this paper, we address these questions by\nconducting various ablation experiments using a recent and widely adopted\napproach called SLAM-ASR. We present novel empirical findings that offer\ninsights on how to effectively utilize the SLAM-ASR architecture across a wide\nrange of settings. Our main findings indicate that the SLAM-ASR exhibits poor\nperformance in cross-domain evaluation settings. Additionally, speech\nperturbations within in-domain data, such as changes in speed or the presence\nof additive noise, can significantly impact performance. Our findings offer\ncritical insights for fine-tuning and configuring robust LLM-based ASR models,\ntailored to different data characteristics and computational resources."
                },
                "authors": [
                    {
                        "name": "Shashi Kumar"
                    },
                    {
                        "name": "Iuliia Thorbecke"
                    },
                    {
                        "name": "Sergio Burdisso"
                    },
                    {
                        "name": "Esaú Villatoro-Tello"
                    },
                    {
                        "name": "Manjunath K E"
                    },
                    {
                        "name": "Kadri Hacioğlu"
                    },
                    {
                        "name": "Pradeep Rangappa"
                    },
                    {
                        "name": "Petr Motlicek"
                    },
                    {
                        "name": "Aravind Ganapathiraju"
                    },
                    {
                        "name": "Andreas Stolcke"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Stolcke"
                },
                "author": "Andreas Stolcke",
                "arxiv_comment": "Submitted to ICASSP 2025 SALMA Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03865v1",
                "updated": "2024-11-06T12:19:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    19,
                    1,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T12:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    19,
                    1,
                    2,
                    311,
                    0
                ],
                "title": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSociety: An Adaptive Environment with Social Structures for\n  Multi-Agent Decision-Making"
                },
                "summary": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional interactive environments limit agents' intelligence growth with\nfixed tasks. Recently, single-agent environments address this by generating new\ntasks based on agent actions, enhancing task diversity. We consider the\ndecision-making problem in multi-agent settings, where tasks are further\ninfluenced by social connections, affecting rewards and information access.\nHowever, existing multi-agent environments lack a combination of adaptive\nphysical surroundings and social connections, hindering the learning of\nintelligent behaviors. To address this, we introduce AdaSociety, a customizable\nmulti-agent environment featuring expanding state and action spaces, alongside\nexplicit and alterable social structures. As agents progress, the environment\nadaptively generates new tasks with social structures for agents to undertake.\nIn AdaSociety, we develop three mini-games showcasing distinct social\nstructures and tasks. Initial results demonstrate that specific social\nstructures can promote both individual and collective benefits, though current\nreinforcement learning and LLM-based algorithms show limited effectiveness in\nleveraging social structures to enhance performance. Overall, AdaSociety serves\nas a valuable research platform for exploring intelligence in diverse physical\nand social settings. The code is available at\nhttps://github.com/bigai-ai/AdaSociety."
                },
                "authors": [
                    {
                        "name": "Yizhe Huang"
                    },
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Aoyang Qin"
                    },
                    {
                        "name": "Min Tang"
                    },
                    {
                        "name": "Xiaoxi Wang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Mingjie Bi"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "arxiv_comment": "Accepted at NeurIPS D&B 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05997v2",
                "updated": "2024-11-06T12:06:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    6,
                    3,
                    2,
                    311,
                    0
                ],
                "published": "2024-04-09T04:04:50Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    4,
                    4,
                    50,
                    1,
                    100,
                    0
                ],
                "title": "Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis"
                },
                "summary": "The black-box nature of deep learning models has raised concerns about their\ninterpretability for successful deployment in real-world clinical applications.\nTo address the concerns, eXplainable Artificial Intelligence (XAI) aims to\nprovide clear and understandable explanations of the decision-making process.\nIn the medical domain, concepts such as attributes of lesions or abnormalities\nserve as key evidence for deriving diagnostic results. Existing concept-based\nmodels mainly depend on concepts that appear independently and require\nfine-grained concept annotations such as bounding boxes. However, a medical\nimage usually contains multiple concepts, and the fine-grained concept\nannotations are difficult to acquire. In this paper, we aim to interpret\nrepresentations in deep neural networks by aligning the axes of the latent\nspace with known concepts of interest. We propose a novel Concept-Attention\nWhitening (CAW) framework for interpretable skin lesion diagnosis. CAW is\ncomprised of a disease diagnosis branch and a concept alignment branch. In the\nformer branch, we train a convolutional neural network (CNN) with an inserted\nCAW layer to perform skin lesion diagnosis. The CAW layer decorrelates features\nand aligns image features to conceptual meanings via an orthogonal matrix. In\nthe latter branch, the orthogonal matrix is calculated under the guidance of\nthe concept attention mask. We particularly introduce a weakly-supervised\nconcept mask generator that only leverages coarse concept labels for filtering\nlocal regions that are relevant to certain concepts, improving the optimization\nof the orthogonal matrix. Extensive experiments on two public skin lesion\ndiagnosis datasets demonstrated that CAW not only enhanced interpretability but\nalso maintained a state-of-the-art diagnostic performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The black-box nature of deep learning models has raised concerns about their\ninterpretability for successful deployment in real-world clinical applications.\nTo address the concerns, eXplainable Artificial Intelligence (XAI) aims to\nprovide clear and understandable explanations of the decision-making process.\nIn the medical domain, concepts such as attributes of lesions or abnormalities\nserve as key evidence for deriving diagnostic results. Existing concept-based\nmodels mainly depend on concepts that appear independently and require\nfine-grained concept annotations such as bounding boxes. However, a medical\nimage usually contains multiple concepts, and the fine-grained concept\nannotations are difficult to acquire. In this paper, we aim to interpret\nrepresentations in deep neural networks by aligning the axes of the latent\nspace with known concepts of interest. We propose a novel Concept-Attention\nWhitening (CAW) framework for interpretable skin lesion diagnosis. CAW is\ncomprised of a disease diagnosis branch and a concept alignment branch. In the\nformer branch, we train a convolutional neural network (CNN) with an inserted\nCAW layer to perform skin lesion diagnosis. The CAW layer decorrelates features\nand aligns image features to conceptual meanings via an orthogonal matrix. In\nthe latter branch, the orthogonal matrix is calculated under the guidance of\nthe concept attention mask. We particularly introduce a weakly-supervised\nconcept mask generator that only leverages coarse concept labels for filtering\nlocal regions that are relevant to certain concepts, improving the optimization\nof the orthogonal matrix. Extensive experiments on two public skin lesion\ndiagnosis datasets demonstrated that CAW not only enhanced interpretability but\nalso maintained a state-of-the-art diagnostic performance."
                },
                "authors": [
                    {
                        "name": "Junlin Hou"
                    },
                    {
                        "name": "Jilan Xu"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "arxiv_comment": "MICCAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01204v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01204v3",
                "updated": "2024-11-06T12:02:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    2,
                    52,
                    2,
                    311,
                    0
                ],
                "published": "2024-04-01T16:00:01Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    16,
                    0,
                    1,
                    0,
                    92,
                    0
                ],
                "title": "The Fine Line: Navigating Large Language Model Pretraining with\n  Down-streaming Capability Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fine Line: Navigating Large Language Model Pretraining with\n  Down-streaming Capability Analysis"
                },
                "summary": "Uncovering early-stage metrics that reflect final model performance is one\ncore principle for large-scale pretraining. The existing scaling law\ndemonstrates the power-law correlation between pretraining loss and training\nflops, which serves as an important indicator of the current training state for\nlarge language models. However, this principle only focuses on the model's\ncompression properties on the training data, resulting in an inconsistency with\nthe ability improvements on the downstream tasks. Some follow-up works\nattempted to extend the scaling-law to more complex metrics (such as\nhyperparameters), but still lacked a comprehensive analysis of the dynamic\ndifferences among various capabilities during pretraining. To address the\naforementioned limitations, this paper undertakes a comprehensive comparison of\nmodel capabilities at various pretraining intermediate checkpoints. Through\nthis analysis, we confirm that specific downstream metrics exhibit similar\ntraining dynamics across models of different sizes, up to 67 billion\nparameters. In addition to our core findings, we've reproduced Amber and\nOpenLLaMA, releasing their intermediate checkpoints. This initiative offers\nvaluable resources to the research community and facilitates the verification\nand exploration of LLM pretraining by open-source researchers. Besides, we\nprovide empirical summaries, including performance comparisons of different\nmodels and capabilities, and tuition of key metrics for different training\nphases. Based on these findings, we provide a more user-friendly strategy for\nevaluating the optimization state, offering guidance for establishing a stable\npretraining process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering early-stage metrics that reflect final model performance is one\ncore principle for large-scale pretraining. The existing scaling law\ndemonstrates the power-law correlation between pretraining loss and training\nflops, which serves as an important indicator of the current training state for\nlarge language models. However, this principle only focuses on the model's\ncompression properties on the training data, resulting in an inconsistency with\nthe ability improvements on the downstream tasks. Some follow-up works\nattempted to extend the scaling-law to more complex metrics (such as\nhyperparameters), but still lacked a comprehensive analysis of the dynamic\ndifferences among various capabilities during pretraining. To address the\naforementioned limitations, this paper undertakes a comprehensive comparison of\nmodel capabilities at various pretraining intermediate checkpoints. Through\nthis analysis, we confirm that specific downstream metrics exhibit similar\ntraining dynamics across models of different sizes, up to 67 billion\nparameters. In addition to our core findings, we've reproduced Amber and\nOpenLLaMA, releasing their intermediate checkpoints. This initiative offers\nvaluable resources to the research community and facilitates the verification\nand exploration of LLM pretraining by open-source researchers. Besides, we\nprovide empirical summaries, including performance comparisons of different\nmodels and capabilities, and tuition of key metrics for different training\nphases. Based on these findings, we provide a more user-friendly strategy for\nevaluating the optimization state, offering guidance for establishing a stable\npretraining process."
                },
                "authors": [
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Junzhuo Li"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zhaoliang Chen"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Stephen W. Huang"
                    },
                    {
                        "name": "Shawn Yue"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01204v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01204v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19453v2",
                "updated": "2024-11-06T11:49:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    11,
                    49,
                    10,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-25T10:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    28,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework"
                },
                "summary": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research"
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Chenming Shang"
                    },
                    {
                        "name": "Sizhe Wang"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Renliang Sun"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03827v1",
                "updated": "2024-11-06T11:00:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    11,
                    0,
                    44,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T11:00:44Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    11,
                    0,
                    44,
                    2,
                    311,
                    0
                ],
                "title": "DesignMinds: Enhancing Video-Based Design Ideation with Vision-Language\n  Model and Context-Injected Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DesignMinds: Enhancing Video-Based Design Ideation with Vision-Language\n  Model and Context-Injected Large Language Model"
                },
                "summary": "Ideation is a critical component of video-based design (VBD), where videos\nserve as the primary medium for design exploration and inspiration. The\nemergence of generative AI offers considerable potential to enhance this\nprocess by streamlining video analysis and facilitating idea generation. In\nthis paper, we present DesignMinds, a prototype that integrates a\nstate-of-the-art Vision-Language Model (VLM) with a context-enhanced Large\nLanguage Model (LLM) to support ideation in VBD. To evaluate DesignMinds, we\nconducted a between-subject study with 35 design practitioners, comparing its\nperformance to a baseline condition. Our results demonstrate that DesignMinds\nsignificantly enhances the flexibility and originality of ideation, while also\nincreasing task engagement. Importantly, the introduction of this technology\ndid not negatively impact user experience, technology acceptance, or usability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ideation is a critical component of video-based design (VBD), where videos\nserve as the primary medium for design exploration and inspiration. The\nemergence of generative AI offers considerable potential to enhance this\nprocess by streamlining video analysis and facilitating idea generation. In\nthis paper, we present DesignMinds, a prototype that integrates a\nstate-of-the-art Vision-Language Model (VLM) with a context-enhanced Large\nLanguage Model (LLM) to support ideation in VBD. To evaluate DesignMinds, we\nconducted a between-subject study with 35 design practitioners, comparing its\nperformance to a baseline condition. Our results demonstrate that DesignMinds\nsignificantly enhances the flexibility and originality of ideation, while also\nincreasing task engagement. Importantly, the introduction of this technology\ndid not negatively impact user experience, technology acceptance, or usability."
                },
                "authors": [
                    {
                        "name": "Tianhao He"
                    },
                    {
                        "name": "Andrija Stankovic"
                    },
                    {
                        "name": "Evangelos Niforatos"
                    },
                    {
                        "name": "Gerd Kortuem"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Kortuem"
                },
                "author": "Gerd Kortuem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03823v1",
                "updated": "2024-11-06T10:44:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    44,
                    15,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T10:44:15Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    44,
                    15,
                    2,
                    311,
                    0
                ],
                "title": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM\n  Data Contamination"
                },
                "summary": "The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting dataset contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is sensitive to varying degrees of contamination and can\nhighlight significant performance improvements due to leakage of the training\nset of multimodal benchmarks. Furthermore, We also explore the possibility of\ncontamination originating from the pre-training phase of LLMs used by MLLMs and\nthe fine-tuning phase of MLLMs, offering new insights into the stages at which\ncontamination may be introduced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progression of multimodal large language models (MLLMs) has\ndemonstrated superior performance on various multimodal benchmarks. However,\nthe issue of data contamination during training creates challenges in\nperformance evaluation and comparison. While numerous methods exist for\ndetecting dataset contamination in large language models (LLMs), they are less\neffective for MLLMs due to their various modalities and multiple training\nphases. In this study, we introduce a multimodal data contamination detection\nframework, MM-Detect, designed for MLLMs. Our experimental results indicate\nthat MM-Detect is sensitive to varying degrees of contamination and can\nhighlight significant performance improvements due to leakage of the training\nset of multimodal benchmarks. Furthermore, We also explore the possibility of\ncontamination originating from the pre-training phase of LLMs used by MLLMs and\nthe fine-tuning phase of MLLMs, offering new insights into the stages at which\ncontamination may be introduced."
                },
                "authors": [
                    {
                        "name": "Dingjie Song"
                    },
                    {
                        "name": "Sicheng Lai"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03817v1",
                "updated": "2024-11-06T10:35:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    35,
                    11,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T10:35:11Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    35,
                    11,
                    2,
                    311,
                    0
                ],
                "title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning"
                },
                "summary": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Zhirui Deng"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Ruibin Xiong"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03814v1",
                "updated": "2024-11-06T10:32:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    32,
                    9,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T10:32:09Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    32,
                    9,
                    2,
                    311,
                    0
                ],
                "title": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue"
                },
                "summary": "Large Language Models (LLMs) demonstrate outstanding performance in their\nreservoir of knowledge and understanding capabilities, but they have also been\nshown to be prone to illegal or unethical reactions when subjected to jailbreak\nattacks. To ensure their responsible deployment in critical applications, it is\ncrucial to understand the safety capabilities and vulnerabilities of LLMs.\nPrevious works mainly focus on jailbreak in single-round dialogue, overlooking\nthe potential jailbreak risks in multi-round dialogues, which are a vital way\nhumans interact with and extract information from LLMs. Some studies have\nincreasingly concentrated on the risks associated with jailbreak in multi-round\ndialogues. These efforts typically involve the use of manually crafted\ntemplates or prompt engineering techniques. However, due to the inherent\ncomplexity of multi-round dialogues, their jailbreak performance is limited. To\nsolve this problem, we propose a novel multi-round dialogue jailbreaking agent,\nemphasizing the importance of stealthiness in identifying and mitigating\npotential threats to human values posed by LLMs. We propose a risk\ndecomposition strategy that distributes risks across multiple rounds of queries\nand utilizes psychological strategies to enhance attack strength. Extensive\nexperiments show that our proposed method surpasses other attack methods and\nachieves state-of-the-art attack success rate. We will make the corresponding\ncode and dataset available for future research. The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate outstanding performance in their\nreservoir of knowledge and understanding capabilities, but they have also been\nshown to be prone to illegal or unethical reactions when subjected to jailbreak\nattacks. To ensure their responsible deployment in critical applications, it is\ncrucial to understand the safety capabilities and vulnerabilities of LLMs.\nPrevious works mainly focus on jailbreak in single-round dialogue, overlooking\nthe potential jailbreak risks in multi-round dialogues, which are a vital way\nhumans interact with and extract information from LLMs. Some studies have\nincreasingly concentrated on the risks associated with jailbreak in multi-round\ndialogues. These efforts typically involve the use of manually crafted\ntemplates or prompt engineering techniques. However, due to the inherent\ncomplexity of multi-round dialogues, their jailbreak performance is limited. To\nsolve this problem, we propose a novel multi-round dialogue jailbreaking agent,\nemphasizing the importance of stealthiness in identifying and mitigating\npotential threats to human values posed by LLMs. We propose a risk\ndecomposition strategy that distributes risks across multiple rounds of queries\nand utilizes psychological strategies to enhance attack strength. Extensive\nexperiments show that our proposed method surpasses other attack methods and\nachieves state-of-the-art attack success rate. We will make the corresponding\ncode and dataset available for future research. The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Fengxiang Wang"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Peng Xiao"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "YueFeng Chen"
                    },
                    {
                        "name": "Chongwen Wang"
                    },
                    {
                        "name": "Jialing Tao"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Hui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xue"
                },
                "author": "Hui Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18680v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18680v3",
                "updated": "2024-11-06T10:27:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    27,
                    5,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-27T12:06:53Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    12,
                    6,
                    53,
                    4,
                    271,
                    0
                ],
                "title": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large\n  Language Models"
                },
                "summary": "Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various audio-LLMs (ALLMs) have been explored recently for tackling different\naudio tasks simultaneously using a single, unified model. While existing\nevaluations of ALLMs primarily focus on single-audio tasks, real-world\napplications often involve processing multiple audio streams simultaneously. To\nbridge this gap, we propose the first multi-audio evaluation (MAE) benchmark\nthat consists of 20 datasets from 11 multi-audio tasks encompassing both speech\nand sound scenarios. Comprehensive experiments on MAE demonstrate that the\nexisting ALLMs, while being powerful in comprehending primary audio elements in\nindividual audio inputs, struggling to handle multi-audio scenarios. To this\nend, we propose a novel multi-audio-LLM (MALLM) to capture audio context among\nmultiple similar audios using discriminative learning on our proposed synthetic\ndata. The results demonstrate that the proposed MALLM outperforms all baselines\nand achieves high data efficiency using synthetic data without requiring human\nannotations. The proposed MALLM opens the door for ALLMs towards multi-audio\nprocessing era and brings us closer to replicating human auditory capabilities\nin machines."
                },
                "authors": [
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Xianghu Yue"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Luis Fernando D'Haro"
                    },
                    {
                        "name": "Robby T. Tan"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "EMNLP24 Findings. Data available at\n  https://github.com/MatthewCYM/MALLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18680v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18680v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06922v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06922v3",
                "updated": "2024-11-06T10:22:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    22,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-02-10T11:07:24Z",
                "published_parsed": [
                    2024,
                    2,
                    10,
                    11,
                    7,
                    24,
                    5,
                    41,
                    0
                ],
                "title": "Whispers in the Machine: Confidentiality in LLM-integrated Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whispers in the Machine: Confidentiality in LLM-integrated Systems"
                },
                "summary": "Large Language Models (LLMs) are increasingly augmented with external tools\nand commercial services into LLM-integrated systems. While these interfaces can\nsignificantly enhance the capabilities of the models, they also introduce a new\nattack surface. Manipulated integrations, for example, can exploit the model\nand compromise sensitive data accessed through other interfaces. While previous\nwork primarily focused on attacks targeting a model's alignment or the leakage\nof training data, the security of data that is only available during inference\nhas escaped scrutiny so far. In this work, we demonstrate the vulnerabilities\nassociated with external components and introduce a systematic approach to\nevaluate confidentiality risks in LLM-integrated systems. We identify two\nspecific attack scenarios unique to these systems and formalize these into a\ntool-robustness framework designed to measure a model's ability to protect\nsensitive information. Our findings show that all examined models are highly\nvulnerable to confidentiality attacks, with the risk increasing significantly\nwhen models are used together with external tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly augmented with external tools\nand commercial services into LLM-integrated systems. While these interfaces can\nsignificantly enhance the capabilities of the models, they also introduce a new\nattack surface. Manipulated integrations, for example, can exploit the model\nand compromise sensitive data accessed through other interfaces. While previous\nwork primarily focused on attacks targeting a model's alignment or the leakage\nof training data, the security of data that is only available during inference\nhas escaped scrutiny so far. In this work, we demonstrate the vulnerabilities\nassociated with external components and introduce a systematic approach to\nevaluate confidentiality risks in LLM-integrated systems. We identify two\nspecific attack scenarios unique to these systems and formalize these into a\ntool-robustness framework designed to measure a model's ability to protect\nsensitive information. Our findings show that all examined models are highly\nvulnerable to confidentiality attacks, with the risk increasing significantly\nwhen models are used together with external tools."
                },
                "authors": [
                    {
                        "name": "Jonathan Evertz"
                    },
                    {
                        "name": "Merlin Chlosta"
                    },
                    {
                        "name": "Lea Schönherr"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Eisenhofer"
                },
                "author": "Thorsten Eisenhofer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06922v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06922v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03806v1",
                "updated": "2024-11-06T10:06:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    6,
                    21,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T10:06:21Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    6,
                    21,
                    2,
                    311,
                    0
                ],
                "title": "Understanding the Effects of Human-written Paraphrases in LLM-generated\n  Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Effects of Human-written Paraphrases in LLM-generated\n  Text Detection"
                },
                "summary": "Natural Language Generation has been rapidly developing with the advent of\nlarge language models (LLMs). While their usage has sparked significant\nattention from the general public, it is important for readers to be aware when\na piece of text is LLM-generated. This has brought about the need for building\nmodels that enable automated LLM-generated text detection, with the aim of\nmitigating potential negative outcomes of such content. Existing LLM-generated\ndetectors show competitive performances in telling apart LLM-generated and\nhuman-written text, but this performance is likely to deteriorate when\nparaphrased texts are considered. In this study, we devise a new data\ncollection strategy to collect Human & LLM Paraphrase Collection (HLPC), a\nfirst-of-its-kind dataset that incorporates human-written texts and\nparaphrases, as well as LLM-generated texts and paraphrases. With the aim of\nunderstanding the effects of human-written paraphrases on the performance of\nstate-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark\ndetectors, we perform classification experiments that incorporate human-written\nparaphrases, watermarked and non-watermarked LLM-generated documents from GPT\nand OPT, and LLM-generated paraphrases from DIPPER and BART. The results show\nthat the inclusion of human-written paraphrases has a significant impact of\nLLM-generated detector performance, promoting TPR@1%FPR with a possible\ntrade-off of AUROC and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Generation has been rapidly developing with the advent of\nlarge language models (LLMs). While their usage has sparked significant\nattention from the general public, it is important for readers to be aware when\na piece of text is LLM-generated. This has brought about the need for building\nmodels that enable automated LLM-generated text detection, with the aim of\nmitigating potential negative outcomes of such content. Existing LLM-generated\ndetectors show competitive performances in telling apart LLM-generated and\nhuman-written text, but this performance is likely to deteriorate when\nparaphrased texts are considered. In this study, we devise a new data\ncollection strategy to collect Human & LLM Paraphrase Collection (HLPC), a\nfirst-of-its-kind dataset that incorporates human-written texts and\nparaphrases, as well as LLM-generated texts and paraphrases. With the aim of\nunderstanding the effects of human-written paraphrases on the performance of\nstate-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark\ndetectors, we perform classification experiments that incorporate human-written\nparaphrases, watermarked and non-watermarked LLM-generated documents from GPT\nand OPT, and LLM-generated paraphrases from DIPPER and BART. The results show\nthat the inclusion of human-written paraphrases has a significant impact of\nLLM-generated detector performance, promoting TPR@1%FPR with a possible\ntrade-off of AUROC and accuracy."
                },
                "authors": [
                    {
                        "name": "Hiu Ting Lau"
                    },
                    {
                        "name": "Arkaitz Zubiaga"
                    }
                ],
                "author_detail": {
                    "name": "Arkaitz Zubiaga"
                },
                "author": "Arkaitz Zubiaga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03752v2",
                "updated": "2024-11-06T10:05:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    5,
                    18,
                    2,
                    311,
                    0
                ],
                "published": "2024-08-07T13:05:59Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    13,
                    5,
                    59,
                    2,
                    220,
                    0
                ],
                "title": "One-Shot Distributed Node-Specific Signal Estimation with\n  Non-Overlapping Latent Subspaces in Acoustic Sensor Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Shot Distributed Node-Specific Signal Estimation with\n  Non-Overlapping Latent Subspaces in Acoustic Sensor Networks"
                },
                "summary": "A one-shot algorithm called iterationless DANSE (iDANSE) is introduced to\nperform distributed adaptive node-specific signal estimation (DANSE) in a fully\nconnected wireless acoustic sensor network (WASN) deployed in an environment\nwith non-overlapping latent signal subspaces. The iDANSE algorithm matches the\nperformance of a centralized algorithm in a single processing cycle while\ndevices exchange fused versions of their multichannel local microphone signals.\nKey advantages of iDANSE over currently available solutions are its\niterationless nature, which favors deployment in real-time applications, and\nthe fact that devices can exchange fewer fused signals than the number of\nlatent sources in the environment. The proposed method is validated in\nnumerical simulations including a speech enhancement scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A one-shot algorithm called iterationless DANSE (iDANSE) is introduced to\nperform distributed adaptive node-specific signal estimation (DANSE) in a fully\nconnected wireless acoustic sensor network (WASN) deployed in an environment\nwith non-overlapping latent signal subspaces. The iDANSE algorithm matches the\nperformance of a centralized algorithm in a single processing cycle while\ndevices exchange fused versions of their multichannel local microphone signals.\nKey advantages of iDANSE over currently available solutions are its\niterationless nature, which favors deployment in real-time applications, and\nthe fact that devices can exchange fewer fused signals than the number of\nlatent sources in the environment. The proposed method is validated in\nnumerical simulations including a speech enhancement scenario."
                },
                "authors": [
                    {
                        "name": "Paul Didier"
                    },
                    {
                        "name": "Pourya Behmandpoor"
                    },
                    {
                        "name": "Toon van Waterschoot"
                    },
                    {
                        "name": "Marc Moonen"
                    }
                ],
                "author_detail": {
                    "name": "Marc Moonen"
                },
                "author": "Marc Moonen",
                "arxiv_doi": "10.1109/IWAENC61483.2024.10694548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IWAENC61483.2024.10694548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.03752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 2024 18th International Workshop on Acoustic\n  Signal Enhancement (IWAENC)",
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03805v1",
                "updated": "2024-11-06T10:02:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    2,
                    50,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T10:02:50Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    2,
                    50,
                    2,
                    311,
                    0
                ],
                "title": "A Comparative Study of Recent Large Language Models on Generating\n  Hospital Discharge Summaries for Lung Cancer Patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study of Recent Large Language Models on Generating\n  Hospital Discharge Summaries for Lung Cancer Patients"
                },
                "summary": "Generating discharge summaries is a crucial yet time-consuming task in\nclinical practice, essential for conveying pertinent patient information and\nfacilitating continuity of care. Recent advancements in large language models\n(LLMs) have significantly enhanced their capability in understanding and\nsummarizing complex medical texts. This research aims to explore how LLMs can\nalleviate the burden of manual summarization, streamline workflow efficiencies,\nand support informed decision-making in healthcare settings. Clinical notes\nfrom a cohort of 1,099 lung cancer patients were utilized, with a subset of 50\npatients for testing purposes, and 102 patients used for model fine-tuning.\nThis study evaluates the performance of multiple LLMs, including GPT-3.5,\nGPT-4, GPT-4o, and LLaMA 3 8b, in generating discharge summaries. Evaluation\nmetrics included token-level analysis (BLEU, ROUGE-1, ROUGE-2, ROUGE-L) and\nsemantic similarity scores between model-generated summaries and\nphysician-written gold standards. LLaMA 3 8b was further tested on clinical\nnotes of varying lengths to examine the stability of its performance. The study\nfound notable variations in summarization capabilities among LLMs. GPT-4o and\nfine-tuned LLaMA 3 demonstrated superior token-level evaluation metrics, while\nLLaMA 3 consistently produced concise summaries across different input lengths.\nSemantic similarity scores indicated GPT-4o and LLaMA 3 as leading models in\ncapturing clinical relevance. This study contributes insights into the efficacy\nof LLMs for generating discharge summaries, highlighting LLaMA 3's robust\nperformance in maintaining clarity and relevance across varying clinical\ncontexts. These findings underscore the potential of automated summarization\ntools to enhance documentation precision and efficiency, ultimately improving\npatient care and operational capability in healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating discharge summaries is a crucial yet time-consuming task in\nclinical practice, essential for conveying pertinent patient information and\nfacilitating continuity of care. Recent advancements in large language models\n(LLMs) have significantly enhanced their capability in understanding and\nsummarizing complex medical texts. This research aims to explore how LLMs can\nalleviate the burden of manual summarization, streamline workflow efficiencies,\nand support informed decision-making in healthcare settings. Clinical notes\nfrom a cohort of 1,099 lung cancer patients were utilized, with a subset of 50\npatients for testing purposes, and 102 patients used for model fine-tuning.\nThis study evaluates the performance of multiple LLMs, including GPT-3.5,\nGPT-4, GPT-4o, and LLaMA 3 8b, in generating discharge summaries. Evaluation\nmetrics included token-level analysis (BLEU, ROUGE-1, ROUGE-2, ROUGE-L) and\nsemantic similarity scores between model-generated summaries and\nphysician-written gold standards. LLaMA 3 8b was further tested on clinical\nnotes of varying lengths to examine the stability of its performance. The study\nfound notable variations in summarization capabilities among LLMs. GPT-4o and\nfine-tuned LLaMA 3 demonstrated superior token-level evaluation metrics, while\nLLaMA 3 consistently produced concise summaries across different input lengths.\nSemantic similarity scores indicated GPT-4o and LLaMA 3 as leading models in\ncapturing clinical relevance. This study contributes insights into the efficacy\nof LLMs for generating discharge summaries, highlighting LLaMA 3's robust\nperformance in maintaining clarity and relevance across varying clinical\ncontexts. These findings underscore the potential of automated summarization\ntools to enhance documentation precision and efficiency, ultimately improving\npatient care and operational capability in healthcare settings."
                },
                "authors": [
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Fang Li"
                    },
                    {
                        "name": "Kirk Roberts"
                    },
                    {
                        "name": "Licong Cui"
                    },
                    {
                        "name": "Cui Tao"
                    },
                    {
                        "name": "Hua Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xu"
                },
                "author": "Hua Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03766v1",
                "updated": "2024-11-06T08:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    59,
                    44,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T08:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    59,
                    44,
                    2,
                    311,
                    0
                ],
                "title": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It"
                },
                "summary": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as special tokenizers, PEs, and\nnumber formats), comprehensively evaluating their effectiveness using our\ntestbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and\nfind that 1) naive finetuning can improve NUPA a lot on many but not all tasks,\nand 2) surprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work takes a preliminary step towards understanding and\nimproving NUPA of LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as special tokenizers, PEs, and\nnumber formats), comprehensively evaluating their effectiveness using our\ntestbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and\nfind that 1) naive finetuning can improve NUPA a lot on many but not all tasks,\nand 2) surprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work takes a preliminary step towards understanding and\nimproving NUPA of LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook."
                },
                "authors": [
                    {
                        "name": "Haotong Yang"
                    },
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Shijia Kang"
                    },
                    {
                        "name": "Zhouchen Lin"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02603v2",
                "updated": "2024-11-06T08:51:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    51,
                    52,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T20:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    20,
                    53,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "FactTest: Factuality Testing in Large Language Models with Finite-Sample\n  and Distribution-Free Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FactTest: Factuality Testing in Large Language Models with Finite-Sample\n  and Distribution-Free Guarantees"
                },
                "summary": "The propensity of Large Language Models (LLMs) to generate hallucinations and\nnon-factual content undermines their reliability in high-stakes domains, where\nrigorous control over Type I errors (the conditional probability of incorrectly\nclassifying hallucinations as truthful content) is essential. Despite its\nimportance, formal verification of LLM factuality with such guarantees remains\nlargely unexplored. In this paper, we introduce FactTest, a novel framework\nthat statistically assesses whether an LLM can confidently provide correct\nanswers to given questions with high-probability correctness guarantees. We\nformulate factuality testing as hypothesis testing problem to enforce an upper\nbound of Type I errors at user-specified significance levels. Notably, we prove\nthat our framework also ensures strong Type II error control under mild\nconditions and can be extended to maintain its effectiveness when covariate\nshifts exist. %These analyses are amenable to the principled NP framework. Our\napproach is distribution-free and works for any number of human-annotated\nsamples. It is model-agnostic and applies to any black-box or white-box LM.\nExtensive experiments on question-answering (QA) and multiple-choice benchmarks\ndemonstrate that \\approach effectively detects hallucinations and improves the\nmodel's ability to abstain from answering unknown questions, leading to an over\n40% accuracy improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The propensity of Large Language Models (LLMs) to generate hallucinations and\nnon-factual content undermines their reliability in high-stakes domains, where\nrigorous control over Type I errors (the conditional probability of incorrectly\nclassifying hallucinations as truthful content) is essential. Despite its\nimportance, formal verification of LLM factuality with such guarantees remains\nlargely unexplored. In this paper, we introduce FactTest, a novel framework\nthat statistically assesses whether an LLM can confidently provide correct\nanswers to given questions with high-probability correctness guarantees. We\nformulate factuality testing as hypothesis testing problem to enforce an upper\nbound of Type I errors at user-specified significance levels. Notably, we prove\nthat our framework also ensures strong Type II error control under mild\nconditions and can be extended to maintain its effectiveness when covariate\nshifts exist. %These analyses are amenable to the principled NP framework. Our\napproach is distribution-free and works for any number of human-annotated\nsamples. It is model-agnostic and applies to any black-box or white-box LM.\nExtensive experiments on question-answering (QA) and multiple-choice benchmarks\ndemonstrate that \\approach effectively detects hallucinations and improves the\nmodel's ability to abstain from answering unknown questions, leading to an over\n40% accuracy improvement."
                },
                "authors": [
                    {
                        "name": "Fan Nie"
                    },
                    {
                        "name": "Xiaotian Hou"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Linjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linjun Zhang"
                },
                "author": "Linjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21052v2",
                "updated": "2024-11-06T08:44:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    44,
                    3,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-28T14:07:41Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    7,
                    41,
                    0,
                    302,
                    0
                ],
                "title": "Getting By Goal Misgeneralization With a Little Help From a Mentor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Getting By Goal Misgeneralization With a Little Help From a Mentor"
                },
                "summary": "While reinforcement learning (RL) agents often perform well during training,\nthey can struggle with distribution shift in real-world deployments. One\nparticularly severe risk of distribution shift is goal misgeneralization, where\nthe agent learns a proxy goal that coincides with the true goal during training\nbut not during deployment. In this paper, we explore whether allowing an agent\nto ask for help from a supervisor in unfamiliar situations can mitigate this\nissue. We focus on agents trained with PPO in the CoinRun environment, a\nsetting known to exhibit goal misgeneralization. We evaluate multiple methods\nfor determining when the agent should request help and find that asking for\nhelp consistently improves performance. However, we also find that methods\nbased on the agent's internal state fail to proactively request help, instead\nwaiting until mistakes have already occurred. Further investigation suggests\nthat the agent's internal state does not represent the coin at all,\nhighlighting the importance of learning nuanced representations, the risks of\nignoring everything not immediately relevant to reward, and the necessity of\ndeveloping ask-for-help strategies tailored to the agent's training algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning (RL) agents often perform well during training,\nthey can struggle with distribution shift in real-world deployments. One\nparticularly severe risk of distribution shift is goal misgeneralization, where\nthe agent learns a proxy goal that coincides with the true goal during training\nbut not during deployment. In this paper, we explore whether allowing an agent\nto ask for help from a supervisor in unfamiliar situations can mitigate this\nissue. We focus on agents trained with PPO in the CoinRun environment, a\nsetting known to exhibit goal misgeneralization. We evaluate multiple methods\nfor determining when the agent should request help and find that asking for\nhelp consistently improves performance. However, we also find that methods\nbased on the agent's internal state fail to proactively request help, instead\nwaiting until mistakes have already occurred. Further investigation suggests\nthat the agent's internal state does not represent the coin at all,\nhighlighting the importance of learning nuanced representations, the risks of\nignoring everything not immediately relevant to reward, and the necessity of\ndeveloping ask-for-help strategies tailored to the agent's training algorithm."
                },
                "authors": [
                    {
                        "name": "Tu Trinh"
                    },
                    {
                        "name": "Mohamad H. Danesh"
                    },
                    {
                        "name": "Nguyen X. Khanh"
                    },
                    {
                        "name": "Benjamin Plaut"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Plaut"
                },
                "author": "Benjamin Plaut",
                "arxiv_comment": "SATA Workshop @ NeurIPS 2024 (Towards Safe and Trustworthy Agents)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13824v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13824v3",
                "updated": "2024-11-06T08:29:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    29,
                    22,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-17T17:48:54Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    48,
                    54,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Webpage UIs for Text-Rich Visual Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Webpage UIs for Text-Rich Visual Understanding"
                },
                "summary": "Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios."
                },
                "authors": [
                    {
                        "name": "Junpeng Liu"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Yuxiao Qu"
                    },
                    {
                        "name": "Wai Lam"
                    },
                    {
                        "name": "Chenyan Xiong"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Xiang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yue"
                },
                "author": "Xiang Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13824v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13824v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03743v1",
                "updated": "2024-11-06T08:16:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    16,
                    56,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T08:16:56Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    16,
                    56,
                    2,
                    311,
                    0
                ],
                "title": "Automating Exploratory Proteomics Research via Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Exploratory Proteomics Research via Language Models"
                },
                "summary": "With the development of artificial intelligence, its contribution to science\nis evolving from simulating a complex problem to automating entire research\nprocesses and producing novel discoveries. Achieving this advancement requires\nboth specialized general models grounded in real-world scientific data and\niterative, exploratory frameworks that mirror human scientific methodologies.\nIn this paper, we present PROTEUS, a fully automated system for scientific\ndiscovery from raw proteomics data. PROTEUS uses large language models (LLMs)\nto perform hierarchical planning, execute specialized bioinformatics tools, and\niteratively refine analysis workflows to generate high-quality scientific\nhypotheses. The system takes proteomics datasets as input and produces a\ncomprehensive set of research objectives, analysis results, and novel\nbiological hypotheses without human intervention. We evaluated PROTEUS on 12\nproteomics datasets collected from various biological samples (e.g. immune\ncells, tumors) and different sample types (single-cell and bulk), generating\n191 scientific hypotheses. These were assessed using both automatic LLM-based\nscoring on 5 metrics and detailed reviews from human experts. Results\ndemonstrate that PROTEUS consistently produces reliable, logically coherent\nresults that align well with existing literature while also proposing novel,\nevaluable hypotheses. The system's flexible architecture facilitates seamless\nintegration of diverse analysis tools and adaptation to different proteomics\ndata types. By automating complex proteomics analysis workflows and hypothesis\ngeneration, PROTEUS has the potential to considerably accelerate the pace of\nscientific discovery in proteomics research, enabling researchers to\nefficiently explore large-scale datasets and uncover biological insights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of artificial intelligence, its contribution to science\nis evolving from simulating a complex problem to automating entire research\nprocesses and producing novel discoveries. Achieving this advancement requires\nboth specialized general models grounded in real-world scientific data and\niterative, exploratory frameworks that mirror human scientific methodologies.\nIn this paper, we present PROTEUS, a fully automated system for scientific\ndiscovery from raw proteomics data. PROTEUS uses large language models (LLMs)\nto perform hierarchical planning, execute specialized bioinformatics tools, and\niteratively refine analysis workflows to generate high-quality scientific\nhypotheses. The system takes proteomics datasets as input and produces a\ncomprehensive set of research objectives, analysis results, and novel\nbiological hypotheses without human intervention. We evaluated PROTEUS on 12\nproteomics datasets collected from various biological samples (e.g. immune\ncells, tumors) and different sample types (single-cell and bulk), generating\n191 scientific hypotheses. These were assessed using both automatic LLM-based\nscoring on 5 metrics and detailed reviews from human experts. Results\ndemonstrate that PROTEUS consistently produces reliable, logically coherent\nresults that align well with existing literature while also proposing novel,\nevaluable hypotheses. The system's flexible architecture facilitates seamless\nintegration of diverse analysis tools and adaptation to different proteomics\ndata types. By automating complex proteomics analysis workflows and hypothesis\ngeneration, PROTEUS has the potential to considerably accelerate the pace of\nscientific discovery in proteomics research, enabling researchers to\nefficiently explore large-scale datasets and uncover biological insights."
                },
                "authors": [
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Linhai Xie"
                    },
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Zaoqu Liu"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Yibai Xiong"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Zhangren Chen"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Fuchu He"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11541v2",
                "updated": "2024-11-06T08:07:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    7,
                    50,
                    2,
                    311,
                    0
                ],
                "published": "2024-05-19T13:11:48Z",
                "published_parsed": [
                    2024,
                    5,
                    19,
                    13,
                    11,
                    48,
                    6,
                    140,
                    0
                ],
                "title": "R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-NeRF: Neural Radiance Fields for Modeling RIS-enabled Wireless\n  Environments"
                },
                "summary": "Recently, ray tracing has gained renewed interest with the advent of\nReflective Intelligent Surfaces (RIS) technology, a key enabler of 6G wireless\ncommunications due to its capability of intelligent manipulation of\nelectromagnetic waves. However, accurately modeling RIS-enabled wireless\nenvironments poses significant challenges due to the complex variations caused\nby various environmental factors and the mobility of RISs. In this paper, we\npropose a novel modeling approach using Neural Radiance Fields (NeRF) to\ncharacterize the dynamics of electromagnetic fields in such environments. Our\nmethod utilizes NeRF-based ray tracing to intuitively capture and visualize the\ncomplex dynamics of signal propagation, effectively modeling the complete\nsignal pathways from the transmitter to the RIS, and from the RIS to the\nreceiver. This two-stage process accurately characterizes multiple complex\ntransmission paths, enhancing our understanding of signal behavior in\nreal-world scenarios. Our approach predicts the signal field for any specified\nRIS placement and receiver location, facilitating efficient RIS deployment.\nExperimental evaluations using both simulated and real-world data validate the\nsignificant benefits of our methodology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, ray tracing has gained renewed interest with the advent of\nReflective Intelligent Surfaces (RIS) technology, a key enabler of 6G wireless\ncommunications due to its capability of intelligent manipulation of\nelectromagnetic waves. However, accurately modeling RIS-enabled wireless\nenvironments poses significant challenges due to the complex variations caused\nby various environmental factors and the mobility of RISs. In this paper, we\npropose a novel modeling approach using Neural Radiance Fields (NeRF) to\ncharacterize the dynamics of electromagnetic fields in such environments. Our\nmethod utilizes NeRF-based ray tracing to intuitively capture and visualize the\ncomplex dynamics of signal propagation, effectively modeling the complete\nsignal pathways from the transmitter to the RIS, and from the RIS to the\nreceiver. This two-stage process accurately characterizes multiple complex\ntransmission paths, enhancing our understanding of signal behavior in\nreal-world scenarios. Our approach predicts the signal field for any specified\nRIS placement and receiver location, facilitating efficient RIS deployment.\nExperimental evaluations using both simulated and real-world data validate the\nsignificant benefits of our methodology."
                },
                "authors": [
                    {
                        "name": "Huiying Yang"
                    },
                    {
                        "name": "Zihan Jin"
                    },
                    {
                        "name": "Chenhao Wu"
                    },
                    {
                        "name": "Rujing Xiong"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Zenan Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zenan Ling"
                },
                "author": "Zenan Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05408v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05408v2",
                "updated": "2024-11-06T07:08:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    8,
                    58,
                    2,
                    311,
                    0
                ],
                "published": "2024-03-08T16:06:54Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    16,
                    6,
                    54,
                    4,
                    68,
                    0
                ],
                "title": "FedFMS: Exploring Federated Foundation Models for Medical Image\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedFMS: Exploring Federated Foundation Models for Medical Image\n  Segmentation"
                },
                "summary": "Medical image segmentation is crucial for clinical diagnosis. The\nSegmentation Anything Model (SAM) serves as a powerful foundation model for\nvisual segmentation and can be adapted for medical image segmentation. However,\nmedical imaging data typically contain privacy-sensitive information, making it\nchallenging to train foundation models with centralized storage and sharing. To\ndate, there are few foundation models tailored for medical image deployment\nwithin the federated learning framework, and the segmentation performance, as\nwell as the efficiency of communication and training, remain unexplored. In\nresponse to these issues, we developed Federated Foundation models for Medical\nimage Segmentation (FedFMS), which includes the Federated SAM (FedSAM) and a\ncommunication and training-efficient Federated SAM with Medical SAM Adapter\n(FedMSA). Comprehensive experiments on diverse datasets are conducted to\ninvestigate the performance disparities between centralized training and\nfederated learning across various configurations of FedFMS. The experiments\nrevealed that FedFMS could achieve performance comparable to models trained via\ncentralized training methods while maintaining privacy. Furthermore, FedMSA\ndemonstrated the potential to enhance communication and training efficiency.\nOur model implementation codes are available at\nhttps://github.com/LIU-YUXI/FedFMS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image segmentation is crucial for clinical diagnosis. The\nSegmentation Anything Model (SAM) serves as a powerful foundation model for\nvisual segmentation and can be adapted for medical image segmentation. However,\nmedical imaging data typically contain privacy-sensitive information, making it\nchallenging to train foundation models with centralized storage and sharing. To\ndate, there are few foundation models tailored for medical image deployment\nwithin the federated learning framework, and the segmentation performance, as\nwell as the efficiency of communication and training, remain unexplored. In\nresponse to these issues, we developed Federated Foundation models for Medical\nimage Segmentation (FedFMS), which includes the Federated SAM (FedSAM) and a\ncommunication and training-efficient Federated SAM with Medical SAM Adapter\n(FedMSA). Comprehensive experiments on diverse datasets are conducted to\ninvestigate the performance disparities between centralized training and\nfederated learning across various configurations of FedFMS. The experiments\nrevealed that FedFMS could achieve performance comparable to models trained via\ncentralized training methods while maintaining privacy. Furthermore, FedMSA\ndemonstrated the potential to enhance communication and training efficiency.\nOur model implementation codes are available at\nhttps://github.com/LIU-YUXI/FedFMS."
                },
                "authors": [
                    {
                        "name": "Yuxi Liu"
                    },
                    {
                        "name": "Guibo Luo"
                    },
                    {
                        "name": "Yuesheng Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuesheng Zhu"
                },
                "author": "Yuesheng Zhu",
                "arxiv_comment": "Accepted by MICCAI'2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05408v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05408v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.6; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03700v1",
                "updated": "2024-11-06T06:50:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    50,
                    50,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T06:50:50Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    50,
                    50,
                    2,
                    311,
                    0
                ],
                "title": "The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms\n  in Aligned Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms\n  in Aligned Language Models"
                },
                "summary": "Natural-language assistants are designed to provide users with helpful\nresponses while avoiding harmful outputs, largely achieved through alignment to\nhuman preferences. Yet there is limited understanding of whether alignment\ntechniques may inadvertently perpetuate or even amplify harmful biases\ninherited from their pre-aligned base models. This issue is compounded by the\nchoice of bias evaluation benchmarks in popular preference-finetuned models,\nwhich predominantly focus on dominant social categories, such as binary gender,\nthereby limiting insights into biases affecting underrepresented groups.\nTowards addressing this gap, we center transgender, nonbinary, and other\ngender-diverse identities to investigate how alignment procedures interact with\npre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a\ncomprehensive survey of bias evaluation modalities across leading\npreference-finetuned LLMs, highlighting critical gaps in gender-diverse\nrepresentation, 2) systematic evaluation of gender-diverse biases across 12\nmodels spanning Direct Preference Optimization (DPO) stages, uncovering harms\npopular bias benchmarks fail to detect, and 3) a flexible framework for\nmeasuring harmful biases in implicit reward signals applicable to other social\ncontexts. Our findings reveal that DPO-aligned models are particularly\nsensitive to supervised finetuning (SFT), and can amplify two forms of\nreal-world gender-diverse harms from their base models: stigmatization and\ngender non-affirmative language. We conclude with recommendations tailored to\nDPO and broader alignment practices, advocating for the adoption of\ncommunity-informed bias evaluation frameworks to more effectively identify and\naddress underrepresented harms in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural-language assistants are designed to provide users with helpful\nresponses while avoiding harmful outputs, largely achieved through alignment to\nhuman preferences. Yet there is limited understanding of whether alignment\ntechniques may inadvertently perpetuate or even amplify harmful biases\ninherited from their pre-aligned base models. This issue is compounded by the\nchoice of bias evaluation benchmarks in popular preference-finetuned models,\nwhich predominantly focus on dominant social categories, such as binary gender,\nthereby limiting insights into biases affecting underrepresented groups.\nTowards addressing this gap, we center transgender, nonbinary, and other\ngender-diverse identities to investigate how alignment procedures interact with\npre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a\ncomprehensive survey of bias evaluation modalities across leading\npreference-finetuned LLMs, highlighting critical gaps in gender-diverse\nrepresentation, 2) systematic evaluation of gender-diverse biases across 12\nmodels spanning Direct Preference Optimization (DPO) stages, uncovering harms\npopular bias benchmarks fail to detect, and 3) a flexible framework for\nmeasuring harmful biases in implicit reward signals applicable to other social\ncontexts. Our findings reveal that DPO-aligned models are particularly\nsensitive to supervised finetuning (SFT), and can amplify two forms of\nreal-world gender-diverse harms from their base models: stigmatization and\ngender non-affirmative language. We conclude with recommendations tailored to\nDPO and broader alignment practices, advocating for the adoption of\ncommunity-informed bias evaluation frameworks to more effectively identify and\naddress underrepresented harms in LLMs."
                },
                "authors": [
                    {
                        "name": "Anaelia Ovalle"
                    },
                    {
                        "name": "Krunoslav Lehman Pavasovic"
                    },
                    {
                        "name": "Louis Martin"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Eric Michael Smith"
                    },
                    {
                        "name": "Adina Williams"
                    },
                    {
                        "name": "Levent Sagun"
                    }
                ],
                "author_detail": {
                    "name": "Levent Sagun"
                },
                "author": "Levent Sagun",
                "arxiv_comment": "Accepted to 2024 Neurips Queer in AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01345v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01345v3",
                "updated": "2024-11-06T06:28:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    28,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-05-02T14:49:50Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    14,
                    49,
                    50,
                    3,
                    123,
                    0
                ],
                "title": "The Power of Question Translation Training in Multilingual Reasoning:\n  Broadened Scope and Deepened Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Question Translation Training in Multilingual Reasoning:\n  Broadened Scope and Deepened Insights"
                },
                "summary": "Bridging the significant gap between large language model's English and\nnon-English performance presents a great challenge. While some previous studies\nattempt to mitigate this gap with translated training data, the recently\nproposed question alignment framework leverages the model's English expertise\nto improve multilingual performance with minimum usage of expensive,\nerror-prone translation. In this paper, we explore how broadly this method can\nbe applied by examining its effects in reasoning with and without\nchain-of-thought, as well as with program-of-thought. We also explore applying\nthis framework to extremely large language models in an efficient manner, such\nas through proxy-tuning. Experiment results on multilingual reasoning\nbenchmarks mGSM, mSVAMP, xCSQA and xNLI demonstrate that we can extend question\nalignment framework to boost multilingual performance across diverse reasoning\nscenarios, model families, and sizes. For instance, when applied to the LLaMA2\nmodels, it brings an average accuracy improvements of 12.2% on mGSM even with\nthe 70B model. To understand the mechanism of its success, we analyze\nrepresentation space, generated response and data scales, and reveal how\nquestion translation training strengthens language alignment within LLMs and\nshapes their working patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the significant gap between large language model's English and\nnon-English performance presents a great challenge. While some previous studies\nattempt to mitigate this gap with translated training data, the recently\nproposed question alignment framework leverages the model's English expertise\nto improve multilingual performance with minimum usage of expensive,\nerror-prone translation. In this paper, we explore how broadly this method can\nbe applied by examining its effects in reasoning with and without\nchain-of-thought, as well as with program-of-thought. We also explore applying\nthis framework to extremely large language models in an efficient manner, such\nas through proxy-tuning. Experiment results on multilingual reasoning\nbenchmarks mGSM, mSVAMP, xCSQA and xNLI demonstrate that we can extend question\nalignment framework to boost multilingual performance across diverse reasoning\nscenarios, model families, and sizes. For instance, when applied to the LLaMA2\nmodels, it brings an average accuracy improvements of 12.2% on mGSM even with\nthe 70B model. To understand the mechanism of its success, we analyze\nrepresentation space, generated response and data scales, and reveal how\nquestion translation training strengthens language alignment within LLMs and\nshapes their working patterns."
                },
                "authors": [
                    {
                        "name": "Wenhao Zhu"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Fei Yuan"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01345v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01345v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11650v2",
                "updated": "2024-11-06T06:25:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    25,
                    49,
                    2,
                    311,
                    0
                ],
                "published": "2024-08-21T14:24:04Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    14,
                    24,
                    4,
                    2,
                    234,
                    0
                ],
                "title": "CIPHER: Cybersecurity Intelligent Penetration-testing Helper for Ethical\n  Researcher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CIPHER: Cybersecurity Intelligent Penetration-testing Helper for Ethical\n  Researcher"
                },
                "summary": "Penetration testing, a critical component of cybersecurity, typically\nrequires extensive time and effort to find vulnerabilities. Beginners in this\nfield often benefit from collaborative approaches with the community or\nexperts. To address this, we develop CIPHER (Cybersecurity Intelligent\nPenetration-testing Helper for Ethical Researchers), a large language model\nspecifically trained to assist in penetration testing tasks. We trained CIPHER\nusing over 300 high-quality write-ups of vulnerable machines, hacking\ntechniques, and documentation of open-source penetration testing tools.\nAdditionally, we introduced the Findings, Action, Reasoning, and Results (FARR)\nFlow augmentation, a novel method to augment penetration testing write-ups to\nestablish a fully automated pentesting simulation benchmark tailored for large\nlanguage models. This approach fills a significant gap in traditional\ncybersecurity Q\\&A benchmarks and provides a realistic and rigorous standard\nfor evaluating AI's technical knowledge, reasoning capabilities, and practical\nutility in dynamic penetration testing scenarios. In our assessments, CIPHER\nachieved the best overall performance in providing accurate suggestion\nresponses compared to other open-source penetration testing models of similar\nsize and even larger state-of-the-art models like Llama 3 70B and Qwen1.5 72B\nChat, particularly on insane difficulty machine setups. This demonstrates that\nthe current capabilities of general LLMs are insufficient for effectively\nguiding users through the penetration testing process. We also discuss the\npotential for improvement through scaling and the development of better\nbenchmarks using FARR Flow augmentation results. Our benchmark will be released\npublicly at https://github.com/ibndias/CIPHER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration testing, a critical component of cybersecurity, typically\nrequires extensive time and effort to find vulnerabilities. Beginners in this\nfield often benefit from collaborative approaches with the community or\nexperts. To address this, we develop CIPHER (Cybersecurity Intelligent\nPenetration-testing Helper for Ethical Researchers), a large language model\nspecifically trained to assist in penetration testing tasks. We trained CIPHER\nusing over 300 high-quality write-ups of vulnerable machines, hacking\ntechniques, and documentation of open-source penetration testing tools.\nAdditionally, we introduced the Findings, Action, Reasoning, and Results (FARR)\nFlow augmentation, a novel method to augment penetration testing write-ups to\nestablish a fully automated pentesting simulation benchmark tailored for large\nlanguage models. This approach fills a significant gap in traditional\ncybersecurity Q\\&A benchmarks and provides a realistic and rigorous standard\nfor evaluating AI's technical knowledge, reasoning capabilities, and practical\nutility in dynamic penetration testing scenarios. In our assessments, CIPHER\nachieved the best overall performance in providing accurate suggestion\nresponses compared to other open-source penetration testing models of similar\nsize and even larger state-of-the-art models like Llama 3 70B and Qwen1.5 72B\nChat, particularly on insane difficulty machine setups. This demonstrates that\nthe current capabilities of general LLMs are insufficient for effectively\nguiding users through the penetration testing process. We also discuss the\npotential for improvement through scaling and the development of better\nbenchmarks using FARR Flow augmentation results. Our benchmark will be released\npublicly at https://github.com/ibndias/CIPHER."
                },
                "authors": [
                    {
                        "name": "Derry Pratama"
                    },
                    {
                        "name": "Naufal Suryanto"
                    },
                    {
                        "name": "Andro Aprila Adiputra"
                    },
                    {
                        "name": "Thi-Thu-Huong Le"
                    },
                    {
                        "name": "Ahmada Yusril Kadiptya"
                    },
                    {
                        "name": "Muhammad Iqbal"
                    },
                    {
                        "name": "Howon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Howon Kim"
                },
                "author": "Howon Kim",
                "arxiv_doi": "10.3390/s24216878",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/s24216878",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.11650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "28 pages, github available",
                "arxiv_journal_ref": "Sensors 2024, 24(21), 6878;",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03687v1",
                "updated": "2024-11-06T06:13:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    13,
                    57,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T06:13:57Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    13,
                    57,
                    2,
                    311,
                    0
                ],
                "title": "Beyond Model Adaptation at Test Time: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Model Adaptation at Test Time: A Survey"
                },
                "summary": "Machine learning algorithms have achieved remarkable success across various\ndisciplines, use cases and applications, under the prevailing assumption that\ntraining and test samples are drawn from the same distribution. Consequently,\nthese algorithms struggle and become brittle even when samples in the test\ndistribution start to deviate from the ones observed during training. Domain\nadaptation and domain generalization have been studied extensively as\napproaches to address distribution shifts across test and train domains, but\neach has its limitations. Test-time adaptation, a recently emerging learning\nparadigm, combines the benefits of domain adaptation and domain generalization\nby training models only on source data and adapting them to target data during\ntest-time inference. In this survey, we provide a comprehensive and systematic\nreview on test-time adaptation, covering more than 400 recent papers. We\nstructure our review by categorizing existing methods into five distinct\ncategories based on what component of the method is adjusted for test-time\nadaptation: the model, the inference, the normalization, the sample, or the\nprompt, providing detailed analysis of each. We further discuss the various\npreparation and adaptation settings for methods within these categories,\noffering deeper insights into the effective deployment for the evaluation of\ndistribution shifts and their real-world application in understanding images,\nvideo and 3D, as well as modalities beyond vision. We close the survey with an\noutlook on emerging research opportunities for test-time adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning algorithms have achieved remarkable success across various\ndisciplines, use cases and applications, under the prevailing assumption that\ntraining and test samples are drawn from the same distribution. Consequently,\nthese algorithms struggle and become brittle even when samples in the test\ndistribution start to deviate from the ones observed during training. Domain\nadaptation and domain generalization have been studied extensively as\napproaches to address distribution shifts across test and train domains, but\neach has its limitations. Test-time adaptation, a recently emerging learning\nparadigm, combines the benefits of domain adaptation and domain generalization\nby training models only on source data and adapting them to target data during\ntest-time inference. In this survey, we provide a comprehensive and systematic\nreview on test-time adaptation, covering more than 400 recent papers. We\nstructure our review by categorizing existing methods into five distinct\ncategories based on what component of the method is adjusted for test-time\nadaptation: the model, the inference, the normalization, the sample, or the\nprompt, providing detailed analysis of each. We further discuss the various\npreparation and adaptation settings for methods within these categories,\noffering deeper insights into the effective deployment for the evaluation of\ndistribution shifts and their real-world application in understanding images,\nvideo and 3D, as well as modalities beyond vision. We close the survey with an\noutlook on emerging research opportunities for test-time adaptation."
                },
                "authors": [
                    {
                        "name": "Zehao Xiao"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    }
                ],
                "author_detail": {
                    "name": "Cees G. M. Snoek"
                },
                "author": "Cees G. M. Snoek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03682v1",
                "updated": "2024-11-06T06:06:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    6,
                    7,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T06:06:07Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    6,
                    6,
                    7,
                    2,
                    311,
                    0
                ],
                "title": "LEGATO: Cross-Embodiment Imitation Using a Grasping Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEGATO: Cross-Embodiment Imitation Using a Grasping Tool"
                },
                "summary": "Cross-embodiment imitation learning enables policies trained on specific\nembodiments to transfer across different robots, unlocking the potential for\nlarge-scale imitation learning that is both cost-effective and highly reusable.\nThis paper presents LEGATO, a cross-embodiment imitation learning framework for\nvisuomotor skill transfer across varied kinematic morphologies. We introduce a\nhandheld gripper that unifies action and observation spaces, allowing tasks to\nbe defined consistently across robots. Using this gripper, we train visuomotor\npolicies via imitation learning, applying a motion-invariant transformation to\ncompute the training loss. Gripper motions are then retargeted into\nhigh-degree-of-freedom whole-body motions using inverse kinematics for\ndeployment across diverse embodiments. Our evaluations in simulation and\nreal-robot experiments highlight the framework's effectiveness in learning and\ntransferring visuomotor skills across various robots. More information can be\nfound at the project page: https://ut-hcrl.github.io/LEGATO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-embodiment imitation learning enables policies trained on specific\nembodiments to transfer across different robots, unlocking the potential for\nlarge-scale imitation learning that is both cost-effective and highly reusable.\nThis paper presents LEGATO, a cross-embodiment imitation learning framework for\nvisuomotor skill transfer across varied kinematic morphologies. We introduce a\nhandheld gripper that unifies action and observation spaces, allowing tasks to\nbe defined consistently across robots. Using this gripper, we train visuomotor\npolicies via imitation learning, applying a motion-invariant transformation to\ncompute the training loss. Gripper motions are then retargeted into\nhigh-degree-of-freedom whole-body motions using inverse kinematics for\ndeployment across diverse embodiments. Our evaluations in simulation and\nreal-robot experiments highlight the framework's effectiveness in learning and\ntransferring visuomotor skills across various robots. More information can be\nfound at the project page: https://ut-hcrl.github.io/LEGATO."
                },
                "authors": [
                    {
                        "name": "Mingyo Seo"
                    },
                    {
                        "name": "H. Andy Park"
                    },
                    {
                        "name": "Shenli Yuan"
                    },
                    {
                        "name": "Yuke Zhu"
                    },
                    {
                        "name": "Luis Sentis"
                    }
                ],
                "author_detail": {
                    "name": "Luis Sentis"
                },
                "author": "Luis Sentis",
                "arxiv_comment": "Submitted to RA-L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17812v2",
                "updated": "2024-11-06T05:33:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    33,
                    16,
                    2,
                    311,
                    0
                ],
                "published": "2024-02-27T14:51:11Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    14,
                    51,
                    11,
                    1,
                    58,
                    0
                ],
                "title": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping\n  Backward Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping\n  Backward Propagation"
                },
                "summary": "Large language models (LLMs) have achieved significant success across various\ndomains. However, training these LLMs typically involves substantial memory and\ncomputational costs during both forward and backward propagation. While\nparameter-efficient fine-tuning (PEFT) considerably reduces the training memory\nassociated with parameters, it does not address the significant computational\ncosts and activation memory. In this paper, we propose Dropping Backward\nPropagation (DropBP), a novel approach designed to reduce computational costs\nand activation memory while maintaining accuracy. DropBP randomly drops layers\nduring backward propagation, which is essentially equivalent to training\nshallow submodules generated by undropped layers and residual connections.\nAdditionally, DropBP calculates the sensitivity of each layer to assign an\nappropriate drop rate, thereby stabilizing the training process. DropBP is not\nonly applicable to full fine-tuning but can also be orthogonally integrated\nwith all types of PEFT by dropping layers during backward propagation.\nSpecifically, DropBP can reduce training time by 44% with comparable accuracy\nto the baseline, accelerate convergence to the same perplexity by 1.5x, and\nenable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.\nFurthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100\nGPU and 117% on an Intel Gaudi2 HPU. The code is available at\nhttps://github.com/WooSunghyeon/dropbp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant success across various\ndomains. However, training these LLMs typically involves substantial memory and\ncomputational costs during both forward and backward propagation. While\nparameter-efficient fine-tuning (PEFT) considerably reduces the training memory\nassociated with parameters, it does not address the significant computational\ncosts and activation memory. In this paper, we propose Dropping Backward\nPropagation (DropBP), a novel approach designed to reduce computational costs\nand activation memory while maintaining accuracy. DropBP randomly drops layers\nduring backward propagation, which is essentially equivalent to training\nshallow submodules generated by undropped layers and residual connections.\nAdditionally, DropBP calculates the sensitivity of each layer to assign an\nappropriate drop rate, thereby stabilizing the training process. DropBP is not\nonly applicable to full fine-tuning but can also be orthogonally integrated\nwith all types of PEFT by dropping layers during backward propagation.\nSpecifically, DropBP can reduce training time by 44% with comparable accuracy\nto the baseline, accelerate convergence to the same perplexity by 1.5x, and\nenable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.\nFurthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100\nGPU and 117% on an Intel Gaudi2 HPU. The code is available at\nhttps://github.com/WooSunghyeon/dropbp."
                },
                "authors": [
                    {
                        "name": "Sunghyeon Woo"
                    },
                    {
                        "name": "Baeseong Park"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Minjung Jo"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsuk Jeon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongsoo Lee"
                },
                "author": "Dongsoo Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03675v1",
                "updated": "2024-11-06T05:24:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    24,
                    9,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T05:24:09Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    24,
                    9,
                    2,
                    311,
                    0
                ],
                "title": "QUILL: Quotation Generation Enhancement of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUILL: Quotation Generation Enhancement of Large Language Models"
                },
                "summary": "While Large language models (LLMs) have become excellent writing assistants,\nthey still struggle with quotation generation. This is because they either\nhallucinate when providing factual quotations or fail to provide quotes that\nexceed human expectations. To bridge the gap, we systematically study how to\nevaluate and improve LLMs' performance in quotation generation tasks. We first\nestablish a holistic and automatic evaluation system for quotation generation\ntask, which consists of five criteria each with corresponding automatic metric.\nTo improve the LLMs' quotation generation abilities, we construct a bilingual\nknowledge base that is broad in scope and rich in dimensions, containing up to\n32,022 quotes. Moreover, guided by our critiria, we further design a\nquotation-specific metric to rerank the retrieved quotations from the knowledge\nbase. Extensive experiments show that our metrics strongly correlate with human\npreferences. Existing LLMs struggle to generate desired quotes, but our\nquotation knowledge base and reranking metric help narrow this gap. Our dataset\nand code are publicly available at https://github.com/GraceXiaoo/QUILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large language models (LLMs) have become excellent writing assistants,\nthey still struggle with quotation generation. This is because they either\nhallucinate when providing factual quotations or fail to provide quotes that\nexceed human expectations. To bridge the gap, we systematically study how to\nevaluate and improve LLMs' performance in quotation generation tasks. We first\nestablish a holistic and automatic evaluation system for quotation generation\ntask, which consists of five criteria each with corresponding automatic metric.\nTo improve the LLMs' quotation generation abilities, we construct a bilingual\nknowledge base that is broad in scope and rich in dimensions, containing up to\n32,022 quotes. Moreover, guided by our critiria, we further design a\nquotation-specific metric to rerank the retrieved quotations from the knowledge\nbase. Extensive experiments show that our metrics strongly correlate with human\npreferences. Existing LLMs struggle to generate desired quotes, but our\nquotation knowledge base and reranking metric help narrow this gap. Our dataset\nand code are publicly available at https://github.com/GraceXiaoo/QUILL."
                },
                "authors": [
                    {
                        "name": "Jin Xiao"
                    },
                    {
                        "name": "Bowei Zhang"
                    },
                    {
                        "name": "Qianyu He"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Feng Wei"
                    },
                    {
                        "name": "Jinglei Chen"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13147v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13147v5",
                "updated": "2024-11-06T05:18:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    18,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-17T02:04:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    2,
                    4,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "Utilizing Large Language Models in an iterative paradigm with Domain\n  feedback for Zero-shot Molecule optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models in an iterative paradigm with Domain\n  feedback for Zero-shot Molecule optimization"
                },
                "summary": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing shows limited performance. In this work, we facilitate utilizing LLMs\nin an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to explicitly guide the LLM to refine the modified\nmolecule. We conduct experiments across both single- and multi-property\nobjectives with 2 thresholds, where $\\text{Re}^3$DF shows significant\nimprovements. Particularly, for 20 single-property objectives, $\\text{Re}^3$DF\nenhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecule optimization is a critical task in drug discovery to optimize\ndesired properties of a given molecule through chemical modification. Despite\nLarge Language Models (LLMs) holding the potential to efficiently simulate this\ntask by using natural language to direct the optimization, straightforwardly\nutilizing shows limited performance. In this work, we facilitate utilizing LLMs\nin an iterative paradigm by proposing a simple yet highly effective domain\nfeedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses\nan external toolkit, RDKit, to handle the molecule hallucination, if the\nmodified molecule is chemically invalid. Otherwise, its desired properties are\ncomputed and compared to the original one, establishing reliable domain\nfeedback with correct direction and distance towards the objective, followed by\na retrieved example, to explicitly guide the LLM to refine the modified\nmolecule. We conduct experiments across both single- and multi-property\nobjectives with 2 thresholds, where $\\text{Re}^3$DF shows significant\nimprovements. Particularly, for 20 single-property objectives, $\\text{Re}^3$DF\nenhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,\nrespectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit\nratio by 6.04% and 5.25%."
                },
                "authors": [
                    {
                        "name": "Khiem Le"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh V. Chawla"
                },
                "author": "Nitesh V. Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13147v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13147v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09774v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09774v3",
                "updated": "2024-11-06T05:16:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    16,
                    59,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-15T15:46:03Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    15,
                    46,
                    3,
                    6,
                    259,
                    0
                ],
                "title": "Generalizing Alignment Paradigm of Text-to-Image Generation with\n  Preferences through $f$-divergence Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing Alignment Paradigm of Text-to-Image Generation with\n  Preferences through $f$-divergence Minimization"
                },
                "summary": "Direct Preference Optimization (DPO) has recently expanded its successful\napplication from aligning large language models (LLMs) to aligning\ntext-to-image models with human preferences, which has generated considerable\ninterest within the community. However, we have observed that these approaches\nrely solely on minimizing the reverse Kullback-Leibler divergence during\nalignment process between the fine-tuned model and the reference model,\nneglecting the incorporation of other divergence constraints. In this study, we\nfocus on extending reverse Kullback-Leibler divergence in the alignment\nparadigm of text-to-image models to $f$-divergence, which aims to garner better\nalignment performance as well as good generation diversity. We provide the\ngeneralized formula of the alignment paradigm under the $f$-divergence\ncondition and thoroughly analyze the impact of different divergence constraints\non alignment process from the perspective of gradient fields. We conduct\ncomprehensive evaluation on image-text alignment performance, human value\nalignment performance and generation diversity performance under different\ndivergence constraints, and the results indicate that alignment based on\nJensen-Shannon divergence achieves the best trade-off among them. The option of\ndivergence employed for aligning text-to-image models significantly impacts the\ntrade-off between alignment performance (especially human value alignment) and\ngeneration diversity, which highlights the necessity of selecting an\nappropriate divergence for practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has recently expanded its successful\napplication from aligning large language models (LLMs) to aligning\ntext-to-image models with human preferences, which has generated considerable\ninterest within the community. However, we have observed that these approaches\nrely solely on minimizing the reverse Kullback-Leibler divergence during\nalignment process between the fine-tuned model and the reference model,\nneglecting the incorporation of other divergence constraints. In this study, we\nfocus on extending reverse Kullback-Leibler divergence in the alignment\nparadigm of text-to-image models to $f$-divergence, which aims to garner better\nalignment performance as well as good generation diversity. We provide the\ngeneralized formula of the alignment paradigm under the $f$-divergence\ncondition and thoroughly analyze the impact of different divergence constraints\non alignment process from the perspective of gradient fields. We conduct\ncomprehensive evaluation on image-text alignment performance, human value\nalignment performance and generation diversity performance under different\ndivergence constraints, and the results indicate that alignment based on\nJensen-Shannon divergence achieves the best trade-off among them. The option of\ndivergence employed for aligning text-to-image models significantly impacts the\ntrade-off between alignment performance (especially human value alignment) and\ngeneration diversity, which highlights the necessity of selecting an\nappropriate divergence for practical applications."
                },
                "authors": [
                    {
                        "name": "Haoyuan Sun"
                    },
                    {
                        "name": "Bo Xia"
                    },
                    {
                        "name": "Yongzhe Chang"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09774v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09774v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03672v1",
                "updated": "2024-11-06T05:11:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    11,
                    25,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T05:11:25Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    11,
                    25,
                    2,
                    311,
                    0
                ],
                "title": "Towards 3D Semantic Scene Completion for Autonomous Driving: A\n  Meta-Learning Framework Empowered by Deformable Large-Kernel Attention and\n  Mamba Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards 3D Semantic Scene Completion for Autonomous Driving: A\n  Meta-Learning Framework Empowered by Deformable Large-Kernel Attention and\n  Mamba Model"
                },
                "summary": "Semantic scene completion (SSC) is essential for achieving comprehensive\nperception in autonomous driving systems. However, existing SSC methods often\noverlook the high deployment costs in real-world applications. Traditional\narchitectures, such as 3D Convolutional Neural Networks (3D CNNs) and\nself-attention mechanisms, face challenges in efficiently capturing long-range\ndependencies within 3D voxel grids, limiting their effectiveness. To address\nthese issues, we introduce MetaSSC, a novel meta-learning-based framework for\nSSC that leverages deformable convolution, large-kernel attention, and the\nMamba (D-LKA-M) model. Our approach begins with a voxel-based semantic\nsegmentation (SS) pretraining task, aimed at exploring the semantics and\ngeometry of incomplete regions while acquiring transferable meta-knowledge.\nUsing simulated cooperative perception datasets, we supervise the perception\ntraining of a single vehicle using aggregated sensor data from multiple nearby\nconnected autonomous vehicles (CAVs), generating richer and more comprehensive\nlabels. This meta-knowledge is then adapted to the target domain through a\ndual-phase training strategy that does not add extra model parameters, enabling\nefficient deployment. To further enhance the model's capability in capturing\nlong-sequence relationships within 3D voxel grids, we integrate Mamba blocks\nwith deformable convolution and large-kernel attention into the backbone\nnetwork. Extensive experiments demonstrate that MetaSSC achieves\nstate-of-the-art performance, significantly outperforming competing models\nwhile also reducing deployment costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic scene completion (SSC) is essential for achieving comprehensive\nperception in autonomous driving systems. However, existing SSC methods often\noverlook the high deployment costs in real-world applications. Traditional\narchitectures, such as 3D Convolutional Neural Networks (3D CNNs) and\nself-attention mechanisms, face challenges in efficiently capturing long-range\ndependencies within 3D voxel grids, limiting their effectiveness. To address\nthese issues, we introduce MetaSSC, a novel meta-learning-based framework for\nSSC that leverages deformable convolution, large-kernel attention, and the\nMamba (D-LKA-M) model. Our approach begins with a voxel-based semantic\nsegmentation (SS) pretraining task, aimed at exploring the semantics and\ngeometry of incomplete regions while acquiring transferable meta-knowledge.\nUsing simulated cooperative perception datasets, we supervise the perception\ntraining of a single vehicle using aggregated sensor data from multiple nearby\nconnected autonomous vehicles (CAVs), generating richer and more comprehensive\nlabels. This meta-knowledge is then adapted to the target domain through a\ndual-phase training strategy that does not add extra model parameters, enabling\nefficient deployment. To further enhance the model's capability in capturing\nlong-sequence relationships within 3D voxel grids, we integrate Mamba blocks\nwith deformable convolution and large-kernel attention into the backbone\nnetwork. Extensive experiments demonstrate that MetaSSC achieves\nstate-of-the-art performance, significantly outperforming competing models\nwhile also reducing deployment costs."
                },
                "authors": [
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Zilin Huang"
                    },
                    {
                        "name": "Zihao Sheng"
                    },
                    {
                        "name": "Tiantian Chen"
                    },
                    {
                        "name": "Sikai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sikai Chen"
                },
                "author": "Sikai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13941v2",
                "updated": "2024-11-06T05:05:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    5,
                    12,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-20T23:04:21Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    23,
                    4,
                    21,
                    4,
                    264,
                    0
                ],
                "title": "TalkMosaic: Interactive PhotoMosaic with Multi-modal LLM Q&A\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TalkMosaic: Interactive PhotoMosaic with Multi-modal LLM Q&A\n  Interactions"
                },
                "summary": "We use images of cars of a wide range of varieties to compose an image of an\nanimal such as a bird or a lion for the theme of environmental protection to\nmaximize the information about cars in a single composed image and to raise the\nawareness about environmental challenges. We present a novel way of image\ninteraction with an artistically-composed photomosaic image, in which a simple\noperation of \"click and display\" is used to demonstrate the interactive switch\nbetween a tile image in a photomosaic image and the corresponding original car\nimage, which will be automatically saved on the Desktop. We build a multimodal\ncustom GPT named TalkMosaic by incorporating car images information and the\nrelated knowledge to ChatGPT. By uploading the original car image to\nTalkMosaic, we can ask questions about the given car image and get the\ncorresponding answers efficiently and effectively such as where to buy the tire\nin the car image that satisfies high environmental standards. We give an\nin-depth analysis on how to speed up the inference of multimodal LLM using\nsparse attention and quantization techniques with presented probabilistic\nFlashAttention (PrFlashAttention) and Staircase Adaptive Quantization (SAQ)\nmethods. The implemented prototype demonstrates the feasibility and\neffectiveness of the presented approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use images of cars of a wide range of varieties to compose an image of an\nanimal such as a bird or a lion for the theme of environmental protection to\nmaximize the information about cars in a single composed image and to raise the\nawareness about environmental challenges. We present a novel way of image\ninteraction with an artistically-composed photomosaic image, in which a simple\noperation of \"click and display\" is used to demonstrate the interactive switch\nbetween a tile image in a photomosaic image and the corresponding original car\nimage, which will be automatically saved on the Desktop. We build a multimodal\ncustom GPT named TalkMosaic by incorporating car images information and the\nrelated knowledge to ChatGPT. By uploading the original car image to\nTalkMosaic, we can ask questions about the given car image and get the\ncorresponding answers efficiently and effectively such as where to buy the tire\nin the car image that satisfies high environmental standards. We give an\nin-depth analysis on how to speed up the inference of multimodal LLM using\nsparse attention and quantization techniques with presented probabilistic\nFlashAttention (PrFlashAttention) and Staircase Adaptive Quantization (SAQ)\nmethods. The implemented prototype demonstrates the feasibility and\neffectiveness of the presented approach."
                },
                "authors": [
                    {
                        "name": "Kevin Li"
                    },
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03665v1",
                "updated": "2024-11-06T04:52:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    4,
                    52,
                    38,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T04:52:38Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    4,
                    52,
                    38,
                    2,
                    311,
                    0
                ],
                "title": "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework"
                },
                "summary": "Proper moral beliefs are fundamental for language models, yet assessing these\nbeliefs poses a significant challenge. This study introduces a novel\nthree-module framework to evaluate the moral beliefs of four prominent large\nlanguage models. Initially, we constructed a dataset containing 472 moral\nchoice scenarios in Chinese, derived from moral words. The decision-making\nprocess of the models in these scenarios reveals their moral principle\npreferences. By ranking these moral choices, we discern the varying moral\nbeliefs held by different language models. Additionally, through moral debates,\nwe investigate the firmness of these models to their moral choices. Our\nfindings indicate that English language models, namely ChatGPT and Gemini,\nclosely mirror moral decisions of the sample of Chinese university students,\ndemonstrating strong adherence to their choices and a preference for\nindividualistic moral beliefs. In contrast, Chinese models such as Ernie and\nChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their\nmoral choices and debates. This study also uncovers gender bias embedded within\nthe moral beliefs of all examined language models. Our methodology offers an\ninnovative means to assess moral beliefs in both artificial and human\nintelligence, facilitating a comparison of moral values across different\ncultures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proper moral beliefs are fundamental for language models, yet assessing these\nbeliefs poses a significant challenge. This study introduces a novel\nthree-module framework to evaluate the moral beliefs of four prominent large\nlanguage models. Initially, we constructed a dataset containing 472 moral\nchoice scenarios in Chinese, derived from moral words. The decision-making\nprocess of the models in these scenarios reveals their moral principle\npreferences. By ranking these moral choices, we discern the varying moral\nbeliefs held by different language models. Additionally, through moral debates,\nwe investigate the firmness of these models to their moral choices. Our\nfindings indicate that English language models, namely ChatGPT and Gemini,\nclosely mirror moral decisions of the sample of Chinese university students,\ndemonstrating strong adherence to their choices and a preference for\nindividualistic moral beliefs. In contrast, Chinese models such as Ernie and\nChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their\nmoral choices and debates. This study also uncovers gender bias embedded within\nthe moral beliefs of all examined language models. Our methodology offers an\ninnovative means to assess moral beliefs in both artificial and human\nintelligence, facilitating a comparison of moral values across different\ncultures."
                },
                "authors": [
                    {
                        "name": "Xuelin Liu"
                    },
                    {
                        "name": "Yanfei Zhu"
                    },
                    {
                        "name": "Shucheng Zhu"
                    },
                    {
                        "name": "Pengyuan Liu"
                    },
                    {
                        "name": "Ying Liu"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09324v3",
                "updated": "2024-11-06T04:43:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    4,
                    43,
                    57,
                    2,
                    311,
                    0
                ],
                "published": "2024-06-13T17:01:40Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    17,
                    1,
                    40,
                    3,
                    165,
                    0
                ],
                "title": "Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated significant\ncapabilities in executing complex tasks in a zero-shot manner, they are\nsusceptible to jailbreak attacks and can be manipulated to produce harmful\noutputs. Recently, a growing body of research has categorized jailbreak attacks\ninto token-level and prompt-level attacks. However, previous work primarily\noverlooks the diverse key factors of jailbreak attacks, with most studies\nconcentrating on LLM vulnerabilities and lacking exploration of\ndefense-enhanced LLMs. To address these issues, we introduced\n$\\textbf{JailTrickBench}$ to evaluate the impact of various attack settings on\nLLM performance and provide a baseline for jailbreak attacks, encouraging the\nadoption of a standardized evaluation framework. Specifically, we evaluate the\neight key factors of implementing jailbreak attacks on LLMs from both\ntarget-level and attack-level perspectives. We further conduct seven\nrepresentative jailbreak attacks on six defense methods across two widely used\ndatasets, encompassing approximately 354 experiments with about 55,000 GPU\nhours on A800-80G. Our experimental results highlight the need for standardized\nbenchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is\navailable at https://github.com/usail-hkust/JailTrickBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated significant\ncapabilities in executing complex tasks in a zero-shot manner, they are\nsusceptible to jailbreak attacks and can be manipulated to produce harmful\noutputs. Recently, a growing body of research has categorized jailbreak attacks\ninto token-level and prompt-level attacks. However, previous work primarily\noverlooks the diverse key factors of jailbreak attacks, with most studies\nconcentrating on LLM vulnerabilities and lacking exploration of\ndefense-enhanced LLMs. To address these issues, we introduced\n$\\textbf{JailTrickBench}$ to evaluate the impact of various attack settings on\nLLM performance and provide a baseline for jailbreak attacks, encouraging the\nadoption of a standardized evaluation framework. Specifically, we evaluate the\neight key factors of implementing jailbreak attacks on LLMs from both\ntarget-level and attack-level perspectives. We further conduct seven\nrepresentative jailbreak attacks on six defense methods across two widely used\ndatasets, encompassing approximately 354 experiments with about 55,000 GPU\nhours on A800-80G. Our experimental results highlight the need for standardized\nbenchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is\navailable at https://github.com/usail-hkust/JailTrickBench."
                },
                "authors": [
                    {
                        "name": "Zhao Xu"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03659v1",
                "updated": "2024-11-06T04:41:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    4,
                    41,
                    13,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T04:41:13Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    4,
                    41,
                    13,
                    2,
                    311,
                    0
                ],
                "title": "Towards Scalable Automated Grading: Leveraging Large Language Models for\n  Conceptual Question Evaluation in Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable Automated Grading: Leveraging Large Language Models for\n  Conceptual Question Evaluation in Engineering"
                },
                "summary": "This study explores the feasibility of using large language models (LLMs),\nspecifically GPT-4o (ChatGPT), for automated grading of conceptual questions in\nan undergraduate Mechanical Engineering course. We compared the grading\nperformance of GPT-4o with that of human teaching assistants (TAs) on ten quiz\nproblems from the MEEN 361 course at Texas A&M University, each answered by\napproximately 225 students. Both the LLM and TAs followed the same\ninstructor-provided rubric to ensure grading consistency. We evaluated\nperformance using Spearman's rank correlation coefficient and Root Mean Square\nError (RMSE) to assess the alignment between rankings and the accuracy of\nscores assigned by GPT-4o and TAs under zero- and few-shot grading settings. In\nthe zero-shot setting, GPT-4o demonstrated a strong correlation with TA\ngrading, with Spearman's rank correlation coefficient exceeding 0.6 in seven\nout of ten datasets and reaching a high of 0.9387. Our analysis reveals that\nGPT-4o performs well when grading criteria are straightforward but struggles\nwith nuanced answers, particularly those involving synonyms not present in the\nrubric. The model also tends to grade more stringently in ambiguous cases\ncompared to human TAs. Overall, ChatGPT shows promise as a tool for grading\nconceptual questions, offering scalability and consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the feasibility of using large language models (LLMs),\nspecifically GPT-4o (ChatGPT), for automated grading of conceptual questions in\nan undergraduate Mechanical Engineering course. We compared the grading\nperformance of GPT-4o with that of human teaching assistants (TAs) on ten quiz\nproblems from the MEEN 361 course at Texas A&M University, each answered by\napproximately 225 students. Both the LLM and TAs followed the same\ninstructor-provided rubric to ensure grading consistency. We evaluated\nperformance using Spearman's rank correlation coefficient and Root Mean Square\nError (RMSE) to assess the alignment between rankings and the accuracy of\nscores assigned by GPT-4o and TAs under zero- and few-shot grading settings. In\nthe zero-shot setting, GPT-4o demonstrated a strong correlation with TA\ngrading, with Spearman's rank correlation coefficient exceeding 0.6 in seven\nout of ten datasets and reaching a high of 0.9387. Our analysis reveals that\nGPT-4o performs well when grading criteria are straightforward but struggles\nwith nuanced answers, particularly those involving synonyms not present in the\nrubric. The model also tends to grade more stringently in ambiguous cases\ncompared to human TAs. Overall, ChatGPT shows promise as a tool for grading\nconceptual questions, offering scalability and consistency."
                },
                "authors": [
                    {
                        "name": "Rujun Gao"
                    },
                    {
                        "name": "Xiaosu Guo"
                    },
                    {
                        "name": "Xiaodi Li"
                    },
                    {
                        "name": "Arun Balajiee Lekshmi Narayanan"
                    },
                    {
                        "name": "Naveen Thomas"
                    },
                    {
                        "name": "Arun R. Srinivasa"
                    }
                ],
                "author_detail": {
                    "name": "Arun R. Srinivasa"
                },
                "author": "Arun R. Srinivasa",
                "arxiv_comment": "21 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00870v4",
                "updated": "2024-11-06T02:55:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    55,
                    1,
                    2,
                    311,
                    0
                ],
                "published": "2023-12-30T01:26:42Z",
                "published_parsed": [
                    2023,
                    12,
                    30,
                    1,
                    26,
                    42,
                    5,
                    364,
                    0
                ],
                "title": "ConfusionPrompt: Practical Private Inference for Online Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusionPrompt: Practical Private Inference for Online Large Language\n  Models"
                },
                "summary": "State-of-the-art large language models (LLMs) are typically deployed as\nonline services, requiring users to transmit detailed prompts to cloud servers.\nThis raises significant privacy concerns. In response, we introduce\nConfusionPrompt, a novel framework for private LLM inference that protects user\nprivacy by: (i) decomposing the original prompt into smaller sub-prompts, and\n(ii) generating pseudo-prompts alongside the genuine sub-prompts, which are\nthen sent to the LLM. The server responses are later recomposed by the user to\nreconstruct the final output. This approach offers key advantages over previous\nLLM privacy protection methods: (i) it integrates seamlessly with existing\nblack-box LLMs, and (ii) it delivers a significantly improved privacy-utility\ntrade-off compared to existing text perturbation methods. We also develop a\n$(\\lambda, \\mu, \\rho)$-privacy model to formulate the requirements for a\nprivacy-preserving group of prompts and provide a complexity analysis to\njustify the role of prompt decomposition. Our empirical evaluation shows that\nConfusionPrompt achieves significantly higher utility than local inference\nmethods using open-source models and perturbation-based techniques, while also\nreducing memory consumption compared to open-source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art large language models (LLMs) are typically deployed as\nonline services, requiring users to transmit detailed prompts to cloud servers.\nThis raises significant privacy concerns. In response, we introduce\nConfusionPrompt, a novel framework for private LLM inference that protects user\nprivacy by: (i) decomposing the original prompt into smaller sub-prompts, and\n(ii) generating pseudo-prompts alongside the genuine sub-prompts, which are\nthen sent to the LLM. The server responses are later recomposed by the user to\nreconstruct the final output. This approach offers key advantages over previous\nLLM privacy protection methods: (i) it integrates seamlessly with existing\nblack-box LLMs, and (ii) it delivers a significantly improved privacy-utility\ntrade-off compared to existing text perturbation methods. We also develop a\n$(\\lambda, \\mu, \\rho)$-privacy model to formulate the requirements for a\nprivacy-preserving group of prompts and provide a complexity analysis to\njustify the role of prompt decomposition. Our empirical evaluation shows that\nConfusionPrompt achieves significantly higher utility than local inference\nmethods using open-source models and perturbation-based techniques, while also\nreducing memory consumption compared to open-source LLMs."
                },
                "authors": [
                    {
                        "name": "Peihua Mai"
                    },
                    {
                        "name": "Youjia Yang"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Yan Pang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Pang"
                },
                "author": "Yan Pang",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02199v2",
                "updated": "2024-11-06T02:51:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    51,
                    16,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T15:54:32Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    54,
                    32,
                    0,
                    309,
                    0
                ],
                "title": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Transformers Harness Multi-Concept Word Semantics for Efficient\n  In-Context Learning"
                },
                "summary": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have displayed remarkable\ncreative prowess and emergence capabilities. Existing empirical studies have\nrevealed a strong connection between these LLMs' impressive emergence abilities\nand their in-context learning (ICL) capacity, allowing them to solve new tasks\nusing only task-specific prompts without further fine-tuning. On the other\nhand, existing empirical and theoretical studies also show that there is a\nlinear regularity of the multi-concept encoded semantic representation behind\ntransformer-based LLMs. However, existing theoretical work fail to build up an\nunderstanding of the connection between this regularity and the innovative\npower of ICL. Additionally, prior work often focuses on simplified, unrealistic\nscenarios involving linear transformers or unrealistic loss functions, and they\nachieve only linear or sub-linear convergence rates. In contrast, this work\nprovides a fine-grained mathematical analysis to show how transformers leverage\nthe multi-concept semantics of words to enable powerful ICL and excellent\nout-of-distribution ICL abilities, offering insights into how transformers\ninnovate solutions for certain unseen tasks encoded with multiple cross-concept\nsemantics. Inspired by empirical studies on the linear latent geometry of LLMs,\nthe analysis is based on a concept-based low-noise sparse coding prompt model.\nLeveraging advanced techniques, this work showcases the exponential 0-1 loss\nconvergence over the highly non-convex training dynamics, which pioneeringly\nincorporates the challenges of softmax self-attention, ReLU-activated MLPs, and\ncross-entropy loss. Empirical simulations corroborate the theoretical findings."
                },
                "authors": [
                    {
                        "name": "Dake Bu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Andi Han"
                    },
                    {
                        "name": "Atsushi Nitanda"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Hau-San Wong"
                    }
                ],
                "author_detail": {
                    "name": "Hau-San Wong"
                },
                "author": "Hau-San Wong",
                "arxiv_comment": "Accepted by the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02937v2",
                "updated": "2024-11-06T02:36:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    36,
                    2,
                    2,
                    311,
                    0
                ],
                "published": "2024-08-06T03:44:06Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    3,
                    44,
                    6,
                    1,
                    219,
                    0
                ],
                "title": "A Real-Time Adaptive Multi-Stream GPU System for Online Approximate\n  Nearest Neighborhood Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Real-Time Adaptive Multi-Stream GPU System for Online Approximate\n  Nearest Neighborhood Search"
                },
                "summary": "In recent years, Approximate Nearest Neighbor Search (ANNS) has played a\npivotal role in modern search and recommendation systems, especially in\nemerging LLM applications like Retrieval-Augmented Generation. There is a\ngrowing exploration into harnessing the parallel computing capabilities of GPUs\nto meet the substantial demands of ANNS. However, existing systems primarily\nfocus on offline scenarios, overlooking the distinct requirements of online\napplications that necessitate real-time insertion of new vectors. This\nlimitation renders such systems inefficient for real-world scenarios. Moreover,\nprevious architectures struggled to effectively support real-time insertion due\nto their reliance on serial execution streams. In this paper, we introduce a\nnovel Real-Time Adaptive Multi-Stream GPU ANNS System (RTAMS-GANNS). Our\narchitecture achieves its objectives through three key advancements: 1) We\ninitially examined the real-time insertion mechanisms in existing GPU ANNS\nsystems and discovered their reliance on repetitive copying and memory\nallocation, which significantly hinders real-time effectiveness on GPUs. As a\nsolution, we introduce a dynamic vector insertion algorithm based on memory\nblocks, which includes in-place rearrangement. 2) To enable real-time vector\ninsertion in parallel, we introduce a multi-stream parallel execution mode,\nwhich differs from existing systems that operate serially within a single\nstream. Our system utilizes a dynamic resource pool, allowing multiple streams\nto execute concurrently without additional execution blocking. 3) Through\nextensive experiments and comparisons, our approach effectively handles varying\nQPS levels across different datasets, reducing latency by up to 40%-80%. The\nproposed system has also been deployed in real-world industrial search and\nrecommendation systems, serving hundreds of millions of users daily, and has\nachieved good results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Approximate Nearest Neighbor Search (ANNS) has played a\npivotal role in modern search and recommendation systems, especially in\nemerging LLM applications like Retrieval-Augmented Generation. There is a\ngrowing exploration into harnessing the parallel computing capabilities of GPUs\nto meet the substantial demands of ANNS. However, existing systems primarily\nfocus on offline scenarios, overlooking the distinct requirements of online\napplications that necessitate real-time insertion of new vectors. This\nlimitation renders such systems inefficient for real-world scenarios. Moreover,\nprevious architectures struggled to effectively support real-time insertion due\nto their reliance on serial execution streams. In this paper, we introduce a\nnovel Real-Time Adaptive Multi-Stream GPU ANNS System (RTAMS-GANNS). Our\narchitecture achieves its objectives through three key advancements: 1) We\ninitially examined the real-time insertion mechanisms in existing GPU ANNS\nsystems and discovered their reliance on repetitive copying and memory\nallocation, which significantly hinders real-time effectiveness on GPUs. As a\nsolution, we introduce a dynamic vector insertion algorithm based on memory\nblocks, which includes in-place rearrangement. 2) To enable real-time vector\ninsertion in parallel, we introduce a multi-stream parallel execution mode,\nwhich differs from existing systems that operate serially within a single\nstream. Our system utilizes a dynamic resource pool, allowing multiple streams\nto execute concurrently without additional execution blocking. 3) Through\nextensive experiments and comparisons, our approach effectively handles varying\nQPS levels across different datasets, reducing latency by up to 40%-80%. The\nproposed system has also been deployed in real-world industrial search and\nrecommendation systems, serving hundreds of millions of users daily, and has\nachieved good results."
                },
                "authors": [
                    {
                        "name": "Yiping Sun"
                    },
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Jiaolong Du"
                    }
                ],
                "author_detail": {
                    "name": "Jiaolong Du"
                },
                "author": "Jiaolong Du",
                "arxiv_comment": "Accepted by CIKM'24, V2 fixes some typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06209v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06209v6",
                "updated": "2024-11-06T02:35:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    35,
                    30,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-08T17:11:24Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    17,
                    11,
                    24,
                    1,
                    282,
                    0
                ],
                "title": "LeanAgent: Lifelong Learning for Formal Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanAgent: Lifelong Learning for Formal Theorem Proving"
                },
                "summary": "Large Language Models (LLMs) have been successful in mathematical reasoning\ntasks such as formal theorem proving when integrated with interactive proof\nassistants like Lean. Existing approaches involve training or fine-tuning an\nLLM on a specific dataset to perform well on particular domains, such as\nundergraduate-level mathematics. These methods struggle with generalizability\nto advanced mathematics. A fundamental limitation is that these approaches\noperate on static domains, failing to capture how mathematicians often work\nacross multiple domains and projects simultaneously or cyclically. We present\nLeanAgent, a novel lifelong learning framework for theorem proving that\ncontinuously generalizes to and improves on ever-expanding mathematical\nknowledge without forgetting previously learned knowledge. LeanAgent introduces\nseveral key innovations, including a curriculum learning strategy that\noptimizes the learning trajectory in terms of mathematical difficulty, a\ndynamic database for efficient management of evolving mathematical knowledge,\nand progressive training to balance stability and plasticity. LeanAgent\nsuccessfully proves 162 theorems previously unproved by humans across 23\ndiverse Lean repositories, many from advanced mathematics. It performs\nsignificantly better than the static LLM baseline, proving challenging theorems\nin domains like abstract algebra and algebraic topology while showcasing a\nclear progression of learning from basic concepts to advanced topics. In\naddition, we analyze LeanAgent's superior performance on key lifelong learning\nmetrics. LeanAgent achieves exceptional scores in stability and backward\ntransfer, where learning new tasks improves performance on previously learned\ntasks. This emphasizes LeanAgent's continuous generalizability and improvement,\nexplaining its superior theorem-proving performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been successful in mathematical reasoning\ntasks such as formal theorem proving when integrated with interactive proof\nassistants like Lean. Existing approaches involve training or fine-tuning an\nLLM on a specific dataset to perform well on particular domains, such as\nundergraduate-level mathematics. These methods struggle with generalizability\nto advanced mathematics. A fundamental limitation is that these approaches\noperate on static domains, failing to capture how mathematicians often work\nacross multiple domains and projects simultaneously or cyclically. We present\nLeanAgent, a novel lifelong learning framework for theorem proving that\ncontinuously generalizes to and improves on ever-expanding mathematical\nknowledge without forgetting previously learned knowledge. LeanAgent introduces\nseveral key innovations, including a curriculum learning strategy that\noptimizes the learning trajectory in terms of mathematical difficulty, a\ndynamic database for efficient management of evolving mathematical knowledge,\nand progressive training to balance stability and plasticity. LeanAgent\nsuccessfully proves 162 theorems previously unproved by humans across 23\ndiverse Lean repositories, many from advanced mathematics. It performs\nsignificantly better than the static LLM baseline, proving challenging theorems\nin domains like abstract algebra and algebraic topology while showcasing a\nclear progression of learning from basic concepts to advanced topics. In\naddition, we analyze LeanAgent's superior performance on key lifelong learning\nmetrics. LeanAgent achieves exceptional scores in stability and backward\ntransfer, where learning new tasks improves performance on previously learned\ntasks. This emphasizes LeanAgent's continuous generalizability and improvement,\nexplaining its superior theorem-proving performance."
                },
                "authors": [
                    {
                        "name": "Adarsh Kumarappan"
                    },
                    {
                        "name": "Mo Tiwari"
                    },
                    {
                        "name": "Peiyang Song"
                    },
                    {
                        "name": "Robert Joseph George"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06209v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06209v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03619v1",
                "updated": "2024-11-06T02:24:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    24,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T02:24:27Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    2,
                    24,
                    27,
                    2,
                    311,
                    0
                ],
                "title": "Real-Time Safe Bipedal Robot Navigation using Linear Discrete Control\n  Barrier Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Safe Bipedal Robot Navigation using Linear Discrete Control\n  Barrier Functions"
                },
                "summary": "Safe navigation in real-time is an essential task for humanoid robots in\nreal-world deployment. Since humanoid robots are inherently underactuated\nthanks to unilateral ground contacts, a path is considered safe if it is\nobstacle-free and respects the robot's physical limitations and underlying\ndynamics. Existing approaches often decouple path planning from gait control\ndue to the significant computational challenge caused by the full-order robot\ndynamics. In this work, we develop a unified, safe path and gait planning\nframework that can be evaluated online in real-time, allowing the robot to\nnavigate clustered environments while sustaining stable locomotion. Our\napproach uses the popular Linear Inverted Pendulum (LIP) model as a template\nmodel to represent walking dynamics. It incorporates heading angles in the\nmodel to evaluate kinematic constraints essential for physically feasible gaits\nproperly. In addition, we leverage discrete control barrier functions (DCBF)\nfor obstacle avoidance, ensuring that the subsequent foot placement provides a\nsafe navigation path within clustered environments. To guarantee real-time\ncomputation, we use a novel approximation of the DCBF to produce linear DCBF\n(LDCBF) constraints. We validate the proposed approach in simulation using a\nDigit robot in randomly generated environments. The results demonstrate that\nour approach can generate safe gaits for a non-trivial humanoid robot to\nnavigate environments with randomly generated obstacles in real-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe navigation in real-time is an essential task for humanoid robots in\nreal-world deployment. Since humanoid robots are inherently underactuated\nthanks to unilateral ground contacts, a path is considered safe if it is\nobstacle-free and respects the robot's physical limitations and underlying\ndynamics. Existing approaches often decouple path planning from gait control\ndue to the significant computational challenge caused by the full-order robot\ndynamics. In this work, we develop a unified, safe path and gait planning\nframework that can be evaluated online in real-time, allowing the robot to\nnavigate clustered environments while sustaining stable locomotion. Our\napproach uses the popular Linear Inverted Pendulum (LIP) model as a template\nmodel to represent walking dynamics. It incorporates heading angles in the\nmodel to evaluate kinematic constraints essential for physically feasible gaits\nproperly. In addition, we leverage discrete control barrier functions (DCBF)\nfor obstacle avoidance, ensuring that the subsequent foot placement provides a\nsafe navigation path within clustered environments. To guarantee real-time\ncomputation, we use a novel approximation of the DCBF to produce linear DCBF\n(LDCBF) constraints. We validate the proposed approach in simulation using a\nDigit robot in randomly generated environments. The results demonstrate that\nour approach can generate safe gaits for a non-trivial humanoid robot to\nnavigate environments with randomly generated obstacles in real-time."
                },
                "authors": [
                    {
                        "name": "Chengyang Peng"
                    },
                    {
                        "name": "Victor Paredes"
                    },
                    {
                        "name": "Guillermo A. Castillo"
                    },
                    {
                        "name": "Ayonga Hereid"
                    }
                ],
                "author_detail": {
                    "name": "Ayonga Hereid"
                },
                "author": "Ayonga Hereid",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19272v2",
                "updated": "2024-11-06T01:58:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    58,
                    20,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-28T07:13:33Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    7,
                    13,
                    33,
                    5,
                    272,
                    0
                ],
                "title": "Perception Compressor:A training-free prompt compression method in long\n  context scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception Compressor:A training-free prompt compression method in long\n  context scenarios"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities in various\nscenarios. However, they suffer from much redundant information and are\nsensitive to the position of key information (relevant to the input question)\nin long context scenarios, leading to inferior performance. To address these\nchallenges, we present Perception Compressor, a training-free prompt\ncompression method. It includes a perception retriever that leverages guiding\nquestions and instruction to retrieve the most relevant demonstrations, a\ndual-slope ratio allocator to dynamically allocate compression ratios and\nopen-book ratios, and a semi-guided iterative compression that retains key\ninformation at the token level while removing tokens that distract the LLM. We\nconduct extensive experiments on long context benchmarks, i.e.,\nNaturalQuestions, LongBench, and MuSiQue. Experiment results show that\nPerception Compressor outperforms existing methods by a large margin, achieving\nstate-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities in various\nscenarios. However, they suffer from much redundant information and are\nsensitive to the position of key information (relevant to the input question)\nin long context scenarios, leading to inferior performance. To address these\nchallenges, we present Perception Compressor, a training-free prompt\ncompression method. It includes a perception retriever that leverages guiding\nquestions and instruction to retrieve the most relevant demonstrations, a\ndual-slope ratio allocator to dynamically allocate compression ratios and\nopen-book ratios, and a semi-guided iterative compression that retains key\ninformation at the token level while removing tokens that distract the LLM. We\nconduct extensive experiments on long context benchmarks, i.e.,\nNaturalQuestions, LongBench, and MuSiQue. Experiment results show that\nPerception Compressor outperforms existing methods by a large margin, achieving\nstate-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Jin Xu"
                    },
                    {
                        "name": "Tingwei Lu"
                    },
                    {
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "name": "Yiming Zhao"
                    },
                    {
                        "name": "Lin Hai"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Tao Zheng"
                },
                "author": "Hai-Tao Zheng",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01222v2",
                "updated": "2024-11-06T01:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    40,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-02T12:01:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    12,
                    1,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "$B^4$: A Black-Box Scrubbing Attack on LLM Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$B^4$: A Black-Box Scrubbing Attack on LLM Watermarks"
                },
                "summary": "Watermarking has emerged as a prominent technique for LLM-generated content\ndetection by embedding imperceptible patterns. Despite supreme performance, its\nrobustness against adversarial attacks remains underexplored. Previous work\ntypically considers a grey-box attack setting, where the specific type of\nwatermark is already known. Some even necessitates knowledge about\nhyperparameters of the watermarking method. Such prerequisites are unattainable\nin real-world scenarios. Targeting at a more realistic black-box threat model\nwith fewer assumptions, we here propose $\\mathcal{B}^4$, a black-box scrubbing\nattack on watermarks. Specifically, we formulate the watermark scrubbing attack\nas a constrained optimization problem by capturing its objectives with two\ndistributions, a Watermark Distribution and a Fidelity Distribution. This\noptimization problem can be approximately solved using two proxy distributions.\nExperimental results across 12 different settings demonstrate the superior\nperformance of $\\mathcal{B}^4$ compared with other baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has emerged as a prominent technique for LLM-generated content\ndetection by embedding imperceptible patterns. Despite supreme performance, its\nrobustness against adversarial attacks remains underexplored. Previous work\ntypically considers a grey-box attack setting, where the specific type of\nwatermark is already known. Some even necessitates knowledge about\nhyperparameters of the watermarking method. Such prerequisites are unattainable\nin real-world scenarios. Targeting at a more realistic black-box threat model\nwith fewer assumptions, we here propose $\\mathcal{B}^4$, a black-box scrubbing\nattack on watermarks. Specifically, we formulate the watermark scrubbing attack\nas a constrained optimization problem by capturing its objectives with two\ndistributions, a Watermark Distribution and a Fidelity Distribution. This\noptimization problem can be approximately solved using two proxy distributions.\nExperimental results across 12 different settings demonstrate the superior\nperformance of $\\mathcal{B}^4$ compared with other baselines."
                },
                "authors": [
                    {
                        "name": "Baizhou Huang"
                    },
                    {
                        "name": "Xiao Pu"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14824v2",
                "updated": "2024-11-06T01:37:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    37,
                    52,
                    2,
                    311,
                    0
                ],
                "published": "2024-04-23T08:24:43Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    8,
                    24,
                    43,
                    1,
                    114,
                    0
                ],
                "title": "Automated Commit Message Generation with Large Language Models: An\n  Empirical Study and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Commit Message Generation with Large Language Models: An\n  Empirical Study and Beyond"
                },
                "summary": "Commit Message Generation (CMG) approaches aim to automatically generate\ncommit messages based on given code diffs, which facilitate collaboration among\ndevelopers and play a critical role in Open-Source Software (OSS). Very\nrecently, Large Language Models (LLMs) have demonstrated extensive\napplicability in diverse code-related task. But few studies systematically\nexplored their effectiveness using LLMs. This paper conducts the first\ncomprehensive experiment to investigate how far we have been in applying LLM to\ngenerate high-quality commit messages. Motivated by a pilot analysis, we first\nclean the most widely-used CMG dataset following practitioners' criteria.\nAfterward, we re-evaluate diverse state-of-the-art CMG approaches and make\ncomparisons with LLMs, demonstrating the superior performance of LLMs against\nstate-of-the-art CMG approaches. Then, we further propose four manual metrics\nfollowing the practice of OSS, including Accuracy, Integrity, Applicability,\nand Readability, and assess various LLMs accordingly. Results reveal that\nGPT-3.5 performs best overall, but different LLMs carry different advantages.\nTo further boost LLMs' performance in the CMG task, we propose an Efficient\nRetrieval-based In-Context Learning (ICL) framework, namely ERICommiter, which\nleverages a two-step filtering to accelerate the retrieval efficiency and\nintroduces semantic/lexical-based retrieval algorithm to construct the ICL\nexamples. Extensive experiments demonstrate the substantial performance\nimprovement of ERICommiter on various LLMs for code diffs of different\nprogramming languages. Meanwhile, ERICommiter also significantly reduces the\nretrieval time while keeping almost the same performance. Our research\ncontributes to the understanding of LLMs' capabilities in the CMG field and\nprovides valuable insights for practitioners seeking to leverage these tools in\ntheir workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commit Message Generation (CMG) approaches aim to automatically generate\ncommit messages based on given code diffs, which facilitate collaboration among\ndevelopers and play a critical role in Open-Source Software (OSS). Very\nrecently, Large Language Models (LLMs) have demonstrated extensive\napplicability in diverse code-related task. But few studies systematically\nexplored their effectiveness using LLMs. This paper conducts the first\ncomprehensive experiment to investigate how far we have been in applying LLM to\ngenerate high-quality commit messages. Motivated by a pilot analysis, we first\nclean the most widely-used CMG dataset following practitioners' criteria.\nAfterward, we re-evaluate diverse state-of-the-art CMG approaches and make\ncomparisons with LLMs, demonstrating the superior performance of LLMs against\nstate-of-the-art CMG approaches. Then, we further propose four manual metrics\nfollowing the practice of OSS, including Accuracy, Integrity, Applicability,\nand Readability, and assess various LLMs accordingly. Results reveal that\nGPT-3.5 performs best overall, but different LLMs carry different advantages.\nTo further boost LLMs' performance in the CMG task, we propose an Efficient\nRetrieval-based In-Context Learning (ICL) framework, namely ERICommiter, which\nleverages a two-step filtering to accelerate the retrieval efficiency and\nintroduces semantic/lexical-based retrieval algorithm to construct the ICL\nexamples. Extensive experiments demonstrate the substantial performance\nimprovement of ERICommiter on various LLMs for code diffs of different\nprogramming languages. Meanwhile, ERICommiter also significantly reduces the\nretrieval time while keeping almost the same performance. Our research\ncontributes to the understanding of LLMs' capabilities in the CMG field and\nprovides valuable insights for practitioners seeking to leverage these tools in\ntheir workflows."
                },
                "authors": [
                    {
                        "name": "Pengyu Xue"
                    },
                    {
                        "name": "Linhao Wu"
                    },
                    {
                        "name": "Zhongxing Yu"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yue Tan"
                    }
                ],
                "author_detail": {
                    "name": "Yue Tan"
                },
                "author": "Yue Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23956v2",
                "updated": "2024-11-06T01:35:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    35,
                    36,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-31T14:09:50Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    14,
                    9,
                    50,
                    3,
                    305,
                    0
                ],
                "title": "Multilingual Pretraining Using a Large Corpus Machine-Translated from a\n  Single Source Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Pretraining Using a Large Corpus Machine-Translated from a\n  Single Source Language"
                },
                "summary": "English, as a very high-resource language, enables the pretraining of\nhigh-quality large language models (LLMs). The same cannot be said for most\nother languages, as leading LLMs still underperform for non-English languages,\nlikely due to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated text from a\nsingle high-quality source language can contribute significantly to the\npretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality\nEnglish web dataset, into French, German, and Spanish, resulting in a final\n300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter\nmodel, CuatroLLM, from scratch on this dataset. Across five non-English\nreasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art\nmultilingual models trained using closed data, such as Llama3.2 and Gemma2,\ndespite using an order of magnitude less data, such as about 6% of the tokens\nused for Llama3.2's training. We further demonstrate that with additional\ndomain-specific pretraining, amounting to less than 1% of TransWeb-Edu,\nCuatroLLM surpasses the state of the art in multilingual reasoning. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nopen licenses at hf.co/britllm/CuatroLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "English, as a very high-resource language, enables the pretraining of\nhigh-quality large language models (LLMs). The same cannot be said for most\nother languages, as leading LLMs still underperform for non-English languages,\nlikely due to a gap in the quality and diversity of the available multilingual\npretraining corpora. In this work, we find that machine-translated text from a\nsingle high-quality source language can contribute significantly to the\npretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality\nEnglish web dataset, into French, German, and Spanish, resulting in a final\n300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter\nmodel, CuatroLLM, from scratch on this dataset. Across five non-English\nreasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art\nmultilingual models trained using closed data, such as Llama3.2 and Gemma2,\ndespite using an order of magnitude less data, such as about 6% of the tokens\nused for Llama3.2's training. We further demonstrate that with additional\ndomain-specific pretraining, amounting to less than 1% of TransWeb-Edu,\nCuatroLLM surpasses the state of the art in multilingual reasoning. To promote\nreproducibility, we release our corpus, models, and training pipeline under\nopen licenses at hf.co/britllm/CuatroLLM."
                },
                "authors": [
                    {
                        "name": "Jiayi Wang"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Maurice Weber"
                    },
                    {
                        "name": "Max Ryabinin"
                    },
                    {
                        "name": "Yihong Chen"
                    },
                    {
                        "name": "Raphael Tang"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    }
                ],
                "author_detail": {
                    "name": "Pontus Stenetorp"
                },
                "author": "Pontus Stenetorp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03590v1",
                "updated": "2024-11-06T01:09:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    9,
                    17,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T01:09:17Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    9,
                    17,
                    2,
                    311,
                    0
                ],
                "title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical\n  Challenge Problems and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical\n  Challenge Problems and Beyond"
                },
                "summary": "Run-time steering strategies like Medprompt are valuable for guiding large\nlanguage models (LLMs) to top performance on challenging tasks. Medprompt\ndemonstrates that a general LLM can be focused to deliver state-of-the-art\nperformance on specialized domains like medicine by using a prompt to elicit a\nrun-time strategy involving chain of thought reasoning and ensembling. OpenAI's\no1-preview model represents a new paradigm, where a model is designed to do\nrun-time reasoning before generating final responses. We seek to understand the\nbehavior of o1-preview on a diverse set of medical challenge problem\nbenchmarks. Following on the Medprompt study with GPT-4, we systematically\nevaluate the o1-preview model across various medical benchmarks. Notably, even\nwithout prompting techniques, o1-preview largely outperforms the GPT-4 series\nwith Medprompt. We further systematically study the efficacy of classic prompt\nengineering strategies, as represented by Medprompt, within the new paradigm of\nreasoning models. We found that few-shot prompting hinders o1's performance,\nsuggesting that in-context learning may no longer be an effective steering\napproach for reasoning-native models. While ensembling remains viable, it is\nresource-intensive and requires careful cost-performance optimization. Our cost\nand accuracy analysis across run-time strategies reveals a Pareto frontier,\nwith GPT-4o representing a more affordable option and o1-preview achieving\nstate-of-the-art performance at higher cost. Although o1-preview offers top\nperformance, GPT-4o with steering strategies like Medprompt retains value in\nspecific contexts. Moreover, we note that the o1-preview model has reached\nnear-saturation on many existing medical benchmarks, underscoring the need for\nnew, challenging benchmarks. We close with reflections on general directions\nfor inference-time computation with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Run-time steering strategies like Medprompt are valuable for guiding large\nlanguage models (LLMs) to top performance on challenging tasks. Medprompt\ndemonstrates that a general LLM can be focused to deliver state-of-the-art\nperformance on specialized domains like medicine by using a prompt to elicit a\nrun-time strategy involving chain of thought reasoning and ensembling. OpenAI's\no1-preview model represents a new paradigm, where a model is designed to do\nrun-time reasoning before generating final responses. We seek to understand the\nbehavior of o1-preview on a diverse set of medical challenge problem\nbenchmarks. Following on the Medprompt study with GPT-4, we systematically\nevaluate the o1-preview model across various medical benchmarks. Notably, even\nwithout prompting techniques, o1-preview largely outperforms the GPT-4 series\nwith Medprompt. We further systematically study the efficacy of classic prompt\nengineering strategies, as represented by Medprompt, within the new paradigm of\nreasoning models. We found that few-shot prompting hinders o1's performance,\nsuggesting that in-context learning may no longer be an effective steering\napproach for reasoning-native models. While ensembling remains viable, it is\nresource-intensive and requires careful cost-performance optimization. Our cost\nand accuracy analysis across run-time strategies reveals a Pareto frontier,\nwith GPT-4o representing a more affordable option and o1-preview achieving\nstate-of-the-art performance at higher cost. Although o1-preview offers top\nperformance, GPT-4o with steering strategies like Medprompt retains value in\nspecific contexts. Moreover, we note that the o1-preview model has reached\nnear-saturation on many existing medical benchmarks, underscoring the need for\nnew, challenging benchmarks. We close with reflections on general directions\nfor inference-time computation with LLMs."
                },
                "authors": [
                    {
                        "name": "Harsha Nori"
                    },
                    {
                        "name": "Naoto Usuyama"
                    },
                    {
                        "name": "Nicholas King"
                    },
                    {
                        "name": "Scott Mayer McKinney"
                    },
                    {
                        "name": "Xavier Fernandes"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Eric Horvitz"
                    }
                ],
                "author_detail": {
                    "name": "Eric Horvitz"
                },
                "author": "Eric Horvitz",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03575v1",
                "updated": "2024-11-06T00:32:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    0,
                    32,
                    21,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T00:32:21Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    0,
                    32,
                    21,
                    2,
                    311,
                    0
                ],
                "title": "Semantic Navigation for AI-assisted Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Navigation for AI-assisted Ideation"
                },
                "summary": "We present a novel AI-based ideation assistant and evaluate it in a user\nstudy with a group of innovators. The key contribution of our work is twofold:\nwe propose a method of idea exploration in a constrained domain by means of\nLLM-supported semantic navigation of problem and solution spaces, and employ\nnovel automated data input filtering to improve generations. We found that\nsemantic exploration is preferred to the traditional prompt-output\ninteractions, measured both in explicit survey rankings, and in terms of\ninnovation assistant engagement, where 2.1x more generations were performed\nusing semantic exploration. We also show that filtering input data with metrics\nsuch as relevancy, coherence and human alignment leads to improved generations\nin the same metrics as well as enhanced quality of experience among innovators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel AI-based ideation assistant and evaluate it in a user\nstudy with a group of innovators. The key contribution of our work is twofold:\nwe propose a method of idea exploration in a constrained domain by means of\nLLM-supported semantic navigation of problem and solution spaces, and employ\nnovel automated data input filtering to improve generations. We found that\nsemantic exploration is preferred to the traditional prompt-output\ninteractions, measured both in explicit survey rankings, and in terms of\ninnovation assistant engagement, where 2.1x more generations were performed\nusing semantic exploration. We also show that filtering input data with metrics\nsuch as relevancy, coherence and human alignment leads to improved generations\nin the same metrics as well as enhanced quality of experience among innovators."
                },
                "authors": [
                    {
                        "name": "Thomas Sandholm"
                    },
                    {
                        "name": "Sarah Dong"
                    },
                    {
                        "name": "Sayandev Mukherjee"
                    },
                    {
                        "name": "John Feland"
                    },
                    {
                        "name": "Bernardo A. Huberman"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo A. Huberman"
                },
                "author": "Bernardo A. Huberman",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2402.06053",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20763v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20763v2",
                "updated": "2024-11-06T00:20:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    0,
                    20,
                    32,
                    2,
                    311,
                    0
                ],
                "published": "2024-10-28T05:56:51Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    56,
                    51,
                    0,
                    302,
                    0
                ],
                "title": "Evaluating LLMs for Targeted Concept Simplification for Domain-Specific\n  Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs for Targeted Concept Simplification for Domain-Specific\n  Texts"
                },
                "summary": "One useful application of NLP models is to support people in reading complex\ntext from unfamiliar domains (e.g., scientific articles). Simplifying the\nentire text makes it understandable but sometimes removes important details. On\nthe contrary, helping adult readers understand difficult concepts in context\ncan enhance their vocabulary and knowledge. In a preliminary human study, we\nfirst identify that lack of context and unfamiliarity with difficult concepts\nis a major reason for adult readers' difficulty with domain-specific text. We\nthen introduce \"targeted concept simplification,\" a simplification task for\nrewriting text to help readers comprehend text containing unfamiliar concepts.\nWe also introduce WikiDomains, a new dataset of 22k definitions from 13\nacademic domains paired with a difficult concept within each definition. We\nbenchmark the performance of open-source and commercial LLMs and a simple\ndictionary baseline on this task across human judgments of ease of\nunderstanding and meaning preservation. Interestingly, our human judges\npreferred explanations about the difficult concept more than simplification of\nthe concept phrase. Further, no single model achieved superior performance\nacross all quality dimensions, and automated metrics also show low correlations\nwith human evaluations of concept simplification ($\\sim0.2$), opening up rich\navenues for research on personalized human reading comprehension support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One useful application of NLP models is to support people in reading complex\ntext from unfamiliar domains (e.g., scientific articles). Simplifying the\nentire text makes it understandable but sometimes removes important details. On\nthe contrary, helping adult readers understand difficult concepts in context\ncan enhance their vocabulary and knowledge. In a preliminary human study, we\nfirst identify that lack of context and unfamiliarity with difficult concepts\nis a major reason for adult readers' difficulty with domain-specific text. We\nthen introduce \"targeted concept simplification,\" a simplification task for\nrewriting text to help readers comprehend text containing unfamiliar concepts.\nWe also introduce WikiDomains, a new dataset of 22k definitions from 13\nacademic domains paired with a difficult concept within each definition. We\nbenchmark the performance of open-source and commercial LLMs and a simple\ndictionary baseline on this task across human judgments of ease of\nunderstanding and meaning preservation. Interestingly, our human judges\npreferred explanations about the difficult concept more than simplification of\nthe concept phrase. Further, no single model achieved superior performance\nacross all quality dimensions, and automated metrics also show low correlations\nwith human evaluations of concept simplification ($\\sim0.2$), opening up rich\navenues for research on personalized human reading comprehension support."
                },
                "authors": [
                    {
                        "name": "Sumit Asthana"
                    },
                    {
                        "name": "Hannah Rashkin"
                    },
                    {
                        "name": "Elizabeth Clark"
                    },
                    {
                        "name": "Fantine Huot"
                    },
                    {
                        "name": "Mirella Lapata"
                    }
                ],
                "author_detail": {
                    "name": "Mirella Lapata"
                },
                "author": "Mirella Lapata",
                "arxiv_comment": "to appear in proceedings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20763v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20763v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07832v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07832v5",
                "updated": "2024-11-05T23:50:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    23,
                    50,
                    14,
                    1,
                    310,
                    0
                ],
                "published": "2024-07-31T14:49:35Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    49,
                    35,
                    2,
                    213,
                    0
                ],
                "title": "LADDER: Language Driven Slice Discovery and Error Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Language Driven Slice Discovery and Error Rectification"
                },
                "summary": "Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Error slice discovery associates structured patterns with model errors.\nExisting methods discover error slices by clustering the error-prone samples\nwith similar patterns or assigning discrete attributes to each sample for\npost-hoc analysis. While these methods aim for interpretability and easier\nmitigation through reweighting or rebalancing, they may not capture the full\ncomplexity of error patterns due to incomplete or missing attributes. Contrary\nto the existing approach, this paper utilizes the reasoning capabilities of the\nLarge Language Model (LLM) to analyze complex error patterns and generate\ntestable hypotheses. This paper proposes LADDER: Language Driven slice\nDiscovery and Error Rectification. It first projects the model's representation\ninto a language-aligned feature space (eg CLIP) to preserve semantics in the\noriginal model feature space. This ensures the accurate retrieval of sentences\nthat highlight the model's errors. Next, the LLM utilizes the sentences and\ngenerates hypotheses to discover error slices. Finally, we mitigate the error\nby fine-tuning the classification head by creating a group-balanced dataset\nusing the hypotheses. Our entire method does not require any attribute\nannotation, either explicitly or through external tagging models. We validate\nour method with \\textbf{five} image classification datasets."
                },
                "authors": [
                    {
                        "name": "Shantanu Ghosh"
                    },
                    {
                        "name": "Rayan Syed"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Clare B. Poynton"
                    },
                    {
                        "name": "Shyam Visweswaran"
                    },
                    {
                        "name": "Kayhan Batmanghelich"
                    }
                ],
                "author_detail": {
                    "name": "Kayhan Batmanghelich"
                },
                "author": "Kayhan Batmanghelich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07832v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07832v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12843v3",
                "updated": "2024-11-05T23:15:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    23,
                    15,
                    46,
                    1,
                    310,
                    0
                ],
                "published": "2024-07-04T15:10:51Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    15,
                    10,
                    51,
                    3,
                    186,
                    0
                ],
                "title": "NutriBench: A Dataset for Evaluating Large Language Models in\n  Carbohydrate Estimation from Meal Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NutriBench: A Dataset for Evaluating Large Language Models in\n  Carbohydrate Estimation from Meal Descriptions"
                },
                "summary": "Accurate nutrition estimation helps people make informed dietary choices and\nis essential in the prevention of serious health complications. We present\nNutriBench, the first publicly available natural language meal description\nnutrition benchmark. NutriBench consists of 11,857 meal descriptions generated\nfrom real-world global dietary intake data. The data is human-verified and\nannotated with macro-nutrient labels, including carbohydrates, proteins, fats,\nand calories. We conduct an extensive evaluation of NutriBench on the task of\ncarbohydrate estimation, testing twelve leading Large Language Models (LLMs),\nincluding GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using\nstandard, Chain-of-Thought and Retrieval-Augmented Generation strategies.\nAdditionally, we present a study involving professional nutritionists, finding\nthat LLMs can provide more accurate and faster estimates. Finally, we perform a\nreal-world risk assessment by simulating the effect of carbohydrate predictions\non the blood glucose levels of individuals with diabetes. Our work highlights\nthe opportunities and challenges of using LLMs for nutrition estimation,\ndemonstrating their potential to aid professionals and laypersons and improve\nhealth outcomes. Our benchmark is publicly available at:\nhttps://mehak126.github.io/nutribench.html",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate nutrition estimation helps people make informed dietary choices and\nis essential in the prevention of serious health complications. We present\nNutriBench, the first publicly available natural language meal description\nnutrition benchmark. NutriBench consists of 11,857 meal descriptions generated\nfrom real-world global dietary intake data. The data is human-verified and\nannotated with macro-nutrient labels, including carbohydrates, proteins, fats,\nand calories. We conduct an extensive evaluation of NutriBench on the task of\ncarbohydrate estimation, testing twelve leading Large Language Models (LLMs),\nincluding GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using\nstandard, Chain-of-Thought and Retrieval-Augmented Generation strategies.\nAdditionally, we present a study involving professional nutritionists, finding\nthat LLMs can provide more accurate and faster estimates. Finally, we perform a\nreal-world risk assessment by simulating the effect of carbohydrate predictions\non the blood glucose levels of individuals with diabetes. Our work highlights\nthe opportunities and challenges of using LLMs for nutrition estimation,\ndemonstrating their potential to aid professionals and laypersons and improve\nhealth outcomes. Our benchmark is publicly available at:\nhttps://mehak126.github.io/nutribench.html"
                },
                "authors": [
                    {
                        "name": "Andong Hua"
                    },
                    {
                        "name": "Mehak Preet Dhaliwal"
                    },
                    {
                        "name": "Ryan Burke"
                    },
                    {
                        "name": "Laya Pullela"
                    },
                    {
                        "name": "Yao Qin"
                    }
                ],
                "author_detail": {
                    "name": "Yao Qin"
                },
                "author": "Yao Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03540v1",
                "updated": "2024-11-05T22:42:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    22,
                    42,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T22:42:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    22,
                    42,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation"
                },
                "summary": "With the recent rise of Large Language Models (LLMs), Vision-Language Models\n(VLMs), and other general foundation models, there is growing potential for\nmultimodal, multi-task embodied agents that can operate in diverse environments\ngiven only natural language as input. One such application area is indoor\nnavigation using natural language instructions. However, despite recent\nprogress, this problem remains challenging due to the spatial reasoning and\nsemantic understanding required, particularly in arbitrary scenes that may\ncontain many objects belonging to fine-grained classes. To address this\nchallenge, we curate the largest real-world dataset for Vision and\nLanguage-guided Action in 3D Scenes (VLA-3D), consisting of over 11.5K scanned\n3D indoor rooms from existing datasets, 23.5M heuristically generated semantic\nrelations between objects, and 9.7M synthetically generated referential\nstatements. Our dataset consists of processed 3D point clouds, semantic object\nand room annotations, scene graphs, navigable free space annotations, and\nreferential language statements that specifically focus on view-independent\nspatial relations for disambiguating objects. The goal of these features is to\naid the downstream task of navigation, especially on real-world systems where\nsome level of robustness must be guaranteed in an open world of changing scenes\nand imperfect language. We benchmark our dataset with current state-of-the-art\nmodels to obtain a performance baseline. All code to generate and visualize the\ndataset is publicly released, see https://github.com/HaochenZ11/VLA-3D. With\nthe release of this dataset, we hope to provide a resource for progress in\nsemantic 3D scene understanding that is robust to changes and one which will\naid the development of interactive indoor navigation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent rise of Large Language Models (LLMs), Vision-Language Models\n(VLMs), and other general foundation models, there is growing potential for\nmultimodal, multi-task embodied agents that can operate in diverse environments\ngiven only natural language as input. One such application area is indoor\nnavigation using natural language instructions. However, despite recent\nprogress, this problem remains challenging due to the spatial reasoning and\nsemantic understanding required, particularly in arbitrary scenes that may\ncontain many objects belonging to fine-grained classes. To address this\nchallenge, we curate the largest real-world dataset for Vision and\nLanguage-guided Action in 3D Scenes (VLA-3D), consisting of over 11.5K scanned\n3D indoor rooms from existing datasets, 23.5M heuristically generated semantic\nrelations between objects, and 9.7M synthetically generated referential\nstatements. Our dataset consists of processed 3D point clouds, semantic object\nand room annotations, scene graphs, navigable free space annotations, and\nreferential language statements that specifically focus on view-independent\nspatial relations for disambiguating objects. The goal of these features is to\naid the downstream task of navigation, especially on real-world systems where\nsome level of robustness must be guaranteed in an open world of changing scenes\nand imperfect language. We benchmark our dataset with current state-of-the-art\nmodels to obtain a performance baseline. All code to generate and visualize the\ndataset is publicly released, see https://github.com/HaochenZ11/VLA-3D. With\nthe release of this dataset, we hope to provide a resource for progress in\nsemantic 3D scene understanding that is robust to changes and one which will\naid the development of interactive indoor navigation systems."
                },
                "authors": [
                    {
                        "name": "Haochen Zhang"
                    },
                    {
                        "name": "Nader Zantout"
                    },
                    {
                        "name": "Pujith Kachana"
                    },
                    {
                        "name": "Zongyuan Wu"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Wenshan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenshan Wang"
                },
                "author": "Wenshan Wang",
                "arxiv_comment": "Accepted and presented at the 1st Workshop on Semantic Reasoning and\n  Goal Understanding in Robotics (SemRob), Robotics Science and Systems\n  Conference (RSS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03538v1",
                "updated": "2024-11-05T22:37:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    22,
                    37,
                    43,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T22:37:43Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    22,
                    37,
                    43,
                    1,
                    310,
                    0
                ],
                "title": "Long Context RAG Performance of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context RAG Performance of Large Language Models"
                },
                "summary": "Retrieval Augmented Generation (RAG) has emerged as a crucial technique for\nenhancing the accuracy of Large Language Models (LLMs) by incorporating\nexternal information. With the advent of LLMs that support increasingly longer\ncontext lengths, there is a growing interest in understanding how these models\nperform in RAG scenarios. Can these new long context models improve RAG\nperformance? This paper presents a comprehensive study of the impact of\nincreased context length on RAG performance across 20 popular open source and\ncommercial LLMs. We ran RAG workflows while varying the total context length\nfrom 2,000 to 128,000 tokens (and 2 million tokens when possible) on three\ndomain-specific datasets, and report key insights on the benefits and\nlimitations of long context in RAG applications. Our findings reveal that while\nretrieving more documents can improve performance, only a handful of the most\nrecent state of the art LLMs can maintain consistent accuracy at long context\nabove 64k tokens. We also identify distinct failure modes in long context\nscenarios, suggesting areas for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has emerged as a crucial technique for\nenhancing the accuracy of Large Language Models (LLMs) by incorporating\nexternal information. With the advent of LLMs that support increasingly longer\ncontext lengths, there is a growing interest in understanding how these models\nperform in RAG scenarios. Can these new long context models improve RAG\nperformance? This paper presents a comprehensive study of the impact of\nincreased context length on RAG performance across 20 popular open source and\ncommercial LLMs. We ran RAG workflows while varying the total context length\nfrom 2,000 to 128,000 tokens (and 2 million tokens when possible) on three\ndomain-specific datasets, and report key insights on the benefits and\nlimitations of long context in RAG applications. Our findings reveal that while\nretrieving more documents can improve performance, only a handful of the most\nrecent state of the art LLMs can maintain consistent accuracy at long context\nabove 64k tokens. We also identify distinct failure modes in long context\nscenarios, suggesting areas for future research."
                },
                "authors": [
                    {
                        "name": "Quinn Leng"
                    },
                    {
                        "name": "Jacob Portes"
                    },
                    {
                        "name": "Sam Havens"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Michael Carbin"
                    }
                ],
                "author_detail": {
                    "name": "Michael Carbin"
                },
                "author": "Michael Carbin",
                "arxiv_comment": "2024 NeurIPS workshop on Adaptive Foundation Models: Evolving AI for\n  Personalized and Efficient Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16450v2",
                "updated": "2024-11-05T22:34:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    22,
                    34,
                    29,
                    1,
                    310,
                    0
                ],
                "published": "2024-06-24T08:43:21Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    8,
                    43,
                    21,
                    0,
                    176,
                    0
                ],
                "title": "Building on Efficient Foundations: Effectively Training LLMs with\n  Structured Feedforward Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building on Efficient Foundations: Effectively Training LLMs with\n  Structured Feedforward Layers"
                },
                "summary": "State-of-the-art results in large language models (LLMs) often rely on scale,\nwhich becomes computationally expensive. This has sparked a research agenda to\nreduce these models' parameter counts and computational costs without\nsignificantly impacting their performance. Our study focuses on\ntransformer-based LLMs, specifically targeting the computationally intensive\nfeedforward networks (FFNs), which are less studied than attention blocks. We\nconsider three structured linear parameterizations of the FFN using efficient\nlow-rank and block-diagonal matrices. In contrast to many previous works that\nexamined these approximations, our study i) explores these structures from a\ntraining-from-scratch perspective, ii) scales up to 1.3B parameters, and iii)\nis conducted within recent Transformer-based LLMs rather than convolutional\narchitectures. We demonstrate that these structures can lead to actual\ncomputational gains in various scenarios, including online decoding when using\na pre-merge technique. Additionally, we propose a novel training regime, called\n\\textit{self-guided training}, aimed at improving the poor training dynamics\nthat these approximations exhibit when used from initialization. Interestingly,\nthe scaling performance of structured matrices is explored, revealing steeper\ncurves in scaling training FLOPs, along with a favorable scaling trend in the\novertraining regime. Specifically, we show that wide and structured networks\ncan utilize training FLOPs more efficiently, with fewer parameters and lower\nloss than dense models at their optimal trade-off. Our code is available at\n\\url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art results in large language models (LLMs) often rely on scale,\nwhich becomes computationally expensive. This has sparked a research agenda to\nreduce these models' parameter counts and computational costs without\nsignificantly impacting their performance. Our study focuses on\ntransformer-based LLMs, specifically targeting the computationally intensive\nfeedforward networks (FFNs), which are less studied than attention blocks. We\nconsider three structured linear parameterizations of the FFN using efficient\nlow-rank and block-diagonal matrices. In contrast to many previous works that\nexamined these approximations, our study i) explores these structures from a\ntraining-from-scratch perspective, ii) scales up to 1.3B parameters, and iii)\nis conducted within recent Transformer-based LLMs rather than convolutional\narchitectures. We demonstrate that these structures can lead to actual\ncomputational gains in various scenarios, including online decoding when using\na pre-merge technique. Additionally, we propose a novel training regime, called\n\\textit{self-guided training}, aimed at improving the poor training dynamics\nthat these approximations exhibit when used from initialization. Interestingly,\nthe scaling performance of structured matrices is explored, revealing steeper\ncurves in scaling training FLOPs, along with a favorable scaling trend in the\novertraining regime. Specifically, we show that wide and structured networks\ncan utilize training FLOPs more efficiently, with fewer parameters and lower\nloss than dense models at their optimal trade-off. Our code is available at\n\\url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main}."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Skander Moalla"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "arxiv_comment": "Accepted by NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05269v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05269v3",
                "updated": "2024-11-05T22:08:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    22,
                    8,
                    14,
                    1,
                    310,
                    0
                ],
                "published": "2023-12-07T19:19:25Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    19,
                    19,
                    25,
                    3,
                    341,
                    0
                ],
                "title": "LifelongMemory: Leveraging LLMs for Answering Queries in Long-form\n  Egocentric Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LifelongMemory: Leveraging LLMs for Answering Queries in Long-form\n  Egocentric Videos"
                },
                "summary": "In this paper we introduce LifelongMemory, a new framework for accessing\nlong-form egocentric videographic memory through natural language question\nanswering and retrieval. LifelongMemory generates concise video activity\ndescriptions of the camera wearer and leverages the zero-shot capabilities of\npretrained large language models to perform reasoning over long-form video\ncontext. Furthermore, LifelongMemory uses a confidence and explanation module\nto produce confident, high-quality, and interpretable answers. Our approach\nachieves state-of-the-art performance on the EgoSchema benchmark for question\nanswering and is highly competitive on the natural language query (NLQ)\nchallenge of Ego4D. Code is available at\nhttps://github.com/agentic-learning-ai-lab/lifelong-memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we introduce LifelongMemory, a new framework for accessing\nlong-form egocentric videographic memory through natural language question\nanswering and retrieval. LifelongMemory generates concise video activity\ndescriptions of the camera wearer and leverages the zero-shot capabilities of\npretrained large language models to perform reasoning over long-form video\ncontext. Furthermore, LifelongMemory uses a confidence and explanation module\nto produce confident, high-quality, and interpretable answers. Our approach\nachieves state-of-the-art performance on the EgoSchema benchmark for question\nanswering and is highly competitive on the natural language query (NLQ)\nchallenge of Ego4D. Code is available at\nhttps://github.com/agentic-learning-ai-lab/lifelong-memory."
                },
                "authors": [
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Yanlai Yang"
                    },
                    {
                        "name": "Mengye Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengye Ren"
                },
                "author": "Mengye Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05269v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05269v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03522v1",
                "updated": "2024-11-05T21:57:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    21,
                    57,
                    38,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T21:57:38Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    21,
                    57,
                    38,
                    1,
                    310,
                    0
                ],
                "title": "Exploring the Potentials and Challenges of Using Large Language Models\n  for the Analysis of Transcriptional Regulation of Long Non-coding RNAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potentials and Challenges of Using Large Language Models\n  for the Analysis of Transcriptional Regulation of Long Non-coding RNAs"
                },
                "summary": "Research on long non-coding RNAs (lncRNAs) has garnered significant attention\ndue to their critical roles in gene regulation and disease mechanisms. However,\nthe complexity and diversity of lncRNA sequences, along with the limited\nknowledge of their functional mechanisms and the regulation of their\nexpressions, pose significant challenges to lncRNA studies. Given the\ntremendous success of large language models (LLMs) in capturing complex\ndependencies in sequential data, this study aims to systematically explore the\npotential and limitations of LLMs in the sequence analysis related to the\ntranscriptional regulation of lncRNA genes. Our extensive experiments\ndemonstrated promising performance of fine-tuned genome foundation models on\nprogressively complex tasks. Furthermore, we conducted an insightful analysis\nof the critical impact of task complexity, model selection, data quality, and\nbiological interpretability for the studies of the regulation of lncRNA gene\nexpression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on long non-coding RNAs (lncRNAs) has garnered significant attention\ndue to their critical roles in gene regulation and disease mechanisms. However,\nthe complexity and diversity of lncRNA sequences, along with the limited\nknowledge of their functional mechanisms and the regulation of their\nexpressions, pose significant challenges to lncRNA studies. Given the\ntremendous success of large language models (LLMs) in capturing complex\ndependencies in sequential data, this study aims to systematically explore the\npotential and limitations of LLMs in the sequence analysis related to the\ntranscriptional regulation of lncRNA genes. Our extensive experiments\ndemonstrated promising performance of fine-tuned genome foundation models on\nprogressively complex tasks. Furthermore, we conducted an insightful analysis\nof the critical impact of task complexity, model selection, data quality, and\nbiological interpretability for the studies of the regulation of lncRNA gene\nexpression."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Zhichao Hou"
                    },
                    {
                        "name": "Xiaorui Liu"
                    },
                    {
                        "name": "Xinxia Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xinxia Peng"
                },
                "author": "Xinxia Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03519v1",
                "updated": "2024-11-05T21:54:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    21,
                    54,
                    14,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T21:54:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    21,
                    54,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation\n  with Out-of-order Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation\n  with Out-of-order Execution"
                },
                "summary": "With more advanced natural language understanding and reasoning capabilities,\nlarge language model (LLM)-powered agents are increasingly developed in\nsimulated environments to perform complex tasks, interact with other agents,\nand exhibit emergent behaviors relevant to social science and gaming. However,\ncurrent multi-agent simulations frequently suffer from inefficiencies due to\nthe limited parallelism caused by false dependencies, resulting in performance\nbottlenecks. In this paper, we introduce AI Metropolis, a simulation engine\nthat improves the efficiency of LLM agent simulations by incorporating\nout-of-order execution scheduling. By dynamically tracking real dependencies\nbetween agents, AI Metropolis minimizes false dependencies, enhancing\nparallelism and enabling efficient hardware utilization. Our evaluations\ndemonstrate that AI Metropolis achieves speedups from 1.3x to 4.15x over\nstandard parallel simulation with global synchronization, approaching optimal\nperformance as the number of agents increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With more advanced natural language understanding and reasoning capabilities,\nlarge language model (LLM)-powered agents are increasingly developed in\nsimulated environments to perform complex tasks, interact with other agents,\nand exhibit emergent behaviors relevant to social science and gaming. However,\ncurrent multi-agent simulations frequently suffer from inefficiencies due to\nthe limited parallelism caused by false dependencies, resulting in performance\nbottlenecks. In this paper, we introduce AI Metropolis, a simulation engine\nthat improves the efficiency of LLM agent simulations by incorporating\nout-of-order execution scheduling. By dynamically tracking real dependencies\nbetween agents, AI Metropolis minimizes false dependencies, enhancing\nparallelism and enabling efficient hardware utilization. Our evaluations\ndemonstrate that AI Metropolis achieves speedups from 1.3x to 4.15x over\nstandard parallel simulation with global synchronization, approaching optimal\nperformance as the number of agents increases."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Kayvon Fatahalian"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03513v1",
                "updated": "2024-11-05T21:19:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    21,
                    19,
                    49,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T21:19:49Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    21,
                    19,
                    49,
                    1,
                    310,
                    0
                ],
                "title": "Change Is the Only Constant: Dynamic LLM Slicing based on Layer\n  Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Change Is the Only Constant: Dynamic LLM Slicing based on Layer\n  Redundancy"
                },
                "summary": "This paper introduces a novel model compression approach through dynamic\nlayer-specific pruning in Large Language Models (LLMs), enhancing the\ntraditional methodology established by SliceGPT. By transitioning from constant\nto dynamic slicing, our method leverages the newly proposed Layer Redundancy\n(LR) score, which assesses how much change each layer changes its input by\nmeasuring the cosine similarity of the input to the output of the layer. We use\nthis score to prune parts of individual layers based on redundancy in such a\nway that the average pruned percentage for all layers is a fixed value. We\nconducted extensive experiments using models like Llama3-8B and Mistral-7B on\nmultiple datasets, evaluating different slicing bases and percentages to\ndetermine optimal configurations that balance efficiency and performance. Our\nfindings show that our dynamic slicing approach not only maintains but, in many\ncases, enhances model performance compared to the baseline established by\nconstant slicing methods. For instance, in several settings, we see performance\nimprovements of up to 5% over the SliceGPT baseline. Additionally, a perplexity\ndecrease by as much as 7% was observed across multiple benchmarks, validating\nthe effectiveness of our method. The code, model weights, and datasets are\nopen-sourced at https://github.com/RazvanDu/DynamicSlicing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel model compression approach through dynamic\nlayer-specific pruning in Large Language Models (LLMs), enhancing the\ntraditional methodology established by SliceGPT. By transitioning from constant\nto dynamic slicing, our method leverages the newly proposed Layer Redundancy\n(LR) score, which assesses how much change each layer changes its input by\nmeasuring the cosine similarity of the input to the output of the layer. We use\nthis score to prune parts of individual layers based on redundancy in such a\nway that the average pruned percentage for all layers is a fixed value. We\nconducted extensive experiments using models like Llama3-8B and Mistral-7B on\nmultiple datasets, evaluating different slicing bases and percentages to\ndetermine optimal configurations that balance efficiency and performance. Our\nfindings show that our dynamic slicing approach not only maintains but, in many\ncases, enhances model performance compared to the baseline established by\nconstant slicing methods. For instance, in several settings, we see performance\nimprovements of up to 5% over the SliceGPT baseline. Additionally, a perplexity\ndecrease by as much as 7% was observed across multiple benchmarks, validating\nthe effectiveness of our method. The code, model weights, and datasets are\nopen-sourced at https://github.com/RazvanDu/DynamicSlicing."
                },
                "authors": [
                    {
                        "name": "Razvan-Gabriel Dumitru"
                    },
                    {
                        "name": "Paul-Ioan Clotan"
                    },
                    {
                        "name": "Vikas Yadav"
                    },
                    {
                        "name": "Darius Peteleaza"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    }
                ],
                "author_detail": {
                    "name": "Mihai Surdeanu"
                },
                "author": "Mihai Surdeanu",
                "arxiv_comment": "Accepted at EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01645v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01645v2",
                "updated": "2024-11-05T21:02:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    21,
                    2,
                    11,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-03T17:45:00Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    17,
                    45,
                    0,
                    6,
                    308,
                    0
                ],
                "title": "Enriching Tabular Data with Contextual LLM Embeddings: A Comprehensive\n  Ablation Study for Ensemble Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enriching Tabular Data with Contextual LLM Embeddings: A Comprehensive\n  Ablation Study for Ensemble Classifiers"
                },
                "summary": "Feature engineering is crucial for optimizing machine learning model\nperformance, particularly in tabular data classification tasks. Leveraging\nadvancements in natural language processing, this study presents a systematic\napproach to enrich tabular datasets with features derived from large language\nmodel embeddings. Through a comprehensive ablation study on diverse datasets,\nwe assess the impact of RoBERTa and GPT-2 embeddings on ensemble classifiers,\nincluding Random Forest, XGBoost, and CatBoost. Results indicate that\nintegrating embeddings with traditional numerical and categorical features\noften enhances predictive performance, especially on datasets with class\nimbalance or limited features and samples, such as UCI Adult, Heart Disease,\nTitanic, and Pima Indian Diabetes, with improvements particularly notable in\nXGBoost and CatBoost classifiers. Additionally, feature importance analysis\nreveals that LLM-derived features frequently rank among the most impactful for\nthe predictions. This study provides a structured approach to embedding-based\nfeature enrichment and illustrates its benefits in ensemble learning for\ntabular data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature engineering is crucial for optimizing machine learning model\nperformance, particularly in tabular data classification tasks. Leveraging\nadvancements in natural language processing, this study presents a systematic\napproach to enrich tabular datasets with features derived from large language\nmodel embeddings. Through a comprehensive ablation study on diverse datasets,\nwe assess the impact of RoBERTa and GPT-2 embeddings on ensemble classifiers,\nincluding Random Forest, XGBoost, and CatBoost. Results indicate that\nintegrating embeddings with traditional numerical and categorical features\noften enhances predictive performance, especially on datasets with class\nimbalance or limited features and samples, such as UCI Adult, Heart Disease,\nTitanic, and Pima Indian Diabetes, with improvements particularly notable in\nXGBoost and CatBoost classifiers. Additionally, feature importance analysis\nreveals that LLM-derived features frequently rank among the most impactful for\nthe predictions. This study provides a structured approach to embedding-based\nfeature enrichment and illustrates its benefits in ensemble learning for\ntabular data."
                },
                "authors": [
                    {
                        "name": "Gjergji Kasneci"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01645v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01645v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03503v1",
                "updated": "2024-11-05T20:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    33,
                    49,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T20:33:49Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    33,
                    49,
                    1,
                    310,
                    0
                ],
                "title": "TwiNet: Connecting Real World Networks to their Digital Twins Through a\n  Live Bidirectional Link",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TwiNet: Connecting Real World Networks to their Digital Twins Through a\n  Live Bidirectional Link"
                },
                "summary": "Only the chairs can edit The wireless spectrum's increasing complexity poses\nchallenges and opportunities, highlighting the necessity for real-time\nsolutions and robust data processing capabilities. Digital Twin (DT), virtual\nreplicas of physical systems, integrate real-time data to mirror their\nreal-world counterparts, enabling precise monitoring and optimization.\nIncorporating DTs into wireless communication enhances predictive maintenance,\nresource allocation, and troubleshooting, thus bolstering network reliability.\nOur paper introduces TwiNet, enabling bidirectional, near-realtime links\nbetween real-world wireless spectrum scenarios and DT replicas. Utilizing the\nprotocol, MQTT, we can achieve data transfer times with an average latency of\n14 ms, suitable for real-time communication. This is confirmed by monitoring\nreal-world traffic and mirroring it in real-time within the DT's wireless\nenvironment. We evaluate TwiNet's performance in two use cases: (i) assessing\nrisky traffic configurations of UEs in a Safe Adaptive Data Rate (SADR) system,\nimproving network performance by approximately 15% compared to original network\nselections; and (ii) deploying new CNNs in response to jammed pilots, achieving\nup to 97% accuracy training on artificial data and deploying a new model in as\nlow as 2 minutes to counter persistent adversaries. TwiNet enables swift\ndeployment and adaptation of DTs, addressing crucial challenges in modern\nwireless communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Only the chairs can edit The wireless spectrum's increasing complexity poses\nchallenges and opportunities, highlighting the necessity for real-time\nsolutions and robust data processing capabilities. Digital Twin (DT), virtual\nreplicas of physical systems, integrate real-time data to mirror their\nreal-world counterparts, enabling precise monitoring and optimization.\nIncorporating DTs into wireless communication enhances predictive maintenance,\nresource allocation, and troubleshooting, thus bolstering network reliability.\nOur paper introduces TwiNet, enabling bidirectional, near-realtime links\nbetween real-world wireless spectrum scenarios and DT replicas. Utilizing the\nprotocol, MQTT, we can achieve data transfer times with an average latency of\n14 ms, suitable for real-time communication. This is confirmed by monitoring\nreal-world traffic and mirroring it in real-time within the DT's wireless\nenvironment. We evaluate TwiNet's performance in two use cases: (i) assessing\nrisky traffic configurations of UEs in a Safe Adaptive Data Rate (SADR) system,\nimproving network performance by approximately 15% compared to original network\nselections; and (ii) deploying new CNNs in response to jammed pilots, achieving\nup to 97% accuracy training on artificial data and deploying a new model in as\nlow as 2 minutes to counter persistent adversaries. TwiNet enables swift\ndeployment and adaptation of DTs, addressing crucial challenges in modern\nwireless communication systems."
                },
                "authors": [
                    {
                        "name": "Clifton Paul Robinson"
                    },
                    {
                        "name": "Andrea Lacava"
                    },
                    {
                        "name": "Pedram Johari"
                    },
                    {
                        "name": "Francesca Cuomo"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_comment": "6 pages, 7 figures, conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03500v1",
                "updated": "2024-11-05T20:29:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    29,
                    50,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T20:29:50Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    29,
                    50,
                    1,
                    310,
                    0
                ],
                "title": "λ-Tune: Harnessing Large Language Models for Automated Database\n  System Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "λ-Tune: Harnessing Large Language Models for Automated Database\n  System Tuning"
                },
                "summary": "We introduce {\\lambda}-Tune, a framework that leverages Large Language Models\n(LLMs) for automated database system tuning. The design of {\\lambda}-Tune is\nmotivated by the capabilities of the latest generation of LLMs. Different from\nprior work, leveraging LLMs to extract tuning hints for single parameters,\n{\\lambda}-Tune generates entire configuration scripts, based on a large input\ndocument, describing the tuning context. {\\lambda}-Tune generates alternative\nconfigurations, using a principled approach to identify the best configuration,\nout of a small set of candidates. In doing so, it minimizes reconfiguration\noverheads and ensures that evaluation costs are bounded as a function of the\noptimal run time. By treating prompt generation as a cost-based optimization\nproblem, {\\lambda}-Tune conveys the most relevant context to the LLM while\nbounding the number of input tokens and, therefore, monetary fees for LLM\ninvocations. We compare {\\lambda}-Tune to various baselines, using multiple\nbenchmarks and PostgreSQL and MySQL as target systems for tuning, showing that\n{\\lambda}-Tune is significantly more robust than prior approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce {\\lambda}-Tune, a framework that leverages Large Language Models\n(LLMs) for automated database system tuning. The design of {\\lambda}-Tune is\nmotivated by the capabilities of the latest generation of LLMs. Different from\nprior work, leveraging LLMs to extract tuning hints for single parameters,\n{\\lambda}-Tune generates entire configuration scripts, based on a large input\ndocument, describing the tuning context. {\\lambda}-Tune generates alternative\nconfigurations, using a principled approach to identify the best configuration,\nout of a small set of candidates. In doing so, it minimizes reconfiguration\noverheads and ensures that evaluation costs are bounded as a function of the\noptimal run time. By treating prompt generation as a cost-based optimization\nproblem, {\\lambda}-Tune conveys the most relevant context to the LLM while\nbounding the number of input tokens and, therefore, monetary fees for LLM\ninvocations. We compare {\\lambda}-Tune to various baselines, using multiple\nbenchmarks and PostgreSQL and MySQL as target systems for tuning, showing that\n{\\lambda}-Tune is significantly more robust than prior approaches."
                },
                "authors": [
                    {
                        "name": "Victor Giannankouris"
                    },
                    {
                        "name": "Immanuel Trummer"
                    }
                ],
                "author_detail": {
                    "name": "Immanuel Trummer"
                },
                "author": "Immanuel Trummer",
                "arxiv_comment": "To be presented at SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03495v1",
                "updated": "2024-11-05T20:18:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    18,
                    53,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T20:18:53Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    18,
                    53,
                    1,
                    310,
                    0
                ],
                "title": "Automatic Generation of Question Hints for Mathematics Problems using\n  Large Language Models in Educational Technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Generation of Question Hints for Mathematics Problems using\n  Large Language Models in Educational Technology"
                },
                "summary": "The automatic generation of hints by Large Language Models (LLMs) within\nIntelligent Tutoring Systems (ITSs) has shown potential to enhance student\nlearning. However, generating pedagogically sound hints that address student\nmisconceptions and adhere to specific educational objectives remains\nchallenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as\nteachers to generate effective hints for students simulated through LLMs\n(GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math\nexercises designed for human high-school students, and designed using cognitive\nscience principles. We present here the study of several dimensions: 1)\nidentifying error patterns made by simulated students on secondary-level math\nexercises; 2) developing various prompts for GPT-4o as a teacher and evaluating\ntheir effectiveness in generating hints that enable simulated students to\nself-correct; and 3) testing the best-performing prompts, based on their\nability to produce relevant hints and facilitate error correction, with\nLlama-3-8B-Instruct as the teacher, allowing for a performance comparison with\nGPT-4o. The results show that model errors increase with higher temperature\nsettings. Notably, when hints are generated by GPT-4o, the most effective\nprompts include prompts tailored to specific errors as well as prompts\nproviding general hints based on common mathematical errors. Interestingly,\nLlama-3-8B-Instruct as a teacher showed better overall performance than GPT-4o.\nAlso the problem-solving and response revision capabilities of the LLMs as\nstudents, particularly GPT-3.5-turbo, improved significantly after receiving\nhints, especially at lower temperature settings. However, models like\nMistral-7B-Instruct demonstrated a decline in performance as the temperature\nincreased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic generation of hints by Large Language Models (LLMs) within\nIntelligent Tutoring Systems (ITSs) has shown potential to enhance student\nlearning. However, generating pedagogically sound hints that address student\nmisconceptions and adhere to specific educational objectives remains\nchallenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as\nteachers to generate effective hints for students simulated through LLMs\n(GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math\nexercises designed for human high-school students, and designed using cognitive\nscience principles. We present here the study of several dimensions: 1)\nidentifying error patterns made by simulated students on secondary-level math\nexercises; 2) developing various prompts for GPT-4o as a teacher and evaluating\ntheir effectiveness in generating hints that enable simulated students to\nself-correct; and 3) testing the best-performing prompts, based on their\nability to produce relevant hints and facilitate error correction, with\nLlama-3-8B-Instruct as the teacher, allowing for a performance comparison with\nGPT-4o. The results show that model errors increase with higher temperature\nsettings. Notably, when hints are generated by GPT-4o, the most effective\nprompts include prompts tailored to specific errors as well as prompts\nproviding general hints based on common mathematical errors. Interestingly,\nLlama-3-8B-Instruct as a teacher showed better overall performance than GPT-4o.\nAlso the problem-solving and response revision capabilities of the LLMs as\nstudents, particularly GPT-3.5-turbo, improved significantly after receiving\nhints, especially at lower temperature settings. However, models like\nMistral-7B-Instruct demonstrated a decline in performance as the temperature\nincreased."
                },
                "authors": [
                    {
                        "name": "Junior Cedric Tonga"
                    },
                    {
                        "name": "Benjamin Clement"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Yves Oudeyer"
                },
                "author": "Pierre-Yves Oudeyer",
                "arxiv_comment": "Accepted at NeurIPS 2024 Workshop on Large Foundation Models for\n  Educational Assessment (FM-Assess)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03493v1",
                "updated": "2024-11-05T20:18:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    18,
                    28,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T20:18:28Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    18,
                    28,
                    1,
                    310,
                    0
                ],
                "title": "LASER: Attention with Exponential Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASER: Attention with Exponential Transformation"
                },
                "summary": "Transformers have had tremendous impact for several sequence related tasks,\nlargely due to their ability to retrieve from any part of the sequence via\nsoftmax based dot-product attention. This mechanism plays a crucial role in\nTransformer's performance. We analyze the gradients backpropagated through the\nsoftmax operation in the attention mechanism and observe that these gradients\ncan often be small. This poor gradient signal backpropagation can lead to\ninefficient learning of parameters preceeding the attention operations. To this\nend, we introduce a new attention mechanism called LASER, which we analytically\nshow to admit a larger gradient signal. We show that LASER Attention can be\nimplemented by making small modifications to existing attention\nimplementations. We conduct experiments on autoregressive large language models\n(LLMs) with upto 2.2 billion parameters where we show upto 3.38% and an average\nof ~1% improvement over standard attention on downstream evaluations. Using\nLASER gives the following relative improvements in generalization performance\nacross a variety of tasks (vision, text and speech): 4.67% accuracy in Vision\nTransformer (ViT) on Imagenet, 2.25% error rate in Conformer on the Librispeech\nspeech-to-text and 0.93% fraction of incorrect predictions in BERT with 2.2\nbillion parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have had tremendous impact for several sequence related tasks,\nlargely due to their ability to retrieve from any part of the sequence via\nsoftmax based dot-product attention. This mechanism plays a crucial role in\nTransformer's performance. We analyze the gradients backpropagated through the\nsoftmax operation in the attention mechanism and observe that these gradients\ncan often be small. This poor gradient signal backpropagation can lead to\ninefficient learning of parameters preceeding the attention operations. To this\nend, we introduce a new attention mechanism called LASER, which we analytically\nshow to admit a larger gradient signal. We show that LASER Attention can be\nimplemented by making small modifications to existing attention\nimplementations. We conduct experiments on autoregressive large language models\n(LLMs) with upto 2.2 billion parameters where we show upto 3.38% and an average\nof ~1% improvement over standard attention on downstream evaluations. Using\nLASER gives the following relative improvements in generalization performance\nacross a variety of tasks (vision, text and speech): 4.67% accuracy in Vision\nTransformer (ViT) on Imagenet, 2.25% error rate in Conformer on the Librispeech\nspeech-to-text and 0.93% fraction of incorrect predictions in BERT with 2.2\nbillion parameters."
                },
                "authors": [
                    {
                        "name": "Sai Surya Duvvuri"
                    },
                    {
                        "name": "Inderjit S. Dhillon"
                    }
                ],
                "author_detail": {
                    "name": "Inderjit S. Dhillon"
                },
                "author": "Inderjit S. Dhillon",
                "arxiv_comment": "15 pages, under review in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03486v1",
                "updated": "2024-11-05T20:10:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    10,
                    25,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T20:10:25Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    10,
                    25,
                    1,
                    310,
                    0
                ],
                "title": "LLM Generated Distribution-Based Prediction of US Electoral Results,\n  Part I",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Generated Distribution-Based Prediction of US Electoral Results,\n  Part I"
                },
                "summary": "This paper introduces distribution-based prediction, a novel approach to\nusing Large Language Models (LLMs) as predictive tools by interpreting output\ntoken probabilities as distributions representing the models' learned\nrepresentation of the world. This distribution-based nature offers an\nalternative perspective for analyzing algorithmic fidelity, complementing the\napproach used in silicon sampling. We demonstrate the use of distribution-based\nprediction in the context of recent United States presidential election,\nshowing that this method can be used to determine task specific bias, prompt\nnoise, and algorithmic fidelity. This approach has significant implications for\nassessing the reliability and increasing transparency of LLM-based predictions\nacross various domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces distribution-based prediction, a novel approach to\nusing Large Language Models (LLMs) as predictive tools by interpreting output\ntoken probabilities as distributions representing the models' learned\nrepresentation of the world. This distribution-based nature offers an\nalternative perspective for analyzing algorithmic fidelity, complementing the\napproach used in silicon sampling. We demonstrate the use of distribution-based\nprediction in the context of recent United States presidential election,\nshowing that this method can be used to determine task specific bias, prompt\nnoise, and algorithmic fidelity. This approach has significant implications for\nassessing the reliability and increasing transparency of LLM-based predictions\nacross various domains."
                },
                "authors": [
                    {
                        "name": "Caleb Bradshaw"
                    },
                    {
                        "name": "Caelen Miller"
                    },
                    {
                        "name": "Sean Warnick"
                    }
                ],
                "author_detail": {
                    "name": "Sean Warnick"
                },
                "author": "Sean Warnick",
                "arxiv_comment": "17 pages, 10 Figures, Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03484v1",
                "updated": "2024-11-05T20:08:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    8,
                    23,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T20:08:23Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    8,
                    23,
                    1,
                    310,
                    0
                ],
                "title": "Automated, LLM enabled extraction of synthesis details for reticular\n  materials from scientific literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated, LLM enabled extraction of synthesis details for reticular\n  materials from scientific literature"
                },
                "summary": "Automated knowledge extraction from scientific literature can potentially\naccelerate materials discovery. We have investigated an approach for extracting\nsynthesis protocols for reticular materials from scientific literature using\nlarge language models (LLMs). To that end, we introduce a Knowledge Extraction\nPipeline (KEP) that automatizes LLM-assisted paragraph classification and\ninformation extraction. By applying prompt engineering with in-context learning\n(ICL) to a set of open-source LLMs, we demonstrate that LLMs can retrieve\nchemical information from PDF documents, without the need for fine-tuning or\ntraining and at a reduced risk of hallucination. By comparing the performance\nof five open-source families of LLMs in both paragraph classification and\ninformation extraction tasks, we observe excellent model performance even if\nonly few example paragraphs are included in the ICL prompts. The results show\nthe potential of the KEP approach for reducing human annotations and data\ncuration efforts in automated scientific knowledge extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated knowledge extraction from scientific literature can potentially\naccelerate materials discovery. We have investigated an approach for extracting\nsynthesis protocols for reticular materials from scientific literature using\nlarge language models (LLMs). To that end, we introduce a Knowledge Extraction\nPipeline (KEP) that automatizes LLM-assisted paragraph classification and\ninformation extraction. By applying prompt engineering with in-context learning\n(ICL) to a set of open-source LLMs, we demonstrate that LLMs can retrieve\nchemical information from PDF documents, without the need for fine-tuning or\ntraining and at a reduced risk of hallucination. By comparing the performance\nof five open-source families of LLMs in both paragraph classification and\ninformation extraction tasks, we observe excellent model performance even if\nonly few example paragraphs are included in the ICL prompts. The results show\nthe potential of the KEP approach for reducing human annotations and data\ncuration efforts in automated scientific knowledge extraction."
                },
                "authors": [
                    {
                        "name": "Viviane Torres da Silva"
                    },
                    {
                        "name": "Alexandre Rademaker"
                    },
                    {
                        "name": "Krystelle Lionti"
                    },
                    {
                        "name": "Ronaldo Giro"
                    },
                    {
                        "name": "Geisa Lima"
                    },
                    {
                        "name": "Sandro Fiorini"
                    },
                    {
                        "name": "Marcelo Archanjo"
                    },
                    {
                        "name": "Breno W. Carvalho"
                    },
                    {
                        "name": "Rodrigo Neumann"
                    },
                    {
                        "name": "Anaximandro Souza"
                    },
                    {
                        "name": "João Pedro Souza"
                    },
                    {
                        "name": "Gabriela de Valnisio"
                    },
                    {
                        "name": "Carmen Nilda Paz"
                    },
                    {
                        "name": "Renato Cerqueira"
                    },
                    {
                        "name": "Mathias Steiner"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Steiner"
                },
                "author": "Mathias Steiner",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03477v1",
                "updated": "2024-11-05T20:01:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    1,
                    20,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T20:01:20Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    20,
                    1,
                    20,
                    1,
                    310,
                    0
                ],
                "title": "CrowdGenUI: Enhancing LLM-Based UI Widget Generation with a Crowdsourced\n  Preference Library",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrowdGenUI: Enhancing LLM-Based UI Widget Generation with a Crowdsourced\n  Preference Library"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable skills across\nvarious design domains, including UI generation. However, current LLMs for UI\ngeneration tend to offer generic solutions that lack a deep understanding of\ntask context and user preferences in specific scenarios. We present\n\\textit{CrowdGenUI}, a framework that enhances LLM-driven UI generation with a\ncrowdsourced user preference library. This approach addresses the limitations\nof existing methods by guiding LLM reasoning with user preferences, enabling\nthe generation of UI widgets that align more closely with user needs and\ntask-specific requirements. Using image editing as a test domain, we built this\nlibrary from 50 users, capturing 720 user preferences, which include the\npredictability, efficiency, and explorability of multiple UI widgets. In a user\nstudy with 72 additional participants, our framework outperformed standard\nLLM-generated widgets in meeting user preferences and task requirements. We\ndiscuss these findings to inform future opportunities for designing\nuser-centered and customizable UIs by comprehensively analyzing the\nextendability of the proposed framework and crowdsourced library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable skills across\nvarious design domains, including UI generation. However, current LLMs for UI\ngeneration tend to offer generic solutions that lack a deep understanding of\ntask context and user preferences in specific scenarios. We present\n\\textit{CrowdGenUI}, a framework that enhances LLM-driven UI generation with a\ncrowdsourced user preference library. This approach addresses the limitations\nof existing methods by guiding LLM reasoning with user preferences, enabling\nthe generation of UI widgets that align more closely with user needs and\ntask-specific requirements. Using image editing as a test domain, we built this\nlibrary from 50 users, capturing 720 user preferences, which include the\npredictability, efficiency, and explorability of multiple UI widgets. In a user\nstudy with 72 additional participants, our framework outperformed standard\nLLM-generated widgets in meeting user preferences and task requirements. We\ndiscuss these findings to inform future opportunities for designing\nuser-centered and customizable UIs by comprehensively analyzing the\nextendability of the proposed framework and crowdsourced library."
                },
                "authors": [
                    {
                        "name": "Yimeng Liu"
                    },
                    {
                        "name": "Misha Sra"
                    },
                    {
                        "name": "Chang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xiao"
                },
                "author": "Chang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03471v1",
                "updated": "2024-11-05T19:52:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    19,
                    52,
                    58,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T19:52:58Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    19,
                    52,
                    58,
                    1,
                    310,
                    0
                ],
                "title": "MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs"
                },
                "summary": "Large Language Models (LLMs) have been applied to various hardware design\ntasks, including Verilog code generation, EDA tool scripting, and RTL bug\nfixing. Despite this extensive exploration, LLMs are yet to be used for the\ntask of post-synthesis metric reasoning and estimation of HDL designs. In this\npaper, we assess the ability of LLMs to reason about post-synthesis metrics of\nVerilog designs. We introduce MetRex, a large-scale dataset comprising 25,868\nVerilog HDL designs and their corresponding post-synthesis metrics, namely\narea, delay, and static power. MetRex incorporates a Chain of Thought (CoT)\ntemplate to enhance LLMs' reasoning about these metrics. Extensive experiments\nshow that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities\non average by 37.0\\%, 25.3\\%, and 25.7\\% on the area, delay, and static power,\nrespectively. While SFT improves performance on our benchmark, it remains far\nfrom achieving optimal results, especially on complex problems. Comparing to\nstate-of-the-art regression models, our approach delivers accurate\npost-synthesis predictions for 17.4\\% more designs (within a 5\\% error margin),\nin addition to offering a 1.7x speedup by eliminating the need for\npre-processing. This work lays the groundwork for advancing LLM-based Verilog\ncode metric reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been applied to various hardware design\ntasks, including Verilog code generation, EDA tool scripting, and RTL bug\nfixing. Despite this extensive exploration, LLMs are yet to be used for the\ntask of post-synthesis metric reasoning and estimation of HDL designs. In this\npaper, we assess the ability of LLMs to reason about post-synthesis metrics of\nVerilog designs. We introduce MetRex, a large-scale dataset comprising 25,868\nVerilog HDL designs and their corresponding post-synthesis metrics, namely\narea, delay, and static power. MetRex incorporates a Chain of Thought (CoT)\ntemplate to enhance LLMs' reasoning about these metrics. Extensive experiments\nshow that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities\non average by 37.0\\%, 25.3\\%, and 25.7\\% on the area, delay, and static power,\nrespectively. While SFT improves performance on our benchmark, it remains far\nfrom achieving optimal results, especially on complex problems. Comparing to\nstate-of-the-art regression models, our approach delivers accurate\npost-synthesis predictions for 17.4\\% more designs (within a 5\\% error margin),\nin addition to offering a 1.7x speedup by eliminating the need for\npre-processing. This work lays the groundwork for advancing LLM-based Verilog\ncode metric reasoning."
                },
                "authors": [
                    {
                        "name": "Manar Abdelatty"
                    },
                    {
                        "name": "Jingxiao Ma"
                    },
                    {
                        "name": "Sherief Reda"
                    }
                ],
                "author_detail": {
                    "name": "Sherief Reda"
                },
                "author": "Sherief Reda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]