[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.24359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24359v1",
                "updated": "2025-10-28T12:28:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:28:02Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine"
                },
                "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual."
                },
                "authors": [
                    {
                        "name": "Pedram Fard"
                    },
                    {
                        "name": "Alaleh Azhir"
                    },
                    {
                        "name": "Neguine Rezaii"
                    },
                    {
                        "name": "Jiazi Tian"
                    },
                    {
                        "name": "Hossein Estiri"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Estiri"
                },
                "author": "Hossein Estiri",
                "arxiv_comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24273v1",
                "updated": "2025-10-28T10:32:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:32:52Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "title": "SALS: Sparse Attention in Latent Space for KV cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALS: Sparse Attention in Latent Space for KV cache Compression"
                },
                "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively."
                },
                "authors": [
                    {
                        "name": "Junlin Mu"
                    },
                    {
                        "name": "Hantao Huang"
                    },
                    {
                        "name": "Jihang Zhang"
                    },
                    {
                        "name": "Minghui Yu"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Yidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Li"
                },
                "author": "Yidong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24051v1",
                "updated": "2025-10-28T04:17:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T04:17:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Pie: A Programmable Serving System for Emerging LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: A Programmable Serving System for Emerging LLM Applications"
                },
                "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Zhiyao Ma"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3731569.3764814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.24051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v4",
                "updated": "2025-10-28T04:00:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    0,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v2",
                "updated": "2025-10-27T21:48:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    21,
                    48,
                    48,
                    0,
                    300,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12362v2",
                "updated": "2025-10-27T17:31:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    17,
                    31,
                    15,
                    0,
                    300,
                    0
                ],
                "published": "2024-04-18T17:45:19Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    17,
                    45,
                    19,
                    3,
                    109,
                    0
                ],
                "title": "KV-weights are all you need for skipless transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-weights are all you need for skipless transformers"
                },
                "summary": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks."
                },
                "authors": [
                    {
                        "name": "Nils Graef"
                    }
                ],
                "author_detail": {
                    "name": "Nils Graef"
                },
                "author": "Nils Graef",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v3",
                "updated": "2025-10-27T16:20:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    16,
                    20,
                    28,
                    0,
                    300,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_doi": "10.1145/3721462.3770776",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721462.3770776",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at Middleware '25",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v2",
                "updated": "2025-10-27T14:59:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    14,
                    59,
                    46,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving"
                },
                "summary": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Accepted in a computer science workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v2",
                "updated": "2025-10-27T11:55:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    11,
                    55,
                    7,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gaëtan Hadjeres"
                    },
                    {
                        "name": "Gaël Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_doi": "10.5334/TISMIR.251",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5334/TISMIR.251",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Transactions of the International Society for Music Information\n  Retrieval, 8(1): 334-352 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22876v1",
                "updated": "2025-10-26T23:59:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T23:59:23Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "title": "Batch Speculative Decoding Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Speculative Decoding Done Right"
                },
                "summary": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec."
                },
                "authors": [
                    {
                        "name": "Ranran Haoran Zhang"
                    },
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Ashirbad Mishra"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22765v1",
                "updated": "2025-10-26T17:28:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T17:28:05Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval"
                },
                "summary": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Binxiao Xu"
                    },
                    {
                        "name": "Junyu Feng"
                    },
                    {
                        "name": "Ruichuan An"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v3",
                "updated": "2025-10-26T13:31:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    13,
                    31,
                    41,
                    6,
                    299,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22556v1",
                "updated": "2025-10-26T07:17:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T07:17:10Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "title": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size"
                },
                "summary": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length."
                },
                "authors": [
                    {
                        "name": "Jinhan Chen"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Xianjun Gao"
                    },
                    {
                        "name": "Shilong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shilong Wang"
                },
                "author": "Shilong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v3",
                "updated": "2025-10-26T04:25:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    4,
                    25,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23657v1",
                "updated": "2025-10-26T01:25:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T01:25:24Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "title": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops"
                },
                "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture."
                },
                "authors": [
                    {
                        "name": "Saklain Niam"
                    },
                    {
                        "name": "Tashfiqur Rahman"
                    },
                    {
                        "name": "Md. Amjad Patwary"
                    },
                    {
                        "name": "Mukarram Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Mukarram Hossain"
                },
                "author": "Mukarram Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22467v1",
                "updated": "2025-10-26T00:50:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T00:50:12Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "title": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints"
                },
                "summary": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore)."
                },
                "authors": [
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Yijia Fan"
                    },
                    {
                        "name": "Yufeng Yang"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v3",
                "updated": "2025-10-25T14:12:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    14,
                    12,
                    56,
                    5,
                    298,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "38 pages, 9 figures, 17 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23649v1",
                "updated": "2025-10-25T11:43:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models"
                },
                "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK."
                },
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22145v1",
                "updated": "2025-10-25T03:34:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T03:34:34Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "title": "Fundamental Limits of Coded Caching with Fixed Subpacketization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Coded Caching with Fixed Subpacketization"
                },
                "summary": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Youlong Wu"
                    },
                    {
                        "name": "Jinyan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinyan Wang"
                },
                "author": "Jinyan Wang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v3",
                "updated": "2025-10-25T02:29:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    2,
                    29,
                    47,
                    5,
                    298,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v4",
                "updated": "2025-10-25T00:33:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    0,
                    33,
                    14,
                    5,
                    298,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22049v1",
                "updated": "2025-10-24T22:17:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T22:17:49Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "title": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders"
                },
                "summary": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users."
                },
                "authors": [
                    {
                        "name": "Zhimin Chen"
                    },
                    {
                        "name": "Chenyu Zhao"
                    },
                    {
                        "name": "Ka Chun Mo"
                    },
                    {
                        "name": "Yunjiang Jiang"
                    },
                    {
                        "name": "Jane H. Lee"
                    },
                    {
                        "name": "Shouwei Chen"
                    },
                    {
                        "name": "Khushhall Chandra Mahajan"
                    },
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Kai Ren"
                    },
                    {
                        "name": "Jinhui Li"
                    },
                    {
                        "name": "Wen-Yun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Yun Yang"
                },
                "author": "Wen-Yun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21696v1",
                "updated": "2025-10-24T17:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "title": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character"
                },
                "summary": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training."
                },
                "authors": [
                    {
                        "name": "Han Yan"
                    },
                    {
                        "name": "Xibin Song"
                    },
                    {
                        "name": "Yifu Wang"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Pan Ji"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "arxiv_comment": "Project page: https://wolfball.github.io/bachvid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20787v2",
                "updated": "2025-10-24T16:56:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    56,
                    22,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T17:53:03Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    17,
                    53,
                    3,
                    3,
                    296,
                    0
                ],
                "title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction"
                },
                "summary": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v2",
                "updated": "2025-10-24T14:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    55,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22922v2",
                "updated": "2025-10-24T11:53:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    53,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-28T15:15:31Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    15,
                    15,
                    31,
                    5,
                    179,
                    0
                ],
                "title": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling"
                },
                "summary": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass."
                },
                "authors": [
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "6 pages, 9 figures including tables. Short theoretical and practical\n  paper on improved dynamic programming for weighted job scheduling with\n  linear-time preprocessing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21361v1",
                "updated": "2025-10-24T11:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Monte Carlo Tree Diffusion for Extendable Planning"
                },
                "summary": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs."
                },
                "authors": [
                    {
                        "name": "Jaesik Yoon"
                    },
                    {
                        "name": "Hyeonseo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "arxiv_comment": "24 pages, 4 figures, NeurIPS 25 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v6",
                "updated": "2025-10-24T08:41:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    41,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025.\n  https://neurips.cc/virtual/2025/poster/116615",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19240v2",
                "updated": "2025-10-24T08:35:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    35,
                    21,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T04:48:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    48,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development"
                },
                "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle."
                },
                "authors": [
                    {
                        "name": "Behnam Agahi"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v2",
                "updated": "2025-10-24T05:39:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    5,
                    39,
                    3,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v2",
                "updated": "2025-10-24T04:48:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    4,
                    48,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v5",
                "updated": "2025-10-23T23:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    23,
                    35,
                    32,
                    3,
                    296,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "Published in NeurIPS 2025 (Spotlight); Project Page:\n  https://github.com/tensorgi/TPA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v2",
                "updated": "2025-10-23T21:31:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    21,
                    31,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Accepted by NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v3",
                "updated": "2025-10-23T19:45:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    19,
                    45,
                    39,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    },
                    {
                        "name": "Panagiotis Karras"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Karras"
                },
                "author": "Panagiotis Karras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v3",
                "updated": "2025-10-23T18:52:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    18,
                    52,
                    25,
                    3,
                    296,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20707v1",
                "updated": "2025-10-23T16:17:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T16:17:47Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models"
                },
                "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}."
                },
                "authors": [
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Xiyan Gui"
                    },
                    {
                        "name": "Yuchao Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "Our code is available at https://github.com/xuyang-liu16/MixKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v2",
                "updated": "2025-10-23T15:26:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    15,
                    26,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "arxiv_comment": "Accepted to ALENEX`26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v4",
                "updated": "2025-10-23T14:23:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    14,
                    23,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "35 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20499v1",
                "updated": "2025-10-23T12:39:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T12:39:59Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "title": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming"
                },
                "summary": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset."
                },
                "authors": [
                    {
                        "name": "Akif Çördük"
                    },
                    {
                        "name": "Piotr Sielski"
                    },
                    {
                        "name": "Alice Boucher"
                    },
                    {
                        "name": "Kumar Aatish"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Aatish"
                },
                "author": "Kumar Aatish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20878v1",
                "updated": "2025-10-23T12:28:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T12:28:58Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "title": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement"
                },
                "summary": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss."
                },
                "authors": [
                    {
                        "name": "Danying Ge"
                    },
                    {
                        "name": "Jianhua Gao"
                    },
                    {
                        "name": "Yixue Yang"
                    },
                    {
                        "name": "Weixing Ji"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Ji"
                },
                "author": "Weixing Ji",
                "arxiv_comment": "13 pages,16 figures,2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; E.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21865v1",
                "updated": "2025-10-23T10:35:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:35:35Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "title": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis"
                },
                "summary": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization."
                },
                "authors": [
                    {
                        "name": "F. I. Qowy"
                    }
                ],
                "author_detail": {
                    "name": "F. I. Qowy"
                },
                "author": "F. I. Qowy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20400v1",
                "updated": "2025-10-23T10:06:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:06:48Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "title": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels"
                },
                "summary": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline."
                },
                "authors": [
                    {
                        "name": "Rubén Langarita"
                    },
                    {
                        "name": "Jesús Alastruey-Benedé"
                    },
                    {
                        "name": "Pablo Ibáñez-Marín"
                    },
                    {
                        "name": "Santiago Marco-Sola"
                    },
                    {
                        "name": "Miquel Moretó"
                    },
                    {
                        "name": "Adrià Armejach"
                    }
                ],
                "author_detail": {
                    "name": "Adrià Armejach"
                },
                "author": "Adrià Armejach",
                "arxiv_comment": "11 pages, 10 figures, 5 tables, 4 algorithms, accepted on PACT25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v2",
                "updated": "2025-10-23T09:55:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    55,
                    50,
                    3,
                    296,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "Accepted in TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19755v2",
                "updated": "2025-10-23T09:09:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    9,
                    15,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation"
                },
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "22 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16807v2",
                "updated": "2025-10-23T08:29:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    8,
                    29,
                    11,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-19T12:17:42Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "title": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads"
                },
                "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhoutong Wu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Cong Fang"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "arxiv_comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20230v1",
                "updated": "2025-10-23T05:22:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T05:22:09Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "title": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$"
                },
                "summary": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Chenchao Xu"
                    },
                    {
                        "name": "Zhimian Wu"
                    },
                    {
                        "name": "Huachen Rao"
                    },
                    {
                        "name": "Zhaoyang Shan"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Guanghan Cao"
                    },
                    {
                        "name": "Michael Smidman"
                    },
                    {
                        "name": "Ming Shi"
                    },
                    {
                        "name": "Huiqiu Yuan"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Xianhui Chen"
                    },
                    {
                        "name": "Chao Cao"
                    },
                    {
                        "name": "Yu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yu Song"
                },
                "author": "Yu Song",
                "arxiv_comment": "submitted to journal in July 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v4",
                "updated": "2025-10-23T00:47:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    47,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "Accepted to ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00744v2",
                "updated": "2025-10-23T00:40:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    40,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-31T23:16:53Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers"
                },
                "summary": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems."
                },
                "authors": [
                    {
                        "name": "Kazuki Irie"
                    },
                    {
                        "name": "Morris Yau"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    }
                ],
                "author_detail": {
                    "name": "Samuel J. Gershman"
                },
                "author": "Samuel J. Gershman",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03712v2",
                "updated": "2025-10-22T23:56:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    23,
                    56,
                    45,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-04T07:22:39Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems"
                },
                "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."
                },
                "authors": [
                    {
                        "name": "Jahidul Arafat"
                    },
                    {
                        "name": "Kh. M. Moniruzzaman"
                    },
                    {
                        "name": "Shamim Hossain"
                    },
                    {
                        "name": "Fariha Tasmin"
                    }
                ],
                "author_detail": {
                    "name": "Fariha Tasmin"
                },
                "author": "Fariha Tasmin",
                "arxiv_comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M15, 90B25, 68T05, 90C29",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; C.2.4; D.2.5; D.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19670v1",
                "updated": "2025-10-22T15:16:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation"
                },
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe",
                "arxiv_comment": "19 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; C.2.4; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v3",
                "updated": "2025-10-22T14:33:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    33,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19875v1",
                "updated": "2025-10-22T09:42:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:42:29Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention"
                },
                "summary": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/."
                },
                "authors": [
                    {
                        "name": "J Rosser"
                    },
                    {
                        "name": "José Luis Redondo García"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Konstantina Palla"
                    },
                    {
                        "name": "Hugues Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Hugues Bouchard"
                },
                "author": "Hugues Bouchard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19264v1",
                "updated": "2025-10-22T05:47:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T05:47:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "LAPRAD: LLM-Assisted PRotocol Attack Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAPRAD: LLM-Assisted PRotocol Attack Discovery"
                },
                "summary": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations."
                },
                "authors": [
                    {
                        "name": "R. Can Aygun"
                    },
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    },
                    {
                        "name": "Leonard Kleinrock"
                    }
                ],
                "author_detail": {
                    "name": "Leonard Kleinrock"
                },
                "arxiv_affiliation": "UCLA",
                "author": "Leonard Kleinrock",
                "arxiv_comment": "IFIP Networking 2025 Proceedings (Accepted on 05.05.2025)",
                "arxiv_journal_ref": "Published in IFIP Networking 2025 Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19183v1",
                "updated": "2025-10-22T02:41:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:41:07Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning"
                },
                "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Fengyuan Sun"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Xinhao Xu"
                    },
                    {
                        "name": "Dandan Zheng"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19171v1",
                "updated": "2025-10-22T02:09:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:09:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG"
                },
                "summary": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference."
                },
                "authors": [
                    {
                        "name": "Jihwan Bang"
                    },
                    {
                        "name": "Juntae Lee"
                    },
                    {
                        "name": "Seunghan Yang"
                    },
                    {
                        "name": "Sungha Choi"
                    }
                ],
                "author_detail": {
                    "name": "Sungha Choi"
                },
                "author": "Sungha Choi",
                "arxiv_comment": "Accepted at NeurIPS 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v3",
                "updated": "2025-10-21T22:37:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    22,
                    37,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet"
                },
                "summary": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v3",
                "updated": "2025-10-21T21:07:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    21,
                    7,
                    17,
                    1,
                    294,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "arxiv_doi": "10.1145/3772052.3772208",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772208",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the version accepted to ACM SoCC 2025. The title has been\n  updated to match the published version",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15878v2",
                "updated": "2025-10-21T16:32:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    32,
                    50,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-21T16:10:26Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    10,
                    26,
                    3,
                    233,
                    0
                ],
                "title": "Putting the Context back into Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putting the Context back into Memory"
                },
                "summary": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices."
                },
                "authors": [
                    {
                        "name": "David A. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "David A. Roberts"
                },
                "author": "David A. Roberts",
                "arxiv_comment": "Fixed errors in paragraph numbering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18716v1",
                "updated": "2025-10-21T15:17:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:17:37Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Siyong Jian"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14576v3",
                "updated": "2025-10-21T15:13:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    13,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2024-01-26T00:27:00Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    0,
                    27,
                    0,
                    4,
                    26,
                    0
                ],
                "title": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints"
                },
                "summary": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings."
                },
                "authors": [
                    {
                        "name": "Steven W. D. Chien"
                    },
                    {
                        "name": "Kento Sato"
                    },
                    {
                        "name": "Artur Podobas"
                    },
                    {
                        "name": "Niclas Jansson"
                    },
                    {
                        "name": "Stefano Markidis"
                    },
                    {
                        "name": "Michio Honda"
                    }
                ],
                "author_detail": {
                    "name": "Michio Honda"
                },
                "author": "Michio Honda",
                "arxiv_doi": "10.1145/3772052.3772212",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772212",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SoCC 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18672v1",
                "updated": "2025-10-21T14:25:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:25:51Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study"
                },
                "summary": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Junpan Wu"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v1",
                "updated": "2025-10-21T12:39:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18546v1",
                "updated": "2025-10-21T11:52:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval"
                },
                "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon."
                },
                "authors": [
                    {
                        "name": "Zebin Yang"
                    },
                    {
                        "name": "Sunjian Zheng"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Shaoshan Liu"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v2",
                "updated": "2025-10-21T10:33:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    33,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching"
                },
                "summary": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v2",
                "updated": "2025-10-21T10:08:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    8,
                    33,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18480v1",
                "updated": "2025-10-21T10:00:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices"
                },
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18413v1",
                "updated": "2025-10-21T08:44:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T08:44:47Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference"
                },
                "summary": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity."
                },
                "authors": [
                    {
                        "name": "Siyuan Yan"
                    },
                    {
                        "name": "Guo-Qing Jiang"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    },
                    {
                        "name": "Ran Zhu"
                    },
                    {
                        "name": "Chun Cao"
                    },
                    {
                        "name": "Jingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jingwei Xu"
                },
                "author": "Jingwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v5",
                "updated": "2025-10-21T06:47:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    47,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was first submitted for review on Sept. 5, 2024, and the\n  initial version was uploaded to Arxiv on Sept. 30, 2024. The latest version\n  has accepted for publication by IEEE Transactions on Information Forensics\n  and Security (TIFS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v3",
                "updated": "2025-10-21T06:30:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    30,
                    21,
                    1,
                    294,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Steffen Christgau"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18269v1",
                "updated": "2025-10-21T03:39:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T03:39:41Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "title": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding"
                },
                "summary": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth."
                },
                "authors": [
                    {
                        "name": "Xueyi Chen"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Kele Shao"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17777v1",
                "updated": "2025-10-20T17:35:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:35:47Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference"
                },
                "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability."
                },
                "authors": [
                    {
                        "name": "Samir Khaki"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Konstantinos N. Plataniotis"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Zhijian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian Liu"
                },
                "author": "Zhijian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17238v1",
                "updated": "2025-10-20T07:27:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T07:27:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "StreamingThinker: Large Language Models Can Think While Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingThinker: Large Language Models Can Think While Reading"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}"
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17098v1",
                "updated": "2025-10-20T02:04:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T02:04:18Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models"
                },
                "summary": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research."
                },
                "authors": [
                    {
                        "name": "Elias Hossain"
                    },
                    {
                        "name": "Swayamjit Saha"
                    },
                    {
                        "name": "Somshubhra Roy"
                    },
                    {
                        "name": "Ravi Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Prasad"
                },
                "author": "Ravi Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17045v1",
                "updated": "2025-10-19T23:17:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T23:17:13Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "title": "Video Reasoning without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Reasoning without Training"
                },
                "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model."
                },
                "authors": [
                    {
                        "name": "Deepak Sridhar"
                    },
                    {
                        "name": "Kartikeya Bhardwaj"
                    },
                    {
                        "name": "Jeya Pradha Jeyaraj"
                    },
                    {
                        "name": "Nuno Vasconcelos"
                    },
                    {
                        "name": "Ankita Nayak"
                    },
                    {
                        "name": "Harris Teague"
                    }
                ],
                "author_detail": {
                    "name": "Harris Teague"
                },
                "author": "Harris Teague",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16871v1",
                "updated": "2025-10-19T15:13:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T15:13:25Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "title": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy"
                },
                "summary": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16805v1",
                "updated": "2025-10-19T12:16:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T12:16:40Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "title": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects"
                },
                "summary": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models."
                },
                "authors": [
                    {
                        "name": "Mariam Rakka"
                    },
                    {
                        "name": "Marios Fournarakis"
                    },
                    {
                        "name": "Olga Krestinskaya"
                    },
                    {
                        "name": "Jinane Bazzi"
                    },
                    {
                        "name": "Khaled N. Salama"
                    },
                    {
                        "name": "Fadi Kurdahi"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "arxiv_comment": "46 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v2",
                "updated": "2025-10-18T11:29:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    11,
                    29,
                    52,
                    5,
                    291,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v2",
                "updated": "2025-10-18T06:04:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    6,
                    4,
                    53,
                    5,
                    291,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08907v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08907v3",
                "updated": "2025-10-18T02:48:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    2,
                    48,
                    35,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-10T01:42:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    1,
                    42,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors"
                },
                "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Junyi Xiao"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shengxiang Gao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "18 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08907v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08907v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16292v1",
                "updated": "2025-10-18T01:31:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T01:31:14Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}."
                },
                "authors": [
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Haiyu Wang"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "arxiv_comment": "Accepted as Spotlight paper by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16276v1",
                "updated": "2025-10-18T00:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T00:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "title": "What Limits Agentic Systems Efficiency?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Limits Agentic Systems Efficiency?"
                },
                "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance."
                },
                "authors": [
                    {
                        "name": "Song Bian"
                    },
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Anand Jayarajan"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "27 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15590v1",
                "updated": "2025-10-17T12:38:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    12,
                    38,
                    25,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-17T12:38:25Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    12,
                    38,
                    25,
                    4,
                    290,
                    0
                ],
                "title": "A single optically detectable tumbling spin in silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single optically detectable tumbling spin in silicon"
                },
                "summary": "Electron spin resonance spectroscopy is a widely used technique for analyzing\nthe microscopic structure, local environment and reorientation of atomic and\nmolecular systems. Conventional inductive detection methods typically require\nto probe more than a billion of electron spins such that single atom motion is\nhidden through ensemble averaging. While several single spin spectroscopy\nmethods are currently available, they have been so far limited to static\nsystems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling\ndefect in silicon called the G center, behaving as a pseudo-molecule randomly\nreorienting itself in the crystalline matrix. Using high-resolution spin\nspectroscopy, we reveal a fine magnetic structure resulting from the spin\nprincipal axes jumping between discrete orientations in the crystal. By\nmodeling the atomic reorientation of the defect, we demonstrate that spin\ntumbling induces variations in the coupling to the microwave magnetic field,\nenabling position-dependent Rabi frequencies to be detected in coherent spin\ncontrol experiments. By virtue of its pseudo-molecule configuration, the G\ncenter in silicon is a unique quantum system to investigate the mutual\ninteraction between optical, spin and rotation properties in a highly versatile\nmaterial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron spin resonance spectroscopy is a widely used technique for analyzing\nthe microscopic structure, local environment and reorientation of atomic and\nmolecular systems. Conventional inductive detection methods typically require\nto probe more than a billion of electron spins such that single atom motion is\nhidden through ensemble averaging. While several single spin spectroscopy\nmethods are currently available, they have been so far limited to static\nsystems. Here we demonstrate single spin spectroscopy of a fluorescent tumbling\ndefect in silicon called the G center, behaving as a pseudo-molecule randomly\nreorienting itself in the crystalline matrix. Using high-resolution spin\nspectroscopy, we reveal a fine magnetic structure resulting from the spin\nprincipal axes jumping between discrete orientations in the crystal. By\nmodeling the atomic reorientation of the defect, we demonstrate that spin\ntumbling induces variations in the coupling to the microwave magnetic field,\nenabling position-dependent Rabi frequencies to be detected in coherent spin\ncontrol experiments. By virtue of its pseudo-molecule configuration, the G\ncenter in silicon is a unique quantum system to investigate the mutual\ninteraction between optical, spin and rotation properties in a highly versatile\nmaterial."
                },
                "authors": [
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Frédéric Milési"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Isabelle Robert-Philip"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    },
                    {
                        "name": "Guillaume Cassabois"
                    },
                    {
                        "name": "Vincent Jacques"
                    },
                    {
                        "name": "Anaïs Dréau"
                    }
                ],
                "author_detail": {
                    "name": "Anaïs Dréau"
                },
                "author": "Anaïs Dréau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15413v1",
                "updated": "2025-10-17T08:07:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    8,
                    7,
                    27,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-17T08:07:27Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    8,
                    7,
                    27,
                    4,
                    290,
                    0
                ],
                "title": "FHE-SQL: Fully Homomorphic Encrypted SQL Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHE-SQL: Fully Homomorphic Encrypted SQL Database"
                },
                "summary": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework."
                },
                "authors": [
                    {
                        "name": "Po-Yu Tseng"
                    },
                    {
                        "name": "Po-Chu Hsu"
                    },
                    {
                        "name": "Shih-Wei Liao"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Wei Liao"
                },
                "author": "Shih-Wei Liao",
                "arxiv_comment": "12 pages, 1 figures, Keywords: Fully Homomorphic Encryption, Private\n  Information Retrieval, Encrypted Databases, Privacy-Preserving Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v4",
                "updated": "2025-10-17T06:54:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    6,
                    54,
                    10,
                    4,
                    290,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV, where\n\"Prefix\" means the top-ranked KV based on importance rather than position in\nthe original sequence. It reframes the challenge of determining KV cache sizes\nfor all layers into the task of searching for the optimal global prefix\nconfiguration. With an adaptive layer-wise KV retention recipe based on binary\nsearch, the maximum contextual information can thus be preserved in each layer,\nfacilitating the generation. Extensive experiments demonstrate that our method\nachieves the state-of-the-art performance compared with others. It exhibits\nsuperior inference efficiency and generation quality trade-offs, showing\npromising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV, where\n\"Prefix\" means the top-ranked KV based on importance rather than position in\nthe original sequence. It reframes the challenge of determining KV cache sizes\nfor all layers into the task of searching for the optimal global prefix\nconfiguration. With an adaptive layer-wise KV retention recipe based on binary\nsearch, the maximum contextual information can thus be preserved in each layer,\nfacilitating the generation. Extensive experiments demonstrate that our method\nachieves the state-of-the-art performance compared with others. It exhibits\nsuperior inference efficiency and generation quality trade-offs, showing\npromising potential for practical applications. Code is available at\nhttps://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "NeurIPS 2025 Camera-ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v2",
                "updated": "2025-10-17T06:45:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    6,
                    45,
                    17,
                    4,
                    290,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Pre-trained Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Pre-trained Large Language Models"
                },
                "summary": "To enhance the efficiency of the attention mechanism within large language\nmodels (LLMs), previous works primarily compress the KV cache or group\nattention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to reduce the redundancy by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights.\n  Driven by these insights, we introduce LISA, a lightweight substitute for\nself-attention in well-trained LLMs. LISA employs tiny feed-forward networks to\nalign attention heads between adjacent layers and low-rank matrices to\napproximate differences in layer-wise attention weights. Evaluations\nencompassing 13 typical benchmarks demonstrate that LISA maintains high\nresponse quality in terms of accuracy and perplexity while reducing redundant\nattention calculations within 53%-84% of the total layers. Our implementations\nof LISA achieve a 6x compression of Q and K matrices within the attention\nmechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for\nLLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enhance the efficiency of the attention mechanism within large language\nmodels (LLMs), previous works primarily compress the KV cache or group\nattention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to reduce the redundancy by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights.\n  Driven by these insights, we introduce LISA, a lightweight substitute for\nself-attention in well-trained LLMs. LISA employs tiny feed-forward networks to\nalign attention heads between adjacent layers and low-rank matrices to\napproximate differences in layer-wise attention weights. Evaluations\nencompassing 13 typical benchmarks demonstrate that LISA maintains high\nresponse quality in terms of accuracy and perplexity while reducing redundant\nattention calculations within 53%-84% of the total layers. Our implementations\nof LISA achieve a 6x compression of Q and K matrices within the attention\nmechanism, with maximum throughput improvements 19.5%, 32.3%, and 40.1% for\nLLaMA3-8B, LLaMA2-7B, and LLaMA2-13B, respectively."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Jiali Zeng"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "A version accepted by TACL, prior to its publication by MIT Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15152v1",
                "updated": "2025-10-16T21:22:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    22,
                    16,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T21:22:16Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    21,
                    22,
                    16,
                    3,
                    289,
                    0
                ],
                "title": "Tail-Optimized Caching for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tail-Optimized Caching for LLM Inference"
                },
                "summary": "Prompt caching is critical for reducing latency and cost in LLM inference:\nOpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.\nDespite its widespread success, little is known about what constitutes an\noptimal prompt caching policy, particularly when optimizing tail latency, a\nmetric of central importance to practitioners. The widely used Least Recently\nUsed (LRU) policy can perform arbitrarily poor on this metric, as it is\noblivious to the heterogeneity of conversation lengths. To address this gap, we\npropose Tail-Optimized LRU, a simple two-line modification that reallocates KV\ncache capacity to prioritize high-latency conversations by evicting cache\nentries that are unlikely to affect future turns. Though the implementation is\nsimple, we prove its optimality under a natural stochastic model of\nconversation dynamics, providing the first theoretical justification for LRU in\nthis setting, a result that may be of independent interest to the caching\ncommunity. Experimentally, on real conversation data WildChat, Tail-Optimized\nLRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and\n23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in\nSLO violations of 200ms. We believe this provides a practical and theoretically\ngrounded option for practitioners seeking to optimize tail latency in\nreal-world LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching is critical for reducing latency and cost in LLM inference:\nOpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.\nDespite its widespread success, little is known about what constitutes an\noptimal prompt caching policy, particularly when optimizing tail latency, a\nmetric of central importance to practitioners. The widely used Least Recently\nUsed (LRU) policy can perform arbitrarily poor on this metric, as it is\noblivious to the heterogeneity of conversation lengths. To address this gap, we\npropose Tail-Optimized LRU, a simple two-line modification that reallocates KV\ncache capacity to prioritize high-latency conversations by evicting cache\nentries that are unlikely to affect future turns. Though the implementation is\nsimple, we prove its optimality under a natural stochastic model of\nconversation dynamics, providing the first theoretical justification for LRU in\nthis setting, a result that may be of independent interest to the caching\ncommunity. Experimentally, on real conversation data WildChat, Tail-Optimized\nLRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and\n23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in\nSLO violations of 200ms. We believe this provides a practical and theoretically\ngrounded option for practitioners seeking to optimize tail latency in\nreal-world LLM deployments."
                },
                "authors": [
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Yueying Li"
                    },
                    {
                        "name": "Ciamac C. Moallemi"
                    },
                    {
                        "name": "Tianyi Peng"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Peng"
                },
                "author": "Tianyi Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15095v1",
                "updated": "2025-10-16T19:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    19,
                    28,
                    30,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T19:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    19,
                    28,
                    30,
                    3,
                    289,
                    0
                ],
                "title": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table\n  for GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table\n  for GPUs"
                },
                "summary": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing."
                },
                "authors": [
                    {
                        "name": "Md Sabbir Hossain Polak"
                    },
                    {
                        "name": "David Troendle"
                    },
                    {
                        "name": "Byunghyun Jang"
                    }
                ],
                "author_detail": {
                    "name": "Byunghyun Jang"
                },
                "author": "Byunghyun Jang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14973v1",
                "updated": "2025-10-16T17:59:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need for KV Cache in Diffusion LLMs"
                },
                "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Quan Nguyen-Tri"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "https://vila-lab.github.io/elastic-cache-webpage/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14891v1",
                "updated": "2025-10-16T17:10:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:10:03Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "title": "A Performance Portable Matrix Free Dense MTTKRP in GenTen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Performance Portable Matrix Free Dense MTTKRP in GenTen"
                },
                "summary": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter."
                },
                "authors": [
                    {
                        "name": "Gabriel Kosmacher"
                    },
                    {
                        "name": "Eric T. Phipps"
                    },
                    {
                        "name": "Sivasankaran Rajamanickam"
                    }
                ],
                "author_detail": {
                    "name": "Sivasankaran Rajamanickam"
                },
                "author": "Sivasankaran Rajamanickam",
                "arxiv_comment": "10 pages, 5 figures, 4 tables, for implementation see\n  https://github.com/sandialabs/GenTen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14686v1",
                "updated": "2025-10-16T13:53:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:53:47Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "title": "xLLM Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLLM Technical Report"
                },
                "summary": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service."
                },
                "authors": [
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Peijun Yang"
                    },
                    {
                        "name": "Xiaoyang Zhao"
                    },
                    {
                        "name": "Xiusheng Lu"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "name": "Zhiwei Liang"
                    },
                    {
                        "name": "Jun Xiong"
                    },
                    {
                        "name": "Donghe Jin"
                    },
                    {
                        "name": "Minchao Zhang"
                    },
                    {
                        "name": "Jinrong Guo"
                    },
                    {
                        "name": "Yingxu Deng"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xianzhe Dong"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Zihan Tang"
                    },
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Jinguang Liu"
                    },
                    {
                        "name": "Meng Kang"
                    },
                    {
                        "name": "Menxin Li"
                    },
                    {
                        "name": "Yunlong Wang"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yichen Zhang"
                    },
                    {
                        "name": "Jinrun Yin"
                    },
                    {
                        "name": "Keyang Zheng"
                    },
                    {
                        "name": "Jiawei Yin"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "Xiaobo Lin"
                    },
                    {
                        "name": "Liangyu Liu"
                    },
                    {
                        "name": "Liwei Lan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chunhua Peng"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Songcheng Ren"
                    },
                    {
                        "name": "Xuezhu Wang"
                    },
                    {
                        "name": "Yunheng Shen"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Ke Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Zhang"
                },
                "author": "Ke Zhang",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v5",
                "updated": "2025-10-16T13:25:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    25,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14622v1",
                "updated": "2025-10-16T12:32:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    32,
                    51,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:32:51Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    32,
                    51,
                    3,
                    289,
                    0
                ],
                "title": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC\n  Systems"
                },
                "summary": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Hyein Woo"
                    },
                    {
                        "name": "Junhee Kim"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Kyungkuk Nam"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Hanyeoreum Bae"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14531v1",
                "updated": "2025-10-16T10:21:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    21,
                    35,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T10:21:35Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    21,
                    35,
                    3,
                    289,
                    0
                ],
                "title": "Design and simulation of a 4H-SiC low gain avalanche diode with\n  trench-isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and simulation of a 4H-SiC low gain avalanche diode with\n  trench-isolation"
                },
                "summary": "We present the design and simulation of a 30 $\\mathrm{\\mu m}$ thick 4H-SiC\nLow Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4\n$\\mathrm{\\mu m}$ thick epitaxially grown gain layer enables controlled internal\namplification up to 1 kV reverse bias, while maintaining full depletion below\n500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were\nsimulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a\nquasi-1D geometry and verified across process-related variations in gain layer\nparameters. To ensure high-voltage stability and proper edge termination, a\nguard structure combining deep etched trenches and deep $p^+$ junction\ntermination extension (JTE) implants was designed. TCAD simulations varying the\nguard structure dimensions yielded an optimized design with a breakdown voltage\nabove 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM,\nBarcelona.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and simulation of a 30 $\\mathrm{\\mu m}$ thick 4H-SiC\nLow Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4\n$\\mathrm{\\mu m}$ thick epitaxially grown gain layer enables controlled internal\namplification up to 1 kV reverse bias, while maintaining full depletion below\n500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were\nsimulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a\nquasi-1D geometry and verified across process-related variations in gain layer\nparameters. To ensure high-voltage stability and proper edge termination, a\nguard structure combining deep etched trenches and deep $p^+$ junction\ntermination extension (JTE) implants was designed. TCAD simulations varying the\nguard structure dimensions yielded an optimized design with a breakdown voltage\nabove 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM,\nBarcelona."
                },
                "authors": [
                    {
                        "name": "Sebastian Onder"
                    },
                    {
                        "name": "Philipp Gaggl"
                    },
                    {
                        "name": "Jürgen Burin"
                    },
                    {
                        "name": "Andreas Gsponer"
                    },
                    {
                        "name": "Matthias Knopf"
                    },
                    {
                        "name": "Simon Waid"
                    },
                    {
                        "name": "Neil Moffat"
                    },
                    {
                        "name": "Giulio Pellegrini"
                    },
                    {
                        "name": "Thomas Bergauer"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Bergauer"
                },
                "author": "Thomas Bergauer",
                "arxiv_doi": "10.1016/j.nima.2025.170740",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nima.2025.170740",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.14531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16040v1",
                "updated": "2025-10-16T07:12:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    7,
                    12,
                    8,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T07:12:08Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    7,
                    12,
                    8,
                    3,
                    289,
                    0
                ],
                "title": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge\n  Computing"
                },
                "summary": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running Large Language Models (LLMs) on edge devices is crucial for reducing\nlatency, improving real-time processing, and enhancing privacy. By performing\ninference directly on the device, data does not need to be sent to the cloud,\nensuring faster responses and reducing reliance on network connectivity.\nHowever, implementing LLMs on edge devices presents challenges, particularly\nwith managing key-value (KV) caches, which plays a pivotal role in LLM serving.\nAs the input text lengthens, the size of the KV cache increases linearly with\nthe sequence length, leading to a significant memory footprint and data access\ncosts. On the other hand, edge devices have limited memory and computational\npower, making it hard to store and efficiently access the large caches needed\nfor LLM inference.\n  To mitigate the substantial overhead caused by KV cache, we propose using\nembedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,\nwhich offers higher storage density compared to SRAM. However, to ensure data\nintegrity, eDRAM needs periodic refresh operations, which are power-intensive.\nTo reduce eDRAM costs and improve overall system performance, we\npropose~\\textit{Kelle}, a software-hardware co-design solution optimized for\ndeploying LLMs on eDRAM-based edge systems. Combined with our fine-grained\nmemory eviction, recomputation, and refresh control algorithms, the\n\\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$\nenergy savings compared to existing baseline solutions."
                },
                "authors": [
                    {
                        "name": "Tianhua Xia"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14126v1",
                "updated": "2025-10-15T21:49:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    49,
                    38,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T21:49:38Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    49,
                    38,
                    2,
                    288,
                    0
                ],
                "title": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic\n  Serving"
                },
                "summary": "We introduce Cortex, a prototype workflow-aware serving platform designed for\nagentic workloads. The core principle of Cortex is stage isolation: it\nprovisions dedicated resource pools for each distinct stage of an agentic\nworkflow. This simple yet powerful strategy mitigates inter-stage interference\nin compute and memory, leading to better KV cache utilization, higher\nthroughput, and more predictable performance. By customizing resource\nallocation and scheduling within each distinct stage of agentic workflows,\nCortex lays the groundwork for more advanced, agent-native serving paradigms,\nincluding malleable resource management, speculative execution of workflow\nbranches, and a shared, multi-tiered cache for \"agentic state.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cortex, a prototype workflow-aware serving platform designed for\nagentic workloads. The core principle of Cortex is stage isolation: it\nprovisions dedicated resource pools for each distinct stage of an agentic\nworkflow. This simple yet powerful strategy mitigates inter-stage interference\nin compute and memory, leading to better KV cache utilization, higher\nthroughput, and more predictable performance. By customizing resource\nallocation and scheduling within each distinct stage of agentic workflows,\nCortex lays the groundwork for more advanced, agent-native serving paradigms,\nincluding malleable resource management, speculative execution of workflow\nbranches, and a shared, multi-tiered cache for \"agentic state.\""
                },
                "authors": [
                    {
                        "name": "Nikos Pagonas"
                    },
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Kostis Kaffes"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Krishnamurthy"
                },
                "author": "Arvind Krishnamurthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13940v1",
                "updated": "2025-10-15T17:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention"
                },
                "summary": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Ganggui Ding"
                    },
                    {
                        "name": "Liang Hou"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Cong Chen"
                },
                "author": "Ying-Cong Chen",
                "arxiv_comment": "Code: https://github.com/EnVision-Research/MTI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13797v1",
                "updated": "2025-10-15T17:57:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:57:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons"
                },
                "summary": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques."
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Yair Feldman"
                    },
                    {
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "name": "Kianté Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v3",
                "updated": "2025-10-15T16:03:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    3,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: More for Keys, Less for Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: More for Keys, Less for Values"
                },
                "summary": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Weicong Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13602v1",
                "updated": "2025-10-15T14:33:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:33:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "NOSA: Native and Offloadable Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOSA: Native and Offloadable Sparse Attention"
                },
                "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2)."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13401v1",
                "updated": "2025-10-15T10:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T10:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "title": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs"
                },
                "summary": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second)."
                },
                "authors": [
                    {
                        "name": "Jude Haris"
                    },
                    {
                        "name": "José Cano"
                    }
                ],
                "author_detail": {
                    "name": "José Cano"
                },
                "author": "José Cano",
                "arxiv_comment": "Accepted to Workshop on New Approaches for Addressing the Computing\n  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.24717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24717v1",
                "updated": "2025-10-28T17:59:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    59,
                    57,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:59:57Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    59,
                    57,
                    1,
                    301,
                    0
                ],
                "title": "Uniform Discrete Diffusion with Metric Path for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uniform Discrete Diffusion with Metric Path for Video Generation"
                },
                "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA"
                },
                "authors": [
                    {
                        "name": "Haoge Deng"
                    },
                    {
                        "name": "Ting Pan"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhuoyan Luo"
                    },
                    {
                        "name": "Yufeng Cui"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    },
                    {
                        "name": "Shiguang Shan"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Xinlong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinlong Wang"
                },
                "author": "Xinlong Wang",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03285v2",
                "updated": "2025-10-28T17:59:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    59,
                    36,
                    1,
                    301,
                    0
                ],
                "published": "2025-07-04T04:23:03Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    4,
                    23,
                    3,
                    4,
                    185,
                    0
                ],
                "title": "Memory Mosaics at scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Mosaics at scale"
                },
                "summary": "Memory Mosaics [Zhang et al., 2025], networks of associative memories, have\ndemonstrated appealing compositional and in-context learning capabilities on\nmedium-scale networks (GPT-2 scale) and synthetic small datasets. This work\nshows that these favorable properties remain when we scale memory mosaics to\nlarge language model sizes (llama-8B scale) and real-world datasets.\n  To this end, we scale memory mosaics to 10B size, we train them on one\ntrillion tokens, we introduce a couple architectural modifications (\"Memory\nMosaics v2\"), we assess their capabilities across three evaluation dimensions:\ntraining-knowledge storage, new-knowledge storage, and in-context learning.\n  Throughout the evaluation, memory mosaics v2 match transformers on the\nlearning of training knowledge (first dimension) and significantly outperforms\ntransformers on carrying out new tasks at inference time (second and third\ndimensions). These improvements cannot be easily replicated by simply\nincreasing the training data for transformers. A memory mosaics v2 trained on\none trillion tokens still perform better on these tasks than a transformer\ntrained on eight trillion tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Mosaics [Zhang et al., 2025], networks of associative memories, have\ndemonstrated appealing compositional and in-context learning capabilities on\nmedium-scale networks (GPT-2 scale) and synthetic small datasets. This work\nshows that these favorable properties remain when we scale memory mosaics to\nlarge language model sizes (llama-8B scale) and real-world datasets.\n  To this end, we scale memory mosaics to 10B size, we train them on one\ntrillion tokens, we introduce a couple architectural modifications (\"Memory\nMosaics v2\"), we assess their capabilities across three evaluation dimensions:\ntraining-knowledge storage, new-knowledge storage, and in-context learning.\n  Throughout the evaluation, memory mosaics v2 match transformers on the\nlearning of training knowledge (first dimension) and significantly outperforms\ntransformers on carrying out new tasks at inference time (second and third\ndimensions). These improvements cannot be easily replicated by simply\nincreasing the training data for transformers. A memory mosaics v2 trained on\none trillion tokens still perform better on these tasks than a transformer\ntrained on eight trillion tokens."
                },
                "authors": [
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Léon Bottou"
                    }
                ],
                "author_detail": {
                    "name": "Léon Bottou"
                },
                "author": "Léon Bottou",
                "arxiv_comment": "Oral @ NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24714v1",
                "updated": "2025-10-28T17:59:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    59,
                    15,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:59:15Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    59,
                    15,
                    1,
                    301,
                    0
                ],
                "title": "Machine-Learning-Assisted Comparison of Regression Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-Learning-Assisted Comparison of Regression Functions"
                },
                "summary": "We revisit the classical problem of comparing regression functions, a\nfundamental question in statistical inference with broad relevance to modern\napplications such as data integration, transfer learning, and causal inference.\nExisting approaches typically rely on smoothing techniques and are thus\nhindered by the curse of dimensionality. We propose a generalized notion of\nkernel-based conditional mean dependence that provides a new characterization\nof the null hypothesis of equal regression functions. Building on this\nreformulation, we develop two novel tests that leverage modern machine learning\nmethods for flexible estimation. We establish the asymptotic properties of the\ntest statistics, which hold under both fixed- and high-dimensional regimes.\nUnlike existing methods that often require restrictive distributional\nassumptions, our framework only imposes mild moment conditions. The efficacy of\nthe proposed tests is demonstrated through extensive numerical studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit the classical problem of comparing regression functions, a\nfundamental question in statistical inference with broad relevance to modern\napplications such as data integration, transfer learning, and causal inference.\nExisting approaches typically rely on smoothing techniques and are thus\nhindered by the curse of dimensionality. We propose a generalized notion of\nkernel-based conditional mean dependence that provides a new characterization\nof the null hypothesis of equal regression functions. Building on this\nreformulation, we develop two novel tests that leverage modern machine learning\nmethods for flexible estimation. We establish the asymptotic properties of the\ntest statistics, which hold under both fixed- and high-dimensional regimes.\nUnlike existing methods that often require restrictive distributional\nassumptions, our framework only imposes mild moment conditions. The efficacy of\nthe proposed tests is demonstrated through extensive numerical studies."
                },
                "authors": [
                    {
                        "name": "Jian Yan"
                    },
                    {
                        "name": "Zhuoxi Li"
                    },
                    {
                        "name": "Yang Ning"
                    },
                    {
                        "name": "Yong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yong Chen"
                },
                "author": "Yong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24711v1",
                "updated": "2025-10-28T17:59:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    59,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:59:02Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    59,
                    2,
                    1,
                    301,
                    0
                ],
                "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available."
                },
                "authors": [
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "Yujin Han"
                    },
                    {
                        "name": "Zhekai Chen"
                    },
                    {
                        "name": "Jiayu Wang"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Hongming Shan"
                    }
                ],
                "author_detail": {
                    "name": "Hongming Shan"
                },
                "author": "Hongming Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13397v2",
                "updated": "2025-10-28T17:56:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    56,
                    27,
                    1,
                    301,
                    0
                ],
                "published": "2024-04-20T14:42:43Z",
                "published_parsed": [
                    2024,
                    4,
                    20,
                    14,
                    42,
                    43,
                    5,
                    111,
                    0
                ],
                "title": "Retrieval-Augmented Generation-based Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation-based Relation Extraction"
                },
                "summary": "Information Extraction (IE) is a transformative process that converts\nunstructured text data into a structured format by employing entity and\nrelation extraction (RE) methodologies. The identification of the relation\nbetween a pair of entities plays a crucial role within this framework. Despite\nthe existence of various techniques for relation extraction, their efficacy\nheavily relies on access to labeled data and substantial computational\nresources. In addressing these challenges, Large Language Models (LLMs) emerge\nas promising solutions; however, they might return hallucinating responses due\nto their own training data. To overcome these limitations, Retrieved-Augmented\nGeneration-based Relation Extraction (RAG4RE) in this work is proposed,\noffering a pathway to enhance the performance of relation extraction tasks.\n  This work evaluated the effectiveness of our RAG4RE approach utilizing\ndifferent LLMs. Through the utilization of established benchmarks, such as\nTACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to\ncomprehensively evaluate the efficacy of our RAG4RE approach. In particularly,\nwe leverage prominent LLMs including Flan T5, Llama2, and Mistral in our\ninvestigation. The results of our study demonstrate that our RAG4RE approach\nsurpasses performance of traditional RE approaches based solely on LLMs,\nparticularly evident in the TACRED dataset and its variations. Furthermore, our\napproach exhibits remarkable performance compared to previous RE methodologies\nacross both TACRED and TACREV datasets, underscoring its efficacy and potential\nfor advancing RE tasks in natural language processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Extraction (IE) is a transformative process that converts\nunstructured text data into a structured format by employing entity and\nrelation extraction (RE) methodologies. The identification of the relation\nbetween a pair of entities plays a crucial role within this framework. Despite\nthe existence of various techniques for relation extraction, their efficacy\nheavily relies on access to labeled data and substantial computational\nresources. In addressing these challenges, Large Language Models (LLMs) emerge\nas promising solutions; however, they might return hallucinating responses due\nto their own training data. To overcome these limitations, Retrieved-Augmented\nGeneration-based Relation Extraction (RAG4RE) in this work is proposed,\noffering a pathway to enhance the performance of relation extraction tasks.\n  This work evaluated the effectiveness of our RAG4RE approach utilizing\ndifferent LLMs. Through the utilization of established benchmarks, such as\nTACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to\ncomprehensively evaluate the efficacy of our RAG4RE approach. In particularly,\nwe leverage prominent LLMs including Flan T5, Llama2, and Mistral in our\ninvestigation. The results of our study demonstrate that our RAG4RE approach\nsurpasses performance of traditional RE approaches based solely on LLMs,\nparticularly evident in the TACRED dataset and its variations. Furthermore, our\napproach exhibits remarkable performance compared to previous RE methodologies\nacross both TACRED and TACREV datasets, underscoring its efficacy and potential\nfor advancing RE tasks in natural language processing."
                },
                "authors": [
                    {
                        "name": "Sefika Efeoglu"
                    },
                    {
                        "name": "Adrian Paschke"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Paschke"
                },
                "author": "Adrian Paschke",
                "arxiv_doi": "10.1177/22104968251385519",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1177/22104968251385519",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "published at the Semantic Web journal. The last version is available:\n  https://doi.org/10.1177/22104968251385519",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24706v1",
                "updated": "2025-10-28T17:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    55,
                    42,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:55:42Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    55,
                    42,
                    1,
                    301,
                    0
                ],
                "title": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality\n  Games?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality\n  Games?"
                },
                "summary": "Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench."
                },
                "authors": [
                    {
                        "name": "Shuqing Li"
                    },
                    {
                        "name": "Jiayi Yan"
                    },
                    {
                        "name": "Chenyu Niu"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Yun Peng"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Yepang Liu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24702v1",
                "updated": "2025-10-28T17:53:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    53,
                    13,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:53:13Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    53,
                    13,
                    1,
                    301,
                    0
                ],
                "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective\n  Fine-tuning of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Data Protocol: Unifying Datasets for Diverse, Effective\n  Fine-tuning of LLM Agents"
                },
                "summary": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training."
                },
                "authors": [
                    {
                        "name": "Yueqi Song"
                    },
                    {
                        "name": "Ketan Ramaneti"
                    },
                    {
                        "name": "Zaid Sheikh"
                    },
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Boyu Gou"
                    },
                    {
                        "name": "Tianbao Xie"
                    },
                    {
                        "name": "Yiheng Xu"
                    },
                    {
                        "name": "Danyang Zhang"
                    },
                    {
                        "name": "Apurva Gandhi"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Zhihao Yuan"
                    },
                    {
                        "name": "Frank Xu"
                    },
                    {
                        "name": "Shuyan Zhou"
                    },
                    {
                        "name": "Xingyao Wang"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Huan Sun"
                    },
                    {
                        "name": "Yu Su"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24699v1",
                "updated": "2025-10-28T17:51:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    51,
                    50,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:51:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    51,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentFold: Long-Horizon Web Agents with Proactive Context Management"
                },
                "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini."
                },
                "authors": [
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Yida Zhao"
                    },
                    {
                        "name": "Liangcai Su"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Yong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jiang"
                },
                "author": "Yong Jiang",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24697v1",
                "updated": "2025-10-28T17:51:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    51,
                    42,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:51:42Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    51,
                    42,
                    1,
                    301,
                    0
                ],
                "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking"
                },
                "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines."
                },
                "authors": [
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Haiyang Shen"
                    },
                    {
                        "name": "Baixuan Li"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Yong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jiang"
                },
                "author": "Yong Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24695v1",
                "updated": "2025-10-28T17:50:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    50,
                    47,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:50:47Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    50,
                    47,
                    1,
                    301,
                    0
                ],
                "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis"
                },
                "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents."
                },
                "authors": [
                    {
                        "name": "Xuanzhong Chen"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Liangcai Su"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Yong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jiang"
                },
                "author": "Yong Jiang",
                "arxiv_comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24694v1",
                "updated": "2025-10-28T17:50:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    50,
                    40,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:50:40Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    50,
                    40,
                    1,
                    301,
                    0
                ],
                "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision"
                },
                "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents."
                },
                "authors": [
                    {
                        "name": "Yida Zhao"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Dingchu Zhang"
                    },
                    {
                        "name": "Baixuan Li"
                    },
                    {
                        "name": "Maojia Song"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Kewei Tu"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Yong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jiang"
                },
                "author": "Yong Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24677v1",
                "updated": "2025-10-28T17:40:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    40,
                    53,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:40:53Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    40,
                    53,
                    1,
                    301,
                    0
                ],
                "title": "Dissecting Role Cognition in Medical LLMs via Neuronal Ablation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Role Cognition in Medical LLMs via Neuronal Ablation"
                },
                "summary": "Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor"
                },
                "authors": [
                    {
                        "name": "Xun Liang"
                    },
                    {
                        "name": "Huayi Lai"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Yanfang Chen"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24670v1",
                "updated": "2025-10-28T17:36:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    36,
                    51,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:36:51Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    36,
                    51,
                    1,
                    301,
                    0
                ],
                "title": "Pearl: A Foundation Model for Placing Every Atom in the Right Location",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pearl: A Foundation Model for Placing Every Atom in the Right Location"
                },
                "summary": "Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining."
                },
                "authors": [
                    {
                        "name": "Genesis Research Team"
                    },
                    {
                        "name": "Alejandro Dobles"
                    },
                    {
                        "name": "Nina Jovic"
                    },
                    {
                        "name": "Kenneth Leidal"
                    },
                    {
                        "name": "Pranav Murugan"
                    },
                    {
                        "name": "David C. Williams"
                    },
                    {
                        "name": "Drausin Wulsin"
                    },
                    {
                        "name": "Nate Gruver"
                    },
                    {
                        "name": "Christina X. Ji"
                    },
                    {
                        "name": "Korrawat Pruegsanusak"
                    },
                    {
                        "name": "Gianluca Scarpellini"
                    },
                    {
                        "name": "Ansh Sharma"
                    },
                    {
                        "name": "Wojciech Swiderski"
                    },
                    {
                        "name": "Andrea Bootsma"
                    },
                    {
                        "name": "Richard Strong Bowen"
                    },
                    {
                        "name": "Charlotte Chen"
                    },
                    {
                        "name": "Jamin Chen"
                    },
                    {
                        "name": "Marc André Dämgen"
                    },
                    {
                        "name": "Roy Tal Dew"
                    },
                    {
                        "name": "Benjamin DiFrancesco"
                    },
                    {
                        "name": "J. D. Fishman"
                    },
                    {
                        "name": "Alla Ivanova"
                    },
                    {
                        "name": "Zach Kagin"
                    },
                    {
                        "name": "David Li-Bland"
                    },
                    {
                        "name": "Zuli Liu"
                    },
                    {
                        "name": "Igor Morozov"
                    },
                    {
                        "name": "Jeffrey Ouyang-Zhang"
                    },
                    {
                        "name": "Frank C. Pickard IV"
                    },
                    {
                        "name": "Kushal S. Shah"
                    },
                    {
                        "name": "Ben Shor"
                    },
                    {
                        "name": "Gabriel Monteiro da Silva"
                    },
                    {
                        "name": "Maxx Tessmer"
                    },
                    {
                        "name": "Carl Tilbury"
                    },
                    {
                        "name": "Cyr Vetcher"
                    },
                    {
                        "name": "Daniel Zeng"
                    },
                    {
                        "name": "Maruan Al-Shedivat"
                    },
                    {
                        "name": "Aleksandra Faust"
                    },
                    {
                        "name": "Evan N. Feinberg"
                    },
                    {
                        "name": "Michael V. LeVine"
                    },
                    {
                        "name": "Matteus Pan"
                    }
                ],
                "author_detail": {
                    "name": "Matteus Pan"
                },
                "author": "Matteus Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01281v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01281v6",
                "updated": "2025-10-28T17:26:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    26,
                    20,
                    1,
                    301,
                    0
                ],
                "published": "2024-11-02T15:23:28Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    23,
                    28,
                    5,
                    307,
                    0
                ],
                "title": "Arena-Lite: Efficient and Reliable Large Language Model Evaluation via\n  Tournament-Based Direct Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arena-Lite: Efficient and Reliable Large Language Model Evaluation via\n  Tournament-Based Direct Comparisons"
                },
                "summary": "As Large Language Models (LLMs) expand across domains, LLM judges have become\nessential for systems evaluation. Current benchmarks typically compare system\noutputs against baselines. This baseline-mediated approach, though convenient,\nyields lower reliability than direct comparison between systems. We propose\nArena-Lite which integrates tournament structure on top of head-to-head\ncomparison. The application of a tournament structure and direct comparison\neliminates the need for baseline outputs, reduces the number of required\ncomparisons, and allows higher reliability in system rankings. We conducted two\nexperiments: (1) controlled stochastic modeling and (2) empirical validation\nwith a real LLM judge. Those experiments collectively demonstrate that\nArena-Lite consistently achieves higher reliability with fewer comparisons,\neven with smaller datasets or weaker judges. We release an easy-to-use web\ndemonstration and code to foster adoption of Arena-Lite, streamlining model\nselection across research and industry communities. Arena-Lite demo and code\nare available on\n\\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) expand across domains, LLM judges have become\nessential for systems evaluation. Current benchmarks typically compare system\noutputs against baselines. This baseline-mediated approach, though convenient,\nyields lower reliability than direct comparison between systems. We propose\nArena-Lite which integrates tournament structure on top of head-to-head\ncomparison. The application of a tournament structure and direct comparison\neliminates the need for baseline outputs, reduces the number of required\ncomparisons, and allows higher reliability in system rankings. We conducted two\nexperiments: (1) controlled stochastic modeling and (2) empirical validation\nwith a real LLM judge. Those experiments collectively demonstrate that\nArena-Lite consistently achieves higher reliability with fewer comparisons,\neven with smaller datasets or weaker judges. We release an easy-to-use web\ndemonstration and code to foster adoption of Arena-Lite, streamlining model\nselection across research and industry communities. Arena-Lite demo and code\nare available on\n\\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}"
                },
                "authors": [
                    {
                        "name": "Seonil Son"
                    },
                    {
                        "name": "Ju-Min Oh"
                    },
                    {
                        "name": "Heegon Jin"
                    },
                    {
                        "name": "Cheolhun Jang"
                    },
                    {
                        "name": "Jeongbeom Jeong"
                    },
                    {
                        "name": "Kuntae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kuntae Kim"
                },
                "author": "Kuntae Kim",
                "arxiv_comment": "8 pages for main body, 19 pages in total",
                "arxiv_journal_ref": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01281v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01281v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24654v1",
                "updated": "2025-10-28T17:19:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    19,
                    47,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:19:47Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    19,
                    47,
                    1,
                    301,
                    0
                ],
                "title": "Evolving Diagnostic Agents in a Virtual Clinical Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Diagnostic Agents in a Virtual Clinical Environment"
                },
                "summary": "In this paper, we present a framework for training large language models\n(LLMs) as diagnostic agents with reinforcement learning, enabling them to\nmanage multi-turn diagnostic processes, adaptively select examinations, and\ncommit to final diagnoses. Unlike instruction-tuned models trained on static\ncase summaries, our method acquires diagnostic strategies through interactive\nexploration and outcome-based feedback. Our contributions are fourfold: (i) We\npresent DiagGym, a diagnostics world model trained with electronic health\nrecords that emits examination outcomes conditioned on patient history and\nrecommended examination, serving as a virtual clinical environment for\nrealistic diagnosis training and evaluation; (ii) We train DiagAgent via\nend-to-end, multi-turn reinforcement learning to learn diagnostic policies that\noptimize both information yield and diagnostic accuracy; (iii) We introduce\nDiagBench, a diagnostic benchmark comprising 750 cases with physician-validated\nexamination recommendations and 99 cases annotated with 973 physician-written\nrubrics on diagnosis process; (iv) we demonstrate superior performance across\ndiverse diagnostic settings. DiagAgent significantly outperforms 10\nstate-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two\nprompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%\nhigher diagnostic accuracy and 44.03% improvement in examination recommendation\nhit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic\naccuracy and 23.09% boost in examination recommendation F1 score. In\nrubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by\n7.1% in weighted rubric score. These findings indicate that learning policies\nin interactive clinical environments confers dynamic and clinically meaningful\ndiagnostic management abilities unattainable through passive training alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a framework for training large language models\n(LLMs) as diagnostic agents with reinforcement learning, enabling them to\nmanage multi-turn diagnostic processes, adaptively select examinations, and\ncommit to final diagnoses. Unlike instruction-tuned models trained on static\ncase summaries, our method acquires diagnostic strategies through interactive\nexploration and outcome-based feedback. Our contributions are fourfold: (i) We\npresent DiagGym, a diagnostics world model trained with electronic health\nrecords that emits examination outcomes conditioned on patient history and\nrecommended examination, serving as a virtual clinical environment for\nrealistic diagnosis training and evaluation; (ii) We train DiagAgent via\nend-to-end, multi-turn reinforcement learning to learn diagnostic policies that\noptimize both information yield and diagnostic accuracy; (iii) We introduce\nDiagBench, a diagnostic benchmark comprising 750 cases with physician-validated\nexamination recommendations and 99 cases annotated with 973 physician-written\nrubrics on diagnosis process; (iv) we demonstrate superior performance across\ndiverse diagnostic settings. DiagAgent significantly outperforms 10\nstate-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two\nprompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%\nhigher diagnostic accuracy and 44.03% improvement in examination recommendation\nhit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic\naccuracy and 23.09% boost in examination recommendation F1 score. In\nrubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by\n7.1% in weighted rubric score. These findings indicate that learning policies\nin interactive clinical environments confers dynamic and clinically meaningful\ndiagnostic management abilities unattainable through passive training alone."
                },
                "authors": [
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Qiaoyu Zheng"
                    },
                    {
                        "name": "Yusheng Liao"
                    },
                    {
                        "name": "Haowen Wang"
                    },
                    {
                        "name": "Yun Yue"
                    },
                    {
                        "name": "Qianrui Fan"
                    },
                    {
                        "name": "Shuai Zhen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24652v1",
                "updated": "2025-10-28T17:18:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    18,
                    30,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:18:30Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    18,
                    30,
                    1,
                    301,
                    0
                ],
                "title": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning"
                },
                "summary": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24650v1",
                "updated": "2025-10-28T17:16:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    16,
                    47,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:16:47Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    16,
                    47,
                    1,
                    301,
                    0
                ],
                "title": "Advancing site-specific disease and pest management in precision\n  agriculture: From reasoning-driven foundation models to adaptive,\n  feedback-based learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing site-specific disease and pest management in precision\n  agriculture: From reasoning-driven foundation models to adaptive,\n  feedback-based learning"
                },
                "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets."
                },
                "authors": [
                    {
                        "name": "Nitin Rai"
                    },
                    {
                        "name": "Daeun"
                    },
                    {
                        "name": "Choi"
                    },
                    {
                        "name": "Nathan S. Boyd"
                    },
                    {
                        "name": "Arnold W. Schumann"
                    }
                ],
                "author_detail": {
                    "name": "Arnold W. Schumann"
                },
                "arxiv_affiliation": "Dana",
                "author": "Arnold W. Schumann",
                "arxiv_comment": "26 pages, 8 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24645v1",
                "updated": "2025-10-28T17:15:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    15,
                    26,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:15:26Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    15,
                    26,
                    1,
                    301,
                    0
                ],
                "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in\n  Multi-Turn Function Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in\n  Multi-Turn Function Calling"
                },
                "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning."
                },
                "authors": [
                    {
                        "name": "Zengzhuang Xu"
                    },
                    {
                        "name": "Bingguang Hao"
                    },
                    {
                        "name": "Zechuan Wang"
                    },
                    {
                        "name": "Yuntao Wen"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Yicheng Chen"
                    },
                    {
                        "name": "Cunyin Peng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Shi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shi Gu"
                },
                "author": "Shi Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24637v1",
                "updated": "2025-10-28T17:03:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    3,
                    33,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:03:33Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    3,
                    33,
                    1,
                    301,
                    0
                ],
                "title": "All in one timestep: Enhancing Sparsity and Energy efficiency in\n  Multi-level Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All in one timestep: Enhancing Sparsity and Energy efficiency in\n  Multi-level Spiking Neural Networks"
                },
                "summary": "Spiking Neural Networks (SNNs) are one of the most promising bio-inspired\nneural networks models and have drawn increasing attention in recent years. The\nevent-driven communication mechanism of SNNs allows for sparse and\ntheoretically low-power operations on dedicated neuromorphic hardware. However,\nthe binary nature of instantaneous spikes also leads to considerable\ninformation loss in SNNs, resulting in accuracy degradation. To address this\nissue, we propose a multi-level spiking neuron model able to provide both\nlow-quantization error and minimal inference latency while approaching the\nperformance of full precision Artificial Neural Networks (ANNs). Experimental\nresults with popular network architectures and datasets, show that multi-level\nspiking neurons provide better information compression, allowing therefore a\nreduction in latency without performance loss. When compared to binary SNNs on\nimage classification scenarios, multi-level SNNs indeed allow reducing by 2 to\n3 times the energy consumption depending on the number of quantization\nintervals. On neuromorphic data, our approach allows us to drastically reduce\nthe inference latency to 1 timestep, which corresponds to a compression factor\nof 10 compared to previously published results. At the architectural level, we\npropose a new residual architecture that we call Sparse-ResNet. Through a\ncareful analysis of the spikes propagation in residual connections we highlight\na spike avalanche effect, that affects most spiking residual architectures.\nUsing our Sparse-ResNet architecture, we can provide state-of-the-art accuracy\nresults in image classification while reducing by more than 20% the network\nactivity compared to the previous spiking ResNets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are one of the most promising bio-inspired\nneural networks models and have drawn increasing attention in recent years. The\nevent-driven communication mechanism of SNNs allows for sparse and\ntheoretically low-power operations on dedicated neuromorphic hardware. However,\nthe binary nature of instantaneous spikes also leads to considerable\ninformation loss in SNNs, resulting in accuracy degradation. To address this\nissue, we propose a multi-level spiking neuron model able to provide both\nlow-quantization error and minimal inference latency while approaching the\nperformance of full precision Artificial Neural Networks (ANNs). Experimental\nresults with popular network architectures and datasets, show that multi-level\nspiking neurons provide better information compression, allowing therefore a\nreduction in latency without performance loss. When compared to binary SNNs on\nimage classification scenarios, multi-level SNNs indeed allow reducing by 2 to\n3 times the energy consumption depending on the number of quantization\nintervals. On neuromorphic data, our approach allows us to drastically reduce\nthe inference latency to 1 timestep, which corresponds to a compression factor\nof 10 compared to previously published results. At the architectural level, we\npropose a new residual architecture that we call Sparse-ResNet. Through a\ncareful analysis of the spikes propagation in residual connections we highlight\na spike avalanche effect, that affects most spiking residual architectures.\nUsing our Sparse-ResNet architecture, we can provide state-of-the-art accuracy\nresults in image classification while reducing by more than 20% the network\nactivity compared to the previous spiking ResNets."
                },
                "authors": [
                    {
                        "name": "Andrea Castagnetti"
                    },
                    {
                        "name": "Alain Pegatoquet"
                    },
                    {
                        "name": "Benoît Miramond"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Miramond"
                },
                "author": "Benoît Miramond",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24636v1",
                "updated": "2025-10-28T17:02:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    2,
                    46,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:02:46Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    2,
                    46,
                    1,
                    301,
                    0
                ],
                "title": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement\n  Learning"
                },
                "summary": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation."
                },
                "authors": [
                    {
                        "name": "Ziyou Hu"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Minghang Zhu"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Zhaochun Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhaochun Ren"
                },
                "author": "Zhaochun Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24631v1",
                "updated": "2025-10-28T16:59:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    59,
                    42,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:59:42Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    59,
                    42,
                    1,
                    301,
                    0
                ],
                "title": "Bridging Simulators with Conditional Optimal Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Simulators with Conditional Optimal Transport"
                },
                "summary": "We propose a new field-level emulator that bridges two simulators using\nunpaired simulation datasets. Our method leverages a flow-based approach to\nlearn the likelihood transport from one simulator to the other. Since multiple\ntransport maps exist, we employ Conditional Optimal Transport Flow Matching\n(COT-FM) to ensure that the transformation minimally distorts the underlying\nstructure of the data. We demonstrate the effectiveness of this approach by\nbridging weak lensing simulators: a Lagrangian Perturbation Theory (LPT) to a\nN-body Particle-Mesh (PM). We demonstrate that our emulator captures the full\ncorrection between the simulators by showing that it enables full-field\ninference to accurately recover the true posterior, validating its accuracy\nbeyond traditional summary statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new field-level emulator that bridges two simulators using\nunpaired simulation datasets. Our method leverages a flow-based approach to\nlearn the likelihood transport from one simulator to the other. Since multiple\ntransport maps exist, we employ Conditional Optimal Transport Flow Matching\n(COT-FM) to ensure that the transformation minimally distorts the underlying\nstructure of the data. We demonstrate the effectiveness of this approach by\nbridging weak lensing simulators: a Lagrangian Perturbation Theory (LPT) to a\nN-body Particle-Mesh (PM). We demonstrate that our emulator captures the full\ncorrection between the simulators by showing that it enables full-field\ninference to accurately recover the true posterior, validating its accuracy\nbeyond traditional summary statistics."
                },
                "authors": [
                    {
                        "name": "Justine Zeghal"
                    },
                    {
                        "name": "Benjamin Remy"
                    },
                    {
                        "name": "Yashar Hezaveh"
                    },
                    {
                        "name": "Francois Lanusse"
                    },
                    {
                        "name": "Laurence Perreault Levasseur"
                    }
                ],
                "author_detail": {
                    "name": "Laurence Perreault Levasseur"
                },
                "author": "Laurence Perreault Levasseur",
                "arxiv_comment": "Accepted at the 2025 Workshop on Machine Learning for Astrophysics,\n  10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22693v2",
                "updated": "2025-10-28T16:57:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    57,
                    22,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-26T14:36:15Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    14,
                    36,
                    15,
                    6,
                    299,
                    0
                ],
                "title": "VADTree: Explainable Training-Free Video Anomaly Detection via\n  Hierarchical Granularity-Aware Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VADTree: Explainable Training-Free Video Anomaly Detection via\n  Hierarchical Granularity-Aware Tree"
                },
                "summary": "Video anomaly detection (VAD) focuses on identifying anomalies in videos.\nSupervised methods demand substantial in-domain training data and fail to\ndeliver clear explanations for anomalies. In contrast, training-free methods\nleverage the knowledge reserves and language interactivity of large pre-trained\nmodels to detect anomalies. However, the current fixed-length temporal window\nsampling approaches struggle to accurately capture anomalies with varying\ntemporal spans. Therefore, we propose VADTree that utilizes a Hierarchical\nGranularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree\nleverages the knowledge embedded in a pre-trained Generic Event Boundary\nDetection (GEBD) model to characterize potential anomaly event boundaries.\nSpecifically, VADTree decomposes the video into generic event nodes based on\nboundary confidence, and performs adaptive coarse-fine hierarchical structuring\nand redundancy removal to construct the HGTree. Then, the multi-dimensional\npriors are injected into the visual language models (VLMs) to enhance the\nnode-wise anomaly perception, and anomaly reasoning for generic event nodes is\nachieved via large language models (LLMs). Finally, an inter-cluster node\ncorrelation method is used to integrate the multi-granularity anomaly scores.\nExtensive experiments on three challenging datasets demonstrate that VADTree\nachieves state-of-the-art performance in training-free settings while\ndrastically reducing the number of sampled video segments. The code will be\navailable at https://github.com/wenlongli10/VADTree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video anomaly detection (VAD) focuses on identifying anomalies in videos.\nSupervised methods demand substantial in-domain training data and fail to\ndeliver clear explanations for anomalies. In contrast, training-free methods\nleverage the knowledge reserves and language interactivity of large pre-trained\nmodels to detect anomalies. However, the current fixed-length temporal window\nsampling approaches struggle to accurately capture anomalies with varying\ntemporal spans. Therefore, we propose VADTree that utilizes a Hierarchical\nGranularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree\nleverages the knowledge embedded in a pre-trained Generic Event Boundary\nDetection (GEBD) model to characterize potential anomaly event boundaries.\nSpecifically, VADTree decomposes the video into generic event nodes based on\nboundary confidence, and performs adaptive coarse-fine hierarchical structuring\nand redundancy removal to construct the HGTree. Then, the multi-dimensional\npriors are injected into the visual language models (VLMs) to enhance the\nnode-wise anomaly perception, and anomaly reasoning for generic event nodes is\nachieved via large language models (LLMs). Finally, an inter-cluster node\ncorrelation method is used to integrate the multi-granularity anomaly scores.\nExtensive experiments on three challenging datasets demonstrate that VADTree\nachieves state-of-the-art performance in training-free settings while\ndrastically reducing the number of sampled video segments. The code will be\navailable at https://github.com/wenlongli10/VADTree."
                },
                "authors": [
                    {
                        "name": "Wenlong Li"
                    },
                    {
                        "name": "Yifei Xu"
                    },
                    {
                        "name": "Yuan Rao"
                    },
                    {
                        "name": "Zhenhua Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "NeurIPS 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24626v1",
                "updated": "2025-10-28T16:55:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    55,
                    22,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:55:22Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    55,
                    22,
                    1,
                    301,
                    0
                ],
                "title": "Relative Scaling Laws for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relative Scaling Laws for LLMs"
                },
                "summary": "Scaling laws describe how language models improve with additional data,\nparameters, and compute. While widely used, they are typically measured on\naggregate test sets. Aggregate evaluations yield clean trends but average over\nheterogeneous subpopulations, obscuring performance disparities. We introduce\nrelative scaling laws, which track how performance gaps between test\ndistributions evolve with scale rather than focusing solely on absolute error.\nUsing 255 decoder-only Transformers trained under matched-compute (IsoFLOP)\nbudgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we\nfind diverse trajectories: academic domains on MMLU converge toward parity;\nregional English dialects shift depending on population size; and clusters of\nAI risk behaviours split, with capability- and influence-related risks\nincreasing during pretraining while adversarial risks do not. These results\nshow that although scaling improves overall performance, it is not a universal\nequalizer. To support further study, we release all model checkpoints from this\nwork to enable practitioners to measure relative alongside traditional scaling\nlaws, in order to better prioritize robustness challenges in light of the\nbitter lesson.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws describe how language models improve with additional data,\nparameters, and compute. While widely used, they are typically measured on\naggregate test sets. Aggregate evaluations yield clean trends but average over\nheterogeneous subpopulations, obscuring performance disparities. We introduce\nrelative scaling laws, which track how performance gaps between test\ndistributions evolve with scale rather than focusing solely on absolute error.\nUsing 255 decoder-only Transformers trained under matched-compute (IsoFLOP)\nbudgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we\nfind diverse trajectories: academic domains on MMLU converge toward parity;\nregional English dialects shift depending on population size; and clusters of\nAI risk behaviours split, with capability- and influence-related risks\nincreasing during pretraining while adversarial risks do not. These results\nshow that although scaling improves overall performance, it is not a universal\nequalizer. To support further study, we release all model checkpoints from this\nwork to enable practitioners to measure relative alongside traditional scaling\nlaws, in order to better prioritize robustness challenges in light of the\nbitter lesson."
                },
                "authors": [
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "David Hall"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24619v1",
                "updated": "2025-10-28T16:48:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    48,
                    3,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:48:03Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    48,
                    3,
                    1,
                    301,
                    0
                ],
                "title": "Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation"
                },
                "summary": "With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings."
                },
                "authors": [
                    {
                        "name": "Snegha A"
                    },
                    {
                        "name": "Sayambhu Sen"
                    },
                    {
                        "name": "Piyush Singh Pasi"
                    },
                    {
                        "name": "Abhishek Singhania"
                    },
                    {
                        "name": "Preethi Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Preethi Jyothi"
                },
                "arxiv_affiliation": "Indian Institute of Technology Bombay",
                "author": "Preethi Jyothi",
                "arxiv_comment": "12 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18513v2",
                "updated": "2025-10-28T16:44:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    44,
                    35,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-21T10:55:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    55,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "DWaste: Greener AI for Waste Sorting using Mobile and Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DWaste: Greener AI for Waste Sorting using Mobile and Edge Devices"
                },
                "summary": "The rise of convenience packaging has led to generation of enormous waste,\nmaking efficient waste sorting crucial for sustainable waste management. To\naddress this, we developed DWaste, a computer vision-powered platform designed\nfor real-time waste sorting on resource-constrained smartphones and edge\ndevices, including offline functionality. We benchmarked various image\nclassification models (EfficientNetV2S/M, ResNet50/101, MobileNet) and object\ndetection (YOLOv8n, YOLOv11n) including our purposed YOLOv8n-CBAM model using\nour annotated dataset designed for recycling. We found a clear trade-off\nbetween accuracy and resource consumption: the best classifier,\nEfficientNetV2S, achieved high accuracy(~ 96%) but suffered from high latency\n(~ 0.22s) and elevated carbon emissions. In contrast, lightweight object\ndetection models delivered strong performance (up to 80% mAP) with ultra-fast\ninference (~ 0.03s) and significantly smaller model sizes (< 7MB ), making them\nideal for real-time, low-power use. Model quantization further maximized\nefficiency, substantially reducing model size and VRAM usage by up to 75%. Our\nwork demonstrates the successful implementation of \"Greener AI\" models to\nsupport real-time, sustainable waste sorting on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of convenience packaging has led to generation of enormous waste,\nmaking efficient waste sorting crucial for sustainable waste management. To\naddress this, we developed DWaste, a computer vision-powered platform designed\nfor real-time waste sorting on resource-constrained smartphones and edge\ndevices, including offline functionality. We benchmarked various image\nclassification models (EfficientNetV2S/M, ResNet50/101, MobileNet) and object\ndetection (YOLOv8n, YOLOv11n) including our purposed YOLOv8n-CBAM model using\nour annotated dataset designed for recycling. We found a clear trade-off\nbetween accuracy and resource consumption: the best classifier,\nEfficientNetV2S, achieved high accuracy(~ 96%) but suffered from high latency\n(~ 0.22s) and elevated carbon emissions. In contrast, lightweight object\ndetection models delivered strong performance (up to 80% mAP) with ultra-fast\ninference (~ 0.03s) and significantly smaller model sizes (< 7MB ), making them\nideal for real-time, low-power use. Model quantization further maximized\nefficiency, substantially reducing model size and VRAM usage by up to 75%. Our\nwork demonstrates the successful implementation of \"Greener AI\" models to\nsupport real-time, sustainable waste sorting on edge devices."
                },
                "authors": [
                    {
                        "name": "Suman Kunwar"
                    }
                ],
                "author_detail": {
                    "name": "Suman Kunwar"
                },
                "author": "Suman Kunwar",
                "arxiv_comment": "8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11390v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11390v3",
                "updated": "2025-10-28T16:44:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    44,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2024-09-17T17:50:15Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    50,
                    15,
                    1,
                    261,
                    0
                ],
                "title": "Says Who? Effective Zero-Shot Annotation of Focalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Says Who? Effective Zero-Shot Annotation of Focalization"
                },
                "summary": "Focalization describes the way in which access to narrative information is\nrestricted or controlled based on the knowledge available to knowledge of the\nnarrator. It is encoded via a wide range of lexico-grammatical features and is\nsubject to reader interpretation. Even trained annotators frequently disagree\non correct labels, suggesting this task is both qualitatively and\ncomputationally challenging. In this work, we test how well five contemporary\nlarge language model (LLM) families and two baselines perform when annotating\nshort literary excerpts for focalization. Despite the challenging nature of the\ntask, we find that LLMs show comparable performance to trained human\nannotators, with GPT-4o achieving an average F1 of 84.79%. Further, we\ndemonstrate that the log probabilities output by GPT-family models frequently\nreflect the difficulty of annotating particular excerpts. Finally, we provide a\ncase study analyzing sixteen Stephen King novels, demonstrating the usefulness\nof this approach for computational literary studies and the insights gleaned\nfrom examining focalization at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focalization describes the way in which access to narrative information is\nrestricted or controlled based on the knowledge available to knowledge of the\nnarrator. It is encoded via a wide range of lexico-grammatical features and is\nsubject to reader interpretation. Even trained annotators frequently disagree\non correct labels, suggesting this task is both qualitatively and\ncomputationally challenging. In this work, we test how well five contemporary\nlarge language model (LLM) families and two baselines perform when annotating\nshort literary excerpts for focalization. Despite the challenging nature of the\ntask, we find that LLMs show comparable performance to trained human\nannotators, with GPT-4o achieving an average F1 of 84.79%. Further, we\ndemonstrate that the log probabilities output by GPT-family models frequently\nreflect the difficulty of annotating particular excerpts. Finally, we provide a\ncase study analyzing sixteen Stephen King novels, demonstrating the usefulness\nof this approach for computational literary studies and the insights gleaned\nfrom examining focalization at scale."
                },
                "authors": [
                    {
                        "name": "Rebecca M. M. Hicke"
                    },
                    {
                        "name": "Yuri Bizzoni"
                    },
                    {
                        "name": "Pascale Feldkamp"
                    },
                    {
                        "name": "Ross Deans Kristensen-McLachlan"
                    }
                ],
                "author_detail": {
                    "name": "Ross Deans Kristensen-McLachlan"
                },
                "author": "Ross Deans Kristensen-McLachlan",
                "arxiv_comment": "Accepted at CHR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11390v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11390v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07046v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07046v3",
                "updated": "2025-10-28T16:43:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    43,
                    19,
                    1,
                    301,
                    0
                ],
                "published": "2024-05-11T16:22:00Z",
                "published_parsed": [
                    2024,
                    5,
                    11,
                    16,
                    22,
                    0,
                    5,
                    132,
                    0
                ],
                "title": "RETTA: Retrieval-Enhanced Test-Time Adaptation for Zero-Shot Video\n  Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RETTA: Retrieval-Enhanced Test-Time Adaptation for Zero-Shot Video\n  Captioning"
                },
                "summary": "Despite the significant progress of fully-supervised video captioning,\nzero-shot methods remain much less explored. In this paper, we propose a novel\nzero-shot video captioning framework named Retrieval-Enhanced Test-Time\nAdaptation (RETTA), which takes advantage of existing pretrained large-scale\nvision and language models to directly generate captions with test-time\nadaptation. Specifically, we bridge video and text using four key models: a\ngeneral video-text retrieval model XCLIP, a general image-text matching model\nCLIP, a text alignment model AnglE, and a text generation model GPT-2, due to\ntheir source-code availability. The main challenge is how to enable the text\ngeneration model to be sufficiently aware of the content in a given video so as\nto generate corresponding captions. To address this problem, we propose using\nlearnable tokens as a communication medium among these four frozen models\nGPT-2, XCLIP, CLIP, and AnglE. Different from the conventional way that trains\nthese tokens with training data, we propose to learn these tokens with soft\ntargets of the inference data under several carefully crafted loss functions,\nwhich enable the tokens to absorb video information catered for GPT-2. This\nprocedure can be efficiently done in just a few iterations (we use 16\niterations in the experiments) and does not require ground truth data.\nExtensive experimental results on three widely used datasets, MSR-VTT, MSVD,\nand VATEX, show absolute 5.1%-32.4% improvements in terms of the main metric\nCIDEr compared to several state-of-the-art zero-shot video captioning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant progress of fully-supervised video captioning,\nzero-shot methods remain much less explored. In this paper, we propose a novel\nzero-shot video captioning framework named Retrieval-Enhanced Test-Time\nAdaptation (RETTA), which takes advantage of existing pretrained large-scale\nvision and language models to directly generate captions with test-time\nadaptation. Specifically, we bridge video and text using four key models: a\ngeneral video-text retrieval model XCLIP, a general image-text matching model\nCLIP, a text alignment model AnglE, and a text generation model GPT-2, due to\ntheir source-code availability. The main challenge is how to enable the text\ngeneration model to be sufficiently aware of the content in a given video so as\nto generate corresponding captions. To address this problem, we propose using\nlearnable tokens as a communication medium among these four frozen models\nGPT-2, XCLIP, CLIP, and AnglE. Different from the conventional way that trains\nthese tokens with training data, we propose to learn these tokens with soft\ntargets of the inference data under several carefully crafted loss functions,\nwhich enable the tokens to absorb video information catered for GPT-2. This\nprocedure can be efficiently done in just a few iterations (we use 16\niterations in the experiments) and does not require ground truth data.\nExtensive experimental results on three widely used datasets, MSR-VTT, MSVD,\nand VATEX, show absolute 5.1%-32.4% improvements in terms of the main metric\nCIDEr compared to several state-of-the-art zero-shot video captioning methods."
                },
                "authors": [
                    {
                        "name": "Yunchuan Ma"
                    },
                    {
                        "name": "Laiyun Qing"
                    },
                    {
                        "name": "Guorong Li"
                    },
                    {
                        "name": "Yuankai Qi"
                    },
                    {
                        "name": "Amin Beheshti"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Qingming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qingming Huang"
                },
                "author": "Qingming Huang",
                "arxiv_comment": "Published in Pattern Recognition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07046v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07046v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24606v1",
                "updated": "2025-10-28T16:34:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    34,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:34:18Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    34,
                    18,
                    1,
                    301,
                    0
                ],
                "title": "Long-Context Modeling with Dynamic Hierarchical Sparse Attention for\n  On-Device LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Modeling with Dynamic Hierarchical Sparse Attention for\n  On-Device LLMs"
                },
                "summary": "The quadratic cost of attention hinders the scalability of long-context LLMs,\nespecially in resource-constrained settings. Existing static sparse methods\nsuch as sliding windows or global tokens utilizes the sparsity of attention to\nreduce the cost of attention, but poorly adapts to the content-dependent\nvariations in attention due to their staticity. While previous work has\nproposed several dynamic approaches to improve flexibility, they still depend\non predefined templates or heuristic mechanisms. Such strategies reduce\ngenerality and prune tokens that remain contextually important, limiting their\naccuracy across diverse tasks. To tackle these bottlenecks of existing methods\nfor long-context modeling, we introduce Dynamic Hierarchical Sparse Attention\n(DHSA), a data-driven framework that dynamically predicts attention sparsity\nonline without retraining. Our proposed DHSA adaptively segments sequences into\nvariable-length chunks, then computes chunk representations by aggregating the\ntoken embeddings within each chunk. To avoid the bias introduced by varying\nchunk lengths, we apply length-normalized aggregation that scales the averaged\nembeddings by the square root of the chunk size. Finally, DHSA upsamples the\nchunk-level similarity scores to token level similarities to calculate\nimportance scores that determine which token-level interactions should be\npreserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and\nLongBench show that DHSA matches dense attention in accuracy, while reducing\nprefill latency by 20-60% and peak memory usage by 35%. Compared to other\nrepresentative baselines such as block sparse attention, DHSA achieves\nconsistently higher accuracy (6-18% relative gains) with comparable or lower\ncost, offering an efficient and adaptable solution for long-context on-device\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic cost of attention hinders the scalability of long-context LLMs,\nespecially in resource-constrained settings. Existing static sparse methods\nsuch as sliding windows or global tokens utilizes the sparsity of attention to\nreduce the cost of attention, but poorly adapts to the content-dependent\nvariations in attention due to their staticity. While previous work has\nproposed several dynamic approaches to improve flexibility, they still depend\non predefined templates or heuristic mechanisms. Such strategies reduce\ngenerality and prune tokens that remain contextually important, limiting their\naccuracy across diverse tasks. To tackle these bottlenecks of existing methods\nfor long-context modeling, we introduce Dynamic Hierarchical Sparse Attention\n(DHSA), a data-driven framework that dynamically predicts attention sparsity\nonline without retraining. Our proposed DHSA adaptively segments sequences into\nvariable-length chunks, then computes chunk representations by aggregating the\ntoken embeddings within each chunk. To avoid the bias introduced by varying\nchunk lengths, we apply length-normalized aggregation that scales the averaged\nembeddings by the square root of the chunk size. Finally, DHSA upsamples the\nchunk-level similarity scores to token level similarities to calculate\nimportance scores that determine which token-level interactions should be\npreserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and\nLongBench show that DHSA matches dense attention in accuracy, while reducing\nprefill latency by 20-60% and peak memory usage by 35%. Compared to other\nrepresentative baselines such as block sparse attention, DHSA achieves\nconsistently higher accuracy (6-18% relative gains) with comparable or lower\ncost, offering an efficient and adaptable solution for long-context on-device\nLLMs."
                },
                "authors": [
                    {
                        "name": "Siheng Xiong"
                    },
                    {
                        "name": "Joe Zou"
                    },
                    {
                        "name": "Faramarz Fekri"
                    },
                    {
                        "name": "Yae Jee Cho"
                    }
                ],
                "author_detail": {
                    "name": "Yae Jee Cho"
                },
                "author": "Yae Jee Cho",
                "arxiv_comment": "Accepted to NeurIPS 2025 Workshop on Efficient Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24605v1",
                "updated": "2025-10-28T16:32:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    32,
                    43,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:32:43Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    32,
                    43,
                    1,
                    301,
                    0
                ],
                "title": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead\n  the Way",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead\n  the Way"
                },
                "summary": "Diffusion-based large language models (dLLMs) have exhibited substantial\npotential for parallel text generation, which may enable more efficient\ngeneration compared to autoregressive models. However, current dLLMs suffer\nfrom fixed generation lengths, which indicates the generation lengths of dLLMs\nhave to be determined before decoding as a hyper-parameter, leading to issues\nin efficiency and flexibility. To solve these problems, in this work, we\npropose to train a diffusion LLM with native variable generation lengths,\nabbreviated as dLLM-Var. Concretely, we aim to train a model to accurately\npredict the [EOS] token in the generated text, which makes a dLLM be able to\nnatively infer in a block diffusion manner, while still maintaining the ability\nof global bi-directional (full) attention and high parallelism. Experiments on\nstandard benchmarks demonstrate that our method achieves a 30.1x speedup over\ntraditional dLLM inference paradigms and a 2.4x speedup relative to\nautoregressive models such as Qwen and Llama. Our method achieves higher\naccuracy and faster inference, elevating dLLMs beyond mere academic novelty and\nsupporting their practical use in real-world applications. Codes and models\nhave been released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have exhibited substantial\npotential for parallel text generation, which may enable more efficient\ngeneration compared to autoregressive models. However, current dLLMs suffer\nfrom fixed generation lengths, which indicates the generation lengths of dLLMs\nhave to be determined before decoding as a hyper-parameter, leading to issues\nin efficiency and flexibility. To solve these problems, in this work, we\npropose to train a diffusion LLM with native variable generation lengths,\nabbreviated as dLLM-Var. Concretely, we aim to train a model to accurately\npredict the [EOS] token in the generated text, which makes a dLLM be able to\nnatively infer in a block diffusion manner, while still maintaining the ability\nof global bi-directional (full) attention and high parallelism. Experiments on\nstandard benchmarks demonstrate that our method achieves a 30.1x speedup over\ntraditional dLLM inference paradigms and a 2.4x speedup relative to\nautoregressive models such as Qwen and Llama. Our method achieves higher\naccuracy and faster inference, elevating dLLMs beyond mere academic novelty and\nsupporting their practical use in real-world applications. Codes and models\nhave been released."
                },
                "authors": [
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Hanlin Xu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15737v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15737v4",
                "updated": "2025-10-28T16:23:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    23,
                    53,
                    1,
                    301,
                    0
                ],
                "published": "2024-11-24T07:02:32Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    7,
                    2,
                    32,
                    6,
                    329,
                    0
                ],
                "title": "TableTime: Reformulating Time Series Classification as Training-Free\n  Table Understanding with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableTime: Reformulating Time Series Classification as Training-Free\n  Table Understanding with Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated their effectiveness in\nmultivariate time series classification (MTSC). Effective adaptation of LLMs\nfor MTSC necessitates informative data representations. Existing LLM-based\nmethods directly encode embeddings for time series within the latent space of\nLLMs from scratch to align with semantic space of LLMs. Despite their\neffectiveness, we reveal that these methods conceal three inherent bottlenecks:\n(1) they struggle to encode temporal and channel-specific information in a\nlossless manner, both of which are critical components of multivariate time\nseries; (2) it is much difficult to align the learned representation space with\nthe semantic space of the LLMs; (3) they require task-specific retraining,\nwhich is both computationally expensive and labor-intensive. To bridge these\ngaps, we propose TableTime, which reformulates MTSC as a table understanding\ntask. Specifically, TableTime introduces the following strategies: (1) convert\nmultivariate time series into a tabular form, thus minimizing information loss\nto the greatest extent; (2) represent tabular time series in text format to\nachieve natural alignment with the semantic space of LLMs; (3) design a\nreasoning framework that integrates contextual text information, neighborhood\nassistance, multi-path inference and problem decomposition to enhance the\nreasoning ability of LLMs and realize zero-shot classification. Extensive\nexperiments performed on 10 publicly representative datasets from UEA archive\nverify the superiorities of the TableTime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated their effectiveness in\nmultivariate time series classification (MTSC). Effective adaptation of LLMs\nfor MTSC necessitates informative data representations. Existing LLM-based\nmethods directly encode embeddings for time series within the latent space of\nLLMs from scratch to align with semantic space of LLMs. Despite their\neffectiveness, we reveal that these methods conceal three inherent bottlenecks:\n(1) they struggle to encode temporal and channel-specific information in a\nlossless manner, both of which are critical components of multivariate time\nseries; (2) it is much difficult to align the learned representation space with\nthe semantic space of the LLMs; (3) they require task-specific retraining,\nwhich is both computationally expensive and labor-intensive. To bridge these\ngaps, we propose TableTime, which reformulates MTSC as a table understanding\ntask. Specifically, TableTime introduces the following strategies: (1) convert\nmultivariate time series into a tabular form, thus minimizing information loss\nto the greatest extent; (2) represent tabular time series in text format to\nachieve natural alignment with the semantic space of LLMs; (3) design a\nreasoning framework that integrates contextual text information, neighborhood\nassistance, multi-path inference and problem decomposition to enhance the\nreasoning ability of LLMs and realize zero-shot classification. Extensive\nexperiments performed on 10 publicly representative datasets from UEA archive\nverify the superiorities of the TableTime."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Qingyang Mao"
                    },
                    {
                        "name": "Yitong Zhou"
                    },
                    {
                        "name": "Daoyu Wang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Feiyang Xu"
                    },
                    {
                        "name": "Xin Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin Li"
                },
                "author": "Xin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15737v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15737v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24594v1",
                "updated": "2025-10-28T16:23:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    23,
                    27,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:23:27Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    23,
                    27,
                    1,
                    301,
                    0
                ],
                "title": "Detecting the Use of Generative AI in Crowdsourced Surveys: Implications\n  for Data Integrity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting the Use of Generative AI in Crowdsourced Surveys: Implications\n  for Data Integrity"
                },
                "summary": "The widespread adoption of generative AI (GenAI) has introduced new\nchallenges in crowdsourced data collection, particularly in survey-based\nresearch. While GenAI offers powerful capabilities, its unintended use in\ncrowdsourcing, such as generating automated survey responses, threatens the\nintegrity of empirical research and complicates efforts to understand public\nopinion and behavior. In this study, we investigate and evaluate two approaches\nfor detecting AI-generated responses in online surveys: LLM-based detection and\nsignature-based detection. We conducted experiments across seven survey\nstudies, comparing responses collected before 2022 with those collected after\nthe release of ChatGPT. Our findings reveal a significant increase in\nAI-generated responses in the post-2022 studies, highlighting how GenAI may\nsilently distort crowdsourced data. This work raises broader concerns about\nevolving landscape of data integrity, where GenAI can compromise data quality,\nmislead researchers, and influence downstream findings in fields such as\nhealth, politics, and social behavior. By surfacing detection strategies and\nempirical evidence of GenAI's impact, we aim to contribute to ongoing\nconversation about safeguarding research integrity and supporting scholars\nnavigating these methodological and ethical challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of generative AI (GenAI) has introduced new\nchallenges in crowdsourced data collection, particularly in survey-based\nresearch. While GenAI offers powerful capabilities, its unintended use in\ncrowdsourcing, such as generating automated survey responses, threatens the\nintegrity of empirical research and complicates efforts to understand public\nopinion and behavior. In this study, we investigate and evaluate two approaches\nfor detecting AI-generated responses in online surveys: LLM-based detection and\nsignature-based detection. We conducted experiments across seven survey\nstudies, comparing responses collected before 2022 with those collected after\nthe release of ChatGPT. Our findings reveal a significant increase in\nAI-generated responses in the post-2022 studies, highlighting how GenAI may\nsilently distort crowdsourced data. This work raises broader concerns about\nevolving landscape of data integrity, where GenAI can compromise data quality,\nmislead researchers, and influence downstream findings in fields such as\nhealth, politics, and social behavior. By surfacing detection strategies and\nempirical evidence of GenAI's impact, we aim to contribute to ongoing\nconversation about safeguarding research integrity and supporting scholars\nnavigating these methodological and ethical challenges."
                },
                "authors": [
                    {
                        "name": "Dapeng Zhang"
                    },
                    {
                        "name": "Marina Katoh"
                    },
                    {
                        "name": "Weiping Pei"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Pei"
                },
                "author": "Weiping Pei",
                "arxiv_comment": "Accepted by CSCW 2025 workshop Beyond Information: Online\n  Participatory Culture and Information Disorder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23458v2",
                "updated": "2025-10-28T16:23:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    23,
                    4,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-27T15:58:51Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    15,
                    58,
                    51,
                    0,
                    300,
                    0
                ],
                "title": "BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents"
                },
                "summary": "Confidence in LLMs is a useful indicator of model uncertainty and answer\nreliability. Existing work mainly focused on single-turn scenarios, while\nresearch on confidence in complex multi-turn interactions is limited. In this\npaper, we investigate whether LLM-based search agents have the ability to\ncommunicate their own confidence through verbalized confidence scores after\nlong sequences of actions, a significantly more challenging task compared to\noutputting confidence in a single interaction. Experimenting on open-source\nagentic models, we first find that models exhibit much higher task accuracy at\nhigh confidence while having near-zero accuracy when confidence is low. Based\non this observation, we propose Test-Time Scaling (TTS) methods that use\nconfidence scores to determine answer quality, encourage the model to try again\nuntil reaching a satisfactory confidence level. Results show that our proposed\nmethods significantly reduce token consumption while demonstrating competitive\nperformance compared to baseline fixed budget TTS methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence in LLMs is a useful indicator of model uncertainty and answer\nreliability. Existing work mainly focused on single-turn scenarios, while\nresearch on confidence in complex multi-turn interactions is limited. In this\npaper, we investigate whether LLM-based search agents have the ability to\ncommunicate their own confidence through verbalized confidence scores after\nlong sequences of actions, a significantly more challenging task compared to\noutputting confidence in a single interaction. Experimenting on open-source\nagentic models, we first find that models exhibit much higher task accuracy at\nhigh confidence while having near-zero accuracy when confidence is low. Based\non this observation, we propose Test-Time Scaling (TTS) methods that use\nconfidence scores to determine answer quality, encourage the model to try again\nuntil reaching a satisfactory confidence level. Results show that our proposed\nmethods significantly reduce token consumption while demonstrating competitive\nperformance compared to baseline fixed budget TTS methods."
                },
                "authors": [
                    {
                        "name": "Litu Ou"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Yong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jiang"
                },
                "author": "Yong Jiang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24593v1",
                "updated": "2025-10-28T16:22:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    22,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:22:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    22,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Brownian motion on spaces of discrete regular curves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brownian motion on spaces of discrete regular curves"
                },
                "summary": "We introduce and study Brownian motion on spaces of discrete regular curves\nin Euclidean space equipped with discrete Sobolev-type metrics. It has been\nestablished that these spaces of discrete regular curves are geodesically\ncomplete if and only if the Sobolev-type metric is of order two or higher. By\nrelying on a general result by Grigor'yan and controlling the volume growth of\ngeodesic balls, we show that all spaces of discrete regular curves that are\ngeodesically complete are also stochastically complete, that is, the associated\nBrownian motion exists for all times. This provides a rigorous footing for\nperforming data statistics, such as data inference and data imputation, on\nthese spaces. Our result is the first stochastic completeness result in shape\nanalysis that applies to the full shape space of interest. For illustrative\npurposes, we include simulations for sample paths of Brownian motion on spaces\nof discrete regular curves. For the space of triangles in the plane modulo\nrotation, translation and scaling, we further provide heuristics which suggest\nthat this space remains stochastically complete even for Sobolev-type metrics\nof order zero and one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study Brownian motion on spaces of discrete regular curves\nin Euclidean space equipped with discrete Sobolev-type metrics. It has been\nestablished that these spaces of discrete regular curves are geodesically\ncomplete if and only if the Sobolev-type metric is of order two or higher. By\nrelying on a general result by Grigor'yan and controlling the volume growth of\ngeodesic balls, we show that all spaces of discrete regular curves that are\ngeodesically complete are also stochastically complete, that is, the associated\nBrownian motion exists for all times. This provides a rigorous footing for\nperforming data statistics, such as data inference and data imputation, on\nthese spaces. Our result is the first stochastic completeness result in shape\nanalysis that applies to the full shape space of interest. For illustrative\npurposes, we include simulations for sample paths of Brownian motion on spaces\nof discrete regular curves. For the space of triangles in the plane modulo\nrotation, translation and scaling, we further provide heuristics which suggest\nthat this space remains stochastically complete even for Sobolev-type metrics\nof order zero and one."
                },
                "authors": [
                    {
                        "name": "Karen Habermann"
                    },
                    {
                        "name": "Emmanuel Hartman"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Hartman"
                },
                "author": "Emmanuel Hartman",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "58J65, 60J65, 62R30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24592v1",
                "updated": "2025-10-28T16:22:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    22,
                    54,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:22:54Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    22,
                    54,
                    1,
                    301,
                    0
                ],
                "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization"
                },
                "summary": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases."
                },
                "authors": [
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Jing Wu"
                    },
                    {
                        "name": "Xinjie Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ruihua Song"
                    },
                    {
                        "name": "Chengxi Li"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Ongoing Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19000v2",
                "updated": "2025-10-28T16:16:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    16,
                    0,
                    1,
                    301,
                    0
                ],
                "published": "2024-05-29T11:28:06Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    11,
                    28,
                    6,
                    2,
                    150,
                    0
                ],
                "title": "FedMAP: Personalised Federated Learning for Real Large-Scale Healthcare\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMAP: Personalised Federated Learning for Real Large-Scale Healthcare\n  Systems"
                },
                "summary": "Federated learning (FL) promises to enable collaborative machine learning\nacross healthcare sites whilst preserving data privacy. Practical deployment\nremains limited by statistical heterogeneity arising from differences in\npatient demographics, treatments, and outcomes, and infrastructure constraints.\nWe introduce FedMAP, a personalised FL (PFL) framework that addresses\nheterogeneity through local Maximum a Posteriori (MAP) estimation with Input\nConvex Neural Network priors. These priors represent global knowledge gathered\nfrom other sites that guides the model while adapting to local data, and we\nprovide a formal proof of convergence. Unlike many PFL methods that rely on\nfixed regularisation, FedMAP's prior adaptively learns patterns that capture\ncomplex inter-site relationships. We demonstrate improved performance compared\nto local training, FedAvg, and several PFL methods across three large-scale\nclinical datasets: 10-year cardiovascular risk prediction (CPRD, 387 general\npractitioner practices, 258,688 patients), iron deficiency detection (INTERVAL,\n4 donor centres, 31,949 blood donors), and mortality prediction (eICU, 150\nhospitals, 44,842 patients). FedMAP incorporates a three-tier design that\nenables participation across healthcare sites with varying infrastructure and\ntechnical capabilities, from full federated training to inference-only\ndeployment. Geographical analysis reveals substantial equity improvements, with\nunderperforming regions achieving up to 14.3% performance gains. This framework\nprovides the first practical pathway for large-scale healthcare FL deployment,\nwhich ensures clinical sites at all scales can benefit, equity is enhanced, and\nprivacy is retained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) promises to enable collaborative machine learning\nacross healthcare sites whilst preserving data privacy. Practical deployment\nremains limited by statistical heterogeneity arising from differences in\npatient demographics, treatments, and outcomes, and infrastructure constraints.\nWe introduce FedMAP, a personalised FL (PFL) framework that addresses\nheterogeneity through local Maximum a Posteriori (MAP) estimation with Input\nConvex Neural Network priors. These priors represent global knowledge gathered\nfrom other sites that guides the model while adapting to local data, and we\nprovide a formal proof of convergence. Unlike many PFL methods that rely on\nfixed regularisation, FedMAP's prior adaptively learns patterns that capture\ncomplex inter-site relationships. We demonstrate improved performance compared\nto local training, FedAvg, and several PFL methods across three large-scale\nclinical datasets: 10-year cardiovascular risk prediction (CPRD, 387 general\npractitioner practices, 258,688 patients), iron deficiency detection (INTERVAL,\n4 donor centres, 31,949 blood donors), and mortality prediction (eICU, 150\nhospitals, 44,842 patients). FedMAP incorporates a three-tier design that\nenables participation across healthcare sites with varying infrastructure and\ntechnical capabilities, from full federated training to inference-only\ndeployment. Geographical analysis reveals substantial equity improvements, with\nunderperforming regions achieving up to 14.3% performance gains. This framework\nprovides the first practical pathway for large-scale healthcare FL deployment,\nwhich ensures clinical sites at all scales can benefit, equity is enhanced, and\nprivacy is retained."
                },
                "authors": [
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Daniel Kreuter"
                    },
                    {
                        "name": "Carlos Esteve-Yagüe"
                    },
                    {
                        "name": "Sören Dittmer"
                    },
                    {
                        "name": "Javier Fernandez-Marques"
                    },
                    {
                        "name": "Samantha Ip"
                    },
                    {
                        "name": "BloodCounts! Consortium"
                    },
                    {
                        "name": "Norbert C. J. de Wit"
                    },
                    {
                        "name": "Angela Wood"
                    },
                    {
                        "name": "James HF Rudd"
                    },
                    {
                        "name": "Nicholas Lane"
                    },
                    {
                        "name": "Nicholas S Gleadall"
                    },
                    {
                        "name": "Carola-Bibiane Schönlieb"
                    },
                    {
                        "name": "Michael Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Michael Roberts"
                },
                "author": "Michael Roberts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21729v2",
                "updated": "2025-10-28T16:15:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    15,
                    47,
                    1,
                    301,
                    0
                ],
                "published": "2025-09-30T00:25:47Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    0,
                    25,
                    47,
                    1,
                    273,
                    0
                ],
                "title": "CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known\n  Document Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known\n  Document Corpora"
                },
                "summary": "Dense embedding models have become critical for modern information retrieval,\nparticularly in RAG pipelines, but their performance often degrades when\napplied to specialized corpora outside their pre-training distribution. To\naddress thi we introduce CustomIR, a framework for unsupervised adaptation of\npre-trained language embedding models to domain-specific corpora using\nsynthetically generated query-document pairs. CustomIR leverages large language\nmodels (LLMs) to create diverse queries grounded in a known target corpus,\npaired with LLM-verified hard negatives, eliminating the need for costly human\nannotation. Experiments on enterprise email and messaging datasets show that\nCustomIR consistently improves retrieval effectiveness with small models\ngaining up to 2.3 points in Recall@10. This performance increase allows these\nsmall models to rival the performance of much larger alternatives, allowing for\ncheaper RAG deployments. These results highlight that targeted synthetic\nfine-tuning offers a scalable and cost-efficient strategy for increasing\ndomain-specific performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense embedding models have become critical for modern information retrieval,\nparticularly in RAG pipelines, but their performance often degrades when\napplied to specialized corpora outside their pre-training distribution. To\naddress thi we introduce CustomIR, a framework for unsupervised adaptation of\npre-trained language embedding models to domain-specific corpora using\nsynthetically generated query-document pairs. CustomIR leverages large language\nmodels (LLMs) to create diverse queries grounded in a known target corpus,\npaired with LLM-verified hard negatives, eliminating the need for costly human\nannotation. Experiments on enterprise email and messaging datasets show that\nCustomIR consistently improves retrieval effectiveness with small models\ngaining up to 2.3 points in Recall@10. This performance increase allows these\nsmall models to rival the performance of much larger alternatives, allowing for\ncheaper RAG deployments. These results highlight that targeted synthetic\nfine-tuning offers a scalable and cost-efficient strategy for increasing\ndomain-specific performance."
                },
                "authors": [
                    {
                        "name": "Nathan Paull"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Paull"
                },
                "author": "Nathan Paull",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24582v1",
                "updated": "2025-10-28T16:15:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    15,
                    17,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:15:17Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    15,
                    17,
                    1,
                    301,
                    0
                ],
                "title": "Politically Speaking: LLMs on Changing International Affairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Politically Speaking: LLMs on Changing International Affairs"
                },
                "summary": "Ask your chatbot to impersonate an expert from Russia and an expert from US\nand query it on Chinese politics. How might the outputs differ? Or, to prepare\nourselves for the worse, how might they converge? Scholars have raised concerns\nLLM based applications can homogenize cultures and flatten perspectives. But\nexactly how much does LLM generated outputs converge despite explicit different\nrole assignment? This study provides empirical evidence to the above question.\nThe critique centres on pretrained models regurgitating ossified political\njargons used in the Western world when speaking about China, Iran, Russian, and\nUS politics, despite changes in these countries happening daily or hourly. The\nexperiments combine role-prompting and similarity metrics. The results show\nthat AI generated discourses from four models about Iran and China are the most\nhomogeneous and unchanging across all four models, including OpenAI GPT, Google\nGemini, Anthropic Claude, and DeepSeek, despite the prompted perspective change\nand the actual changes in real life. This study does not engage with history,\npolitics, or literature as traditional disciplinary approaches would; instead,\nit takes cues from international and area studies and offers insight on the\nfuture trajectory of shifting political discourse in a digital space\nincreasingly cannibalised by AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask your chatbot to impersonate an expert from Russia and an expert from US\nand query it on Chinese politics. How might the outputs differ? Or, to prepare\nourselves for the worse, how might they converge? Scholars have raised concerns\nLLM based applications can homogenize cultures and flatten perspectives. But\nexactly how much does LLM generated outputs converge despite explicit different\nrole assignment? This study provides empirical evidence to the above question.\nThe critique centres on pretrained models regurgitating ossified political\njargons used in the Western world when speaking about China, Iran, Russian, and\nUS politics, despite changes in these countries happening daily or hourly. The\nexperiments combine role-prompting and similarity metrics. The results show\nthat AI generated discourses from four models about Iran and China are the most\nhomogeneous and unchanging across all four models, including OpenAI GPT, Google\nGemini, Anthropic Claude, and DeepSeek, despite the prompted perspective change\nand the actual changes in real life. This study does not engage with history,\npolitics, or literature as traditional disciplinary approaches would; instead,\nit takes cues from international and area studies and offers insight on the\nfuture trajectory of shifting political discourse in a digital space\nincreasingly cannibalised by AI."
                },
                "authors": [
                    {
                        "name": "Xuenan Cao"
                    },
                    {
                        "name": "Wai Kei Chung"
                    },
                    {
                        "name": "Ye Zhao"
                    },
                    {
                        "name": "Lidia Mengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Lidia Mengyuan Zhou"
                },
                "author": "Lidia Mengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05177v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05177v3",
                "updated": "2025-10-28T16:02:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    2,
                    48,
                    1,
                    301,
                    0
                ],
                "published": "2025-02-07T18:59:56Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    59,
                    56,
                    4,
                    38,
                    0
                ],
                "title": "Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with\n  Leading Short-Context Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with\n  Leading Short-Context Accuracy"
                },
                "summary": "We introduce Long-VITA, a simple yet effective large multi-modal model for\nlong-context visual-language understanding tasks. It is adept at concurrently\nprocessing and analyzing modalities of image, video, and text over 4K frames or\n1M tokens while delivering advanced performances on short-context multi-modal\ntasks. We propose an effective multi-modal training schema that starts with\nlarge language models and proceeds through vision-language alignment, general\nknowledge learning, and two sequential stages of long-sequence fine-tuning. We\nfurther implement context-parallelism distributed inference and logits-masked\nlanguage modeling head to scale Long-VITA to infinitely long inputs of images\nand texts during model inference. Regarding training data, Long-VITA is built\non a mix of 17M samples from public datasets only and demonstrates\nstate-of-the-art performance on various multi-modal benchmarks, compared\nagainst recent cutting-edge models with internal data. Long-VITA is fully\nopen-source and reproducible.. By leveraging our inference designs, Long-VITA\nmodels achieve a remarkable 2x prefill speedup and 4x context length extension\nin a single node with 8 GPUs. We hope Long-VITA can serve as a competitive\nbaseline and offer valuable insights for the open-source community in advancing\nlong-context multi-modal understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Long-VITA, a simple yet effective large multi-modal model for\nlong-context visual-language understanding tasks. It is adept at concurrently\nprocessing and analyzing modalities of image, video, and text over 4K frames or\n1M tokens while delivering advanced performances on short-context multi-modal\ntasks. We propose an effective multi-modal training schema that starts with\nlarge language models and proceeds through vision-language alignment, general\nknowledge learning, and two sequential stages of long-sequence fine-tuning. We\nfurther implement context-parallelism distributed inference and logits-masked\nlanguage modeling head to scale Long-VITA to infinitely long inputs of images\nand texts during model inference. Regarding training data, Long-VITA is built\non a mix of 17M samples from public datasets only and demonstrates\nstate-of-the-art performance on various multi-modal benchmarks, compared\nagainst recent cutting-edge models with internal data. Long-VITA is fully\nopen-source and reproducible.. By leveraging our inference designs, Long-VITA\nmodels achieve a remarkable 2x prefill speedup and 4x context length extension\nin a single node with 8 GPUs. We hope Long-VITA can serve as a competitive\nbaseline and offer valuable insights for the open-source community in advancing\nlong-context multi-modal understanding."
                },
                "authors": [
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Shaoqi Dong"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Peixian Chen"
                    },
                    {
                        "name": "Mengdan Zhang"
                    },
                    {
                        "name": "Haoyu Cao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Shaohui Lin"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "https://github.com/VITA-MLLM/Long-VITA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05177v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05177v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14617v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14617v3",
                "updated": "2025-10-28T16:02:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    2,
                    10,
                    1,
                    301,
                    0
                ],
                "published": "2025-05-20T17:03:12Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    3,
                    12,
                    1,
                    140,
                    0
                ],
                "title": "The Hawthorne Effect in Reasoning Models: Evaluating and Steering Test\n  Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hawthorne Effect in Reasoning Models: Evaluating and Steering Test\n  Awareness"
                },
                "summary": "Reasoning-focused LLMs sometimes alter their behavior when they detect that\nthey are being evaluated, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its performance on\nsafety-related tasks. We introduce a white-box probing framework that (i)\nlinearly identifies awareness-related activations and (ii) steers models toward\nor away from test awareness while monitoring downstream performance. We apply\nour method to different state-of-the-art open-weight reasoning LLMs across both\nrealistic and hypothetical tasks (denoting tests or simulations). Our results\ndemonstrate that test awareness significantly impacts safety alignment (such as\ncompliance with harmful requests and conforming to stereotypes) with effects\nvarying in both magnitude and direction across models. By providing control\nover this latent effect, our work aims to provide a stress-test mechanism and\nincrease trust in how we perform safety evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-focused LLMs sometimes alter their behavior when they detect that\nthey are being evaluated, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its performance on\nsafety-related tasks. We introduce a white-box probing framework that (i)\nlinearly identifies awareness-related activations and (ii) steers models toward\nor away from test awareness while monitoring downstream performance. We apply\nour method to different state-of-the-art open-weight reasoning LLMs across both\nrealistic and hypothetical tasks (denoting tests or simulations). Our results\ndemonstrate that test awareness significantly impacts safety alignment (such as\ncompliance with harmful requests and conforming to stereotypes) with effects\nvarying in both magnitude and direction across models. By providing control\nover this latent effect, our work aims to provide a stress-test mechanism and\nincrease trust in how we perform safety evaluations."
                },
                "authors": [
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Ahmed Salem"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Salem"
                },
                "author": "Ahmed Salem",
                "arxiv_comment": "NeurIPS 2025 (Spotlight). Code is available at:\n  https://github.com/microsoft/Test_Awareness_Steering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14617v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14617v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05295v2",
                "updated": "2025-10-28T16:01:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    1,
                    40,
                    1,
                    301,
                    0
                ],
                "published": "2025-02-07T19:56:01Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    19,
                    56,
                    1,
                    4,
                    38,
                    0
                ],
                "title": "GST-UNet: A Neural Framework for Spatiotemporal Causal Inference with\n  Time-Varying Confounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GST-UNet: A Neural Framework for Spatiotemporal Causal Inference with\n  Time-Varying Confounding"
                },
                "summary": "Estimating causal effects from spatiotemporal observational data is essential\nin public health, environmental science, and policy evaluation, where\nrandomized experiments are often infeasible. Existing approaches, however,\neither rely on strong structural assumptions or fail to handle key challenges\nsuch as interference, spatial confounding, temporal carryover, and time-varying\nconfounding -- where covariates are influenced by past treatments and, in turn,\naffect future ones. We introduce GST-UNet (G-computation Spatio-Temporal UNet),\na theoretically grounded neural framework that combines a U-Net-based\nspatiotemporal encoder with regression-based iterative G-computation to\nestimate location-specific potential outcomes under complex intervention\nsequences. GST-UNet explicitly adjusts for time-varying confounders and\ncaptures non-linear spatial and temporal dependencies, enabling valid causal\ninference from a single observed trajectory in data-scarce settings. We\nvalidate its effectiveness in synthetic experiments and in a real-world\nanalysis of wildfire smoke exposure and respiratory hospitalizations during the\n2018 California Camp Fire. Together, these results position GST-UNet as a\nprincipled and ready-to-use framework for spatiotemporal causal inference,\nadvancing reliable estimation in policy-relevant and scientific domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating causal effects from spatiotemporal observational data is essential\nin public health, environmental science, and policy evaluation, where\nrandomized experiments are often infeasible. Existing approaches, however,\neither rely on strong structural assumptions or fail to handle key challenges\nsuch as interference, spatial confounding, temporal carryover, and time-varying\nconfounding -- where covariates are influenced by past treatments and, in turn,\naffect future ones. We introduce GST-UNet (G-computation Spatio-Temporal UNet),\na theoretically grounded neural framework that combines a U-Net-based\nspatiotemporal encoder with regression-based iterative G-computation to\nestimate location-specific potential outcomes under complex intervention\nsequences. GST-UNet explicitly adjusts for time-varying confounders and\ncaptures non-linear spatial and temporal dependencies, enabling valid causal\ninference from a single observed trajectory in data-scarce settings. We\nvalidate its effectiveness in synthetic experiments and in a real-world\nanalysis of wildfire smoke exposure and respiratory hospitalizations during the\n2018 California Camp Fire. Together, these results position GST-UNet as a\nprincipled and ready-to-use framework for spatiotemporal causal inference,\nadvancing reliable estimation in policy-relevant and scientific domains."
                },
                "authors": [
                    {
                        "name": "Miruna Oprescu"
                    },
                    {
                        "name": "David K. Park"
                    },
                    {
                        "name": "Xihaier Luo"
                    },
                    {
                        "name": "Shinjae Yoo"
                    },
                    {
                        "name": "Nathan Kallus"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Kallus"
                },
                "author": "Nathan Kallus",
                "arxiv_comment": "29 pages, 6 figures, 6 tables, NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24561v1",
                "updated": "2025-10-28T15:55:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    55,
                    36,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:55:36Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    55,
                    36,
                    1,
                    301,
                    0
                ],
                "title": "LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via\n  Asymptotic Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via\n  Asymptotic Analysis"
                },
                "summary": "With the widespread adoption of LLMs, LoRA has become a dominant method for\nPEFT, and its initialization methods have attracted increasing attention.\nHowever, existing methods have notable limitations: many methods do not\nincorporate target-domain data, while gradient-based methods exploit data only\nat a shallow level by relying on one-step gradient decomposition, which remains\nunsatisfactory due to the weak empirical performance of the one-step\nfine-tuning model that serves as their basis, as well as the fact that these\nmethods either lack a rigorous theoretical foundation or depend heavily on\nrestrictive isotropic assumptions. In this paper, we establish a theoretical\nframework for data-aware LoRA initialization based on asymptotic analysis.\nStarting from a general optimization objective that minimizes the expectation\nof the parameter discrepancy between the fine-tuned and target models, we\nderive an optimization problem with two components: a bias term, which is\nrelated to the parameter distance between the fine-tuned and target models, and\nis approximated using a Fisher-gradient formulation to preserve anisotropy; and\na variance term, which accounts for the uncertainty introduced by sampling\nstochasticity through the Fisher information. By solving this problem, we\nobtain an optimal initialization strategy for LoRA. Building on this\ntheoretical framework, we develop an efficient algorithm, LoRA-DA, which\nestimates the terms in the optimization problem from a small set of target\ndomain samples and obtains the optimal LoRA initialization. Empirical results\nacross multiple benchmarks demonstrate that LoRA-DA consistently improves final\naccuracy over existing initialization methods. Additional studies show faster,\nmore stable convergence, robustness across ranks, and only a small\ninitialization overhead for LoRA-DA. The source code will be released upon\npublication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of LLMs, LoRA has become a dominant method for\nPEFT, and its initialization methods have attracted increasing attention.\nHowever, existing methods have notable limitations: many methods do not\nincorporate target-domain data, while gradient-based methods exploit data only\nat a shallow level by relying on one-step gradient decomposition, which remains\nunsatisfactory due to the weak empirical performance of the one-step\nfine-tuning model that serves as their basis, as well as the fact that these\nmethods either lack a rigorous theoretical foundation or depend heavily on\nrestrictive isotropic assumptions. In this paper, we establish a theoretical\nframework for data-aware LoRA initialization based on asymptotic analysis.\nStarting from a general optimization objective that minimizes the expectation\nof the parameter discrepancy between the fine-tuned and target models, we\nderive an optimization problem with two components: a bias term, which is\nrelated to the parameter distance between the fine-tuned and target models, and\nis approximated using a Fisher-gradient formulation to preserve anisotropy; and\na variance term, which accounts for the uncertainty introduced by sampling\nstochasticity through the Fisher information. By solving this problem, we\nobtain an optimal initialization strategy for LoRA. Building on this\ntheoretical framework, we develop an efficient algorithm, LoRA-DA, which\nestimates the terms in the optimization problem from a small set of target\ndomain samples and obtains the optimal LoRA initialization. Empirical results\nacross multiple benchmarks demonstrate that LoRA-DA consistently improves final\naccuracy over existing initialization methods. Additional studies show faster,\nmore stable convergence, robustness across ranks, and only a small\ninitialization overhead for LoRA-DA. The source code will be released upon\npublication."
                },
                "authors": [
                    {
                        "name": "Qingyue Zhang"
                    },
                    {
                        "name": "Chang Chu"
                    },
                    {
                        "name": "Tianren Peng"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Xiangyang Luo"
                    },
                    {
                        "name": "Zhihao Jiang"
                    },
                    {
                        "name": "Shao-Lun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shao-Lun Huang"
                },
                "author": "Shao-Lun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22931v2",
                "updated": "2025-10-28T15:51:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    51,
                    13,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-27T02:15:51Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    2,
                    15,
                    51,
                    0,
                    300,
                    0
                ],
                "title": "Robust Uncertainty Quantification for Self-Evolving Large Language\n  Models via Continual Domain Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Uncertainty Quantification for Self-Evolving Large Language\n  Models via Continual Domain Pretraining"
                },
                "summary": "Continual Learning (CL) is essential for enabling self-evolving large\nlanguage models (LLMs) to adapt and remain effective amid rapid knowledge\ngrowth. Yet, despite its importance, little attention has been given to\nestablishing statistical reliability guarantees for LLMs under CL, particularly\nin the setting of continual domain pretraining (CDP). Conformal Prediction (CP)\nhas shown promise in offering correctness guarantees for LLMs, but it faces\nmajor challenges in CDP: testing data often stems from unknown or shifting\ndomain distributions, under which CP may no longer provide valid guarantees.\nMoreover, when high coverage is required, CP can yield excessively large\nprediction sets for unanswerable queries, reducing informativeness. To address\nthese challenges, we introduce an adaptive rejection and non-exchangeable CP\nframework. Our method first estimates the distribution of questions across\ndomains in the test set using transformer-based clustering, then reweights or\nresamples the calibration data accordingly. Building on this, adaptive\nrejection CP allows the LLM to selectively abstain from answering when its\nconfidence or competence shifts significantly. Extensive experiments\ndemonstrate that our framework enhances both the effectiveness and reliability\nof CP under CDP scenarios. Our code is available at:\nhttps://anonymous.4open.science/r/CPCL-8C12/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Learning (CL) is essential for enabling self-evolving large\nlanguage models (LLMs) to adapt and remain effective amid rapid knowledge\ngrowth. Yet, despite its importance, little attention has been given to\nestablishing statistical reliability guarantees for LLMs under CL, particularly\nin the setting of continual domain pretraining (CDP). Conformal Prediction (CP)\nhas shown promise in offering correctness guarantees for LLMs, but it faces\nmajor challenges in CDP: testing data often stems from unknown or shifting\ndomain distributions, under which CP may no longer provide valid guarantees.\nMoreover, when high coverage is required, CP can yield excessively large\nprediction sets for unanswerable queries, reducing informativeness. To address\nthese challenges, we introduce an adaptive rejection and non-exchangeable CP\nframework. Our method first estimates the distribution of questions across\ndomains in the test set using transformer-based clustering, then reweights or\nresamples the calibration data accordingly. Building on this, adaptive\nrejection CP allows the LLM to selectively abstain from answering when its\nconfidence or competence shifts significantly. Extensive experiments\ndemonstrate that our framework enhances both the effectiveness and reliability\nof CP under CDP scenarios. Our code is available at:\nhttps://anonymous.4open.science/r/CPCL-8C12/"
                },
                "authors": [
                    {
                        "name": "Xiaofan Zhou"
                    },
                    {
                        "name": "Lu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Cheng"
                },
                "author": "Lu Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24551v1",
                "updated": "2025-10-28T15:47:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    47,
                    44,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:47:44Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    47,
                    44,
                    1,
                    301,
                    0
                ],
                "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives"
                },
                "summary": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery."
                },
                "authors": [
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Changshuo Liu"
                    },
                    {
                        "name": "Gene Anne Ooi"
                    },
                    {
                        "name": "Marcus Tan"
                    },
                    {
                        "name": "Zhongle Xie"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "James Wei Luen Yip"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Beng Chin Ooi"
                    }
                ],
                "author_detail": {
                    "name": "Beng Chin Ooi"
                },
                "author": "Beng Chin Ooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24538v1",
                "updated": "2025-10-28T15:42:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    42,
                    3,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:42:03Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    42,
                    3,
                    1,
                    301,
                    0
                ],
                "title": "Dark & Stormy: Modeling Humor in the Worst Sentences Ever Written",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark & Stormy: Modeling Humor in the Worst Sentences Ever Written"
                },
                "summary": "Textual humor is enormously diverse and computational studies need to account\nfor this range, including intentionally bad humor. In this paper, we curate and\nanalyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to\nbetter understand \"bad\" humor in English. Standard humor detection models\nperform poorly on our corpus, and an analysis of literary devices finds that\nthese sentences combine features common in existing humor datasets (e.g., puns,\nirony) with metaphor, metafiction and simile. LLMs prompted to synthesize\ncontest-style sentences imitate the form but exaggerate the effect by\nover-using certain literary devices, and including far more novel\nadjective-noun bigrams than human writers. Data, code and analysis are\navailable at https://github.com/venkatasg/bulwer-lytton",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual humor is enormously diverse and computational studies need to account\nfor this range, including intentionally bad humor. In this paper, we curate and\nanalyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to\nbetter understand \"bad\" humor in English. Standard humor detection models\nperform poorly on our corpus, and an analysis of literary devices finds that\nthese sentences combine features common in existing humor datasets (e.g., puns,\nirony) with metaphor, metafiction and simile. LLMs prompted to synthesize\ncontest-style sentences imitate the form but exaggerate the effect by\nover-using certain literary devices, and including far more novel\nadjective-noun bigrams than human writers. Data, code and analysis are\navailable at https://github.com/venkatasg/bulwer-lytton"
                },
                "authors": [
                    {
                        "name": "Venkata S Govindarajan"
                    },
                    {
                        "name": "Laura Biester"
                    }
                ],
                "author_detail": {
                    "name": "Laura Biester"
                },
                "author": "Laura Biester",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24528v1",
                "updated": "2025-10-28T15:37:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    37,
                    51,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:37:51Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    37,
                    51,
                    1,
                    301,
                    0
                ],
                "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based\n  Pseudo-Labeling Framework for In-context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cross-Task Examples to In-Task Prompts: A Graph-Based\n  Pseudo-Labeling Framework for In-context Learning"
                },
                "summary": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs."
                },
                "authors": [
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Xingbo Fu"
                    },
                    {
                        "name": "Chengshuai Shi"
                    },
                    {
                        "name": "Zhenyu Lei"
                    },
                    {
                        "name": "Cong Shen"
                    },
                    {
                        "name": "Jundong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jundong Li"
                },
                "author": "Jundong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24526v1",
                "updated": "2025-10-28T15:36:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    36,
                    3,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:36:03Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    36,
                    3,
                    1,
                    301,
                    0
                ],
                "title": "Bayesian nonparametric modeling of multivariate count data with an\n  unknown number of traits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian nonparametric modeling of multivariate count data with an\n  unknown number of traits"
                },
                "summary": "Feature and trait allocation models are fundamental objects in Bayesian\nnonparametrics and play a prominent role in several applications. Existing\napproaches, however, typically assume full exchangeability of the data, which\nmay be restrictive in settings characterized by heterogeneous but related\ngroups. In this paper, we introduce a general and tractable class of Bayesian\nnonparametric priors for partially exchangeable trait allocation models,\nrelying on completely random vectors. We provide a comprehensive theoretical\nanalysis, including closed-form expressions for marginal and posterior\ndistributions, and illustrate the tractability of our framework in the cases of\nbinary and Poisson-distributed traits. A distinctive aspect of our approach is\nthat the number of traits is a random quantity, thereby allowing us to model\nand estimate unobserved traits. Building on these results, we also develop a\nnovel mixture model that infers the group partition structure from the data,\neffectively clustering trait allocations. This extension generalizes Bayesian\nnonparametric latent class models and avoids the systematic overclustering that\narises when the number of traits is assumed to be fixed. We demonstrate the\npractical usefulness of our methodology through an application to the\n`Ndrangheta criminal network from the Operazione Infinito investigation, where\nour model provides insights into the organization of illicit activities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature and trait allocation models are fundamental objects in Bayesian\nnonparametrics and play a prominent role in several applications. Existing\napproaches, however, typically assume full exchangeability of the data, which\nmay be restrictive in settings characterized by heterogeneous but related\ngroups. In this paper, we introduce a general and tractable class of Bayesian\nnonparametric priors for partially exchangeable trait allocation models,\nrelying on completely random vectors. We provide a comprehensive theoretical\nanalysis, including closed-form expressions for marginal and posterior\ndistributions, and illustrate the tractability of our framework in the cases of\nbinary and Poisson-distributed traits. A distinctive aspect of our approach is\nthat the number of traits is a random quantity, thereby allowing us to model\nand estimate unobserved traits. Building on these results, we also develop a\nnovel mixture model that infers the group partition structure from the data,\neffectively clustering trait allocations. This extension generalizes Bayesian\nnonparametric latent class models and avoids the systematic overclustering that\narises when the number of traits is assumed to be fixed. We demonstrate the\npractical usefulness of our methodology through an application to the\n`Ndrangheta criminal network from the Operazione Infinito investigation, where\nour model provides insights into the organization of illicit activities."
                },
                "authors": [
                    {
                        "name": "Lorenzo Ghilotti"
                    },
                    {
                        "name": "Federico Camerlenghi"
                    },
                    {
                        "name": "Tommaso Rigon"
                    },
                    {
                        "name": "Michele Guindani"
                    }
                ],
                "author_detail": {
                    "name": "Michele Guindani"
                },
                "author": "Michele Guindani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15545v2",
                "updated": "2025-10-28T15:23:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    23,
                    35,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-17T11:25:36Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    11,
                    25,
                    36,
                    4,
                    290,
                    0
                ],
                "title": "TokenTiming: A Dynamic Alignment Method for Universal Speculative\n  Decoding Model Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenTiming: A Dynamic Alignment Method for Universal Speculative\n  Decoding Model Pairs"
                },
                "summary": "Accelerating the inference of large language models (LLMs) has been a\ncritical challenge in generative AI. Speculative decoding (SD) substantially\nimproves LLM inference efficiency. However, its utility is limited by a\nfundamental constraint: the draft and target models must share the same\nvocabulary, thus limiting the herd of available draft models and often\nnecessitating the training of a new model from scratch. Inspired by Dynamic\nTime Warping (DTW), a classic algorithm for aligning time series, we propose\nthe algorithm TokenTiming for universal speculative decoding. It operates by\nre-encoding the draft token sequence to get a new target token sequence, and\nthen uses DTW to build a mapping to transfer the probability distributions for\nspeculative sampling. Benefiting from this, our method accommodates mismatched\nvocabularies and works with any off-the-shelf models without retraining and\nmodification. We conduct comprehensive experiments on various tasks,\ndemonstrating 1.57x speedup. This work enables a universal approach for draft\nmodel selection, making SD a more versatile and practical tool for LLM\nacceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating the inference of large language models (LLMs) has been a\ncritical challenge in generative AI. Speculative decoding (SD) substantially\nimproves LLM inference efficiency. However, its utility is limited by a\nfundamental constraint: the draft and target models must share the same\nvocabulary, thus limiting the herd of available draft models and often\nnecessitating the training of a new model from scratch. Inspired by Dynamic\nTime Warping (DTW), a classic algorithm for aligning time series, we propose\nthe algorithm TokenTiming for universal speculative decoding. It operates by\nre-encoding the draft token sequence to get a new target token sequence, and\nthen uses DTW to build a mapping to transfer the probability distributions for\nspeculative sampling. Benefiting from this, our method accommodates mismatched\nvocabularies and works with any off-the-shelf models without retraining and\nmodification. We conduct comprehensive experiments on various tasks,\ndemonstrating 1.57x speedup. This work enables a universal approach for draft\nmodel selection, making SD a more versatile and practical tool for LLM\nacceleration."
                },
                "authors": [
                    {
                        "name": "Sibo Xiao"
                    },
                    {
                        "name": "Jinyuan Fu"
                    },
                    {
                        "name": "Zhongle Xie"
                    },
                    {
                        "name": "Lidan Shou"
                    }
                ],
                "author_detail": {
                    "name": "Lidan Shou"
                },
                "author": "Lidan Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00799v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00799v3",
                "updated": "2025-10-28T15:20:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    20,
                    47,
                    1,
                    301,
                    0
                ],
                "published": "2025-06-01T03:00:09Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    3,
                    0,
                    9,
                    6,
                    152,
                    0
                ],
                "title": "Uni-LoRA: One Vector is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-LoRA: One Vector is All You Need"
                },
                "summary": "Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient\nfine-tuning (PEFT) method for large language models (LLMs) by constraining\nweight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and\nVB-LoRA push efficiency further by introducing additional constraints to reduce\nthe trainable parameter space. In this paper, we show that the parameter space\nreduction strategies employed by these LoRA variants can be formulated within a\nunified framework, Uni-LoRA, where the LoRA parameter space, flattened as a\nhigh-dimensional vector space $R^D$, can be reconstructed through a projection\nfrom a subspace R^d, with $d \\ll D$. We demonstrate that the fundamental\ndifference among various LoRA methods lies in the choice of the projection\nmatrix, $P \\in R^{D \\times d}$.Most existing LoRA variants rely on layer-wise\nor structure-specific projections that limit cross-layer parameter sharing,\nthereby compromising parameter efficiency. In light of this, we introduce an\nefficient and theoretically grounded projection matrix that is isometric,\nenabling global parameter sharing and reducing computation overhead.\nFurthermore, under the unified view of Uni-LoRA, this design requires only a\nsingle trainable vector to reconstruct LoRA parameters for the entire LLM -\nmaking Uni-LoRA both a unified framework and a \"one-vector-only\" solution.\nExtensive experiments on GLUE, mathematical reasoning, and instruction tuning\nbenchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter\nefficiency while outperforming or matching prior approaches in predictive\nperformance. Our code is available at\nhttps://github.com/KaiyangLi1992/Uni-LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient\nfine-tuning (PEFT) method for large language models (LLMs) by constraining\nweight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and\nVB-LoRA push efficiency further by introducing additional constraints to reduce\nthe trainable parameter space. In this paper, we show that the parameter space\nreduction strategies employed by these LoRA variants can be formulated within a\nunified framework, Uni-LoRA, where the LoRA parameter space, flattened as a\nhigh-dimensional vector space $R^D$, can be reconstructed through a projection\nfrom a subspace R^d, with $d \\ll D$. We demonstrate that the fundamental\ndifference among various LoRA methods lies in the choice of the projection\nmatrix, $P \\in R^{D \\times d}$.Most existing LoRA variants rely on layer-wise\nor structure-specific projections that limit cross-layer parameter sharing,\nthereby compromising parameter efficiency. In light of this, we introduce an\nefficient and theoretically grounded projection matrix that is isometric,\nenabling global parameter sharing and reducing computation overhead.\nFurthermore, under the unified view of Uni-LoRA, this design requires only a\nsingle trainable vector to reconstruct LoRA parameters for the entire LLM -\nmaking Uni-LoRA both a unified framework and a \"one-vector-only\" solution.\nExtensive experiments on GLUE, mathematical reasoning, and instruction tuning\nbenchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter\nefficiency while outperforming or matching prior approaches in predictive\nperformance. Our code is available at\nhttps://github.com/KaiyangLi1992/Uni-LoRA."
                },
                "authors": [
                    {
                        "name": "Kaiyang Li"
                    },
                    {
                        "name": "Shaobo Han"
                    },
                    {
                        "name": "Qing Su"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Zhipeng Cai"
                    },
                    {
                        "name": "Shihao Ji"
                    }
                ],
                "author_detail": {
                    "name": "Shihao Ji"
                },
                "author": "Shihao Ji",
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00799v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00799v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24509v1",
                "updated": "2025-10-28T15:20:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    20,
                    38,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:20:38Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    20,
                    38,
                    1,
                    301,
                    0
                ],
                "title": "Quantum Combinatorial Reasoning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Combinatorial Reasoning for Large Language Models"
                },
                "summary": "We design and implement a quantum combinatorial reasoning framework for large\nlanguage models (QCR-LLM), integrating a real quantum computer in the hybrid\nworkflow. QCR-LLM reformulates reasoning aggregation as a higher-order\nunconstrained binary optimization (HUBO) problem. In this sense, reasoning\nfragments are represented as binary variables and their interactions encode\nstatistical relevance, logical coherence, and semantic redundancy. We tackle\nthe resulting high-order optimization problem both classically, via simulated\nannealing, and quantumly through the bias-field digitized counterdiabatic\nquantum optimizer (BF-DCQO) executed on IBM's superconducting digital quantum\nprocessors. Experiments on BIG-Bench Extra Hard (BBEH) benchmarks demonstrate\nthat our QCR-LLM consistently improves reasoning accuracy across multiple LLM\nbackbones, surpassing reasoning-native systems such as o3-high and DeepSeek R1\nby up to $+9\\,$pp. Despite requiring multiple reasoning samples per query, our\nQCR-LLM remains approximately five times more energy-efficient than o3-high,\nowing to the low per-token energy footprint of its GPT-4o backbone. These\nresults constitute the first experimental evidence of quantum-assisted\nreasoning, showing that hybrid quantum-classical optimization can efficiently\nenhance reasoning coherence, interpretability, and sustainability in\nlarge-scale language models. We have opened the doors to the emergence of\nquantum intelligence, where harder prompts require quantum optimizers at\nquantum-advantage level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We design and implement a quantum combinatorial reasoning framework for large\nlanguage models (QCR-LLM), integrating a real quantum computer in the hybrid\nworkflow. QCR-LLM reformulates reasoning aggregation as a higher-order\nunconstrained binary optimization (HUBO) problem. In this sense, reasoning\nfragments are represented as binary variables and their interactions encode\nstatistical relevance, logical coherence, and semantic redundancy. We tackle\nthe resulting high-order optimization problem both classically, via simulated\nannealing, and quantumly through the bias-field digitized counterdiabatic\nquantum optimizer (BF-DCQO) executed on IBM's superconducting digital quantum\nprocessors. Experiments on BIG-Bench Extra Hard (BBEH) benchmarks demonstrate\nthat our QCR-LLM consistently improves reasoning accuracy across multiple LLM\nbackbones, surpassing reasoning-native systems such as o3-high and DeepSeek R1\nby up to $+9\\,$pp. Despite requiring multiple reasoning samples per query, our\nQCR-LLM remains approximately five times more energy-efficient than o3-high,\nowing to the low per-token energy footprint of its GPT-4o backbone. These\nresults constitute the first experimental evidence of quantum-assisted\nreasoning, showing that hybrid quantum-classical optimization can efficiently\nenhance reasoning coherence, interpretability, and sustainability in\nlarge-scale language models. We have opened the doors to the emergence of\nquantum intelligence, where harder prompts require quantum optimizers at\nquantum-advantage level."
                },
                "authors": [
                    {
                        "name": "Carlos Flores-Garrigos"
                    },
                    {
                        "name": "Gaurav Dev"
                    },
                    {
                        "name": "Michael Falkenthal"
                    },
                    {
                        "name": "Alejandro Gomez Cadavid"
                    },
                    {
                        "name": "Anton Simen"
                    },
                    {
                        "name": "Shubham Kumar"
                    },
                    {
                        "name": "Enrique Solano"
                    },
                    {
                        "name": "Narendra N. Hegade"
                    }
                ],
                "author_detail": {
                    "name": "Narendra N. Hegade"
                },
                "author": "Narendra N. Hegade",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24505v1",
                "updated": "2025-10-28T15:16:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    16,
                    6,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:16:06Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    16,
                    6,
                    1,
                    301,
                    0
                ],
                "title": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?"
                },
                "summary": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability."
                },
                "authors": [
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Baixuan Xu"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24498v1",
                "updated": "2025-10-28T15:13:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    13,
                    32,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:13:32Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    13,
                    32,
                    1,
                    301,
                    0
                ],
                "title": "Design and Optimization of Cloud Native Homomorphic Encryption Workflows\n  for Privacy-Preserving ML Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Optimization of Cloud Native Homomorphic Encryption Workflows\n  for Privacy-Preserving ML Inference"
                },
                "summary": "As machine learning (ML) models become increasingly deployed through cloud\ninfrastructures, the confidentiality of user data during inference poses a\nsignificant security challenge. Homomorphic Encryption (HE) has emerged as a\ncompelling cryptographic technique that enables computation on encrypted data,\nallowing predictions to be generated without decrypting sensitive inputs.\nHowever, the integration of HE within large scale cloud native pipelines\nremains constrained by high computational overhead, orchestration complexity,\nand model compatibility issues.\n  This paper presents a systematic framework for the design and optimization of\ncloud native homomorphic encryption workflows that support privacy-preserving\nML inference. The proposed architecture integrates containerized HE modules\nwith Kubernetes-based orchestration, enabling elastic scaling and parallel\nencrypted computation across distributed environments. Furthermore,\noptimization strategies including ciphertext packing, polynomial modulus\nadjustment, and operator fusion are employed to minimize latency and resource\nconsumption while preserving cryptographic integrity. Experimental results\ndemonstrate that the proposed system achieves up to 3.2times inference\nacceleration and 40% reduction in memory utilization compared to conventional\nHE pipelines. These findings illustrate a practical pathway for deploying\nsecure ML-as-a-Service (MLaaS) systems that guarantee data confidentiality\nunder zero-trust cloud conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning (ML) models become increasingly deployed through cloud\ninfrastructures, the confidentiality of user data during inference poses a\nsignificant security challenge. Homomorphic Encryption (HE) has emerged as a\ncompelling cryptographic technique that enables computation on encrypted data,\nallowing predictions to be generated without decrypting sensitive inputs.\nHowever, the integration of HE within large scale cloud native pipelines\nremains constrained by high computational overhead, orchestration complexity,\nand model compatibility issues.\n  This paper presents a systematic framework for the design and optimization of\ncloud native homomorphic encryption workflows that support privacy-preserving\nML inference. The proposed architecture integrates containerized HE modules\nwith Kubernetes-based orchestration, enabling elastic scaling and parallel\nencrypted computation across distributed environments. Furthermore,\noptimization strategies including ciphertext packing, polynomial modulus\nadjustment, and operator fusion are employed to minimize latency and resource\nconsumption while preserving cryptographic integrity. Experimental results\ndemonstrate that the proposed system achieves up to 3.2times inference\nacceleration and 40% reduction in memory utilization compared to conventional\nHE pipelines. These findings illustrate a practical pathway for deploying\nsecure ML-as-a-Service (MLaaS) systems that guarantee data confidentiality\nunder zero-trust cloud conditions."
                },
                "authors": [
                    {
                        "name": "Tejaswini Bollikonda"
                    }
                ],
                "author_detail": {
                    "name": "Tejaswini Bollikonda"
                },
                "author": "Tejaswini Bollikonda",
                "arxiv_comment": "6 pages 2 figures, 2 tABLES",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24496v1",
                "updated": "2025-10-28T15:12:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    12,
                    15,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:12:15Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    12,
                    15,
                    1,
                    301,
                    0
                ],
                "title": "Panel data models with randomly generated groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panel data models with randomly generated groups"
                },
                "summary": "We develop a structural framework for modeling and inferring unobserved\nheterogeneity in dynamic panel-data models. Unlike methods treating clustering\nas a descriptive device, we model heterogeneity as arising from a latent\nclustering mechanism, where the number of clusters is unknown and estimated.\nBuilding on the mixture of finite mixtures (MFM) approach, our method avoids\nthe clustering inconsistency issues of Dirichlet process mixtures and provides\nan interpretable representation of the population clustering structure. We\nextend the Telescoping Sampler of Fruhwirth-Schnatter et al. (2021) to dynamic\npanels with covariates, yielding an efficient MCMC algorithm that delivers full\nBayesian inference and credible sets. We show that asymptotically the posterior\ndistribution of the mixing measure contracts around the truth at parametric\nrates in Wasserstein distance, ensuring recovery of clustering and structural\nparameters. Simulations demonstrate strong finite-sample performance. Finally,\nan application to the income-democracy relationship reveals latent\nheterogeneity only when controlling for additional covariates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a structural framework for modeling and inferring unobserved\nheterogeneity in dynamic panel-data models. Unlike methods treating clustering\nas a descriptive device, we model heterogeneity as arising from a latent\nclustering mechanism, where the number of clusters is unknown and estimated.\nBuilding on the mixture of finite mixtures (MFM) approach, our method avoids\nthe clustering inconsistency issues of Dirichlet process mixtures and provides\nan interpretable representation of the population clustering structure. We\nextend the Telescoping Sampler of Fruhwirth-Schnatter et al. (2021) to dynamic\npanels with covariates, yielding an efficient MCMC algorithm that delivers full\nBayesian inference and credible sets. We show that asymptotically the posterior\ndistribution of the mixing measure contracts around the truth at parametric\nrates in Wasserstein distance, ensuring recovery of clustering and structural\nparameters. Simulations demonstrate strong finite-sample performance. Finally,\nan application to the income-democracy relationship reveals latent\nheterogeneity only when controlling for additional covariates."
                },
                "authors": [
                    {
                        "name": "Jean-Pierre Florens"
                    },
                    {
                        "name": "Anna Simoni"
                    }
                ],
                "author_detail": {
                    "name": "Anna Simoni"
                },
                "author": "Anna Simoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10978v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10978v3",
                "updated": "2025-10-28T15:11:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    11,
                    36,
                    1,
                    301,
                    0
                ],
                "published": "2025-05-16T08:26:59Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    8,
                    26,
                    59,
                    4,
                    136,
                    0
                ],
                "title": "Group-in-Group Policy Optimization for LLM Agent Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group-in-Group Policy Optimization for LLM Agent Training"
                },
                "summary": "Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to multi-turn LLM agent training remains\nlimited. Unlike static tasks, agent-environment interactions unfold over many\nsteps and often yield sparse or delayed rewards, making credit assignment\nacross individual steps significantly more challenging. In this work, we\npropose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that\nachieves fine-grained credit assignment for LLM agents while preserving the\nappealing properties of group-based RL: critic-free, low memory, and stable\nconvergence. GiGPO introduces a two-level structure for estimating relative\nadvantage: (i) At the episode-level, GiGPO computes macro relative advantages\nbased on groups of complete trajectories; (ii) At the step-level, GiGPO\nintroduces an anchor state grouping mechanism that retroactively constructs\nstep-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop,\nas well as tool-integrated reasoning on search-augmented QA tasks, using\nQwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step\ncredit signals, achieves performance gains of > 12% on ALFWorld and > 9% on\nWebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B\nand 47.2% on 7B): all while maintaining the same GPU memory overhead, identical\nLLM rollout, and incurring little to no additional time cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to multi-turn LLM agent training remains\nlimited. Unlike static tasks, agent-environment interactions unfold over many\nsteps and often yield sparse or delayed rewards, making credit assignment\nacross individual steps significantly more challenging. In this work, we\npropose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that\nachieves fine-grained credit assignment for LLM agents while preserving the\nappealing properties of group-based RL: critic-free, low memory, and stable\nconvergence. GiGPO introduces a two-level structure for estimating relative\nadvantage: (i) At the episode-level, GiGPO computes macro relative advantages\nbased on groups of complete trajectories; (ii) At the step-level, GiGPO\nintroduces an anchor state grouping mechanism that retroactively constructs\nstep-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop,\nas well as tool-integrated reasoning on search-augmented QA tasks, using\nQwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step\ncredit signals, achieves performance gains of > 12% on ALFWorld and > 9% on\nWebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B\nand 47.2% on 7B): all while maintaining the same GPU memory overhead, identical\nLLM rollout, and incurring little to no additional time cost."
                },
                "authors": [
                    {
                        "name": "Lang Feng"
                    },
                    {
                        "name": "Zhenghai Xue"
                    },
                    {
                        "name": "Tingcong Liu"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10978v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10978v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24488v1",
                "updated": "2025-10-28T15:03:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    3,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:03:18Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    3,
                    18,
                    1,
                    301,
                    0
                ],
                "title": "A word association network methodology for evaluating implicit biases in\n  LLMs compared to humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A word association network methodology for evaluating implicit biases in\n  LLMs compared to humans"
                },
                "summary": "As Large language models (LLMs) become increasingly integrated into our\nlives, their inherent social biases remain a pressing concern. Detecting and\nevaluating these biases can be challenging because they are often implicit\nrather than explicit in nature, so developing evaluation methods that assess\nthe implicit knowledge representations of LLMs is essential. We present a novel\nword association network methodology for evaluating implicit biases in LLMs\nbased on simulating semantic priming within LLM-generated word association\nnetworks. Our prompt-based approach taps into the implicit relational\nstructures encoded in LLMs, providing both quantitative and qualitative\nassessments of bias. Unlike most prompt-based evaluation methods, our method\nenables direct comparisons between various LLMs and humans, providing a\nvaluable point of reference and offering new insights into the alignment of\nLLMs with human cognition. To demonstrate the utility of our methodology, we\napply it to both humans and several widely used LLMs to investigate social\nbiases related to gender, religion, ethnicity, sexual orientation, and\npolitical party. Our results reveal both convergences and divergences between\nLLM and human biases, providing new perspectives on the potential risks of\nusing LLMs. Our methodology contributes to a systematic, scalable, and\ngeneralizable framework for evaluating and comparing biases across multiple\nLLMs and humans, advancing the goal of transparent and socially responsible\nlanguage technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large language models (LLMs) become increasingly integrated into our\nlives, their inherent social biases remain a pressing concern. Detecting and\nevaluating these biases can be challenging because they are often implicit\nrather than explicit in nature, so developing evaluation methods that assess\nthe implicit knowledge representations of LLMs is essential. We present a novel\nword association network methodology for evaluating implicit biases in LLMs\nbased on simulating semantic priming within LLM-generated word association\nnetworks. Our prompt-based approach taps into the implicit relational\nstructures encoded in LLMs, providing both quantitative and qualitative\nassessments of bias. Unlike most prompt-based evaluation methods, our method\nenables direct comparisons between various LLMs and humans, providing a\nvaluable point of reference and offering new insights into the alignment of\nLLMs with human cognition. To demonstrate the utility of our methodology, we\napply it to both humans and several widely used LLMs to investigate social\nbiases related to gender, religion, ethnicity, sexual orientation, and\npolitical party. Our results reveal both convergences and divergences between\nLLM and human biases, providing new perspectives on the potential risks of\nusing LLMs. Our methodology contributes to a systematic, scalable, and\ngeneralizable framework for evaluating and comparing biases across multiple\nLLMs and humans, advancing the goal of transparent and socially responsible\nlanguage technologies."
                },
                "authors": [
                    {
                        "name": "Katherine Abramski"
                    },
                    {
                        "name": "Giulio Rossetti"
                    },
                    {
                        "name": "Massimo Stella"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Stella"
                },
                "author": "Massimo Stella",
                "arxiv_comment": "24 pages, 13 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24476v1",
                "updated": "2025-10-28T14:48:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    48,
                    57,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:48:57Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    48,
                    57,
                    1,
                    301,
                    0
                ],
                "title": "Mitigating Hallucination in Large Language Models (LLMs): An\n  Application-Oriented Survey on RAG, Reasoning, and Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Hallucination in Large Language Models (LLMs): An\n  Application-Oriented Survey on RAG, Reasoning, and Agentic Systems"
                },
                "summary": "Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks."
                },
                "authors": [
                    {
                        "name": "Yihan Li"
                    },
                    {
                        "name": "Xiyuan Fu"
                    },
                    {
                        "name": "Ghanshyam Verma"
                    },
                    {
                        "name": "Paul Buitelaar"
                    },
                    {
                        "name": "Mingming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Liu"
                },
                "author": "Mingming Liu",
                "arxiv_comment": "25 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06387v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06387v4",
                "updated": "2025-10-28T14:46:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    46,
                    58,
                    1,
                    301,
                    0
                ],
                "published": "2024-07-08T20:56:17Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    20,
                    56,
                    17,
                    0,
                    190,
                    0
                ],
                "title": "Conditional Rank-Rank Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Rank-Rank Regression"
                },
                "summary": "Rank-rank regression is commonly employed in economic research as a way of\ncapturing the relationship between two economic variables. The slope of this\nregression is the Spearman rank correlation, a classical measure of\nassociation. However, in many applications it is common practice to include\ncovariates to account for differences in association levels between groups as\ndefined by the values of these covariates. This is either done by including the\ncovariates or by modeling the residuals obtained after partialing out the\nimpact of the covariates. In each of these instances the resulting rank-rank\nregression coefficients can be difficult to interpret. We propose the\nconditional rank-rank regression, which uses conditional ranks instead of\nunconditional ranks, to measure average within-group persistence. The\ncoefficient of this new regression corresponds to the average Spearman rank\ncorrelation conditional on the covariates, a natural summary measure of\nwithin-group association. We develop a flexible estimation approach using\ndistribution regression and establish a theoretical framework for large sample\ninference. An empirical study on intergenerational income mobility in\nSwitzerland demonstrates the advantages of this approach. The study reveals\nstronger intergenerational persistence between fathers and sons compared to\nfathers and daughters, with the within-group persistence explaining 62% of the\noverall income persistence for sons and 52% for daughters. Smaller families and\nthose with highly educated fathers exhibit greater persistence in economic\nstatus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rank-rank regression is commonly employed in economic research as a way of\ncapturing the relationship between two economic variables. The slope of this\nregression is the Spearman rank correlation, a classical measure of\nassociation. However, in many applications it is common practice to include\ncovariates to account for differences in association levels between groups as\ndefined by the values of these covariates. This is either done by including the\ncovariates or by modeling the residuals obtained after partialing out the\nimpact of the covariates. In each of these instances the resulting rank-rank\nregression coefficients can be difficult to interpret. We propose the\nconditional rank-rank regression, which uses conditional ranks instead of\nunconditional ranks, to measure average within-group persistence. The\ncoefficient of this new regression corresponds to the average Spearman rank\ncorrelation conditional on the covariates, a natural summary measure of\nwithin-group association. We develop a flexible estimation approach using\ndistribution regression and establish a theoretical framework for large sample\ninference. An empirical study on intergenerational income mobility in\nSwitzerland demonstrates the advantages of this approach. The study reveals\nstronger intergenerational persistence between fathers and sons compared to\nfathers and daughters, with the within-group persistence explaining 62% of the\noverall income persistence for sons and 52% for daughters. Smaller families and\nthose with highly educated fathers exhibit greater persistence in economic\nstatus."
                },
                "authors": [
                    {
                        "name": "Victor Chernozhukov"
                    },
                    {
                        "name": "Iván Fernández-Val"
                    },
                    {
                        "name": "Jonas Meier"
                    },
                    {
                        "name": "Aico van Vuuren"
                    },
                    {
                        "name": "Francis Vella"
                    }
                ],
                "author_detail": {
                    "name": "Francis Vella"
                },
                "author": "Francis Vella",
                "arxiv_comment": "46 pages, 4 figures, 8 tables; main changes in introduction and\n  section 2 (former section 3)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06387v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06387v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24474v1",
                "updated": "2025-10-28T14:43:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    43,
                    48,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:43:48Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    43,
                    48,
                    1,
                    301,
                    0
                ],
                "title": "Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated\n  Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated\n  Sampling"
                },
                "summary": "Denoising generative models, such as diffusion and flow-based models, produce\nhigh-quality samples but require many denoising steps due to discretization\nerror. Flow maps, which estimate the average velocity between timesteps,\nmitigate this error and enable faster sampling. However, their training\ntypically demands architectural changes that limit compatibility with\npretrained flow models. We introduce Decoupled MeanFlow, a simple decoding\nstrategy that converts flow models into flow map models without architectural\nmodifications. Our method conditions the final blocks of diffusion transformers\non the subsequent timestep, allowing pretrained flow models to be directly\nrepurposed as flow maps. Combined with enhanced training techniques, this\ndesign enables high-quality generation in as few as 1 to 4 steps. Notably, we\nfind that training flow models and subsequently converting them is more\nefficient and effective than training flow maps from scratch. On ImageNet\n256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,\nrespectively, surpassing prior art by a large margin. Furthermore, we achieve\nFID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the\nperformance of flow models while delivering over 100x faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising generative models, such as diffusion and flow-based models, produce\nhigh-quality samples but require many denoising steps due to discretization\nerror. Flow maps, which estimate the average velocity between timesteps,\nmitigate this error and enable faster sampling. However, their training\ntypically demands architectural changes that limit compatibility with\npretrained flow models. We introduce Decoupled MeanFlow, a simple decoding\nstrategy that converts flow models into flow map models without architectural\nmodifications. Our method conditions the final blocks of diffusion transformers\non the subsequent timestep, allowing pretrained flow models to be directly\nrepurposed as flow maps. Combined with enhanced training techniques, this\ndesign enables high-quality generation in as few as 1 to 4 steps. Notably, we\nfind that training flow models and subsequently converting them is more\nefficient and effective than training flow maps from scratch. On ImageNet\n256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,\nrespectively, surpassing prior art by a large margin. Furthermore, we achieve\nFID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the\nperformance of flow models while delivering over 100x faster inference."
                },
                "authors": [
                    {
                        "name": "Kyungmin Lee"
                    },
                    {
                        "name": "Sihyun Yu"
                    },
                    {
                        "name": "Jinwoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Shin"
                },
                "author": "Jinwoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24469v1",
                "updated": "2025-10-28T14:36:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    36,
                    22,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:36:22Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    36,
                    22,
                    1,
                    301,
                    0
                ],
                "title": "Iterative Critique-Refine Framework for Enhancing LLM Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Critique-Refine Framework for Enhancing LLM Personalization"
                },
                "summary": "Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic."
                },
                "authors": [
                    {
                        "name": "Durga Prasad Maram"
                    },
                    {
                        "name": "Dhruvin Gandhi"
                    },
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Gayathri Akkinapalli"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Nesreen K. Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Nesreen K. Ahmed"
                },
                "author": "Nesreen K. Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20039v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20039v3",
                "updated": "2025-10-28T14:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    35,
                    32,
                    1,
                    301,
                    0
                ],
                "published": "2025-04-28T17:59:28Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    59,
                    28,
                    0,
                    118,
                    0
                ],
                "title": "AutoJudge: Judge Decoding Without Manual Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoJudge: Judge Decoding Without Manual Annotation"
                },
                "summary": "We introduce AutoJudge, a method that accelerates large language model (LLM)\ninference with task-specific lossy speculative decoding. Instead of matching\nthe original model output distribution token-by-token, we identify which of the\ngenerated tokens affect the downstream quality of the response, relaxing the\ndistribution match guarantee so that the \"unimportant\" tokens can be generated\nfaster. Our approach relies on a semi-greedy search algorithm to test which of\nthe mismatches between target and draft models should be corrected to preserve\nquality and which ones may be skipped. We then train a lightweight classifier\nbased on existing LLM embeddings to predict, at inference time, which\nmismatching tokens can be safely accepted without compromising the final answer\nquality. We evaluate the effectiveness of AutoJudge with multiple draft/target\nmodel pairs on mathematical reasoning and programming benchmarks, achieving\nsignificant speedups at the cost of a minor accuracy reduction. Notably, on\nGSM8k with the Llama 3.1 70B target model, our approach achieves up to\n$\\approx2\\times$ speedup over speculative decoding at the cost of $\\le 1\\%$\ndrop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge\nautomatically detects programming-specific important tokens, accepting $\\ge 25$\ntokens per speculation cycle at $2\\%$ drop in Pass@1. Our approach requires no\nhuman annotation and is easy to integrate with modern LLM inference frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AutoJudge, a method that accelerates large language model (LLM)\ninference with task-specific lossy speculative decoding. Instead of matching\nthe original model output distribution token-by-token, we identify which of the\ngenerated tokens affect the downstream quality of the response, relaxing the\ndistribution match guarantee so that the \"unimportant\" tokens can be generated\nfaster. Our approach relies on a semi-greedy search algorithm to test which of\nthe mismatches between target and draft models should be corrected to preserve\nquality and which ones may be skipped. We then train a lightweight classifier\nbased on existing LLM embeddings to predict, at inference time, which\nmismatching tokens can be safely accepted without compromising the final answer\nquality. We evaluate the effectiveness of AutoJudge with multiple draft/target\nmodel pairs on mathematical reasoning and programming benchmarks, achieving\nsignificant speedups at the cost of a minor accuracy reduction. Notably, on\nGSM8k with the Llama 3.1 70B target model, our approach achieves up to\n$\\approx2\\times$ speedup over speculative decoding at the cost of $\\le 1\\%$\ndrop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge\nautomatically detects programming-specific important tokens, accepting $\\ge 25$\ntokens per speculation cycle at $2\\%$ drop in Pass@1. Our approach requires no\nhuman annotation and is easy to integrate with modern LLM inference frameworks."
                },
                "authors": [
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Fedor Velikonivtsev"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20039v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20039v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07729v2",
                "updated": "2025-10-28T14:35:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    35,
                    15,
                    1,
                    301,
                    0
                ],
                "published": "2025-07-10T13:08:55Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    13,
                    8,
                    55,
                    3,
                    191,
                    0
                ],
                "title": "Efficient Stochastic BFGS methods Inspired by Bayesian Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Stochastic BFGS methods Inspired by Bayesian Principles"
                },
                "summary": "Quasi-Newton methods are ubiquitous in deterministic local search due to\ntheir efficiency and low computational cost. This class of methods uses the\nhistory of gradient evaluations to approximate second-order derivatives.\nHowever, only noisy gradient observations are accessible in stochastic\noptimization; thus, deriving quasi-Newton methods in this setting is\nchallenging. Although most existing quasi-Newton methods for stochastic\noptimization rely on deterministic equations that are modified to circumvent\nnoise, we propose a new approach inspired by Bayesian inference to assimilate\nnoisy gradient information and derive the stochastic counterparts to standard\nquasi-Newton methods. We focus on the derivations of stochastic BFGS and\nL-BFGS, but our methodology can also be employed to derive stochastic analogs\nof other quasi-Newton methods. The resulting stochastic BFGS (S-BFGS) and\nstochastic L-BFGS (L-S-BFGS) can effectively learn an inverse Hessian\napproximation even with small batch sizes. For a problem of dimension $d$, the\niteration cost of S-BFGS is $\\mathcal{O}(d^2)$, and the cost of L-S-BFGS is\n$\\mathcal{O}(d)$. Numerical experiments with a dimensionality of up to $30,720$\ndemonstrate the efficiency and robustness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasi-Newton methods are ubiquitous in deterministic local search due to\ntheir efficiency and low computational cost. This class of methods uses the\nhistory of gradient evaluations to approximate second-order derivatives.\nHowever, only noisy gradient observations are accessible in stochastic\noptimization; thus, deriving quasi-Newton methods in this setting is\nchallenging. Although most existing quasi-Newton methods for stochastic\noptimization rely on deterministic equations that are modified to circumvent\nnoise, we propose a new approach inspired by Bayesian inference to assimilate\nnoisy gradient information and derive the stochastic counterparts to standard\nquasi-Newton methods. We focus on the derivations of stochastic BFGS and\nL-BFGS, but our methodology can also be employed to derive stochastic analogs\nof other quasi-Newton methods. The resulting stochastic BFGS (S-BFGS) and\nstochastic L-BFGS (L-S-BFGS) can effectively learn an inverse Hessian\napproximation even with small batch sizes. For a problem of dimension $d$, the\niteration cost of S-BFGS is $\\mathcal{O}(d^2)$, and the cost of L-S-BFGS is\n$\\mathcal{O}(d)$. Numerical experiments with a dimensionality of up to $30,720$\ndemonstrate the efficiency and robustness of the proposed method."
                },
                "authors": [
                    {
                        "name": "André Carlon"
                    },
                    {
                        "name": "Luis Espath"
                    },
                    {
                        "name": "Raúl Tempone"
                    }
                ],
                "author_detail": {
                    "name": "Raúl Tempone"
                },
                "author": "Raúl Tempone",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21724v2",
                "updated": "2025-10-28T14:26:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    26,
                    23,
                    1,
                    301,
                    0
                ],
                "published": "2025-05-27T20:12:46Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    20,
                    12,
                    46,
                    1,
                    147,
                    0
                ],
                "title": "OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions"
                },
                "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task designed to produce synchronized verbal and\nnon-verbal listener feedback online, based on the speaker's multimodal inputs.\nOMCRG captures natural dyadic interactions and introduces new challenges in\naligning generated audio with listeners' facial responses. To tackle these\nchallenges, we incorporate text as an intermediate modality to connect audio\nand facial responses. We propose OmniResponse, a Multimodal Large Language\nModel (MLLM) that autoregressively generates accurate multimodal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two core\ncomponents: Chrono-Text Markup, which precisely timestamps generated text\ntokens, and TempoVoice, a controllable online text-to-speech (TTS) module that\noutputs speech synchronized with facial responses. To advance OMCRG research,\nwe offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring\nsynchronized split-screen videos, multichannel audio, transcripts, and\nannotated facial behaviors. Comprehensive evaluations on ResponseNet\ndemonstrate that OmniResponse outperforms baseline models in terms of semantic\nspeech content, audio-visual synchronization, and generation quality. Our\ndataset, code, and models are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task designed to produce synchronized verbal and\nnon-verbal listener feedback online, based on the speaker's multimodal inputs.\nOMCRG captures natural dyadic interactions and introduces new challenges in\naligning generated audio with listeners' facial responses. To tackle these\nchallenges, we incorporate text as an intermediate modality to connect audio\nand facial responses. We propose OmniResponse, a Multimodal Large Language\nModel (MLLM) that autoregressively generates accurate multimodal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two core\ncomponents: Chrono-Text Markup, which precisely timestamps generated text\ntokens, and TempoVoice, a controllable online text-to-speech (TTS) module that\noutputs speech synchronized with facial responses. To advance OMCRG research,\nwe offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring\nsynchronized split-screen videos, multichannel audio, transcripts, and\nannotated facial behaviors. Comprehensive evaluations on ResponseNet\ndemonstrate that OmniResponse outperforms baseline models in terms of semantic\nspeech content, audio-visual synchronization, and generation quality. Our\ndataset, code, and models are publicly available."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Jianghui Wang"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Siyang Song"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06923v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06923v4",
                "updated": "2025-10-28T14:21:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    21,
                    34,
                    1,
                    301,
                    0
                ],
                "published": "2025-04-09T14:30:30Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    30,
                    30,
                    2,
                    99,
                    0
                ],
                "title": "The Importance of Being Discrete: Measuring the Impact of Discretization\n  in End-to-End Differentially Private Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Importance of Being Discrete: Measuring the Impact of Discretization\n  in End-to-End Differentially Private Synthetic Data"
                },
                "summary": "Differentially Private (DP) generative marginal models are often used in the\nwild to release synthetic tabular datasets in lieu of sensitive data while\nproviding formal privacy guarantees. These models approximate low-dimensional\nmarginals or query workloads; crucially, they require the training data to be\npre-discretized, i.e., continuous values need to first be partitioned into\nbins. However, as the range of values (or their domain) is often inferred\ndirectly from the training data, with the number of bins and bin edges\ntypically defined arbitrarily, this approach can ultimately break end-to-end DP\nguarantees and may not always yield optimal utility.\n  In this paper, we present an extensive measurement study of four\ndiscretization strategies in the context of DP marginal generative models. More\nprecisely, we design DP versions of three discretizers (uniform, quantile, and\nk-means) and reimplement the PrivTree algorithm. We find that optimizing both\nthe choice of discretizer and bin count can improve utility, on average, by\nalmost 30% across six DP marginal models, compared to the default strategy and\nnumber of bins, with PrivTree being the best-performing discretizer in the\nmajority of cases. We demonstrate that, while DP generative models with\nnon-private discretization remain vulnerable to membership inference attacks,\napplying DP during discretization effectively mitigates this risk. Finally, we\nimprove on an existing approach for automatically selecting the optimal number\nof bins, and achieve high utility while reducing both privacy budget\nconsumption and computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private (DP) generative marginal models are often used in the\nwild to release synthetic tabular datasets in lieu of sensitive data while\nproviding formal privacy guarantees. These models approximate low-dimensional\nmarginals or query workloads; crucially, they require the training data to be\npre-discretized, i.e., continuous values need to first be partitioned into\nbins. However, as the range of values (or their domain) is often inferred\ndirectly from the training data, with the number of bins and bin edges\ntypically defined arbitrarily, this approach can ultimately break end-to-end DP\nguarantees and may not always yield optimal utility.\n  In this paper, we present an extensive measurement study of four\ndiscretization strategies in the context of DP marginal generative models. More\nprecisely, we design DP versions of three discretizers (uniform, quantile, and\nk-means) and reimplement the PrivTree algorithm. We find that optimizing both\nthe choice of discretizer and bin count can improve utility, on average, by\nalmost 30% across six DP marginal models, compared to the default strategy and\nnumber of bins, with PrivTree being the best-performing discretizer in the\nmajority of cases. We demonstrate that, while DP generative models with\nnon-private discretization remain vulnerable to membership inference attacks,\napplying DP during discretization effectively mitigates this risk. Finally, we\nimprove on an existing approach for automatically selecting the optimal number\nof bins, and achieve high utility while reducing both privacy budget\nconsumption and computational overhead."
                },
                "authors": [
                    {
                        "name": "Georgi Ganev"
                    },
                    {
                        "name": "Meenatchi Sundaram Muthu Selva Annamalai"
                    },
                    {
                        "name": "Sofiane Mahiou"
                    },
                    {
                        "name": "Emiliano De Cristofaro"
                    }
                ],
                "author_detail": {
                    "name": "Emiliano De Cristofaro"
                },
                "author": "Emiliano De Cristofaro",
                "arxiv_journal_ref": "Published in the Proceedings of the 32nd ACM Conference on\n  Computer and Communications Security (ACM CCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06923v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06923v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22200v2",
                "updated": "2025-10-28T14:19:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    19,
                    57,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-25T07:41:02Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    7,
                    41,
                    2,
                    5,
                    298,
                    0
                ],
                "title": "LongCat-Video Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongCat-Video Technical Report"
                },
                "summary": "Video generation is a critical pathway toward world models, with efficient\nlong video inference as a key capability. Toward this end, we introduce\nLongCat-Video, a foundational video generation model with 13.6B parameters,\ndelivering strong performance across multiple video generation tasks. It\nparticularly excels in efficient and high-quality long video generation,\nrepresenting our first step toward world models. Key features include: Unified\narchitecture for multiple tasks: Built on the Diffusion Transformer (DiT)\nframework, LongCat-Video supports Text-to-Video, Image-to-Video, and\nVideo-Continuation tasks with a single model; Long video generation:\nPretraining on Video-Continuation tasks enables LongCat-Video to maintain high\nquality and temporal coherence in the generation of minutes-long videos;\nEfficient inference: LongCat-Video generates 720p, 30fps videos within minutes\nby employing a coarse-to-fine generation strategy along both the temporal and\nspatial axes. Block Sparse Attention further enhances efficiency, particularly\nat high resolutions; Strong performance with multi-reward RLHF: Multi-reward\nRLHF training enables LongCat-Video to achieve performance on par with the\nlatest closed-source and leading open-source models. Code and model weights are\npublicly available to accelerate progress in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation is a critical pathway toward world models, with efficient\nlong video inference as a key capability. Toward this end, we introduce\nLongCat-Video, a foundational video generation model with 13.6B parameters,\ndelivering strong performance across multiple video generation tasks. It\nparticularly excels in efficient and high-quality long video generation,\nrepresenting our first step toward world models. Key features include: Unified\narchitecture for multiple tasks: Built on the Diffusion Transformer (DiT)\nframework, LongCat-Video supports Text-to-Video, Image-to-Video, and\nVideo-Continuation tasks with a single model; Long video generation:\nPretraining on Video-Continuation tasks enables LongCat-Video to maintain high\nquality and temporal coherence in the generation of minutes-long videos;\nEfficient inference: LongCat-Video generates 720p, 30fps videos within minutes\nby employing a coarse-to-fine generation strategy along both the temporal and\nspatial axes. Block Sparse Attention further enhances efficiency, particularly\nat high resolutions; Strong performance with multi-reward RLHF: Multi-reward\nRLHF training enables LongCat-Video to achieve performance on par with the\nlatest closed-source and leading open-source models. Code and model weights are\npublicly available to accelerate progress in the field."
                },
                "authors": [
                    {
                        "name": "Meituan LongCat Team"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Qilong Huang"
                    },
                    {
                        "name": "Zhuoliang Kang"
                    },
                    {
                        "name": "Hongyu Li"
                    },
                    {
                        "name": "Shijun Liang"
                    },
                    {
                        "name": "Liya Ma"
                    },
                    {
                        "name": "Siyu Ren"
                    },
                    {
                        "name": "Xiaoming Wei"
                    },
                    {
                        "name": "Rixu Xie"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24450v1",
                "updated": "2025-10-28T14:13:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    13,
                    44,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:13:44Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    13,
                    44,
                    1,
                    301,
                    0
                ],
                "title": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a\n  Set of Best Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a\n  Set of Best Practices"
                },
                "summary": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods."
                },
                "authors": [
                    {
                        "name": "Špela Vintar"
                    },
                    {
                        "name": "Taja Kuzman Pungeršek"
                    },
                    {
                        "name": "Mojca Brglez"
                    },
                    {
                        "name": "Nikola Ljubešić"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Ljubešić"
                },
                "author": "Nikola Ljubešić",
                "arxiv_comment": "12 pages, 1 figure. Submitted to the LREC 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24448v1",
                "updated": "2025-10-28T14:12:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    12,
                    11,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:12:11Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    12,
                    11,
                    1,
                    301,
                    0
                ],
                "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Visual Intelligence: Insights from Video Pretraining"
                },
                "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models."
                },
                "authors": [
                    {
                        "name": "Pablo Acuaviva"
                    },
                    {
                        "name": "Aram Davtyan"
                    },
                    {
                        "name": "Mariam Hassan"
                    },
                    {
                        "name": "Sebastian Stapf"
                    },
                    {
                        "name": "Ahmad Rahimi"
                    },
                    {
                        "name": "Alexandre Alahi"
                    },
                    {
                        "name": "Paolo Favaro"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Favaro"
                },
                "author": "Paolo Favaro",
                "arxiv_comment": "Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T45, 68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8; I.5.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24442v1",
                "updated": "2025-10-28T14:07:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    7,
                    10,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:07:10Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    7,
                    10,
                    1,
                    301,
                    0
                ],
                "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Law in Silico: Simulating Legal Society with LLM-Based Agents"
                },
                "summary": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals."
                },
                "authors": [
                    {
                        "name": "Yiding Wang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Xifan Chen"
                    },
                    {
                        "name": "Xiaolei Yang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24438v1",
                "updated": "2025-10-28T14:05:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    5,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:05:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    5,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated\n  Islamic Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated\n  Islamic Content"
                },
                "summary": "Large language models are increasingly used for Islamic guidance, but risk\nmisquoting texts, misapplying jurisprudence, or producing culturally\ninconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar\non prompts from authentic Islamic blogs. Our dual-agent framework uses a\nquantitative agent for citation verification and six-dimensional scoring (e.g.,\nStructure, Islamic Consistency, Citations) and a qualitative agent for\nfive-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).\nGPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI\nfollowed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong\nperformance, models still fall short in reliably producing accurate Islamic\ncontent and citations -- a paramount requirement in faith-sensitive writing.\nGPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led\nqualitative pairwise wins (116/200). Fanar, though trailing, introduces\ninnovations for Islamic and Arabic contexts. This study underscores the need\nfor community-driven benchmarks centering Muslim perspectives, offering an\nearly step toward more reliable AI in Islamic knowledge and other high-stakes\ndomains such as medicine, law, and journalism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly used for Islamic guidance, but risk\nmisquoting texts, misapplying jurisprudence, or producing culturally\ninconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar\non prompts from authentic Islamic blogs. Our dual-agent framework uses a\nquantitative agent for citation verification and six-dimensional scoring (e.g.,\nStructure, Islamic Consistency, Citations) and a qualitative agent for\nfive-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).\nGPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI\nfollowed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong\nperformance, models still fall short in reliably producing accurate Islamic\ncontent and citations -- a paramount requirement in faith-sensitive writing.\nGPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led\nqualitative pairwise wins (116/200). Fanar, though trailing, introduces\ninnovations for Islamic and Arabic contexts. This study underscores the need\nfor community-driven benchmarks centering Muslim perspectives, offering an\nearly step toward more reliable AI in Islamic knowledge and other high-stakes\ndomains such as medicine, law, and journalism."
                },
                "authors": [
                    {
                        "name": "Abdullah Mushtaq"
                    },
                    {
                        "name": "Rafay Naeem"
                    },
                    {
                        "name": "Ezieddin Elmahjub"
                    },
                    {
                        "name": "Ibrahim Ghaznavi"
                    },
                    {
                        "name": "Shawqi Al-Maliki"
                    },
                    {
                        "name": "Mohamed Abdallah"
                    },
                    {
                        "name": "Ala Al-Fuqaha"
                    },
                    {
                        "name": "Junaid Qadir"
                    }
                ],
                "author_detail": {
                    "name": "Junaid Qadir"
                },
                "author": "Junaid Qadir",
                "arxiv_comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Workshop: 5th Muslims in Machine Learning (MusIML) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24435v1",
                "updated": "2025-10-28T14:02:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    2,
                    58,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:02:58Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    2,
                    58,
                    1,
                    301,
                    0
                ],
                "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on\n  Logical and Abstract Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Level Reasoning: A Comparative Study of Large Language Models on\n  Logical and Abstract Reasoning"
                },
                "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction."
                },
                "authors": [
                    {
                        "name": "Benjamin Grando Moreira"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Grando Moreira"
                },
                "author": "Benjamin Grando Moreira",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24434v1",
                "updated": "2025-10-28T14:02:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    2,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:02:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    2,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed\n  Data"
                },
                "summary": "The effectiveness of instruction-tuned Large Language Models (LLMs) is often\nlimited in low-resource linguistic settings due to a lack of high-quality\ntraining data. We introduce LuxIT, a novel, monolingual instruction tuning\ndataset for Luxembourgish developed to mitigate this challenge. We synthesize\nthe dataset from a corpus of native Luxembourgish texts, utilizing\nDeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following\ngeneration, we apply a quality assurance process, employing an LLM-as-a-judge\napproach. To investigate the practical utility of the dataset, we fine-tune\nseveral smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base\nmodels on Luxembourgish language proficiency examinations, however, yields\nmixed results, with performance varying significantly across different models.\nLuxIT represents a critical contribution to Luxembourgish natural language\nprocessing and offers a replicable monolingual methodology, though our findings\nhighlight the need for further research to optimize its application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of instruction-tuned Large Language Models (LLMs) is often\nlimited in low-resource linguistic settings due to a lack of high-quality\ntraining data. We introduce LuxIT, a novel, monolingual instruction tuning\ndataset for Luxembourgish developed to mitigate this challenge. We synthesize\nthe dataset from a corpus of native Luxembourgish texts, utilizing\nDeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following\ngeneration, we apply a quality assurance process, employing an LLM-as-a-judge\napproach. To investigate the practical utility of the dataset, we fine-tune\nseveral smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base\nmodels on Luxembourgish language proficiency examinations, however, yields\nmixed results, with performance varying significantly across different models.\nLuxIT represents a critical contribution to Luxembourgish natural language\nprocessing and offers a replicable monolingual methodology, though our findings\nhighlight the need for further research to optimize its application."
                },
                "authors": [
                    {
                        "name": "Julian Valline"
                    },
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Jordi Cabot"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Cabot"
                },
                "author": "Jordi Cabot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24431v1",
                "updated": "2025-10-28T13:58:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    58,
                    36,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:58:36Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    58,
                    36,
                    1,
                    301,
                    0
                ],
                "title": "MiniOneRec: An Open-Source Framework for Scaling Generative\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniOneRec: An Open-Source Framework for Scaling Generative\n  Recommendation"
                },
                "summary": "The recent success of large language models (LLMs) has renewed interest in\nwhether recommender systems can achieve similar scaling benefits. Conventional\nrecommenders, dominated by massive embedding tables, tend to plateau as\nembedding dimensions grow. In contrast, the emerging generative paradigm\nreplaces embeddings with compact Semantic ID (SID) sequences produced by\nautoregressive Transformers. Yet most industrial deployments remain\nproprietary, leaving two fundamental questions open: (1) Do the expected\nscaling laws hold on public benchmarks? (2) What is the minimal post-training\nrecipe that enables competitive performance?\n  We present MiniOneRec, to the best of our knowledge, the first fully\nopen-source generative recommendation framework, which provides an end-to-end\nworkflow spanning SID construction, supervised fine-tuning, and\nrecommendation-oriented reinforcement learning. We generate SIDs via a Residual\nQuantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters\non the Amazon Review dataset. Our experiments reveal a consistent downward\ntrend in both training and evaluation losses with increasing model size,\nvalidating the parameter efficiency of the generative approach. To further\nenhance performance, we propose a lightweight yet effective post-training\npipeline that (1) enforces full-process SID alignment and (2) applies\nreinforcement learning with constrained decoding and hybrid rewards. Together,\nthese techniques yield significant improvements in both ranking accuracy and\ncandidate diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent success of large language models (LLMs) has renewed interest in\nwhether recommender systems can achieve similar scaling benefits. Conventional\nrecommenders, dominated by massive embedding tables, tend to plateau as\nembedding dimensions grow. In contrast, the emerging generative paradigm\nreplaces embeddings with compact Semantic ID (SID) sequences produced by\nautoregressive Transformers. Yet most industrial deployments remain\nproprietary, leaving two fundamental questions open: (1) Do the expected\nscaling laws hold on public benchmarks? (2) What is the minimal post-training\nrecipe that enables competitive performance?\n  We present MiniOneRec, to the best of our knowledge, the first fully\nopen-source generative recommendation framework, which provides an end-to-end\nworkflow spanning SID construction, supervised fine-tuning, and\nrecommendation-oriented reinforcement learning. We generate SIDs via a Residual\nQuantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters\non the Amazon Review dataset. Our experiments reveal a consistent downward\ntrend in both training and evaluation losses with increasing model size,\nvalidating the parameter efficiency of the generative approach. To further\nenhance performance, we propose a lightweight yet effective post-training\npipeline that (1) enforces full-process SID alignment and (2) applies\nreinforcement learning with constrained decoding and hybrid rewards. Together,\nthese techniques yield significant improvements in both ranking accuracy and\ncandidate diversity."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Kong"
                    },
                    {
                        "name": "Leheng Sheng"
                    },
                    {
                        "name": "Junfei Tan"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24430v1",
                "updated": "2025-10-28T13:57:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    57,
                    23,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:57:23Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    57,
                    23,
                    1,
                    301,
                    0
                ],
                "title": "From Time and Place to Preference: LLM-Driven Geo-Temporal Context in\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Time and Place to Preference: LLM-Driven Geo-Temporal Context in\n  Recommendations"
                },
                "summary": "Most recommender systems treat timestamps as numeric or cyclical values,\noverlooking real-world context such as holidays, events, and seasonal patterns.\nWe propose a scalable framework that uses large language models (LLMs) to\ngenerate geo-temporal embeddings from only a timestamp and coarse location,\ncapturing holidays, seasonal trends, and local/global events. We then introduce\na geo-temporal embedding informativeness test as a lightweight diagnostic,\ndemonstrating on MovieLens, LastFM, and a production dataset that these\nembeddings provide predictive signal consistent with the outcomes of full model\nintegrations. Geo-temporal embeddings are incorporated into sequential models\nthrough (1) direct feature fusion with metadata embeddings or (2) an auxiliary\nloss that enforces semantic and geo-temporal alignment. Our findings highlight\nthe need for adaptive or hybrid recommendation strategies, and we release a\ncontext-enriched MovieLens dataset to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most recommender systems treat timestamps as numeric or cyclical values,\noverlooking real-world context such as holidays, events, and seasonal patterns.\nWe propose a scalable framework that uses large language models (LLMs) to\ngenerate geo-temporal embeddings from only a timestamp and coarse location,\ncapturing holidays, seasonal trends, and local/global events. We then introduce\na geo-temporal embedding informativeness test as a lightweight diagnostic,\ndemonstrating on MovieLens, LastFM, and a production dataset that these\nembeddings provide predictive signal consistent with the outcomes of full model\nintegrations. Geo-temporal embeddings are incorporated into sequential models\nthrough (1) direct feature fusion with metadata embeddings or (2) an auxiliary\nloss that enforces semantic and geo-temporal alignment. Our findings highlight\nthe need for adaptive or hybrid recommendation strategies, and we release a\ncontext-enriched MovieLens dataset to support future research."
                },
                "authors": [
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Shaghayegh Agah"
                    },
                    {
                        "name": "Mayur Nankani"
                    },
                    {
                        "name": "Neeraj Sharma"
                    },
                    {
                        "name": "Feifei Peng"
                    },
                    {
                        "name": "Maria Peifer"
                    },
                    {
                        "name": "Sardar Hamidian"
                    },
                    {
                        "name": "H Howie Huang"
                    }
                ],
                "author_detail": {
                    "name": "H Howie Huang"
                },
                "author": "H Howie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24428v1",
                "updated": "2025-10-28T13:52:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    52,
                    46,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:52:46Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    52,
                    46,
                    1,
                    301,
                    0
                ],
                "title": "CodeWiki: Automated Repository-Level Documentation at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeWiki: Automated Repository-Level Documentation at Scale"
                },
                "summary": "Developers spend nearly 58% of their time understanding codebases, yet\nmaintaining comprehensive documentation remains challenging due to complexity\nand manual effort. While recent Large Language Models (LLMs) show promise for\nfunction-level documentation, they fail at the repository level, where\ncapturing architectural patterns and cross-module interactions is essential. We\nintroduce CodeWiki, the first open-source framework for holistic\nrepository-level documentation across seven programming languages. CodeWiki\nemploys three innovations: (i) hierarchical decomposition that preserves\narchitectural context, (ii) recursive agentic processing with dynamic\ndelegation, and (iii) synthesis of textual and visual artifacts including\narchitecture diagrams and data flows. We also present CodeWikiBench, the first\nrepository-level documentation benchmark with multi-level rubrics and agentic\nassessment. CodeWiki achieves 68.79% quality score with proprietary models and\n64.80% with open-source alternatives, outperforming existing closed-source\nsystems and demonstrating scalable, accurate documentation for real-world\nrepositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developers spend nearly 58% of their time understanding codebases, yet\nmaintaining comprehensive documentation remains challenging due to complexity\nand manual effort. While recent Large Language Models (LLMs) show promise for\nfunction-level documentation, they fail at the repository level, where\ncapturing architectural patterns and cross-module interactions is essential. We\nintroduce CodeWiki, the first open-source framework for holistic\nrepository-level documentation across seven programming languages. CodeWiki\nemploys three innovations: (i) hierarchical decomposition that preserves\narchitectural context, (ii) recursive agentic processing with dynamic\ndelegation, and (iii) synthesis of textual and visual artifacts including\narchitecture diagrams and data flows. We also present CodeWikiBench, the first\nrepository-level documentation benchmark with multi-level rubrics and agentic\nassessment. CodeWiki achieves 68.79% quality score with proprietary models and\n64.80% with open-source alternatives, outperforming existing closed-source\nsystems and demonstrating scalable, accurate documentation for real-world\nrepositories."
                },
                "authors": [
                    {
                        "name": "Nguyen Hoang Anh"
                    },
                    {
                        "name": "Minh Le-Anh"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17281v2",
                "updated": "2025-10-28T13:52:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    52,
                    29,
                    1,
                    301,
                    0
                ],
                "published": "2025-08-24T10:02:51Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    10,
                    2,
                    51,
                    6,
                    236,
                    0
                ],
                "title": "From Language to Action: A Review of Large Language Models as Autonomous\n  Agents and Tool Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Language to Action: A Review of Large Language Models as Autonomous\n  Agents and Tool Users"
                },
                "summary": "The pursuit of human-level artificial intelligence (AI) has significantly\nadvanced the development of autonomous agents and Large Language Models (LLMs).\nLLMs are now widely utilized as decision-making agents for their ability to\ninterpret instructions, manage sequential tasks, and adapt through feedback.\nThis review examines recent developments in employing LLMs as autonomous agents\nand tool users and comprises seven research questions. We only used the papers\npublished between 2023 and 2025 in conferences of the A* and A rank and Q1\njournals. A structured analysis of the LLM agents' architectural design\nprinciples, dividing their applications into single-agent and multi-agent\nsystems, and strategies for integrating external tools is presented. In\naddition, the cognitive mechanisms of LLM, including reasoning, planning, and\nmemory, and the impact of prompting methods and fine-tuning procedures on agent\nperformance are also investigated. Furthermore, we evaluated current benchmarks\nand assessment protocols and have provided an analysis of 68 publicly available\ndatasets to assess the performance of LLM-based agents in various tasks. In\nconducting this review, we have identified critical findings on verifiable\nreasoning of LLMs, the capacity for self-improvement, and the personalization\nof LLM-based agents. Finally, we have discussed ten future research directions\nto overcome these gaps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of human-level artificial intelligence (AI) has significantly\nadvanced the development of autonomous agents and Large Language Models (LLMs).\nLLMs are now widely utilized as decision-making agents for their ability to\ninterpret instructions, manage sequential tasks, and adapt through feedback.\nThis review examines recent developments in employing LLMs as autonomous agents\nand tool users and comprises seven research questions. We only used the papers\npublished between 2023 and 2025 in conferences of the A* and A rank and Q1\njournals. A structured analysis of the LLM agents' architectural design\nprinciples, dividing their applications into single-agent and multi-agent\nsystems, and strategies for integrating external tools is presented. In\naddition, the cognitive mechanisms of LLM, including reasoning, planning, and\nmemory, and the impact of prompting methods and fine-tuning procedures on agent\nperformance are also investigated. Furthermore, we evaluated current benchmarks\nand assessment protocols and have provided an analysis of 68 publicly available\ndatasets to assess the performance of LLM-based agents in various tasks. In\nconducting this review, we have identified critical findings on verifiable\nreasoning of LLMs, the capacity for self-improvement, and the personalization\nof LLM-based agents. Finally, we have discussed ten future research directions\nto overcome these gaps."
                },
                "authors": [
                    {
                        "name": "Sadia Sultana Chowa"
                    },
                    {
                        "name": "Riasad Alvi"
                    },
                    {
                        "name": "Subhey Sadi Rahman"
                    },
                    {
                        "name": "Md Abdur Rahman"
                    },
                    {
                        "name": "Mohaimenul Azam Khan Raiaan"
                    },
                    {
                        "name": "Md Rafiqul Islam"
                    },
                    {
                        "name": "Mukhtar Hussain"
                    },
                    {
                        "name": "Sami Azam"
                    }
                ],
                "author_detail": {
                    "name": "Sami Azam"
                },
                "author": "Sami Azam",
                "arxiv_comment": "Submitted to Artificial Intelligence Review for peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10856v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10856v4",
                "updated": "2025-10-28T13:45:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    45,
                    25,
                    1,
                    301,
                    0
                ],
                "published": "2024-12-14T15:11:07Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    15,
                    11,
                    7,
                    5,
                    349,
                    0
                ],
                "title": "RWKV-edge: Deeply Compressed RWKV for Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RWKV-edge: Deeply Compressed RWKV for Resource-Constrained Devices"
                },
                "summary": "To deploy LLMs on resource-contained platforms such as mobile robots and\nsmartphones, non-transformers LLMs have achieved major breakthroughs. Recently,\na novel RNN-based LLM family, Repentance Weighted Key Value (RWKV) has shown\nstrong computational efficiency; nevertheless, RWKV models still have high\nparameter counts which limited their deployment. In this paper, we propose a\nsuite of compression techniques, ranging from model architecture optimizations\nto post-training compression, tailored to the RWKV architecture. Combined, our\ntechniques reduce the memory footprint of RWKV models by 3.4x -- 5x with only\nnegligible degradation in accuracy; compared to transformer LLMs with similar\naccuracy, our models require 4x less memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To deploy LLMs on resource-contained platforms such as mobile robots and\nsmartphones, non-transformers LLMs have achieved major breakthroughs. Recently,\na novel RNN-based LLM family, Repentance Weighted Key Value (RWKV) has shown\nstrong computational efficiency; nevertheless, RWKV models still have high\nparameter counts which limited their deployment. In this paper, we propose a\nsuite of compression techniques, ranging from model architecture optimizations\nto post-training compression, tailored to the RWKV architecture. Combined, our\ntechniques reduce the memory footprint of RWKV models by 3.4x -- 5x with only\nnegligible degradation in accuracy; compared to transformer LLMs with similar\naccuracy, our models require 4x less memory footprint."
                },
                "authors": [
                    {
                        "name": "Wonkyo Choe"
                    },
                    {
                        "name": "Yangfeng Ji"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10856v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10856v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24408v1",
                "updated": "2025-10-28T13:19:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    19,
                    46,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:19:46Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    19,
                    46,
                    1,
                    301,
                    0
                ],
                "title": "Uncovering Gaps Between RFC Updates and TCP/IP Implementations:\n  LLM-Facilitated Differential Checks on Intermediate Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Gaps Between RFC Updates and TCP/IP Implementations:\n  LLM-Facilitated Differential Checks on Intermediate Representations"
                },
                "summary": "As the core of the Internet infrastructure, the TCP/IP protocol stack\nundertakes the task of network data transmission. However, due to the\ncomplexity of the protocol and the uncertainty of cross-layer interaction,\nthere are often inconsistencies between the implementation of the protocol\nstack code and the RFC standard. This inconsistency may not only lead to\ndifferences in protocol functions but also cause serious security\nvulnerabilities. At present, with the continuous expansion of protocol stack\nfunctions and the rapid iteration of RFC documents, it is increasingly\nimportant to detect and fix these inconsistencies. With the rise of large\nlanguage models, researchers have begun to explore how to extract protocol\nspecifications from RFC documents through these models, including protocol\nstack modeling, state machine extraction, text ambiguity analysis, and other\nrelated content. However, existing methods rely on predefined patterns or\nrule-based approaches that fail to generalize across different protocol\nspecifications. Automated and scalable detection of these inconsistencies\nremains a significant challenge. In this study, we propose an automated\nanalysis framework based on LLM and differential models. By modeling the\niterative relationship of the protocol and based on the iterative update\nrelationship of the RFC standard, we perform incremental code function analysis\non different versions of kernel code implementations to automatically perform\ncode detection and vulnerability analysis. We conduct extensive evaluations to\nvalidate the effectiveness of our framework, demonstrating its effectiveness in\nidentifying potential vulnerabilities caused by RFC code inconsistencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the core of the Internet infrastructure, the TCP/IP protocol stack\nundertakes the task of network data transmission. However, due to the\ncomplexity of the protocol and the uncertainty of cross-layer interaction,\nthere are often inconsistencies between the implementation of the protocol\nstack code and the RFC standard. This inconsistency may not only lead to\ndifferences in protocol functions but also cause serious security\nvulnerabilities. At present, with the continuous expansion of protocol stack\nfunctions and the rapid iteration of RFC documents, it is increasingly\nimportant to detect and fix these inconsistencies. With the rise of large\nlanguage models, researchers have begun to explore how to extract protocol\nspecifications from RFC documents through these models, including protocol\nstack modeling, state machine extraction, text ambiguity analysis, and other\nrelated content. However, existing methods rely on predefined patterns or\nrule-based approaches that fail to generalize across different protocol\nspecifications. Automated and scalable detection of these inconsistencies\nremains a significant challenge. In this study, we propose an automated\nanalysis framework based on LLM and differential models. By modeling the\niterative relationship of the protocol and based on the iterative update\nrelationship of the RFC standard, we perform incremental code function analysis\non different versions of kernel code implementations to automatically perform\ncode detection and vulnerability analysis. We conduct extensive evaluations to\nvalidate the effectiveness of our framework, demonstrating its effectiveness in\nidentifying potential vulnerabilities caused by RFC code inconsistencies."
                },
                "authors": [
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Xuewei Feng"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Ke Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ke Xu"
                },
                "author": "Ke Xu",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24402v1",
                "updated": "2025-10-28T13:16:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    16,
                    36,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:16:36Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    16,
                    36,
                    1,
                    301,
                    0
                ],
                "title": "Metadata-Driven Retrieval-Augmented Generation for Financial Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metadata-Driven Retrieval-Augmented Generation for Financial Question\n  Answering"
                },
                "summary": "Retrieval-Augmented Generation (RAG) struggles on long, structured financial\nfilings where relevant evidence is sparse and cross-referenced. This paper\npresents a systematic investigation of advanced metadata-driven\nRetrieval-Augmented Generation (RAG) techniques, proposing and evaluating a\nnovel, multi-stage RAG architecture that leverages LLM-generated metadata. We\nintroduce a sophisticated indexing pipeline to create contextually rich\ndocument chunks and benchmark a spectrum of enhancements, including\npre-retrieval filtering, post-retrieval reranking, and enriched embeddings,\nbenchmarked on the FinanceBench dataset. Our results reveal that while a\npowerful reranker is essential for precision, the most significant performance\ngains come from embedding chunk metadata directly with text (\"contextual\nchunks\"). Our proposed optimal architecture combines LLM-driven pre-retrieval\noptimizations with these contextual embeddings to achieve superior performance.\nAdditionally, we present a custom metadata reranker that offers a compelling,\ncost-effective alternative to commercial solutions, highlighting a practical\ntrade-off between peak performance and operational efficiency. This study\nprovides a blueprint for building robust, metadata-aware RAG systems for\nfinancial document analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) struggles on long, structured financial\nfilings where relevant evidence is sparse and cross-referenced. This paper\npresents a systematic investigation of advanced metadata-driven\nRetrieval-Augmented Generation (RAG) techniques, proposing and evaluating a\nnovel, multi-stage RAG architecture that leverages LLM-generated metadata. We\nintroduce a sophisticated indexing pipeline to create contextually rich\ndocument chunks and benchmark a spectrum of enhancements, including\npre-retrieval filtering, post-retrieval reranking, and enriched embeddings,\nbenchmarked on the FinanceBench dataset. Our results reveal that while a\npowerful reranker is essential for precision, the most significant performance\ngains come from embedding chunk metadata directly with text (\"contextual\nchunks\"). Our proposed optimal architecture combines LLM-driven pre-retrieval\noptimizations with these contextual embeddings to achieve superior performance.\nAdditionally, we present a custom metadata reranker that offers a compelling,\ncost-effective alternative to commercial solutions, highlighting a practical\ntrade-off between peak performance and operational efficiency. This study\nprovides a blueprint for building robust, metadata-aware RAG systems for\nfinancial document analysis."
                },
                "authors": [
                    {
                        "name": "Michail Dadopoulos"
                    },
                    {
                        "name": "Anestis Ladas"
                    },
                    {
                        "name": "Stratos Moschidis"
                    },
                    {
                        "name": "Ioannis Negkakis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Negkakis"
                },
                "author": "Ioannis Negkakis",
                "arxiv_comment": "Preprint version submitted to the International Journal of Accounting\n  Information Systems; currently under major revision. 20 pages, 1 figure, 1\n  table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24401v1",
                "updated": "2025-10-28T13:15:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    15,
                    27,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:15:27Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    15,
                    27,
                    1,
                    301,
                    0
                ],
                "title": "Comprehensive Inclusion of Higher-order Ca$^+$ Isotope Shifts in the\n  King's Plot Yields an Order Improvement on the $e^-$-$n$ Coupling Limit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Inclusion of Higher-order Ca$^+$ Isotope Shifts in the\n  King's Plot Yields an Order Improvement on the $e^-$-$n$ Coupling Limit"
                },
                "summary": "By critically evaluating higher-order nonlinear effects to the isotope shifts\n(ISs) in the low-lying transition frequencies of the singly charged calcium\nion, stringent constraint on the electron-neutron coupling due to a\nhypothetical boson describing physics beyond the Standard Model is inferred. It\nshows an order magnitude difference compared to the previously reported limit\ndemonstrating importance of higher-order effects in the analysis of\nnonlinearity in the King's plot. The first-order IS parameters and enhancement\nfactor ($D$) were evaluated using two complementary approaches in the\nrelativistic coupled-cluster theory framework: namely finite-field (FF) and\nanalytical response (AR) approaches. Extraction of the second-order IS\nparameters in the FF approach show numerical instabilities, so they are\ndetermined in the AR approach. Comparison of these factors with previous\ncalculation shows substantial differences in the magnitudes. However, $D$\nvalues from both the FF and AR approaches display excellent agreement. We also\nshow explicitly roles of electron correlation effects in the evaluation of $D$\nvalues accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By critically evaluating higher-order nonlinear effects to the isotope shifts\n(ISs) in the low-lying transition frequencies of the singly charged calcium\nion, stringent constraint on the electron-neutron coupling due to a\nhypothetical boson describing physics beyond the Standard Model is inferred. It\nshows an order magnitude difference compared to the previously reported limit\ndemonstrating importance of higher-order effects in the analysis of\nnonlinearity in the King's plot. The first-order IS parameters and enhancement\nfactor ($D$) were evaluated using two complementary approaches in the\nrelativistic coupled-cluster theory framework: namely finite-field (FF) and\nanalytical response (AR) approaches. Extraction of the second-order IS\nparameters in the FF approach show numerical instabilities, so they are\ndetermined in the AR approach. Comparison of these factors with previous\ncalculation shows substantial differences in the magnitudes. However, $D$\nvalues from both the FF and AR approaches display excellent agreement. We also\nshow explicitly roles of electron correlation effects in the evaluation of $D$\nvalues accurately."
                },
                "authors": [
                    {
                        "name": "Vaibhav Katyal"
                    },
                    {
                        "name": "A. Chakraborty"
                    },
                    {
                        "name": "B. K. Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "B. K. Sahoo"
                },
                "author": "B. K. Sahoo",
                "arxiv_comment": "v1: 5 pages, 3 tables, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24397v1",
                "updated": "2025-10-28T13:11:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    11,
                    22,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:11:22Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    11,
                    22,
                    1,
                    301,
                    0
                ],
                "title": "APTBench: Benchmarking Agentic Potential of Base LLMs During\n  Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APTBench: Benchmarking Agentic Potential of Base LLMs During\n  Pre-Training"
                },
                "summary": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training."
                },
                "authors": [
                    {
                        "name": "Jiarui Qin"
                    },
                    {
                        "name": "Yunjia Xi"
                    },
                    {
                        "name": "Junjie Huang"
                    },
                    {
                        "name": "Renting Rui"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "46 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22705v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22705v2",
                "updated": "2025-10-28T13:08:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    8,
                    3,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-26T14:56:06Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    14,
                    56,
                    6,
                    6,
                    299,
                    0
                ],
                "title": "Using \"AI Poincare\" to analyze non-linear integrable optics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using \"AI Poincare\" to analyze non-linear integrable optics"
                },
                "summary": "This study dives into the applicability of using automated discovery of\nconserved quantities in dynamical systems relevant to accelerator physics.\nSpecifically, we explore the performance of AI Poincar\\'e in analyzing\nnumerical trajectory data obtained using the McMillan system of non-linear\nintegrable optics. A comprehensive evaluation of the algorithm's performance is\nconducted through diverse methodologies. These include the analysis of the\nestimated number of conserved quantities embedded in a dataset and the\ndeviation of interpolated points on the inferred manifold with respect to\npoints in actually in the dataset. the investigation identifies an optimal\nrange of perturbation distances where the underlying manifold extraction\nalgorithm inside AI Poincar\\'e exhibits optimal performance. Additionally, an\nimproved neural network architecture is proposed based on the observed results.\nFinally, we apply the algorithm to preliminary experimental data from the\nIntegrable Optics Test Accelerator at Fermilab to successfully infer the number\nof conserved quantities even in the presence of fast decoherence of the\nmeasured signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study dives into the applicability of using automated discovery of\nconserved quantities in dynamical systems relevant to accelerator physics.\nSpecifically, we explore the performance of AI Poincar\\'e in analyzing\nnumerical trajectory data obtained using the McMillan system of non-linear\nintegrable optics. A comprehensive evaluation of the algorithm's performance is\nconducted through diverse methodologies. These include the analysis of the\nestimated number of conserved quantities embedded in a dataset and the\ndeviation of interpolated points on the inferred manifold with respect to\npoints in actually in the dataset. the investigation identifies an optimal\nrange of perturbation distances where the underlying manifold extraction\nalgorithm inside AI Poincar\\'e exhibits optimal performance. Additionally, an\nimproved neural network architecture is proposed based on the observed results.\nFinally, we apply the algorithm to preliminary experimental data from the\nIntegrable Optics Test Accelerator at Fermilab to successfully infer the number\nof conserved quantities even in the presence of fast decoherence of the\nmeasured signal."
                },
                "authors": [
                    {
                        "name": "Lazare Osmanov"
                    },
                    {
                        "name": "Nilanjan Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Nilanjan Banerjee"
                },
                "author": "Nilanjan Banerjee",
                "arxiv_comment": "12 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22705v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22705v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24390v1",
                "updated": "2025-10-28T13:05:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    5,
                    23,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:05:23Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    5,
                    23,
                    1,
                    301,
                    0
                ],
                "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and\n  Logic-Parallel Content Expansion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and\n  Logic-Parallel Content Expansion"
                },
                "summary": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies."
                },
                "authors": [
                    {
                        "name": "Xianjun Gao"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26080v2",
                "updated": "2025-10-28T13:00:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    0,
                    46,
                    1,
                    301,
                    0
                ],
                "published": "2025-09-30T10:53:54Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    10,
                    53,
                    54,
                    1,
                    273,
                    0
                ],
                "title": "Evaluating the Use of Large Language Models as Synthetic Social Agents\n  in Social Science Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Use of Large Language Models as Synthetic Social Agents\n  in Social Science Research"
                },
                "summary": "Large Language Models (LLMs) are being increasingly used as synthetic agents\nin social science, in applications ranging from augmenting survey responses to\npowering multi-agent simulations. This paper outlines cautions that should be\ntaken when interpreting LLM outputs and proposes a pragmatic reframing for the\nsocial sciences in which LLMs are used as high-capacity pattern matchers for\nquasi-predictive interpolation under explicit scope conditions and not as\nsubstitutes for probabilistic inference. Practical guardrails such as\nindependent draws, preregistered human baselines, reliability-aware validation,\nand subgroup calibration, are introduced so that researchers may engage in\nuseful prototyping and forecasting while avoiding category errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being increasingly used as synthetic agents\nin social science, in applications ranging from augmenting survey responses to\npowering multi-agent simulations. This paper outlines cautions that should be\ntaken when interpreting LLM outputs and proposes a pragmatic reframing for the\nsocial sciences in which LLMs are used as high-capacity pattern matchers for\nquasi-predictive interpolation under explicit scope conditions and not as\nsubstitutes for probabilistic inference. Practical guardrails such as\nindependent draws, preregistered human baselines, reliability-aware validation,\nand subgroup calibration, are introduced so that researchers may engage in\nuseful prototyping and forecasting while avoiding category errors."
                },
                "authors": [
                    {
                        "name": "Emma Rose Madden"
                    }
                ],
                "author_detail": {
                    "name": "Emma Rose Madden"
                },
                "author": "Emma Rose Madden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14788v2",
                "updated": "2025-10-28T12:58:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    58,
                    38,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-16T15:20:49Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    20,
                    49,
                    3,
                    289,
                    0
                ],
                "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Scenario Unified Modeling of User Interests at Billion Scale"
                },
                "summary": "User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms."
                },
                "authors": [
                    {
                        "name": "Manjie Xu"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Jingyi Zhou"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Zejian Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Kai Zuo"
                    },
                    {
                        "name": "Yibo Chen"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Yixin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Zhu"
                },
                "author": "Yixin Zhu",
                "arxiv_comment": "https://github.com/ariesssxu/RedSeqRec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24378v1",
                "updated": "2025-10-28T12:56:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    56,
                    48,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:56:48Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    56,
                    48,
                    1,
                    301,
                    0
                ],
                "title": "Stroke Lesion Segmentation in Clinical Workflows: A Modular,\n  Lightweight, and Deployment-Ready Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stroke Lesion Segmentation in Clinical Workflows: A Modular,\n  Lightweight, and Deployment-Ready Tool"
                },
                "summary": "Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools."
                },
                "authors": [
                    {
                        "name": "Yann Kerverdo"
                    },
                    {
                        "name": "Florent Leray"
                    },
                    {
                        "name": "Youwan Mahé"
                    },
                    {
                        "name": "Stéphanie Leplaideur"
                    },
                    {
                        "name": "Francesca Galassi"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Galassi"
                },
                "author": "Francesca Galassi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24368v1",
                "updated": "2025-10-28T12:45:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    45,
                    20,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:45:20Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    45,
                    20,
                    1,
                    301,
                    0
                ],
                "title": "Filtering instances and rejecting predictions to obtain reliable models\n  in healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filtering instances and rejecting predictions to obtain reliable models\n  in healthcare"
                },
                "summary": "Machine Learning (ML) models are widely used in high-stakes domains such as\nhealthcare, where the reliability of predictions is critical. However, these\nmodels often fail to account for uncertainty, providing predictions even with\nlow confidence. This work proposes a novel two-step data-centric approach to\nenhance the performance of ML models by improving data quality and filtering\nlow-confidence predictions. The first step involves leveraging Instance\nHardness (IH) to filter problematic instances during training, thereby refining\nthe dataset. The second step introduces a confidence-based rejection mechanism\nduring inference, ensuring that only reliable predictions are retained. We\nevaluate our approach using three real-world healthcare datasets, demonstrating\nits effectiveness at improving model reliability while balancing predictive\nperformance and rejection rate. Additionally, we use alternative criteria -\ninfluence values for filtering and uncertainty for rejection - as baselines to\nevaluate the efficiency of the proposed method. The results demonstrate that\nintegrating IH filtering with confidence-based rejection effectively enhances\nmodel performance while preserving a large proportion of instances. This\napproach provides a practical method for deploying ML systems in\nsafety-critical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning (ML) models are widely used in high-stakes domains such as\nhealthcare, where the reliability of predictions is critical. However, these\nmodels often fail to account for uncertainty, providing predictions even with\nlow confidence. This work proposes a novel two-step data-centric approach to\nenhance the performance of ML models by improving data quality and filtering\nlow-confidence predictions. The first step involves leveraging Instance\nHardness (IH) to filter problematic instances during training, thereby refining\nthe dataset. The second step introduces a confidence-based rejection mechanism\nduring inference, ensuring that only reliable predictions are retained. We\nevaluate our approach using three real-world healthcare datasets, demonstrating\nits effectiveness at improving model reliability while balancing predictive\nperformance and rejection rate. Additionally, we use alternative criteria -\ninfluence values for filtering and uncertainty for rejection - as baselines to\nevaluate the efficiency of the proposed method. The results demonstrate that\nintegrating IH filtering with confidence-based rejection effectively enhances\nmodel performance while preserving a large proportion of instances. This\napproach provides a practical method for deploying ML systems in\nsafety-critical applications."
                },
                "authors": [
                    {
                        "name": "Maria Gabriela Valeriano"
                    },
                    {
                        "name": "David Kohan Marzagão"
                    },
                    {
                        "name": "Alfredo Montelongo"
                    },
                    {
                        "name": "Carlos Roberto Veiga Kiffer"
                    },
                    {
                        "name": "Natan Katz"
                    },
                    {
                        "name": "Ana Carolina Lorena"
                    }
                ],
                "author_detail": {
                    "name": "Ana Carolina Lorena"
                },
                "author": "Ana Carolina Lorena",
                "arxiv_comment": "This paper is under review at Machine Learning (Springer)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00175v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00175v3",
                "updated": "2025-10-28T12:44:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    44,
                    57,
                    1,
                    301,
                    0
                ],
                "published": "2025-09-30T18:51:16Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    18,
                    51,
                    16,
                    1,
                    273,
                    0
                ],
                "title": "Icy or rocky? Convective or stable? New interior models of Uranus and\n  Neptune",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Icy or rocky? Convective or stable? New interior models of Uranus and\n  Neptune"
                },
                "summary": "We present a new framework for constructing agnostic and yet physical models\nfor planetary interiors and apply it to Uranus and Neptune. Unlike previous\nresearch that either impose rigid assumptions or rely on simplified empirical\nprofiles, our approach bridges both paradigms. Starting from randomly generated\ndensity profiles, we applied an iterative algorithm that converges towards\nmodels that simultaneously satisfy hydrostatic equilibrium, match the observed\ngravitational moments, and remain thermodynamically and compositionally\nconsistent. The inferred interior models for Uranus and Neptune span a wide\nrange of possible interior structures, in particular encompassing both\nwater-dominated and rock-dominated configurations (rock-to-water mass ratios\nbetween 0.04-3.92 for Uranus and 0.20-1.78 for Neptune). All models contain\nconvective regions with ionic water and have temperature-pressure profiles that\nremain above the demixing curves for hydrogen-helium-water mixtures. This\noffers both a plausible explanation for the observed non-dipolar magnetic\nfields and indicates that no hydrogen-helium-water demixing occurs. We find a\nhigher H-He mass fraction in the outermost convection zones for Uranus\n(0.62-0.73) compared to Neptune (0.25-0.49) and that Uranus' magnetic field is\nlikely generated deeper in the interior compared to Neptune. We infer upper\nlimits of 0.69-0.74 (Uranus) versus 0.78-0.92 (Neptune) for the outer edges of\nthe dynamo regions in units of normalised radii. Overall, our findings\nchallenge the conventional classification of Uranus and Neptune as 'ice giants'\nand underscore the need for improved observational data or formation\nconstraints to break compositional degeneracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new framework for constructing agnostic and yet physical models\nfor planetary interiors and apply it to Uranus and Neptune. Unlike previous\nresearch that either impose rigid assumptions or rely on simplified empirical\nprofiles, our approach bridges both paradigms. Starting from randomly generated\ndensity profiles, we applied an iterative algorithm that converges towards\nmodels that simultaneously satisfy hydrostatic equilibrium, match the observed\ngravitational moments, and remain thermodynamically and compositionally\nconsistent. The inferred interior models for Uranus and Neptune span a wide\nrange of possible interior structures, in particular encompassing both\nwater-dominated and rock-dominated configurations (rock-to-water mass ratios\nbetween 0.04-3.92 for Uranus and 0.20-1.78 for Neptune). All models contain\nconvective regions with ionic water and have temperature-pressure profiles that\nremain above the demixing curves for hydrogen-helium-water mixtures. This\noffers both a plausible explanation for the observed non-dipolar magnetic\nfields and indicates that no hydrogen-helium-water demixing occurs. We find a\nhigher H-He mass fraction in the outermost convection zones for Uranus\n(0.62-0.73) compared to Neptune (0.25-0.49) and that Uranus' magnetic field is\nlikely generated deeper in the interior compared to Neptune. We infer upper\nlimits of 0.69-0.74 (Uranus) versus 0.78-0.92 (Neptune) for the outer edges of\nthe dynamo regions in units of normalised radii. Overall, our findings\nchallenge the conventional classification of Uranus and Neptune as 'ice giants'\nand underscore the need for improved observational data or formation\nconstraints to break compositional degeneracy."
                },
                "authors": [
                    {
                        "name": "Luca Morf"
                    },
                    {
                        "name": "Ravit Helled"
                    }
                ],
                "author_detail": {
                    "name": "Ravit Helled"
                },
                "author": "Ravit Helled",
                "arxiv_comment": "Accepted for publication in A&A; v2: Language compliance update with\n  A&A guidelines; v3: Unit corrections for the entropies depicted in Figures\n  A.1 and A.3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00175v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00175v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24367v1",
                "updated": "2025-10-28T12:44:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    44,
                    54,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:44:54Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    44,
                    54,
                    1,
                    301,
                    0
                ],
                "title": "LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and\n  the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and\n  the Road Ahead"
                },
                "summary": "The rapid integration of Large Language Models (LLMs) into software\nengineering (SE) has revolutionized tasks like code generation, producing a\nmassive volume of software artifacts. This surge has exposed a critical\nbottleneck: the lack of scalable, reliable methods to evaluate these outputs.\nHuman evaluation is costly and time-consuming, while traditional automated\nmetrics like BLEU fail to capture nuanced quality aspects. In response, the\nLLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.\nThis approach leverages the advanced reasoning of LLMs, offering a path toward\nhuman-like nuance at automated scale. However, LLM-as-a-Judge research in SE is\nstill in its early stages. This forward-looking SE 2030 paper aims to steer the\ncommunity toward advancing LLM-as-a-Judge for evaluating LLM-generated software\nartifacts. We provide a literature review of existing SE studies, analyze their\nlimitations, identify key research gaps, and outline a detailed roadmap. We\nenvision these frameworks as reliable, robust, and scalable human surrogates\ncapable of consistent, multi-faceted artifact evaluation by 2030. Our work aims\nto foster research and adoption of LLM-as-a-Judge frameworks, ultimately\nimproving the scalability of software artifact evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid integration of Large Language Models (LLMs) into software\nengineering (SE) has revolutionized tasks like code generation, producing a\nmassive volume of software artifacts. This surge has exposed a critical\nbottleneck: the lack of scalable, reliable methods to evaluate these outputs.\nHuman evaluation is costly and time-consuming, while traditional automated\nmetrics like BLEU fail to capture nuanced quality aspects. In response, the\nLLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.\nThis approach leverages the advanced reasoning of LLMs, offering a path toward\nhuman-like nuance at automated scale. However, LLM-as-a-Judge research in SE is\nstill in its early stages. This forward-looking SE 2030 paper aims to steer the\ncommunity toward advancing LLM-as-a-Judge for evaluating LLM-generated software\nartifacts. We provide a literature review of existing SE studies, analyze their\nlimitations, identify key research gaps, and outline a detailed roadmap. We\nenvision these frameworks as reliable, robust, and scalable human surrogates\ncapable of consistent, multi-faceted artifact evaluation by 2030. Our work aims\nto foster research and adoption of LLM-as-a-Judge frameworks, ultimately\nimproving the scalability of software artifact evaluation."
                },
                "authors": [
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Christoph Treude"
                    },
                    {
                        "name": "Jiamou Sun"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Xiaoning Du"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11842v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11842v3",
                "updated": "2025-10-28T12:44:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    44,
                    7,
                    1,
                    301,
                    0
                ],
                "published": "2025-05-17T05:06:38Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    5,
                    6,
                    38,
                    5,
                    137,
                    0
                ],
                "title": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs"
                },
                "summary": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies."
                },
                "authors": [
                    {
                        "name": "Xuannan Liu"
                    },
                    {
                        "name": "Zekun Li"
                    },
                    {
                        "name": "Zheqi He"
                    },
                    {
                        "name": "Peipei Li"
                    },
                    {
                        "name": "Shuhan Xia"
                    },
                    {
                        "name": "Xing Cui"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "Accepted by NeurIPS 2025 Dataset and Benchmark Track, Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11842v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11842v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24365v1",
                "updated": "2025-10-28T12:41:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    41,
                    10,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:41:10Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    41,
                    10,
                    1,
                    301,
                    0
                ],
                "title": "Text Simplification with Sentence Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Simplification with Sentence Embeddings"
                },
                "summary": "Sentence embeddings can be decoded to give approximations of the original\ntexts used to create them. We explore this effect in the context of text\nsimplification, demonstrating that reconstructed text embeddings preserve\ncomplexity levels. We experiment with a small feed forward neural network to\neffectively learn a transformation between sentence embeddings representing\nhigh-complexity and low-complexity texts. We provide comparison to a Seq2Seq\nand LLM-based approach, showing encouraging results in our much smaller\nlearning setting. Finally, we demonstrate the applicability of our\ntransformation to an unseen simplification dataset (MedEASI), as well as\ndatasets from languages outside the training data (ES,DE). We conclude that\nlearning transformations in sentence embedding space is a promising direction\nfor future research and has potential to unlock the ability to develop small,\nbut powerful models for text simplification and other natural language\ngeneration tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentence embeddings can be decoded to give approximations of the original\ntexts used to create them. We explore this effect in the context of text\nsimplification, demonstrating that reconstructed text embeddings preserve\ncomplexity levels. We experiment with a small feed forward neural network to\neffectively learn a transformation between sentence embeddings representing\nhigh-complexity and low-complexity texts. We provide comparison to a Seq2Seq\nand LLM-based approach, showing encouraging results in our much smaller\nlearning setting. Finally, we demonstrate the applicability of our\ntransformation to an unseen simplification dataset (MedEASI), as well as\ndatasets from languages outside the training data (ES,DE). We conclude that\nlearning transformations in sentence embedding space is a promising direction\nfor future research and has potential to unlock the ability to develop small,\nbut powerful models for text simplification and other natural language\ngeneration tasks."
                },
                "authors": [
                    {
                        "name": "Matthew Shardlow"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Shardlow"
                },
                "author": "Matthew Shardlow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24362v1",
                "updated": "2025-10-28T12:35:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    35,
                    33,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:35:33Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    35,
                    33,
                    1,
                    301,
                    0
                ],
                "title": "Implicit quantile preferences of the Fed and the Taylor rule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit quantile preferences of the Fed and the Taylor rule"
                },
                "summary": "We study optimal monetary policy when a central bank maximizes a quantile\nutility objective rather than expected utility. In our framework, the central\nbank's risk attitude is indexed by the quantile index level, providing a\ntransparent mapping between hawkish/dovish stances and attention to adverse\nmacroeconomic realizations. We formulate the infinite-horizon problem using a\nBellman equation with the quantile operator. Implementing an Euler-equation\napproach, we derive Taylor-rule-type reaction functions. Using an indirect\ninference approach, we derive a central bank risk aversion implicit quantile\nindex. An empirical implementation for the US is outlined based on reduced-form\nlaws of motion with conditional heteroskedasticity, enabling estimation of the\nnew monetary policy rule and its dependence on the Fed risk attitudes. The\nresults reveal that the Fed has mostly a dovish-type behavior but with some\nperiods of hawkish attitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study optimal monetary policy when a central bank maximizes a quantile\nutility objective rather than expected utility. In our framework, the central\nbank's risk attitude is indexed by the quantile index level, providing a\ntransparent mapping between hawkish/dovish stances and attention to adverse\nmacroeconomic realizations. We formulate the infinite-horizon problem using a\nBellman equation with the quantile operator. Implementing an Euler-equation\napproach, we derive Taylor-rule-type reaction functions. Using an indirect\ninference approach, we derive a central bank risk aversion implicit quantile\nindex. An empirical implementation for the US is outlined based on reduced-form\nlaws of motion with conditional heteroskedasticity, enabling estimation of the\nnew monetary policy rule and its dependence on the Fed risk attitudes. The\nresults reveal that the Fed has mostly a dovish-type behavior but with some\nperiods of hawkish attitudes."
                },
                "authors": [
                    {
                        "name": "Gabriel Montes-Rojas"
                    },
                    {
                        "name": "Fernando Toledo"
                    },
                    {
                        "name": "Nicolás Bertholet"
                    },
                    {
                        "name": "Kevin Corfield"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Corfield"
                },
                "author": "Kevin Corfield",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09536v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09536v3",
                "updated": "2025-10-28T12:34:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    34,
                    26,
                    1,
                    301,
                    0
                ],
                "published": "2024-08-18T16:44:01Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    16,
                    44,
                    1,
                    6,
                    231,
                    0
                ],
                "title": "Galapagos: Automated N-Version Programming with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galapagos: Automated N-Version Programming with LLMs"
                },
                "summary": "N-Version Programming is a well-known methodology for developing\nfault-tolerant systems. It achieves fault detection and correction at runtime\nby adding diverse redundancy into programs, minimizing fault mode overlap\nbetween redundant program variants. In this work, we propose the automated\ngeneration of program variants using large language models. We design, develop\nand evaluate Gal\\'apagos: a tool for generating program variants using LLMs,\nvalidating their correctness and equivalence, and using them to assemble\nN-Version binaries. We evaluate Gal\\'apagos by creating N-Version components of\nreal-world C code. Our original results show that Gal\\'apagos can produce\nprogram variants that are proven to be functionally equivalent, even when the\nvariants are written in a different programming language. Our systematic\ndiversity measurement indicates that functionally equivalent variants produced\nby Gal\\'apagos, are statically different after compilation, and present\ndiverging internal behavior at runtime. We demonstrate that the variants\nproduced by Gal\\'apagos can protect C code against real miscompilation bugs\nwhich affect the Clang compiler. Overall, our paper shows that producing\nN-Version software can be drastically automated by advanced usage of practical\nformal verification and generative language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-Version Programming is a well-known methodology for developing\nfault-tolerant systems. It achieves fault detection and correction at runtime\nby adding diverse redundancy into programs, minimizing fault mode overlap\nbetween redundant program variants. In this work, we propose the automated\ngeneration of program variants using large language models. We design, develop\nand evaluate Gal\\'apagos: a tool for generating program variants using LLMs,\nvalidating their correctness and equivalence, and using them to assemble\nN-Version binaries. We evaluate Gal\\'apagos by creating N-Version components of\nreal-world C code. Our original results show that Gal\\'apagos can produce\nprogram variants that are proven to be functionally equivalent, even when the\nvariants are written in a different programming language. Our systematic\ndiversity measurement indicates that functionally equivalent variants produced\nby Gal\\'apagos, are statically different after compilation, and present\ndiverging internal behavior at runtime. We demonstrate that the variants\nproduced by Gal\\'apagos can protect C code against real miscompilation bugs\nwhich affect the Clang compiler. Overall, our paper shows that producing\nN-Version software can be drastically automated by advanced usage of practical\nformal verification and generative language models."
                },
                "authors": [
                    {
                        "name": "Javier Ron"
                    },
                    {
                        "name": "Diogo Gaspar"
                    },
                    {
                        "name": "Javier Cabrera-Arteaga"
                    },
                    {
                        "name": "Benoit Baudry"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09536v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09536v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07883v2",
                "updated": "2025-10-28T12:30:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    30,
                    22,
                    1,
                    301,
                    0
                ],
                "published": "2024-05-13T16:17:10Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    16,
                    17,
                    10,
                    0,
                    134,
                    0
                ],
                "title": "Zero-Shot Tokenizer Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Tokenizer Transfer"
                },
                "summary": "Language models (LMs) are bound to their tokenizer, which maps raw text to a\nsequence of vocabulary items (tokens). This restricts their flexibility: for\nexample, LMs trained primarily on English may still perform well in other\nnatural and programming languages, but have vastly decreased efficiency due to\ntheir English-centric tokenizer. To mitigate this, we should be able to swap\nthe original LM tokenizer with an arbitrary one, on the fly, without degrading\nperformance. Hence, in this work we define a new problem: Zero-Shot Tokenizer\nTransfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for\nthe tokens in the vocabulary of the new tokenizer. Since prior heuristics for\ninitializing embeddings often perform at chance level in a ZeTT setting, we\npropose a new solution: we train a hypernetwork taking a tokenizer as input and\npredicting the corresponding embeddings. We empirically demonstrate that the\nhypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and\ndecoder LLMs (e.g., Mistral-7B). Our method comes close to the original models'\nperformance in cross-lingual and coding tasks while markedly reducing the\nlength of the tokenized sequence. We also find that the remaining gap can be\nquickly closed by continued training on less than 1B tokens. Finally, we show\nthat a ZeTT hypernetwork trained for a base (L)LM can also be applied to\nfine-tuned variants without extra training. Overall, our results make\nsubstantial strides toward detaching LMs from their tokenizer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) are bound to their tokenizer, which maps raw text to a\nsequence of vocabulary items (tokens). This restricts their flexibility: for\nexample, LMs trained primarily on English may still perform well in other\nnatural and programming languages, but have vastly decreased efficiency due to\ntheir English-centric tokenizer. To mitigate this, we should be able to swap\nthe original LM tokenizer with an arbitrary one, on the fly, without degrading\nperformance. Hence, in this work we define a new problem: Zero-Shot Tokenizer\nTransfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for\nthe tokens in the vocabulary of the new tokenizer. Since prior heuristics for\ninitializing embeddings often perform at chance level in a ZeTT setting, we\npropose a new solution: we train a hypernetwork taking a tokenizer as input and\npredicting the corresponding embeddings. We empirically demonstrate that the\nhypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and\ndecoder LLMs (e.g., Mistral-7B). Our method comes close to the original models'\nperformance in cross-lingual and coding tasks while markedly reducing the\nlength of the tokenized sequence. We also find that the remaining gap can be\nquickly closed by continued training on less than 1B tokens. Finally, we show\nthat a ZeTT hypernetwork trained for a base (L)LM can also be applied to\nfine-tuned variants without extra training. Overall, our results make\nsubstantial strides toward detaching LMs from their tokenizer."
                },
                "authors": [
                    {
                        "name": "Benjamin Minixhofer"
                    },
                    {
                        "name": "Edoardo Maria Ponti"
                    },
                    {
                        "name": "Ivan Vulić"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Vulić"
                },
                "author": "Ivan Vulić",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24358v1",
                "updated": "2025-10-28T12:26:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    26,
                    45,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:26:45Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    26,
                    45,
                    1,
                    301,
                    0
                ],
                "title": "Automatically Benchmarking LLM Code Agents through Agent-Driven\n  Annotation and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Benchmarking LLM Code Agents through Agent-Driven\n  Annotation and Evaluation"
                },
                "summary": "Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation."
                },
                "authors": [
                    {
                        "name": "Lingyue Fu"
                    },
                    {
                        "name": "Bolun Zhang"
                    },
                    {
                        "name": "Hao Guan"
                    },
                    {
                        "name": "Yaoming Zhu"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Yong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Yu"
                },
                "author": "Yong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24352v1",
                "updated": "2025-10-28T12:17:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    17,
                    31,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:17:31Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    17,
                    31,
                    1,
                    301,
                    0
                ],
                "title": "Self-Normalized Quantile Empirical Saddlepoint Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Normalized Quantile Empirical Saddlepoint Approximation"
                },
                "summary": "We propose a density-free method for frequentist inference on population\nquantiles, termed Self-Normalized Quantile Empirical Saddlepoint Approximation\n(SNQESA). The approach builds a self-normalized pivot from the indicator score\nfor a fixed quantile threshold and then employs a constrained empirical\nsaddlepoint approximation to obtain highly accurate tail probabilities.\nInverting these tail areas yields confidence intervals and tests without\nestimating the unknown density at the target quantile, thereby eliminating\nbandwidth selection and the boundary issues that affect kernel-based\nWald/Hall-Sheather intervals. Under mild local regularity, the resulting\nprocedures attain higher-order tail accuracy and second-order coverage after\ninversion. Because the pivot is anchored in a bounded Bernoulli reduction, the\nmethod remains reliable for skewed and heavy-tailed distributions and for\nextreme quantiles. Extensive Monte Carlo experiments across light, heavy, and\nmultimodal distributions demonstrate that SNQESA delivers stable coverage and\ncompetitive interval lengths in small to moderate samples while being orders of\nmagnitude faster than large-B resampling schemes. An empirical study on\nValue-at-Risk with rolling windows further highlights the gains in tail\nperformance and computational efficiency. The framework naturally extends to\ntwo-sample quantile differences and to regression-type settings, offering a\npractical, analytically transparent alternative to kernel, bootstrap, and\nempirical-likelihood methods for distribution-free quantile inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a density-free method for frequentist inference on population\nquantiles, termed Self-Normalized Quantile Empirical Saddlepoint Approximation\n(SNQESA). The approach builds a self-normalized pivot from the indicator score\nfor a fixed quantile threshold and then employs a constrained empirical\nsaddlepoint approximation to obtain highly accurate tail probabilities.\nInverting these tail areas yields confidence intervals and tests without\nestimating the unknown density at the target quantile, thereby eliminating\nbandwidth selection and the boundary issues that affect kernel-based\nWald/Hall-Sheather intervals. Under mild local regularity, the resulting\nprocedures attain higher-order tail accuracy and second-order coverage after\ninversion. Because the pivot is anchored in a bounded Bernoulli reduction, the\nmethod remains reliable for skewed and heavy-tailed distributions and for\nextreme quantiles. Extensive Monte Carlo experiments across light, heavy, and\nmultimodal distributions demonstrate that SNQESA delivers stable coverage and\ncompetitive interval lengths in small to moderate samples while being orders of\nmagnitude faster than large-B resampling schemes. An empirical study on\nValue-at-Risk with rolling windows further highlights the gains in tail\nperformance and computational efficiency. The framework naturally extends to\ntwo-sample quantile differences and to regression-type settings, offering a\npractical, analytically transparent alternative to kernel, bootstrap, and\nempirical-likelihood methods for distribution-free quantile inference."
                },
                "authors": [
                    {
                        "name": "Hou Jian"
                    },
                    {
                        "name": "Meng Tan"
                    },
                    {
                        "name": "Tian Maozai"
                    }
                ],
                "author_detail": {
                    "name": "Tian Maozai"
                },
                "author": "Tian Maozai",
                "arxiv_comment": "24 pages, 3 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24345v1",
                "updated": "2025-10-28T12:11:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    11,
                    12,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:11:12Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    11,
                    12,
                    1,
                    301,
                    0
                ],
                "title": "LongWeave: A Long-Form Generation Benchmark Bridging Real-World\n  Relevance and Verifiability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongWeave: A Long-Form Generation Benchmark Bridging Real-World\n  Relevance and Verifiability"
                },
                "summary": "Generating long, informative, and factual outputs remains a major challenge\nfor Large Language Models (LLMs). Existing benchmarks for long-form generation\ntypically assess real-world queries with hard-to-verify metrics or use\nsynthetic setups that ease evaluation but overlook real-world intricacies. In\nthis paper, we introduce \\textbf{LongWeave}, which balances real-world and\nverifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval\nconstructs tasks by first defining verifiable targets within real-world\nscenarios, then systematically generating corresponding queries, textual\nmaterials, and constraints based on these targets. This ensures that tasks are\nboth realistic and objectively assessable, enabling rigorous assessment of\nmodel capabilities in meeting complex real-world constraints. LongWeave\nsupports customizable input/output lengths (up to 64K/8K tokens) across seven\ndistinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models\nencounter significant challenges in long-form generation as real-world\ncomplexity and output length increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, informative, and factual outputs remains a major challenge\nfor Large Language Models (LLMs). Existing benchmarks for long-form generation\ntypically assess real-world queries with hard-to-verify metrics or use\nsynthetic setups that ease evaluation but overlook real-world intricacies. In\nthis paper, we introduce \\textbf{LongWeave}, which balances real-world and\nverifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval\nconstructs tasks by first defining verifiable targets within real-world\nscenarios, then systematically generating corresponding queries, textual\nmaterials, and constraints based on these targets. This ensures that tasks are\nboth realistic and objectively assessable, enabling rigorous assessment of\nmodel capabilities in meeting complex real-world constraints. LongWeave\nsupports customizable input/output lengths (up to 64K/8K tokens) across seven\ndistinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models\nencounter significant challenges in long-form generation as real-world\ncomplexity and output length increase."
                },
                "authors": [
                    {
                        "name": "Zikai Xiao"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Jianhui Wei"
                    },
                    {
                        "name": "Wen Ma"
                    },
                    {
                        "name": "Yuxuan Zhou"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Zuozhu Liu"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "arxiv_comment": "EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24340v1",
                "updated": "2025-10-28T12:08:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    8,
                    23,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:08:23Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    8,
                    23,
                    1,
                    301,
                    0
                ],
                "title": "On the Dusty Proximate Damped Lyman-$α$ System towards Q2310-3358\n  at $z=2.40$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Dusty Proximate Damped Lyman-$α$ System towards Q2310-3358\n  at $z=2.40$"
                },
                "summary": "Quasar absorption systems not only affect the way quasars are selected, but\nalso serve as key probes of galaxies, providing insight into their chemical\nevolution and interstellar medium (ISM). Recently, a method based on Gaia\nastrometric measurements has aided the selection of quasars reddened by dust\nhitherto overlooked. We conducted a spectroscopic study using VLT/X-Shooter on\none such dust-reddened quasar, Q2310-3358. This quasar, at $z =\n2.3908\\pm0.0003$, is associated with a Damped Lyman-alpha absorber (DLA) at\nnearly the same redshift $2.4007\\pm0.0003$, with a neutral hydrogen column\ndensity of $\\log N(\\mathrm{H\\,I}) = 21.214 \\pm 0.003$. The DLA is very\nmetal-rich (close to the Solar metallicity after correction for depletion on\ndust grains). Its properties align with the metal-to-dust ratio and the\nmass-metallicity relation established in previous large samples of DLAs.\nSurprisingly, given its proximity to the quasar in redshift, the absorber has\nstrong cold gas characteristics, including CI and H$_2$. Based on the derived\nkinetic temperature of $71^{+28}_{-15}$~K, we infer the presence of a strong UV\nradiation field, which in turn suggests that the quasar and the DLA are in\nclose proximity, i.e. part of the same galaxy and not just different objects in\nthe same overdensity of galaxies. We use the line ratios of the CI\nfine-structure lines to constrain the density of the cold gas, yielding $n_{\\rm\nH} \\sim 10^{3}~\\mathrm{cm}^{-3}$. Our analysis extends the understanding of\n$z_{abs} \\approx z_{em}$ absorption line systems and provides valuable\nconstraints on the interplay between dust, metals, and neutral gas in the ISM\nof early galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasar absorption systems not only affect the way quasars are selected, but\nalso serve as key probes of galaxies, providing insight into their chemical\nevolution and interstellar medium (ISM). Recently, a method based on Gaia\nastrometric measurements has aided the selection of quasars reddened by dust\nhitherto overlooked. We conducted a spectroscopic study using VLT/X-Shooter on\none such dust-reddened quasar, Q2310-3358. This quasar, at $z =\n2.3908\\pm0.0003$, is associated with a Damped Lyman-alpha absorber (DLA) at\nnearly the same redshift $2.4007\\pm0.0003$, with a neutral hydrogen column\ndensity of $\\log N(\\mathrm{H\\,I}) = 21.214 \\pm 0.003$. The DLA is very\nmetal-rich (close to the Solar metallicity after correction for depletion on\ndust grains). Its properties align with the metal-to-dust ratio and the\nmass-metallicity relation established in previous large samples of DLAs.\nSurprisingly, given its proximity to the quasar in redshift, the absorber has\nstrong cold gas characteristics, including CI and H$_2$. Based on the derived\nkinetic temperature of $71^{+28}_{-15}$~K, we infer the presence of a strong UV\nradiation field, which in turn suggests that the quasar and the DLA are in\nclose proximity, i.e. part of the same galaxy and not just different objects in\nthe same overdensity of galaxies. We use the line ratios of the CI\nfine-structure lines to constrain the density of the cold gas, yielding $n_{\\rm\nH} \\sim 10^{3}~\\mathrm{cm}^{-3}$. Our analysis extends the understanding of\n$z_{abs} \\approx z_{em}$ absorption line systems and provides valuable\nconstraints on the interplay between dust, metals, and neutral gas in the ISM\nof early galaxies."
                },
                "authors": [
                    {
                        "name": "S. Han"
                    },
                    {
                        "name": "J. -K. Krogager"
                    },
                    {
                        "name": "C. Ledoux"
                    },
                    {
                        "name": "G. Ma"
                    },
                    {
                        "name": "K. E. Heintz"
                    },
                    {
                        "name": "S. J. Geier"
                    },
                    {
                        "name": "L. Christensen"
                    },
                    {
                        "name": "P. Møller"
                    },
                    {
                        "name": "J. P. U. Fynbo"
                    }
                ],
                "author_detail": {
                    "name": "J. P. U. Fynbo"
                },
                "arxiv_affiliation": "Niels Bohr Institute, University of Copenhagen",
                "author": "J. P. U. Fynbo",
                "arxiv_comment": "8 pages, 8 figures, 2 tables.This version corresponds to the\n  manuscript submitted to A&A on Oct 28, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24339v2",
                "updated": "2025-10-29T04:05:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    5,
                    13,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-28T12:07:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    7,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science\n  Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science\n  Automation"
                },
                "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation."
                },
                "authors": [
                    {
                        "name": "Yunxuan Jiang"
                    },
                    {
                        "name": "Silan Hu"
                    },
                    {
                        "name": "Xiaoning Wang"
                    },
                    {
                        "name": "Yuanyuan Zhang"
                    },
                    {
                        "name": "Xiangyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Chang"
                },
                "arxiv_affiliation": "School of Management, Xi'an Jiaotong University",
                "author": "Xiangyu Chang",
                "arxiv_comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents . Submitted to Stat\n  (manuscript ID: STAT-25-0222.R1, under review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15189v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15189v3",
                "updated": "2025-10-29T05:47:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    5,
                    47,
                    44,
                    2,
                    302,
                    0
                ],
                "published": "2024-12-19T18:57:11Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    57,
                    11,
                    3,
                    354,
                    0
                ],
                "title": "Face the Facts! Evaluating RAG-based Pipelines for Professional\n  Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face the Facts! Evaluating RAG-based Pipelines for Professional\n  Fact-Checking"
                },
                "summary": "Natural Language Processing and Generation systems have recently shown the\npotential to complement and streamline the costly and time-consuming job of\nprofessional fact-checkers. In this work, we lift several constraints of\ncurrent state-of-the-art pipelines for automated fact-checking based on the\nRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark,\nfollowing professional fact-checking practices, RAG-based methods for the\ngeneration of verdicts - i.e., short texts discussing the veracity of a claim -\nevaluating them on stylistically complex claims and heterogeneous, yet\nreliable, knowledge bases. Our findings show a complex landscape, where, for\nexample, LLM-based retrievers outperform other retrieval techniques, though\nthey still struggle with heterogeneous knowledge bases; larger models excel in\nverdict faithfulness, while smaller models provide better context adherence,\nwith human evaluations favouring zero-shot and one-shot approaches for\ninformativeness, and fine-tuned models for emotional alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing and Generation systems have recently shown the\npotential to complement and streamline the costly and time-consuming job of\nprofessional fact-checkers. In this work, we lift several constraints of\ncurrent state-of-the-art pipelines for automated fact-checking based on the\nRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark,\nfollowing professional fact-checking practices, RAG-based methods for the\ngeneration of verdicts - i.e., short texts discussing the veracity of a claim -\nevaluating them on stylistically complex claims and heterogeneous, yet\nreliable, knowledge bases. Our findings show a complex landscape, where, for\nexample, LLM-based retrievers outperform other retrieval techniques, though\nthey still struggle with heterogeneous knowledge bases; larger models excel in\nverdict faithfulness, while smaller models provide better context adherence,\nwith human evaluations favouring zero-shot and one-shot approaches for\ninformativeness, and fine-tuned models for emotional alignment."
                },
                "authors": [
                    {
                        "name": "Daniel Russo"
                    },
                    {
                        "name": "Stefano Menini"
                    },
                    {
                        "name": "Jacopo Staiano"
                    },
                    {
                        "name": "Marco Guerini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Guerini"
                },
                "author": "Marco Guerini",
                "arxiv_comment": "Code and data at https://github.com/drusso98/face-the-facts -\n  Accepted for publication at INLG 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15189v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15189v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19477v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19477v5",
                "updated": "2025-10-28T11:59:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    59,
                    43,
                    1,
                    301,
                    0
                ],
                "published": "2024-11-29T05:29:47Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    29,
                    47,
                    4,
                    334,
                    0
                ],
                "title": "Provable Scaling Laws for the Test-Time Compute of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provable Scaling Laws for the Test-Time Compute of Large Language Models"
                },
                "summary": "We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms."
                },
                "authors": [
                    {
                        "name": "Yanxi Chen"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "NeurIPS 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19477v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19477v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24331v1",
                "updated": "2025-10-28T11:55:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    55,
                    24,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T11:55:24Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    55,
                    24,
                    1,
                    301,
                    0
                ],
                "title": "What do vision-language models see in the context? Investigating\n  multimodal in-context learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What do vision-language models see in the context? Investigating\n  multimodal in-context learning"
                },
                "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples."
                },
                "authors": [
                    {
                        "name": "Gabriel O. dos Santos"
                    },
                    {
                        "name": "Esther Colombini"
                    },
                    {
                        "name": "Sandra Avila"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Avila"
                },
                "author": "Sandra Avila",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24328v1",
                "updated": "2025-10-28T11:52:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    52,
                    51,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T11:52:51Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    52,
                    51,
                    1,
                    301,
                    0
                ],
                "title": "Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect\n  Variants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect\n  Variants"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to answer everyday\nquestions, yet their performance on culturally grounded and dialectal content\nremains uneven across languages. We propose a comprehensive method that (i)\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\nEnglish and several Arabic dialects, (ii) converts them into open-ended\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\nrationales to fine-tune models for step-by-step reasoning. Using this method,\nwe extend an existing dataset in which QAs are parallelly aligned across\nmultiple language varieties, making it, to our knowledge, the first of its\nkind. We conduct extensive experiments with both open and closed models. Our\nfindings show that (i) models underperform on Arabic dialects, revealing\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\ndeveloped dataset will be publicly released to support further research on\nculturally and linguistically inclusive evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to answer everyday\nquestions, yet their performance on culturally grounded and dialectal content\nremains uneven across languages. We propose a comprehensive method that (i)\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\nEnglish and several Arabic dialects, (ii) converts them into open-ended\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\nrationales to fine-tune models for step-by-step reasoning. Using this method,\nwe extend an existing dataset in which QAs are parallelly aligned across\nmultiple language varieties, making it, to our knowledge, the first of its\nkind. We conduct extensive experiments with both open and closed models. Our\nfindings show that (i) models underperform on Arabic dialects, revealing\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\ndeveloped dataset will be publicly released to support further research on\nculturally and linguistically inclusive evaluation."
                },
                "authors": [
                    {
                        "name": "Hunzalah Hassan Bhatti"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "Cultural Knowledge, Everyday Knowledge, Open-Ended Question,\n  Chain-of-Thought, Large Language Models, Native, Multilingual, Language\n  Diversity",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.24711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24711v1",
                "updated": "2025-10-28T17:59:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    59,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:59:02Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    59,
                    2,
                    1,
                    301,
                    0
                ],
                "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available."
                },
                "authors": [
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "Yujin Han"
                    },
                    {
                        "name": "Zhekai Chen"
                    },
                    {
                        "name": "Jiayu Wang"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Hongming Shan"
                    }
                ],
                "author_detail": {
                    "name": "Hongming Shan"
                },
                "author": "Hongming Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13397v2",
                "updated": "2025-10-28T17:56:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    56,
                    27,
                    1,
                    301,
                    0
                ],
                "published": "2024-04-20T14:42:43Z",
                "published_parsed": [
                    2024,
                    4,
                    20,
                    14,
                    42,
                    43,
                    5,
                    111,
                    0
                ],
                "title": "Retrieval-Augmented Generation-based Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation-based Relation Extraction"
                },
                "summary": "Information Extraction (IE) is a transformative process that converts\nunstructured text data into a structured format by employing entity and\nrelation extraction (RE) methodologies. The identification of the relation\nbetween a pair of entities plays a crucial role within this framework. Despite\nthe existence of various techniques for relation extraction, their efficacy\nheavily relies on access to labeled data and substantial computational\nresources. In addressing these challenges, Large Language Models (LLMs) emerge\nas promising solutions; however, they might return hallucinating responses due\nto their own training data. To overcome these limitations, Retrieved-Augmented\nGeneration-based Relation Extraction (RAG4RE) in this work is proposed,\noffering a pathway to enhance the performance of relation extraction tasks.\n  This work evaluated the effectiveness of our RAG4RE approach utilizing\ndifferent LLMs. Through the utilization of established benchmarks, such as\nTACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to\ncomprehensively evaluate the efficacy of our RAG4RE approach. In particularly,\nwe leverage prominent LLMs including Flan T5, Llama2, and Mistral in our\ninvestigation. The results of our study demonstrate that our RAG4RE approach\nsurpasses performance of traditional RE approaches based solely on LLMs,\nparticularly evident in the TACRED dataset and its variations. Furthermore, our\napproach exhibits remarkable performance compared to previous RE methodologies\nacross both TACRED and TACREV datasets, underscoring its efficacy and potential\nfor advancing RE tasks in natural language processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Extraction (IE) is a transformative process that converts\nunstructured text data into a structured format by employing entity and\nrelation extraction (RE) methodologies. The identification of the relation\nbetween a pair of entities plays a crucial role within this framework. Despite\nthe existence of various techniques for relation extraction, their efficacy\nheavily relies on access to labeled data and substantial computational\nresources. In addressing these challenges, Large Language Models (LLMs) emerge\nas promising solutions; however, they might return hallucinating responses due\nto their own training data. To overcome these limitations, Retrieved-Augmented\nGeneration-based Relation Extraction (RAG4RE) in this work is proposed,\noffering a pathway to enhance the performance of relation extraction tasks.\n  This work evaluated the effectiveness of our RAG4RE approach utilizing\ndifferent LLMs. Through the utilization of established benchmarks, such as\nTACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to\ncomprehensively evaluate the efficacy of our RAG4RE approach. In particularly,\nwe leverage prominent LLMs including Flan T5, Llama2, and Mistral in our\ninvestigation. The results of our study demonstrate that our RAG4RE approach\nsurpasses performance of traditional RE approaches based solely on LLMs,\nparticularly evident in the TACRED dataset and its variations. Furthermore, our\napproach exhibits remarkable performance compared to previous RE methodologies\nacross both TACRED and TACREV datasets, underscoring its efficacy and potential\nfor advancing RE tasks in natural language processing."
                },
                "authors": [
                    {
                        "name": "Sefika Efeoglu"
                    },
                    {
                        "name": "Adrian Paschke"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Paschke"
                },
                "author": "Adrian Paschke",
                "arxiv_doi": "10.1177/22104968251385519",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1177/22104968251385519",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "published at the Semantic Web journal. The last version is available:\n  https://doi.org/10.1177/22104968251385519",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24706v1",
                "updated": "2025-10-28T17:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    55,
                    42,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:55:42Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    55,
                    42,
                    1,
                    301,
                    0
                ],
                "title": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality\n  Games?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality\n  Games?"
                },
                "summary": "Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) games require players to translate high-level semantic\nactions into precise device manipulations using controllers and head-mounted\ndisplays (HMDs). While humans intuitively perform this translation based on\ncommon sense and embodied understanding, whether Large Language Models (LLMs)\ncan effectively replicate this ability remains underexplored. This paper\nintroduces a benchmark, ComboBench, evaluating LLMs' capability to translate\nsemantic actions into VR device manipulation sequences across 262 scenarios\nfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,\nand Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,\nGemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against\nannotated ground truth and human performance. Our results reveal that while\ntop-performing models like Gemini-1.5-Pro demonstrate strong task decomposition\ncapabilities, they still struggle with procedural reasoning and spatial\nunderstanding compared to humans. Performance varies significantly across\ngames, suggesting sensitivity to interaction complexity. Few-shot examples\nsubstantially improve performance, indicating potential for targeted\nenhancement of LLMs' VR manipulation capabilities. We release all materials at\nhttps://sites.google.com/view/combobench."
                },
                "authors": [
                    {
                        "name": "Shuqing Li"
                    },
                    {
                        "name": "Jiayi Yan"
                    },
                    {
                        "name": "Chenyu Niu"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Yun Peng"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Yepang Liu"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.08848v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.08848v4",
                "updated": "2025-10-28T17:53:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    53,
                    16,
                    1,
                    301,
                    0
                ],
                "published": "2023-06-15T04:24:13Z",
                "published_parsed": [
                    2023,
                    6,
                    15,
                    4,
                    24,
                    13,
                    3,
                    166,
                    0
                ],
                "title": "Datasheets for Machine Learning Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datasheets for Machine Learning Sensors"
                },
                "summary": "Machine learning (ML) is becoming prevalent in embedded AI sensing systems.\nThese \"ML sensors\" enable context-sensitive, real-time data collection and\ndecision-making across diverse applications ranging from anomaly detection in\nindustrial settings to wildlife tracking for conservation efforts. As such,\nthere is a need to provide transparency in the operation of such ML-enabled\nsensing systems through comprehensive documentation. This is needed to enable\ntheir reproducibility, to address new compliance and auditing regimes mandated\nin regulation and industry-specific policy, and to verify and validate the\nresponsible nature of their operation. To address this gap, we introduce the\ndatasheet for ML sensors framework. We provide a comprehensive template,\ncollaboratively developed in academia-industry partnerships, that captures the\ndistinct attributes of ML sensors, including hardware specifications, ML model\nand dataset characteristics, end-to-end performance metrics, and environmental\nimpacts. Our framework addresses the continuous streaming nature of sensor\ndata, real-time processing requirements, and embeds benchmarking methodologies\nthat reflect real-world deployment conditions, ensuring practical viability.\nAligned with the FAIR principles (Findability, Accessibility, Interoperability,\nand Reusability), our approach enhances the transparency and reusability of ML\nsensor documentation across academic, industrial, and regulatory domains. To\nshow the application of our approach, we present two datasheets: the first for\nan open-source ML sensor designed in-house and the second for a commercial ML\nsensor developed by industry collaborators, both performing computer\nvision-based person detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) is becoming prevalent in embedded AI sensing systems.\nThese \"ML sensors\" enable context-sensitive, real-time data collection and\ndecision-making across diverse applications ranging from anomaly detection in\nindustrial settings to wildlife tracking for conservation efforts. As such,\nthere is a need to provide transparency in the operation of such ML-enabled\nsensing systems through comprehensive documentation. This is needed to enable\ntheir reproducibility, to address new compliance and auditing regimes mandated\nin regulation and industry-specific policy, and to verify and validate the\nresponsible nature of their operation. To address this gap, we introduce the\ndatasheet for ML sensors framework. We provide a comprehensive template,\ncollaboratively developed in academia-industry partnerships, that captures the\ndistinct attributes of ML sensors, including hardware specifications, ML model\nand dataset characteristics, end-to-end performance metrics, and environmental\nimpacts. Our framework addresses the continuous streaming nature of sensor\ndata, real-time processing requirements, and embeds benchmarking methodologies\nthat reflect real-world deployment conditions, ensuring practical viability.\nAligned with the FAIR principles (Findability, Accessibility, Interoperability,\nand Reusability), our approach enhances the transparency and reusability of ML\nsensor documentation across academic, industrial, and regulatory domains. To\nshow the application of our approach, we present two datasheets: the first for\nan open-source ML sensor designed in-house and the second for a commercial ML\nsensor developed by industry collaborators, both performing computer\nvision-based person detection."
                },
                "authors": [
                    {
                        "name": "Matthew Stewart"
                    },
                    {
                        "name": "Yuke Zhang"
                    },
                    {
                        "name": "Pete Warden"
                    },
                    {
                        "name": "Yasmine Omri"
                    },
                    {
                        "name": "Shvetank Prakash"
                    },
                    {
                        "name": "Jacob Huckelberry"
                    },
                    {
                        "name": "Joao Henrique Santos"
                    },
                    {
                        "name": "Shawn Hymel"
                    },
                    {
                        "name": "Benjamin Yeager Brown"
                    },
                    {
                        "name": "Jim MacArthur"
                    },
                    {
                        "name": "Nat Jeffries"
                    },
                    {
                        "name": "Emanuel Moss"
                    },
                    {
                        "name": "Mona Sloane"
                    },
                    {
                        "name": "Brian Plancher"
                    },
                    {
                        "name": "Vijay Janapa Reddi"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Janapa Reddi"
                },
                "author": "Vijay Janapa Reddi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.08848v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.08848v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24702v1",
                "updated": "2025-10-28T17:53:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    53,
                    13,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:53:13Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    53,
                    13,
                    1,
                    301,
                    0
                ],
                "title": "Agent Data Protocol: Unifying Datasets for Diverse, Effective\n  Fine-tuning of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Data Protocol: Unifying Datasets for Diverse, Effective\n  Fine-tuning of LLM Agents"
                },
                "summary": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public research results on large-scale supervised finetuning of AI agents\nremain relatively rare, since the collection of agent training data presents\nunique challenges. In this work, we argue that the bottleneck is not a lack of\nunderlying data sources, but that a large variety of data is fragmented across\nheterogeneous formats, tools, and interfaces. To this end, we introduce the\nagent data protocol (ADP), a light-weight representation language that serves\nas an \"interlingua\" between agent datasets in diverse formats and unified agent\ntraining pipelines downstream. The design of ADP is expressive enough to\ncapture a large variety of tasks, including API/tool use, browsing, coding,\nsoftware engineering, and general agentic workflows, while remaining simple to\nparse and train on without engineering at a per-dataset level. In experiments,\nwe unified a broad collection of 13 existing agent training datasets into ADP\nformat, and converted the standardized ADP data into training-ready formats for\nmultiple agent frameworks. We performed SFT on these data, and demonstrated an\naverage performance gain of ~20% over corresponding base models, and delivers\nstate-of-the-art or near-SOTA performance on standard coding, browsing, tool\nuse, and research benchmarks, without domain-specific tuning. All code and data\nare released publicly, in the hope that ADP could help lower the barrier to\nstandardized, scalable, and reproducible agent training."
                },
                "authors": [
                    {
                        "name": "Yueqi Song"
                    },
                    {
                        "name": "Ketan Ramaneti"
                    },
                    {
                        "name": "Zaid Sheikh"
                    },
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Boyu Gou"
                    },
                    {
                        "name": "Tianbao Xie"
                    },
                    {
                        "name": "Yiheng Xu"
                    },
                    {
                        "name": "Danyang Zhang"
                    },
                    {
                        "name": "Apurva Gandhi"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Zhihao Yuan"
                    },
                    {
                        "name": "Frank Xu"
                    },
                    {
                        "name": "Shuyan Zhou"
                    },
                    {
                        "name": "Xingyao Wang"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Huan Sun"
                    },
                    {
                        "name": "Yu Su"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24699v1",
                "updated": "2025-10-28T17:51:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    51,
                    50,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:51:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    51,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentFold: Long-Horizon Web Agents with Proactive Context Management"
                },
                "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini."
                },
                "authors": [
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Yida Zhao"
                    },
                    {
                        "name": "Liangcai Su"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Siheng Chen"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Yong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jiang"
                },
                "author": "Yong Jiang",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24697v1",
                "updated": "2025-10-28T17:51:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    51,
                    42,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:51:42Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    51,
                    42,
                    1,
                    301,
                    0
                ],
                "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking"
                },
                "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines."
                },
                "authors": [
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Haiyang Shen"
                    },
                    {
                        "name": "Baixuan Li"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Yong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jiang"
                },
                "author": "Yong Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24695v1",
                "updated": "2025-10-28T17:50:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    50,
                    47,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:50:47Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    50,
                    47,
                    1,
                    301,
                    0
                ],
                "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis"
                },
                "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents."
                },
                "authors": [
                    {
                        "name": "Xuanzhong Chen"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Liangcai Su"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Yong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jiang"
                },
                "author": "Yong Jiang",
                "arxiv_comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24694v1",
                "updated": "2025-10-28T17:50:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    50,
                    40,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:50:40Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    50,
                    40,
                    1,
                    301,
                    0
                ],
                "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision"
                },
                "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents."
                },
                "authors": [
                    {
                        "name": "Yida Zhao"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Dingchu Zhang"
                    },
                    {
                        "name": "Baixuan Li"
                    },
                    {
                        "name": "Maojia Song"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Kewei Tu"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Yong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jiang"
                },
                "author": "Yong Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24688v1",
                "updated": "2025-10-28T17:49:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    49,
                    42,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:49:42Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    49,
                    42,
                    1,
                    301,
                    0
                ],
                "title": "MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with\n  Relation-Aware Fusion for 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with\n  Relation-Aware Fusion for 3D Object Detection"
                },
                "summary": "Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV."
                },
                "authors": [
                    {
                        "name": "Yun Zhang"
                    },
                    {
                        "name": "Zhaoliang Zheng"
                    },
                    {
                        "name": "Johnson Liu"
                    },
                    {
                        "name": "Zhiyu Huang"
                    },
                    {
                        "name": "Zewei Zhou"
                    },
                    {
                        "name": "Zonglin Meng"
                    },
                    {
                        "name": "Tianhui Cai"
                    },
                    {
                        "name": "Jiaqi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Ma"
                },
                "author": "Jiaqi Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24677v1",
                "updated": "2025-10-28T17:40:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    40,
                    53,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:40:53Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    40,
                    53,
                    1,
                    301,
                    0
                ],
                "title": "Dissecting Role Cognition in Medical LLMs via Neuronal Ablation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Role Cognition in Medical LLMs via Neuronal Ablation"
                },
                "summary": "Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained significant traction in medical\ndecision support systems, particularly in the\n  context of medical question answering and role-playing simulations. A common\npractice, Prompt-Based Role Playing (PBRP),\n  instructs models to adopt different clinical roles (e.g., medical students,\nresidents, attending physicians) to simulate varied\n  professional behaviors. However, the impact of such role prompts on model\nreasoning capabilities remains unclear. This\n  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to\nevaluate whether role prompts induce distinct,\n  role-specific cognitive processes in LLMs or merely modify linguistic style.\nWe test this framework on three medical QA\n  datasets, employing neuron ablation and representation analysis techniques to\nassess changes in reasoning pathways. Our\n  results demonstrate that role prompts do not significantly enhance the\nmedical reasoning abilities of LLMs. Instead, they\n  primarily affect surface-level linguistic features, with no evidence of\ndistinct reasoning pathways or cognitive differentiation\n  across clinical roles. Despite superficial stylistic changes, the core\ndecision-making mechanisms of LLMs remain uniform\n  across roles, indicating that current PBRP methods fail to replicate the\ncognitive complexity found in real-world medical\n  practice. This highlights the limitations of role-playing in medical AI and\nemphasizes the need for models that simulate genuine\n  cognitive processes rather than linguistic imitation.We have released the\nrelated code in the following repository:https:\n  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor"
                },
                "authors": [
                    {
                        "name": "Xun Liang"
                    },
                    {
                        "name": "Huayi Lai"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Yanfang Chen"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01281v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01281v6",
                "updated": "2025-10-28T17:26:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    26,
                    20,
                    1,
                    301,
                    0
                ],
                "published": "2024-11-02T15:23:28Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    23,
                    28,
                    5,
                    307,
                    0
                ],
                "title": "Arena-Lite: Efficient and Reliable Large Language Model Evaluation via\n  Tournament-Based Direct Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arena-Lite: Efficient and Reliable Large Language Model Evaluation via\n  Tournament-Based Direct Comparisons"
                },
                "summary": "As Large Language Models (LLMs) expand across domains, LLM judges have become\nessential for systems evaluation. Current benchmarks typically compare system\noutputs against baselines. This baseline-mediated approach, though convenient,\nyields lower reliability than direct comparison between systems. We propose\nArena-Lite which integrates tournament structure on top of head-to-head\ncomparison. The application of a tournament structure and direct comparison\neliminates the need for baseline outputs, reduces the number of required\ncomparisons, and allows higher reliability in system rankings. We conducted two\nexperiments: (1) controlled stochastic modeling and (2) empirical validation\nwith a real LLM judge. Those experiments collectively demonstrate that\nArena-Lite consistently achieves higher reliability with fewer comparisons,\neven with smaller datasets or weaker judges. We release an easy-to-use web\ndemonstration and code to foster adoption of Arena-Lite, streamlining model\nselection across research and industry communities. Arena-Lite demo and code\nare available on\n\\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) expand across domains, LLM judges have become\nessential for systems evaluation. Current benchmarks typically compare system\noutputs against baselines. This baseline-mediated approach, though convenient,\nyields lower reliability than direct comparison between systems. We propose\nArena-Lite which integrates tournament structure on top of head-to-head\ncomparison. The application of a tournament structure and direct comparison\neliminates the need for baseline outputs, reduces the number of required\ncomparisons, and allows higher reliability in system rankings. We conducted two\nexperiments: (1) controlled stochastic modeling and (2) empirical validation\nwith a real LLM judge. Those experiments collectively demonstrate that\nArena-Lite consistently achieves higher reliability with fewer comparisons,\neven with smaller datasets or weaker judges. We release an easy-to-use web\ndemonstration and code to foster adoption of Arena-Lite, streamlining model\nselection across research and industry communities. Arena-Lite demo and code\nare available on\n\\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}"
                },
                "authors": [
                    {
                        "name": "Seonil Son"
                    },
                    {
                        "name": "Ju-Min Oh"
                    },
                    {
                        "name": "Heegon Jin"
                    },
                    {
                        "name": "Cheolhun Jang"
                    },
                    {
                        "name": "Jeongbeom Jeong"
                    },
                    {
                        "name": "Kuntae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kuntae Kim"
                },
                "author": "Kuntae Kim",
                "arxiv_comment": "8 pages for main body, 19 pages in total",
                "arxiv_journal_ref": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01281v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01281v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24654v1",
                "updated": "2025-10-28T17:19:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    19,
                    47,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:19:47Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    19,
                    47,
                    1,
                    301,
                    0
                ],
                "title": "Evolving Diagnostic Agents in a Virtual Clinical Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Diagnostic Agents in a Virtual Clinical Environment"
                },
                "summary": "In this paper, we present a framework for training large language models\n(LLMs) as diagnostic agents with reinforcement learning, enabling them to\nmanage multi-turn diagnostic processes, adaptively select examinations, and\ncommit to final diagnoses. Unlike instruction-tuned models trained on static\ncase summaries, our method acquires diagnostic strategies through interactive\nexploration and outcome-based feedback. Our contributions are fourfold: (i) We\npresent DiagGym, a diagnostics world model trained with electronic health\nrecords that emits examination outcomes conditioned on patient history and\nrecommended examination, serving as a virtual clinical environment for\nrealistic diagnosis training and evaluation; (ii) We train DiagAgent via\nend-to-end, multi-turn reinforcement learning to learn diagnostic policies that\noptimize both information yield and diagnostic accuracy; (iii) We introduce\nDiagBench, a diagnostic benchmark comprising 750 cases with physician-validated\nexamination recommendations and 99 cases annotated with 973 physician-written\nrubrics on diagnosis process; (iv) we demonstrate superior performance across\ndiverse diagnostic settings. DiagAgent significantly outperforms 10\nstate-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two\nprompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%\nhigher diagnostic accuracy and 44.03% improvement in examination recommendation\nhit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic\naccuracy and 23.09% boost in examination recommendation F1 score. In\nrubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by\n7.1% in weighted rubric score. These findings indicate that learning policies\nin interactive clinical environments confers dynamic and clinically meaningful\ndiagnostic management abilities unattainable through passive training alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a framework for training large language models\n(LLMs) as diagnostic agents with reinforcement learning, enabling them to\nmanage multi-turn diagnostic processes, adaptively select examinations, and\ncommit to final diagnoses. Unlike instruction-tuned models trained on static\ncase summaries, our method acquires diagnostic strategies through interactive\nexploration and outcome-based feedback. Our contributions are fourfold: (i) We\npresent DiagGym, a diagnostics world model trained with electronic health\nrecords that emits examination outcomes conditioned on patient history and\nrecommended examination, serving as a virtual clinical environment for\nrealistic diagnosis training and evaluation; (ii) We train DiagAgent via\nend-to-end, multi-turn reinforcement learning to learn diagnostic policies that\noptimize both information yield and diagnostic accuracy; (iii) We introduce\nDiagBench, a diagnostic benchmark comprising 750 cases with physician-validated\nexamination recommendations and 99 cases annotated with 973 physician-written\nrubrics on diagnosis process; (iv) we demonstrate superior performance across\ndiverse diagnostic settings. DiagAgent significantly outperforms 10\nstate-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two\nprompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%\nhigher diagnostic accuracy and 44.03% improvement in examination recommendation\nhit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic\naccuracy and 23.09% boost in examination recommendation F1 score. In\nrubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by\n7.1% in weighted rubric score. These findings indicate that learning policies\nin interactive clinical environments confers dynamic and clinically meaningful\ndiagnostic management abilities unattainable through passive training alone."
                },
                "authors": [
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Qiaoyu Zheng"
                    },
                    {
                        "name": "Yusheng Liao"
                    },
                    {
                        "name": "Haowen Wang"
                    },
                    {
                        "name": "Yun Yue"
                    },
                    {
                        "name": "Qianrui Fan"
                    },
                    {
                        "name": "Shuai Zhen"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24652v1",
                "updated": "2025-10-28T17:18:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    18,
                    30,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:18:30Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    18,
                    30,
                    1,
                    301,
                    0
                ],
                "title": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning"
                },
                "summary": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24650v1",
                "updated": "2025-10-28T17:16:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    16,
                    47,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:16:47Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    16,
                    47,
                    1,
                    301,
                    0
                ],
                "title": "Advancing site-specific disease and pest management in precision\n  agriculture: From reasoning-driven foundation models to adaptive,\n  feedback-based learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing site-specific disease and pest management in precision\n  agriculture: From reasoning-driven foundation models to adaptive,\n  feedback-based learning"
                },
                "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets."
                },
                "authors": [
                    {
                        "name": "Nitin Rai"
                    },
                    {
                        "name": "Daeun"
                    },
                    {
                        "name": "Choi"
                    },
                    {
                        "name": "Nathan S. Boyd"
                    },
                    {
                        "name": "Arnold W. Schumann"
                    }
                ],
                "author_detail": {
                    "name": "Arnold W. Schumann"
                },
                "arxiv_affiliation": "Dana",
                "author": "Arnold W. Schumann",
                "arxiv_comment": "26 pages, 8 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24645v1",
                "updated": "2025-10-28T17:15:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    15,
                    26,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:15:26Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    15,
                    26,
                    1,
                    301,
                    0
                ],
                "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in\n  Multi-Turn Function Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in\n  Multi-Turn Function Calling"
                },
                "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning."
                },
                "authors": [
                    {
                        "name": "Zengzhuang Xu"
                    },
                    {
                        "name": "Bingguang Hao"
                    },
                    {
                        "name": "Zechuan Wang"
                    },
                    {
                        "name": "Yuntao Wen"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Yicheng Chen"
                    },
                    {
                        "name": "Cunyin Peng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Shi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shi Gu"
                },
                "author": "Shi Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24638v1",
                "updated": "2025-10-28T17:05:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    5,
                    50,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:05:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    5,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "What Does It Take? Developing a Smartphone App that Motivates Older\n  Adults to be Physically Active",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Does It Take? Developing a Smartphone App that Motivates Older\n  Adults to be Physically Active"
                },
                "summary": "Maintaining physical activity is essential for older adults' health and\nwell-being, yet participation remains low. Traditional paper-based and\nin-person interventions have been effective but face scalability issues.\nSmartphone apps offer a potential solution, but their effectiveness in\nreal-world use remains underexplored. Most prior studies take place in\ncontrolled environments, use specialized hardware, or rely on in-person\ntraining sessions or researcher-led setup. This study examines the feasibility\nand engagement of Senior Fit, a standalone mobile fitness app designed for\nolder adults. We conducted continuous testing with 25 participants aged 65-85,\nrefining the app based on their feedback to improve usability and\naccessibility. Our findings underscore both the potential and key challenges in\ndesigning digital health interventions. Older adults valued features such as\nvideo demonstrations and reminders that made activity feel accessible and\nmotivating, yet some expressed frustration with manual logging and limited\npersonalization. The Facebook group provided encouragement for some but\nexcluded others unfamiliar with the platform. These results highlight the need\nfor fitness apps that integrate flexible tracking, clear feedback, and\nlow-barrier social support. We contribute design recommendations for creating\ninclusive mobile fitness tools that align with older adults' routines and\ncapabilities, offering insights for future long-term, real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining physical activity is essential for older adults' health and\nwell-being, yet participation remains low. Traditional paper-based and\nin-person interventions have been effective but face scalability issues.\nSmartphone apps offer a potential solution, but their effectiveness in\nreal-world use remains underexplored. Most prior studies take place in\ncontrolled environments, use specialized hardware, or rely on in-person\ntraining sessions or researcher-led setup. This study examines the feasibility\nand engagement of Senior Fit, a standalone mobile fitness app designed for\nolder adults. We conducted continuous testing with 25 participants aged 65-85,\nrefining the app based on their feedback to improve usability and\naccessibility. Our findings underscore both the potential and key challenges in\ndesigning digital health interventions. Older adults valued features such as\nvideo demonstrations and reminders that made activity feel accessible and\nmotivating, yet some expressed frustration with manual logging and limited\npersonalization. The Facebook group provided encouragement for some but\nexcluded others unfamiliar with the platform. These results highlight the need\nfor fitness apps that integrate flexible tracking, clear feedback, and\nlow-barrier social support. We contribute design recommendations for creating\ninclusive mobile fitness tools that align with older adults' routines and\ncapabilities, offering insights for future long-term, real-world deployments."
                },
                "authors": [
                    {
                        "name": "Sabrina Haque"
                    },
                    {
                        "name": "Kyle Henry"
                    },
                    {
                        "name": "Troyee Saha"
                    },
                    {
                        "name": "Kimberly Vanhoose"
                    },
                    {
                        "name": "Jobaidul Boni"
                    },
                    {
                        "name": "Samantha Moss"
                    },
                    {
                        "name": "Kate Hyun"
                    },
                    {
                        "name": "Kathy Siepker"
                    },
                    {
                        "name": "Xiangli Gu"
                    },
                    {
                        "name": "Angela Liegey-Dougall"
                    },
                    {
                        "name": "Stephen Mattingly"
                    },
                    {
                        "name": "Christoph Csallner"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Csallner"
                },
                "author": "Christoph Csallner",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24636v1",
                "updated": "2025-10-28T17:02:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    2,
                    46,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T17:02:46Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    2,
                    46,
                    1,
                    301,
                    0
                ],
                "title": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement\n  Learning"
                },
                "summary": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) have become essential for aligning large language models\n(LLMs), serving as scalable proxies for human evaluation in both training and\ninference. However, existing RMs struggle on knowledge-intensive and long-form\ntasks, where evaluating correctness requires grounding beyond the model's\ninternal knowledge. This limitation hinders them from reliably discriminating\nsubtle quality differences, especially when external evidence is necessary. To\naddress this, we introduce OpenRM, a tool-augmented long-form reward model that\nsystematically judges open-ended responses by invoking external tools to gather\nrelevant evidence. We train OpenRM with Group Relative Policy Optimization\n(GRPO) on over 27K synthesized pairwise examples generated through a\ncontrollable data synthesis framework. The training objective jointly\nsupervises intermediate tool usage and final outcome accuracy, incentivizing\nour reward model to learn effective evidence-based judgment strategies.\nExtensive experiments on three newly-collected datasets and two widely-used\nbenchmarks demonstrate that OpenRM substantially outperforms existing reward\nmodeling approaches. As a further step, we integrate OpenRM into both\ninference-time response selection and training-time data selection. This yields\nconsistent gains in downstream LLM alignment tasks, highlighting the potential\nof tool-augmented reward models for scaling reliable long-form evaluation."
                },
                "authors": [
                    {
                        "name": "Ziyou Hu"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Minghang Zhu"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Zhaochun Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhaochun Ren"
                },
                "author": "Zhaochun Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13499v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13499v3",
                "updated": "2025-10-28T17:00:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    17,
                    0,
                    42,
                    1,
                    301,
                    0
                ],
                "published": "2025-09-16T19:55:25Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    19,
                    55,
                    25,
                    1,
                    259,
                    0
                ],
                "title": "Reproducible workflow for online AI in digital health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducible workflow for online AI in digital health"
                },
                "summary": "Online artificial intelligence (AI) algorithms are an important component of\ndigital health interventions. These online algorithms are designed to\ncontinually learn and improve their performance as streaming data is collected\non individuals. Deploying online AI presents a key challenge: balancing\nadaptability of online AI with reproducibility. Online AI in digital\ninterventions is a rapidly evolving area, driven by advances in algorithms,\nsensors, software, and devices. Digital health intervention development and\ndeployment is a continuous process, where implementation - including the AI\ndecision-making algorithm - is interspersed with cycles of re-development and\noptimization. Each deployment informs the next, making iterative deployment a\ndefining characteristic of this field. This iterative nature underscores the\nimportance of reproducibility: data collected across deployments must be\naccurately stored to have scientific utility, algorithm behavior must be\nauditable, and results must be comparable over time to facilitate scientific\ndiscovery and trustworthy refinement. This paper proposes a reproducible\nscientific workflow for developing, deploying, and analyzing online AI\ndecision-making algorithms in digital health interventions. Grounded in\npractical experience from multiple real-world deployments, this workflow\naddresses key challenges to reproducibility across all phases of the online AI\nalgorithm development life-cycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online artificial intelligence (AI) algorithms are an important component of\ndigital health interventions. These online algorithms are designed to\ncontinually learn and improve their performance as streaming data is collected\non individuals. Deploying online AI presents a key challenge: balancing\nadaptability of online AI with reproducibility. Online AI in digital\ninterventions is a rapidly evolving area, driven by advances in algorithms,\nsensors, software, and devices. Digital health intervention development and\ndeployment is a continuous process, where implementation - including the AI\ndecision-making algorithm - is interspersed with cycles of re-development and\noptimization. Each deployment informs the next, making iterative deployment a\ndefining characteristic of this field. This iterative nature underscores the\nimportance of reproducibility: data collected across deployments must be\naccurately stored to have scientific utility, algorithm behavior must be\nauditable, and results must be comparable over time to facilitate scientific\ndiscovery and trustworthy refinement. This paper proposes a reproducible\nscientific workflow for developing, deploying, and analyzing online AI\ndecision-making algorithms in digital health interventions. Grounded in\npractical experience from multiple real-world deployments, this workflow\naddresses key challenges to reproducibility across all phases of the online AI\nalgorithm development life-cycle."
                },
                "authors": [
                    {
                        "name": "Susobhan Ghosh"
                    },
                    {
                        "name": "Bhanu T. Gullapalli"
                    },
                    {
                        "name": "Daiqi Gao"
                    },
                    {
                        "name": "Asim Gazi"
                    },
                    {
                        "name": "Anna Trella"
                    },
                    {
                        "name": "Ziping Xu"
                    },
                    {
                        "name": "Kelly Zhang"
                    },
                    {
                        "name": "Susan A. Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Susan A. Murphy"
                },
                "author": "Susan A. Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13499v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13499v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22693v2",
                "updated": "2025-10-28T16:57:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    57,
                    22,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-26T14:36:15Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    14,
                    36,
                    15,
                    6,
                    299,
                    0
                ],
                "title": "VADTree: Explainable Training-Free Video Anomaly Detection via\n  Hierarchical Granularity-Aware Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VADTree: Explainable Training-Free Video Anomaly Detection via\n  Hierarchical Granularity-Aware Tree"
                },
                "summary": "Video anomaly detection (VAD) focuses on identifying anomalies in videos.\nSupervised methods demand substantial in-domain training data and fail to\ndeliver clear explanations for anomalies. In contrast, training-free methods\nleverage the knowledge reserves and language interactivity of large pre-trained\nmodels to detect anomalies. However, the current fixed-length temporal window\nsampling approaches struggle to accurately capture anomalies with varying\ntemporal spans. Therefore, we propose VADTree that utilizes a Hierarchical\nGranularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree\nleverages the knowledge embedded in a pre-trained Generic Event Boundary\nDetection (GEBD) model to characterize potential anomaly event boundaries.\nSpecifically, VADTree decomposes the video into generic event nodes based on\nboundary confidence, and performs adaptive coarse-fine hierarchical structuring\nand redundancy removal to construct the HGTree. Then, the multi-dimensional\npriors are injected into the visual language models (VLMs) to enhance the\nnode-wise anomaly perception, and anomaly reasoning for generic event nodes is\nachieved via large language models (LLMs). Finally, an inter-cluster node\ncorrelation method is used to integrate the multi-granularity anomaly scores.\nExtensive experiments on three challenging datasets demonstrate that VADTree\nachieves state-of-the-art performance in training-free settings while\ndrastically reducing the number of sampled video segments. The code will be\navailable at https://github.com/wenlongli10/VADTree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video anomaly detection (VAD) focuses on identifying anomalies in videos.\nSupervised methods demand substantial in-domain training data and fail to\ndeliver clear explanations for anomalies. In contrast, training-free methods\nleverage the knowledge reserves and language interactivity of large pre-trained\nmodels to detect anomalies. However, the current fixed-length temporal window\nsampling approaches struggle to accurately capture anomalies with varying\ntemporal spans. Therefore, we propose VADTree that utilizes a Hierarchical\nGranularityaware Tree (HGTree) structure for flexible sampling in VAD. VADTree\nleverages the knowledge embedded in a pre-trained Generic Event Boundary\nDetection (GEBD) model to characterize potential anomaly event boundaries.\nSpecifically, VADTree decomposes the video into generic event nodes based on\nboundary confidence, and performs adaptive coarse-fine hierarchical structuring\nand redundancy removal to construct the HGTree. Then, the multi-dimensional\npriors are injected into the visual language models (VLMs) to enhance the\nnode-wise anomaly perception, and anomaly reasoning for generic event nodes is\nachieved via large language models (LLMs). Finally, an inter-cluster node\ncorrelation method is used to integrate the multi-granularity anomaly scores.\nExtensive experiments on three challenging datasets demonstrate that VADTree\nachieves state-of-the-art performance in training-free settings while\ndrastically reducing the number of sampled video segments. The code will be\navailable at https://github.com/wenlongli10/VADTree."
                },
                "authors": [
                    {
                        "name": "Wenlong Li"
                    },
                    {
                        "name": "Yifei Xu"
                    },
                    {
                        "name": "Yuan Rao"
                    },
                    {
                        "name": "Zhenhua Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "NeurIPS 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24626v1",
                "updated": "2025-10-28T16:55:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    55,
                    22,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:55:22Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    55,
                    22,
                    1,
                    301,
                    0
                ],
                "title": "Relative Scaling Laws for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relative Scaling Laws for LLMs"
                },
                "summary": "Scaling laws describe how language models improve with additional data,\nparameters, and compute. While widely used, they are typically measured on\naggregate test sets. Aggregate evaluations yield clean trends but average over\nheterogeneous subpopulations, obscuring performance disparities. We introduce\nrelative scaling laws, which track how performance gaps between test\ndistributions evolve with scale rather than focusing solely on absolute error.\nUsing 255 decoder-only Transformers trained under matched-compute (IsoFLOP)\nbudgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we\nfind diverse trajectories: academic domains on MMLU converge toward parity;\nregional English dialects shift depending on population size; and clusters of\nAI risk behaviours split, with capability- and influence-related risks\nincreasing during pretraining while adversarial risks do not. These results\nshow that although scaling improves overall performance, it is not a universal\nequalizer. To support further study, we release all model checkpoints from this\nwork to enable practitioners to measure relative alongside traditional scaling\nlaws, in order to better prioritize robustness challenges in light of the\nbitter lesson.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws describe how language models improve with additional data,\nparameters, and compute. While widely used, they are typically measured on\naggregate test sets. Aggregate evaluations yield clean trends but average over\nheterogeneous subpopulations, obscuring performance disparities. We introduce\nrelative scaling laws, which track how performance gaps between test\ndistributions evolve with scale rather than focusing solely on absolute error.\nUsing 255 decoder-only Transformers trained under matched-compute (IsoFLOP)\nbudgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we\nfind diverse trajectories: academic domains on MMLU converge toward parity;\nregional English dialects shift depending on population size; and clusters of\nAI risk behaviours split, with capability- and influence-related risks\nincreasing during pretraining while adversarial risks do not. These results\nshow that although scaling improves overall performance, it is not a universal\nequalizer. To support further study, we release all model checkpoints from this\nwork to enable practitioners to measure relative alongside traditional scaling\nlaws, in order to better prioritize robustness challenges in light of the\nbitter lesson."
                },
                "authors": [
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "David Hall"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24619v1",
                "updated": "2025-10-28T16:48:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    48,
                    3,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:48:03Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    48,
                    3,
                    1,
                    301,
                    0
                ],
                "title": "Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation"
                },
                "summary": "With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the release of new large language models (LLMs) like Llama and Mistral,\nzero-shot cross-lingual transfer has become increasingly feasible due to their\nmultilingual pretraining and strong generalization capabilities. However,\nadapting these decoder-only LLMs to new tasks across languages remains\nchallenging. While parameter-efficient fine-tuning (PeFT) techniques like\nLow-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as\nsoft prompt tuning, prefix tuning, and Llama Adapter are less explored,\nespecially for zero-shot transfer in decoder-only models. We present a\ncomprehensive study of three prefix-based methods for zero-shot cross-lingual\ntransfer from English to 35+ high- and low-resource languages. Our analysis\nfurther explores transfer across linguistic families and scripts, as well as\nthe impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix\nmethods outperform LoRA-baselines by up to 6% on the Belebele benchmark.\nSimilar improvements were observed with Mistral v0.3 7B as well. Despite using\nonly 1.23M learning parameters with prefix tuning, we achieve consistent\nimprovements across diverse benchmarks. These findings highlight the potential\nof prefix-based techniques as an effective and scalable alternative to LoRA,\nparticularly in low-resource multilingual settings."
                },
                "authors": [
                    {
                        "name": "Snegha A"
                    },
                    {
                        "name": "Sayambhu Sen"
                    },
                    {
                        "name": "Piyush Singh Pasi"
                    },
                    {
                        "name": "Abhishek Singhania"
                    },
                    {
                        "name": "Preethi Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Preethi Jyothi"
                },
                "arxiv_affiliation": "Indian Institute of Technology Bombay",
                "author": "Preethi Jyothi",
                "arxiv_comment": "12 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11390v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11390v3",
                "updated": "2025-10-28T16:44:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    44,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2024-09-17T17:50:15Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    50,
                    15,
                    1,
                    261,
                    0
                ],
                "title": "Says Who? Effective Zero-Shot Annotation of Focalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Says Who? Effective Zero-Shot Annotation of Focalization"
                },
                "summary": "Focalization describes the way in which access to narrative information is\nrestricted or controlled based on the knowledge available to knowledge of the\nnarrator. It is encoded via a wide range of lexico-grammatical features and is\nsubject to reader interpretation. Even trained annotators frequently disagree\non correct labels, suggesting this task is both qualitatively and\ncomputationally challenging. In this work, we test how well five contemporary\nlarge language model (LLM) families and two baselines perform when annotating\nshort literary excerpts for focalization. Despite the challenging nature of the\ntask, we find that LLMs show comparable performance to trained human\nannotators, with GPT-4o achieving an average F1 of 84.79%. Further, we\ndemonstrate that the log probabilities output by GPT-family models frequently\nreflect the difficulty of annotating particular excerpts. Finally, we provide a\ncase study analyzing sixteen Stephen King novels, demonstrating the usefulness\nof this approach for computational literary studies and the insights gleaned\nfrom examining focalization at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focalization describes the way in which access to narrative information is\nrestricted or controlled based on the knowledge available to knowledge of the\nnarrator. It is encoded via a wide range of lexico-grammatical features and is\nsubject to reader interpretation. Even trained annotators frequently disagree\non correct labels, suggesting this task is both qualitatively and\ncomputationally challenging. In this work, we test how well five contemporary\nlarge language model (LLM) families and two baselines perform when annotating\nshort literary excerpts for focalization. Despite the challenging nature of the\ntask, we find that LLMs show comparable performance to trained human\nannotators, with GPT-4o achieving an average F1 of 84.79%. Further, we\ndemonstrate that the log probabilities output by GPT-family models frequently\nreflect the difficulty of annotating particular excerpts. Finally, we provide a\ncase study analyzing sixteen Stephen King novels, demonstrating the usefulness\nof this approach for computational literary studies and the insights gleaned\nfrom examining focalization at scale."
                },
                "authors": [
                    {
                        "name": "Rebecca M. M. Hicke"
                    },
                    {
                        "name": "Yuri Bizzoni"
                    },
                    {
                        "name": "Pascale Feldkamp"
                    },
                    {
                        "name": "Ross Deans Kristensen-McLachlan"
                    }
                ],
                "author_detail": {
                    "name": "Ross Deans Kristensen-McLachlan"
                },
                "author": "Ross Deans Kristensen-McLachlan",
                "arxiv_comment": "Accepted at CHR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11390v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11390v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24606v1",
                "updated": "2025-10-28T16:34:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    34,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:34:18Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    34,
                    18,
                    1,
                    301,
                    0
                ],
                "title": "Long-Context Modeling with Dynamic Hierarchical Sparse Attention for\n  On-Device LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Modeling with Dynamic Hierarchical Sparse Attention for\n  On-Device LLMs"
                },
                "summary": "The quadratic cost of attention hinders the scalability of long-context LLMs,\nespecially in resource-constrained settings. Existing static sparse methods\nsuch as sliding windows or global tokens utilizes the sparsity of attention to\nreduce the cost of attention, but poorly adapts to the content-dependent\nvariations in attention due to their staticity. While previous work has\nproposed several dynamic approaches to improve flexibility, they still depend\non predefined templates or heuristic mechanisms. Such strategies reduce\ngenerality and prune tokens that remain contextually important, limiting their\naccuracy across diverse tasks. To tackle these bottlenecks of existing methods\nfor long-context modeling, we introduce Dynamic Hierarchical Sparse Attention\n(DHSA), a data-driven framework that dynamically predicts attention sparsity\nonline without retraining. Our proposed DHSA adaptively segments sequences into\nvariable-length chunks, then computes chunk representations by aggregating the\ntoken embeddings within each chunk. To avoid the bias introduced by varying\nchunk lengths, we apply length-normalized aggregation that scales the averaged\nembeddings by the square root of the chunk size. Finally, DHSA upsamples the\nchunk-level similarity scores to token level similarities to calculate\nimportance scores that determine which token-level interactions should be\npreserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and\nLongBench show that DHSA matches dense attention in accuracy, while reducing\nprefill latency by 20-60% and peak memory usage by 35%. Compared to other\nrepresentative baselines such as block sparse attention, DHSA achieves\nconsistently higher accuracy (6-18% relative gains) with comparable or lower\ncost, offering an efficient and adaptable solution for long-context on-device\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic cost of attention hinders the scalability of long-context LLMs,\nespecially in resource-constrained settings. Existing static sparse methods\nsuch as sliding windows or global tokens utilizes the sparsity of attention to\nreduce the cost of attention, but poorly adapts to the content-dependent\nvariations in attention due to their staticity. While previous work has\nproposed several dynamic approaches to improve flexibility, they still depend\non predefined templates or heuristic mechanisms. Such strategies reduce\ngenerality and prune tokens that remain contextually important, limiting their\naccuracy across diverse tasks. To tackle these bottlenecks of existing methods\nfor long-context modeling, we introduce Dynamic Hierarchical Sparse Attention\n(DHSA), a data-driven framework that dynamically predicts attention sparsity\nonline without retraining. Our proposed DHSA adaptively segments sequences into\nvariable-length chunks, then computes chunk representations by aggregating the\ntoken embeddings within each chunk. To avoid the bias introduced by varying\nchunk lengths, we apply length-normalized aggregation that scales the averaged\nembeddings by the square root of the chunk size. Finally, DHSA upsamples the\nchunk-level similarity scores to token level similarities to calculate\nimportance scores that determine which token-level interactions should be\npreserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and\nLongBench show that DHSA matches dense attention in accuracy, while reducing\nprefill latency by 20-60% and peak memory usage by 35%. Compared to other\nrepresentative baselines such as block sparse attention, DHSA achieves\nconsistently higher accuracy (6-18% relative gains) with comparable or lower\ncost, offering an efficient and adaptable solution for long-context on-device\nLLMs."
                },
                "authors": [
                    {
                        "name": "Siheng Xiong"
                    },
                    {
                        "name": "Joe Zou"
                    },
                    {
                        "name": "Faramarz Fekri"
                    },
                    {
                        "name": "Yae Jee Cho"
                    }
                ],
                "author_detail": {
                    "name": "Yae Jee Cho"
                },
                "author": "Yae Jee Cho",
                "arxiv_comment": "Accepted to NeurIPS 2025 Workshop on Efficient Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24605v1",
                "updated": "2025-10-28T16:32:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    32,
                    43,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:32:43Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    32,
                    43,
                    1,
                    301,
                    0
                ],
                "title": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead\n  the Way",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead\n  the Way"
                },
                "summary": "Diffusion-based large language models (dLLMs) have exhibited substantial\npotential for parallel text generation, which may enable more efficient\ngeneration compared to autoregressive models. However, current dLLMs suffer\nfrom fixed generation lengths, which indicates the generation lengths of dLLMs\nhave to be determined before decoding as a hyper-parameter, leading to issues\nin efficiency and flexibility. To solve these problems, in this work, we\npropose to train a diffusion LLM with native variable generation lengths,\nabbreviated as dLLM-Var. Concretely, we aim to train a model to accurately\npredict the [EOS] token in the generated text, which makes a dLLM be able to\nnatively infer in a block diffusion manner, while still maintaining the ability\nof global bi-directional (full) attention and high parallelism. Experiments on\nstandard benchmarks demonstrate that our method achieves a 30.1x speedup over\ntraditional dLLM inference paradigms and a 2.4x speedup relative to\nautoregressive models such as Qwen and Llama. Our method achieves higher\naccuracy and faster inference, elevating dLLMs beyond mere academic novelty and\nsupporting their practical use in real-world applications. Codes and models\nhave been released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have exhibited substantial\npotential for parallel text generation, which may enable more efficient\ngeneration compared to autoregressive models. However, current dLLMs suffer\nfrom fixed generation lengths, which indicates the generation lengths of dLLMs\nhave to be determined before decoding as a hyper-parameter, leading to issues\nin efficiency and flexibility. To solve these problems, in this work, we\npropose to train a diffusion LLM with native variable generation lengths,\nabbreviated as dLLM-Var. Concretely, we aim to train a model to accurately\npredict the [EOS] token in the generated text, which makes a dLLM be able to\nnatively infer in a block diffusion manner, while still maintaining the ability\nof global bi-directional (full) attention and high parallelism. Experiments on\nstandard benchmarks demonstrate that our method achieves a 30.1x speedup over\ntraditional dLLM inference paradigms and a 2.4x speedup relative to\nautoregressive models such as Qwen and Llama. Our method achieves higher\naccuracy and faster inference, elevating dLLMs beyond mere academic novelty and\nsupporting their practical use in real-world applications. Codes and models\nhave been released."
                },
                "authors": [
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Hanlin Xu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15737v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15737v4",
                "updated": "2025-10-28T16:23:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    23,
                    53,
                    1,
                    301,
                    0
                ],
                "published": "2024-11-24T07:02:32Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    7,
                    2,
                    32,
                    6,
                    329,
                    0
                ],
                "title": "TableTime: Reformulating Time Series Classification as Training-Free\n  Table Understanding with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableTime: Reformulating Time Series Classification as Training-Free\n  Table Understanding with Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated their effectiveness in\nmultivariate time series classification (MTSC). Effective adaptation of LLMs\nfor MTSC necessitates informative data representations. Existing LLM-based\nmethods directly encode embeddings for time series within the latent space of\nLLMs from scratch to align with semantic space of LLMs. Despite their\neffectiveness, we reveal that these methods conceal three inherent bottlenecks:\n(1) they struggle to encode temporal and channel-specific information in a\nlossless manner, both of which are critical components of multivariate time\nseries; (2) it is much difficult to align the learned representation space with\nthe semantic space of the LLMs; (3) they require task-specific retraining,\nwhich is both computationally expensive and labor-intensive. To bridge these\ngaps, we propose TableTime, which reformulates MTSC as a table understanding\ntask. Specifically, TableTime introduces the following strategies: (1) convert\nmultivariate time series into a tabular form, thus minimizing information loss\nto the greatest extent; (2) represent tabular time series in text format to\nachieve natural alignment with the semantic space of LLMs; (3) design a\nreasoning framework that integrates contextual text information, neighborhood\nassistance, multi-path inference and problem decomposition to enhance the\nreasoning ability of LLMs and realize zero-shot classification. Extensive\nexperiments performed on 10 publicly representative datasets from UEA archive\nverify the superiorities of the TableTime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated their effectiveness in\nmultivariate time series classification (MTSC). Effective adaptation of LLMs\nfor MTSC necessitates informative data representations. Existing LLM-based\nmethods directly encode embeddings for time series within the latent space of\nLLMs from scratch to align with semantic space of LLMs. Despite their\neffectiveness, we reveal that these methods conceal three inherent bottlenecks:\n(1) they struggle to encode temporal and channel-specific information in a\nlossless manner, both of which are critical components of multivariate time\nseries; (2) it is much difficult to align the learned representation space with\nthe semantic space of the LLMs; (3) they require task-specific retraining,\nwhich is both computationally expensive and labor-intensive. To bridge these\ngaps, we propose TableTime, which reformulates MTSC as a table understanding\ntask. Specifically, TableTime introduces the following strategies: (1) convert\nmultivariate time series into a tabular form, thus minimizing information loss\nto the greatest extent; (2) represent tabular time series in text format to\nachieve natural alignment with the semantic space of LLMs; (3) design a\nreasoning framework that integrates contextual text information, neighborhood\nassistance, multi-path inference and problem decomposition to enhance the\nreasoning ability of LLMs and realize zero-shot classification. Extensive\nexperiments performed on 10 publicly representative datasets from UEA archive\nverify the superiorities of the TableTime."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Qingyang Mao"
                    },
                    {
                        "name": "Yitong Zhou"
                    },
                    {
                        "name": "Daoyu Wang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Feiyang Xu"
                    },
                    {
                        "name": "Xin Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin Li"
                },
                "author": "Xin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15737v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15737v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24594v1",
                "updated": "2025-10-28T16:23:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    23,
                    27,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:23:27Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    23,
                    27,
                    1,
                    301,
                    0
                ],
                "title": "Detecting the Use of Generative AI in Crowdsourced Surveys: Implications\n  for Data Integrity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting the Use of Generative AI in Crowdsourced Surveys: Implications\n  for Data Integrity"
                },
                "summary": "The widespread adoption of generative AI (GenAI) has introduced new\nchallenges in crowdsourced data collection, particularly in survey-based\nresearch. While GenAI offers powerful capabilities, its unintended use in\ncrowdsourcing, such as generating automated survey responses, threatens the\nintegrity of empirical research and complicates efforts to understand public\nopinion and behavior. In this study, we investigate and evaluate two approaches\nfor detecting AI-generated responses in online surveys: LLM-based detection and\nsignature-based detection. We conducted experiments across seven survey\nstudies, comparing responses collected before 2022 with those collected after\nthe release of ChatGPT. Our findings reveal a significant increase in\nAI-generated responses in the post-2022 studies, highlighting how GenAI may\nsilently distort crowdsourced data. This work raises broader concerns about\nevolving landscape of data integrity, where GenAI can compromise data quality,\nmislead researchers, and influence downstream findings in fields such as\nhealth, politics, and social behavior. By surfacing detection strategies and\nempirical evidence of GenAI's impact, we aim to contribute to ongoing\nconversation about safeguarding research integrity and supporting scholars\nnavigating these methodological and ethical challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of generative AI (GenAI) has introduced new\nchallenges in crowdsourced data collection, particularly in survey-based\nresearch. While GenAI offers powerful capabilities, its unintended use in\ncrowdsourcing, such as generating automated survey responses, threatens the\nintegrity of empirical research and complicates efforts to understand public\nopinion and behavior. In this study, we investigate and evaluate two approaches\nfor detecting AI-generated responses in online surveys: LLM-based detection and\nsignature-based detection. We conducted experiments across seven survey\nstudies, comparing responses collected before 2022 with those collected after\nthe release of ChatGPT. Our findings reveal a significant increase in\nAI-generated responses in the post-2022 studies, highlighting how GenAI may\nsilently distort crowdsourced data. This work raises broader concerns about\nevolving landscape of data integrity, where GenAI can compromise data quality,\nmislead researchers, and influence downstream findings in fields such as\nhealth, politics, and social behavior. By surfacing detection strategies and\nempirical evidence of GenAI's impact, we aim to contribute to ongoing\nconversation about safeguarding research integrity and supporting scholars\nnavigating these methodological and ethical challenges."
                },
                "authors": [
                    {
                        "name": "Dapeng Zhang"
                    },
                    {
                        "name": "Marina Katoh"
                    },
                    {
                        "name": "Weiping Pei"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Pei"
                },
                "author": "Weiping Pei",
                "arxiv_comment": "Accepted by CSCW 2025 workshop Beyond Information: Online\n  Participatory Culture and Information Disorder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23458v2",
                "updated": "2025-10-28T16:23:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    23,
                    4,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-27T15:58:51Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    15,
                    58,
                    51,
                    0,
                    300,
                    0
                ],
                "title": "BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents"
                },
                "summary": "Confidence in LLMs is a useful indicator of model uncertainty and answer\nreliability. Existing work mainly focused on single-turn scenarios, while\nresearch on confidence in complex multi-turn interactions is limited. In this\npaper, we investigate whether LLM-based search agents have the ability to\ncommunicate their own confidence through verbalized confidence scores after\nlong sequences of actions, a significantly more challenging task compared to\noutputting confidence in a single interaction. Experimenting on open-source\nagentic models, we first find that models exhibit much higher task accuracy at\nhigh confidence while having near-zero accuracy when confidence is low. Based\non this observation, we propose Test-Time Scaling (TTS) methods that use\nconfidence scores to determine answer quality, encourage the model to try again\nuntil reaching a satisfactory confidence level. Results show that our proposed\nmethods significantly reduce token consumption while demonstrating competitive\nperformance compared to baseline fixed budget TTS methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence in LLMs is a useful indicator of model uncertainty and answer\nreliability. Existing work mainly focused on single-turn scenarios, while\nresearch on confidence in complex multi-turn interactions is limited. In this\npaper, we investigate whether LLM-based search agents have the ability to\ncommunicate their own confidence through verbalized confidence scores after\nlong sequences of actions, a significantly more challenging task compared to\noutputting confidence in a single interaction. Experimenting on open-source\nagentic models, we first find that models exhibit much higher task accuracy at\nhigh confidence while having near-zero accuracy when confidence is low. Based\non this observation, we propose Test-Time Scaling (TTS) methods that use\nconfidence scores to determine answer quality, encourage the model to try again\nuntil reaching a satisfactory confidence level. Results show that our proposed\nmethods significantly reduce token consumption while demonstrating competitive\nperformance compared to baseline fixed budget TTS methods."
                },
                "authors": [
                    {
                        "name": "Litu Ou"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Zile Qiao"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Yong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jiang"
                },
                "author": "Yong Jiang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24592v1",
                "updated": "2025-10-28T16:22:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    22,
                    54,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:22:54Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    22,
                    54,
                    1,
                    301,
                    0
                ],
                "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization"
                },
                "summary": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases."
                },
                "authors": [
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Jing Wu"
                    },
                    {
                        "name": "Xinjie Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ruihua Song"
                    },
                    {
                        "name": "Chengxi Li"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Ongoing Work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19000v2",
                "updated": "2025-10-28T16:16:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    16,
                    0,
                    1,
                    301,
                    0
                ],
                "published": "2024-05-29T11:28:06Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    11,
                    28,
                    6,
                    2,
                    150,
                    0
                ],
                "title": "FedMAP: Personalised Federated Learning for Real Large-Scale Healthcare\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMAP: Personalised Federated Learning for Real Large-Scale Healthcare\n  Systems"
                },
                "summary": "Federated learning (FL) promises to enable collaborative machine learning\nacross healthcare sites whilst preserving data privacy. Practical deployment\nremains limited by statistical heterogeneity arising from differences in\npatient demographics, treatments, and outcomes, and infrastructure constraints.\nWe introduce FedMAP, a personalised FL (PFL) framework that addresses\nheterogeneity through local Maximum a Posteriori (MAP) estimation with Input\nConvex Neural Network priors. These priors represent global knowledge gathered\nfrom other sites that guides the model while adapting to local data, and we\nprovide a formal proof of convergence. Unlike many PFL methods that rely on\nfixed regularisation, FedMAP's prior adaptively learns patterns that capture\ncomplex inter-site relationships. We demonstrate improved performance compared\nto local training, FedAvg, and several PFL methods across three large-scale\nclinical datasets: 10-year cardiovascular risk prediction (CPRD, 387 general\npractitioner practices, 258,688 patients), iron deficiency detection (INTERVAL,\n4 donor centres, 31,949 blood donors), and mortality prediction (eICU, 150\nhospitals, 44,842 patients). FedMAP incorporates a three-tier design that\nenables participation across healthcare sites with varying infrastructure and\ntechnical capabilities, from full federated training to inference-only\ndeployment. Geographical analysis reveals substantial equity improvements, with\nunderperforming regions achieving up to 14.3% performance gains. This framework\nprovides the first practical pathway for large-scale healthcare FL deployment,\nwhich ensures clinical sites at all scales can benefit, equity is enhanced, and\nprivacy is retained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) promises to enable collaborative machine learning\nacross healthcare sites whilst preserving data privacy. Practical deployment\nremains limited by statistical heterogeneity arising from differences in\npatient demographics, treatments, and outcomes, and infrastructure constraints.\nWe introduce FedMAP, a personalised FL (PFL) framework that addresses\nheterogeneity through local Maximum a Posteriori (MAP) estimation with Input\nConvex Neural Network priors. These priors represent global knowledge gathered\nfrom other sites that guides the model while adapting to local data, and we\nprovide a formal proof of convergence. Unlike many PFL methods that rely on\nfixed regularisation, FedMAP's prior adaptively learns patterns that capture\ncomplex inter-site relationships. We demonstrate improved performance compared\nto local training, FedAvg, and several PFL methods across three large-scale\nclinical datasets: 10-year cardiovascular risk prediction (CPRD, 387 general\npractitioner practices, 258,688 patients), iron deficiency detection (INTERVAL,\n4 donor centres, 31,949 blood donors), and mortality prediction (eICU, 150\nhospitals, 44,842 patients). FedMAP incorporates a three-tier design that\nenables participation across healthcare sites with varying infrastructure and\ntechnical capabilities, from full federated training to inference-only\ndeployment. Geographical analysis reveals substantial equity improvements, with\nunderperforming regions achieving up to 14.3% performance gains. This framework\nprovides the first practical pathway for large-scale healthcare FL deployment,\nwhich ensures clinical sites at all scales can benefit, equity is enhanced, and\nprivacy is retained."
                },
                "authors": [
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Daniel Kreuter"
                    },
                    {
                        "name": "Carlos Esteve-Yagüe"
                    },
                    {
                        "name": "Sören Dittmer"
                    },
                    {
                        "name": "Javier Fernandez-Marques"
                    },
                    {
                        "name": "Samantha Ip"
                    },
                    {
                        "name": "BloodCounts! Consortium"
                    },
                    {
                        "name": "Norbert C. J. de Wit"
                    },
                    {
                        "name": "Angela Wood"
                    },
                    {
                        "name": "James HF Rudd"
                    },
                    {
                        "name": "Nicholas Lane"
                    },
                    {
                        "name": "Nicholas S Gleadall"
                    },
                    {
                        "name": "Carola-Bibiane Schönlieb"
                    },
                    {
                        "name": "Michael Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Michael Roberts"
                },
                "author": "Michael Roberts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21729v2",
                "updated": "2025-10-28T16:15:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    15,
                    47,
                    1,
                    301,
                    0
                ],
                "published": "2025-09-30T00:25:47Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    0,
                    25,
                    47,
                    1,
                    273,
                    0
                ],
                "title": "CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known\n  Document Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CustomIR: Unsupervised Fine-Tuning of Dense Embeddings for Known\n  Document Corpora"
                },
                "summary": "Dense embedding models have become critical for modern information retrieval,\nparticularly in RAG pipelines, but their performance often degrades when\napplied to specialized corpora outside their pre-training distribution. To\naddress thi we introduce CustomIR, a framework for unsupervised adaptation of\npre-trained language embedding models to domain-specific corpora using\nsynthetically generated query-document pairs. CustomIR leverages large language\nmodels (LLMs) to create diverse queries grounded in a known target corpus,\npaired with LLM-verified hard negatives, eliminating the need for costly human\nannotation. Experiments on enterprise email and messaging datasets show that\nCustomIR consistently improves retrieval effectiveness with small models\ngaining up to 2.3 points in Recall@10. This performance increase allows these\nsmall models to rival the performance of much larger alternatives, allowing for\ncheaper RAG deployments. These results highlight that targeted synthetic\nfine-tuning offers a scalable and cost-efficient strategy for increasing\ndomain-specific performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense embedding models have become critical for modern information retrieval,\nparticularly in RAG pipelines, but their performance often degrades when\napplied to specialized corpora outside their pre-training distribution. To\naddress thi we introduce CustomIR, a framework for unsupervised adaptation of\npre-trained language embedding models to domain-specific corpora using\nsynthetically generated query-document pairs. CustomIR leverages large language\nmodels (LLMs) to create diverse queries grounded in a known target corpus,\npaired with LLM-verified hard negatives, eliminating the need for costly human\nannotation. Experiments on enterprise email and messaging datasets show that\nCustomIR consistently improves retrieval effectiveness with small models\ngaining up to 2.3 points in Recall@10. This performance increase allows these\nsmall models to rival the performance of much larger alternatives, allowing for\ncheaper RAG deployments. These results highlight that targeted synthetic\nfine-tuning offers a scalable and cost-efficient strategy for increasing\ndomain-specific performance."
                },
                "authors": [
                    {
                        "name": "Nathan Paull"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Paull"
                },
                "author": "Nathan Paull",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24582v1",
                "updated": "2025-10-28T16:15:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    15,
                    17,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T16:15:17Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    15,
                    17,
                    1,
                    301,
                    0
                ],
                "title": "Politically Speaking: LLMs on Changing International Affairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Politically Speaking: LLMs on Changing International Affairs"
                },
                "summary": "Ask your chatbot to impersonate an expert from Russia and an expert from US\nand query it on Chinese politics. How might the outputs differ? Or, to prepare\nourselves for the worse, how might they converge? Scholars have raised concerns\nLLM based applications can homogenize cultures and flatten perspectives. But\nexactly how much does LLM generated outputs converge despite explicit different\nrole assignment? This study provides empirical evidence to the above question.\nThe critique centres on pretrained models regurgitating ossified political\njargons used in the Western world when speaking about China, Iran, Russian, and\nUS politics, despite changes in these countries happening daily or hourly. The\nexperiments combine role-prompting and similarity metrics. The results show\nthat AI generated discourses from four models about Iran and China are the most\nhomogeneous and unchanging across all four models, including OpenAI GPT, Google\nGemini, Anthropic Claude, and DeepSeek, despite the prompted perspective change\nand the actual changes in real life. This study does not engage with history,\npolitics, or literature as traditional disciplinary approaches would; instead,\nit takes cues from international and area studies and offers insight on the\nfuture trajectory of shifting political discourse in a digital space\nincreasingly cannibalised by AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask your chatbot to impersonate an expert from Russia and an expert from US\nand query it on Chinese politics. How might the outputs differ? Or, to prepare\nourselves for the worse, how might they converge? Scholars have raised concerns\nLLM based applications can homogenize cultures and flatten perspectives. But\nexactly how much does LLM generated outputs converge despite explicit different\nrole assignment? This study provides empirical evidence to the above question.\nThe critique centres on pretrained models regurgitating ossified political\njargons used in the Western world when speaking about China, Iran, Russian, and\nUS politics, despite changes in these countries happening daily or hourly. The\nexperiments combine role-prompting and similarity metrics. The results show\nthat AI generated discourses from four models about Iran and China are the most\nhomogeneous and unchanging across all four models, including OpenAI GPT, Google\nGemini, Anthropic Claude, and DeepSeek, despite the prompted perspective change\nand the actual changes in real life. This study does not engage with history,\npolitics, or literature as traditional disciplinary approaches would; instead,\nit takes cues from international and area studies and offers insight on the\nfuture trajectory of shifting political discourse in a digital space\nincreasingly cannibalised by AI."
                },
                "authors": [
                    {
                        "name": "Xuenan Cao"
                    },
                    {
                        "name": "Wai Kei Chung"
                    },
                    {
                        "name": "Ye Zhao"
                    },
                    {
                        "name": "Lidia Mengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Lidia Mengyuan Zhou"
                },
                "author": "Lidia Mengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14617v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14617v3",
                "updated": "2025-10-28T16:02:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    2,
                    10,
                    1,
                    301,
                    0
                ],
                "published": "2025-05-20T17:03:12Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    3,
                    12,
                    1,
                    140,
                    0
                ],
                "title": "The Hawthorne Effect in Reasoning Models: Evaluating and Steering Test\n  Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hawthorne Effect in Reasoning Models: Evaluating and Steering Test\n  Awareness"
                },
                "summary": "Reasoning-focused LLMs sometimes alter their behavior when they detect that\nthey are being evaluated, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its performance on\nsafety-related tasks. We introduce a white-box probing framework that (i)\nlinearly identifies awareness-related activations and (ii) steers models toward\nor away from test awareness while monitoring downstream performance. We apply\nour method to different state-of-the-art open-weight reasoning LLMs across both\nrealistic and hypothetical tasks (denoting tests or simulations). Our results\ndemonstrate that test awareness significantly impacts safety alignment (such as\ncompliance with harmful requests and conforming to stereotypes) with effects\nvarying in both magnitude and direction across models. By providing control\nover this latent effect, our work aims to provide a stress-test mechanism and\nincrease trust in how we perform safety evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-focused LLMs sometimes alter their behavior when they detect that\nthey are being evaluated, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its performance on\nsafety-related tasks. We introduce a white-box probing framework that (i)\nlinearly identifies awareness-related activations and (ii) steers models toward\nor away from test awareness while monitoring downstream performance. We apply\nour method to different state-of-the-art open-weight reasoning LLMs across both\nrealistic and hypothetical tasks (denoting tests or simulations). Our results\ndemonstrate that test awareness significantly impacts safety alignment (such as\ncompliance with harmful requests and conforming to stereotypes) with effects\nvarying in both magnitude and direction across models. By providing control\nover this latent effect, our work aims to provide a stress-test mechanism and\nincrease trust in how we perform safety evaluations."
                },
                "authors": [
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Ahmed Salem"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Salem"
                },
                "author": "Ahmed Salem",
                "arxiv_comment": "NeurIPS 2025 (Spotlight). Code is available at:\n  https://github.com/microsoft/Test_Awareness_Steering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14617v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14617v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24561v1",
                "updated": "2025-10-28T15:55:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    55,
                    36,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:55:36Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    55,
                    36,
                    1,
                    301,
                    0
                ],
                "title": "LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via\n  Asymptotic Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via\n  Asymptotic Analysis"
                },
                "summary": "With the widespread adoption of LLMs, LoRA has become a dominant method for\nPEFT, and its initialization methods have attracted increasing attention.\nHowever, existing methods have notable limitations: many methods do not\nincorporate target-domain data, while gradient-based methods exploit data only\nat a shallow level by relying on one-step gradient decomposition, which remains\nunsatisfactory due to the weak empirical performance of the one-step\nfine-tuning model that serves as their basis, as well as the fact that these\nmethods either lack a rigorous theoretical foundation or depend heavily on\nrestrictive isotropic assumptions. In this paper, we establish a theoretical\nframework for data-aware LoRA initialization based on asymptotic analysis.\nStarting from a general optimization objective that minimizes the expectation\nof the parameter discrepancy between the fine-tuned and target models, we\nderive an optimization problem with two components: a bias term, which is\nrelated to the parameter distance between the fine-tuned and target models, and\nis approximated using a Fisher-gradient formulation to preserve anisotropy; and\na variance term, which accounts for the uncertainty introduced by sampling\nstochasticity through the Fisher information. By solving this problem, we\nobtain an optimal initialization strategy for LoRA. Building on this\ntheoretical framework, we develop an efficient algorithm, LoRA-DA, which\nestimates the terms in the optimization problem from a small set of target\ndomain samples and obtains the optimal LoRA initialization. Empirical results\nacross multiple benchmarks demonstrate that LoRA-DA consistently improves final\naccuracy over existing initialization methods. Additional studies show faster,\nmore stable convergence, robustness across ranks, and only a small\ninitialization overhead for LoRA-DA. The source code will be released upon\npublication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of LLMs, LoRA has become a dominant method for\nPEFT, and its initialization methods have attracted increasing attention.\nHowever, existing methods have notable limitations: many methods do not\nincorporate target-domain data, while gradient-based methods exploit data only\nat a shallow level by relying on one-step gradient decomposition, which remains\nunsatisfactory due to the weak empirical performance of the one-step\nfine-tuning model that serves as their basis, as well as the fact that these\nmethods either lack a rigorous theoretical foundation or depend heavily on\nrestrictive isotropic assumptions. In this paper, we establish a theoretical\nframework for data-aware LoRA initialization based on asymptotic analysis.\nStarting from a general optimization objective that minimizes the expectation\nof the parameter discrepancy between the fine-tuned and target models, we\nderive an optimization problem with two components: a bias term, which is\nrelated to the parameter distance between the fine-tuned and target models, and\nis approximated using a Fisher-gradient formulation to preserve anisotropy; and\na variance term, which accounts for the uncertainty introduced by sampling\nstochasticity through the Fisher information. By solving this problem, we\nobtain an optimal initialization strategy for LoRA. Building on this\ntheoretical framework, we develop an efficient algorithm, LoRA-DA, which\nestimates the terms in the optimization problem from a small set of target\ndomain samples and obtains the optimal LoRA initialization. Empirical results\nacross multiple benchmarks demonstrate that LoRA-DA consistently improves final\naccuracy over existing initialization methods. Additional studies show faster,\nmore stable convergence, robustness across ranks, and only a small\ninitialization overhead for LoRA-DA. The source code will be released upon\npublication."
                },
                "authors": [
                    {
                        "name": "Qingyue Zhang"
                    },
                    {
                        "name": "Chang Chu"
                    },
                    {
                        "name": "Tianren Peng"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Xiangyang Luo"
                    },
                    {
                        "name": "Zhihao Jiang"
                    },
                    {
                        "name": "Shao-Lun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shao-Lun Huang"
                },
                "author": "Shao-Lun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24554v1",
                "updated": "2025-10-28T15:51:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    51,
                    14,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:51:14Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    51,
                    14,
                    1,
                    301,
                    0
                ],
                "title": "An Adaptive Inspection Planning Approach Towards Routine Monitoring in\n  Uncertain Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Adaptive Inspection Planning Approach Towards Routine Monitoring in\n  Uncertain Environments"
                },
                "summary": "In this work, we present a hierarchical framework designed to support robotic\ninspection under environment uncertainty. By leveraging a known environment\nmodel, existing methods plan and safely track inspection routes to visit points\nof interest. However, discrepancies between the model and actual site\nconditions, caused by either natural or human activities, can alter the surface\nmorphology or introduce path obstructions. To address this challenge, the\nproposed framework divides the inspection task into: (a) generating the initial\nglobal view-plan for region of interests based on a historical map and (b)\nlocal view replanning to adapt to the current morphology of the inspection\nscene. The proposed hierarchy preserves global coverage objectives while\nenabling reactive adaptation to the local surface morphology. This enables the\nlocal autonomy to remain robust against environment uncertainty and complete\nthe inspection tasks. We validate the approach through deployments in\nreal-world subterranean mines using quadrupedal robot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present a hierarchical framework designed to support robotic\ninspection under environment uncertainty. By leveraging a known environment\nmodel, existing methods plan and safely track inspection routes to visit points\nof interest. However, discrepancies between the model and actual site\nconditions, caused by either natural or human activities, can alter the surface\nmorphology or introduce path obstructions. To address this challenge, the\nproposed framework divides the inspection task into: (a) generating the initial\nglobal view-plan for region of interests based on a historical map and (b)\nlocal view replanning to adapt to the current morphology of the inspection\nscene. The proposed hierarchy preserves global coverage objectives while\nenabling reactive adaptation to the local surface morphology. This enables the\nlocal autonomy to remain robust against environment uncertainty and complete\nthe inspection tasks. We validate the approach through deployments in\nreal-world subterranean mines using quadrupedal robot."
                },
                "authors": [
                    {
                        "name": "Vignesh Kottayam Viswanathan"
                    },
                    {
                        "name": "Yifan Bai"
                    },
                    {
                        "name": "Scott Fredriksson"
                    },
                    {
                        "name": "Sumeet Satpute"
                    },
                    {
                        "name": "Christoforos Kanellakis"
                    },
                    {
                        "name": "George Nikolakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "George Nikolakopoulos"
                },
                "author": "George Nikolakopoulos",
                "arxiv_comment": "Submitted for ICRA 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22931v2",
                "updated": "2025-10-28T15:51:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    51,
                    13,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-27T02:15:51Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    2,
                    15,
                    51,
                    0,
                    300,
                    0
                ],
                "title": "Robust Uncertainty Quantification for Self-Evolving Large Language\n  Models via Continual Domain Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Uncertainty Quantification for Self-Evolving Large Language\n  Models via Continual Domain Pretraining"
                },
                "summary": "Continual Learning (CL) is essential for enabling self-evolving large\nlanguage models (LLMs) to adapt and remain effective amid rapid knowledge\ngrowth. Yet, despite its importance, little attention has been given to\nestablishing statistical reliability guarantees for LLMs under CL, particularly\nin the setting of continual domain pretraining (CDP). Conformal Prediction (CP)\nhas shown promise in offering correctness guarantees for LLMs, but it faces\nmajor challenges in CDP: testing data often stems from unknown or shifting\ndomain distributions, under which CP may no longer provide valid guarantees.\nMoreover, when high coverage is required, CP can yield excessively large\nprediction sets for unanswerable queries, reducing informativeness. To address\nthese challenges, we introduce an adaptive rejection and non-exchangeable CP\nframework. Our method first estimates the distribution of questions across\ndomains in the test set using transformer-based clustering, then reweights or\nresamples the calibration data accordingly. Building on this, adaptive\nrejection CP allows the LLM to selectively abstain from answering when its\nconfidence or competence shifts significantly. Extensive experiments\ndemonstrate that our framework enhances both the effectiveness and reliability\nof CP under CDP scenarios. Our code is available at:\nhttps://anonymous.4open.science/r/CPCL-8C12/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Learning (CL) is essential for enabling self-evolving large\nlanguage models (LLMs) to adapt and remain effective amid rapid knowledge\ngrowth. Yet, despite its importance, little attention has been given to\nestablishing statistical reliability guarantees for LLMs under CL, particularly\nin the setting of continual domain pretraining (CDP). Conformal Prediction (CP)\nhas shown promise in offering correctness guarantees for LLMs, but it faces\nmajor challenges in CDP: testing data often stems from unknown or shifting\ndomain distributions, under which CP may no longer provide valid guarantees.\nMoreover, when high coverage is required, CP can yield excessively large\nprediction sets for unanswerable queries, reducing informativeness. To address\nthese challenges, we introduce an adaptive rejection and non-exchangeable CP\nframework. Our method first estimates the distribution of questions across\ndomains in the test set using transformer-based clustering, then reweights or\nresamples the calibration data accordingly. Building on this, adaptive\nrejection CP allows the LLM to selectively abstain from answering when its\nconfidence or competence shifts significantly. Extensive experiments\ndemonstrate that our framework enhances both the effectiveness and reliability\nof CP under CDP scenarios. Our code is available at:\nhttps://anonymous.4open.science/r/CPCL-8C12/"
                },
                "authors": [
                    {
                        "name": "Xiaofan Zhou"
                    },
                    {
                        "name": "Lu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Cheng"
                },
                "author": "Lu Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24551v1",
                "updated": "2025-10-28T15:47:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    47,
                    44,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:47:44Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    47,
                    44,
                    1,
                    301,
                    0
                ],
                "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives"
                },
                "summary": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery."
                },
                "authors": [
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Changshuo Liu"
                    },
                    {
                        "name": "Gene Anne Ooi"
                    },
                    {
                        "name": "Marcus Tan"
                    },
                    {
                        "name": "Zhongle Xie"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "James Wei Luen Yip"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Beng Chin Ooi"
                    }
                ],
                "author_detail": {
                    "name": "Beng Chin Ooi"
                },
                "author": "Beng Chin Ooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24538v1",
                "updated": "2025-10-28T15:42:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    42,
                    3,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:42:03Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    42,
                    3,
                    1,
                    301,
                    0
                ],
                "title": "Dark & Stormy: Modeling Humor in the Worst Sentences Ever Written",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark & Stormy: Modeling Humor in the Worst Sentences Ever Written"
                },
                "summary": "Textual humor is enormously diverse and computational studies need to account\nfor this range, including intentionally bad humor. In this paper, we curate and\nanalyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to\nbetter understand \"bad\" humor in English. Standard humor detection models\nperform poorly on our corpus, and an analysis of literary devices finds that\nthese sentences combine features common in existing humor datasets (e.g., puns,\nirony) with metaphor, metafiction and simile. LLMs prompted to synthesize\ncontest-style sentences imitate the form but exaggerate the effect by\nover-using certain literary devices, and including far more novel\nadjective-noun bigrams than human writers. Data, code and analysis are\navailable at https://github.com/venkatasg/bulwer-lytton",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual humor is enormously diverse and computational studies need to account\nfor this range, including intentionally bad humor. In this paper, we curate and\nanalyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to\nbetter understand \"bad\" humor in English. Standard humor detection models\nperform poorly on our corpus, and an analysis of literary devices finds that\nthese sentences combine features common in existing humor datasets (e.g., puns,\nirony) with metaphor, metafiction and simile. LLMs prompted to synthesize\ncontest-style sentences imitate the form but exaggerate the effect by\nover-using certain literary devices, and including far more novel\nadjective-noun bigrams than human writers. Data, code and analysis are\navailable at https://github.com/venkatasg/bulwer-lytton"
                },
                "authors": [
                    {
                        "name": "Venkata S Govindarajan"
                    },
                    {
                        "name": "Laura Biester"
                    }
                ],
                "author_detail": {
                    "name": "Laura Biester"
                },
                "author": "Laura Biester",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24528v1",
                "updated": "2025-10-28T15:37:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    37,
                    51,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:37:51Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    37,
                    51,
                    1,
                    301,
                    0
                ],
                "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based\n  Pseudo-Labeling Framework for In-context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cross-Task Examples to In-Task Prompts: A Graph-Based\n  Pseudo-Labeling Framework for In-context Learning"
                },
                "summary": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs."
                },
                "authors": [
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Xingbo Fu"
                    },
                    {
                        "name": "Chengshuai Shi"
                    },
                    {
                        "name": "Zhenyu Lei"
                    },
                    {
                        "name": "Cong Shen"
                    },
                    {
                        "name": "Jundong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jundong Li"
                },
                "author": "Jundong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15545v2",
                "updated": "2025-10-28T15:23:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    23,
                    35,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-17T11:25:36Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    11,
                    25,
                    36,
                    4,
                    290,
                    0
                ],
                "title": "TokenTiming: A Dynamic Alignment Method for Universal Speculative\n  Decoding Model Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenTiming: A Dynamic Alignment Method for Universal Speculative\n  Decoding Model Pairs"
                },
                "summary": "Accelerating the inference of large language models (LLMs) has been a\ncritical challenge in generative AI. Speculative decoding (SD) substantially\nimproves LLM inference efficiency. However, its utility is limited by a\nfundamental constraint: the draft and target models must share the same\nvocabulary, thus limiting the herd of available draft models and often\nnecessitating the training of a new model from scratch. Inspired by Dynamic\nTime Warping (DTW), a classic algorithm for aligning time series, we propose\nthe algorithm TokenTiming for universal speculative decoding. It operates by\nre-encoding the draft token sequence to get a new target token sequence, and\nthen uses DTW to build a mapping to transfer the probability distributions for\nspeculative sampling. Benefiting from this, our method accommodates mismatched\nvocabularies and works with any off-the-shelf models without retraining and\nmodification. We conduct comprehensive experiments on various tasks,\ndemonstrating 1.57x speedup. This work enables a universal approach for draft\nmodel selection, making SD a more versatile and practical tool for LLM\nacceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating the inference of large language models (LLMs) has been a\ncritical challenge in generative AI. Speculative decoding (SD) substantially\nimproves LLM inference efficiency. However, its utility is limited by a\nfundamental constraint: the draft and target models must share the same\nvocabulary, thus limiting the herd of available draft models and often\nnecessitating the training of a new model from scratch. Inspired by Dynamic\nTime Warping (DTW), a classic algorithm for aligning time series, we propose\nthe algorithm TokenTiming for universal speculative decoding. It operates by\nre-encoding the draft token sequence to get a new target token sequence, and\nthen uses DTW to build a mapping to transfer the probability distributions for\nspeculative sampling. Benefiting from this, our method accommodates mismatched\nvocabularies and works with any off-the-shelf models without retraining and\nmodification. We conduct comprehensive experiments on various tasks,\ndemonstrating 1.57x speedup. This work enables a universal approach for draft\nmodel selection, making SD a more versatile and practical tool for LLM\nacceleration."
                },
                "authors": [
                    {
                        "name": "Sibo Xiao"
                    },
                    {
                        "name": "Jinyuan Fu"
                    },
                    {
                        "name": "Zhongle Xie"
                    },
                    {
                        "name": "Lidan Shou"
                    }
                ],
                "author_detail": {
                    "name": "Lidan Shou"
                },
                "author": "Lidan Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00799v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00799v3",
                "updated": "2025-10-28T15:20:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    20,
                    47,
                    1,
                    301,
                    0
                ],
                "published": "2025-06-01T03:00:09Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    3,
                    0,
                    9,
                    6,
                    152,
                    0
                ],
                "title": "Uni-LoRA: One Vector is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-LoRA: One Vector is All You Need"
                },
                "summary": "Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient\nfine-tuning (PEFT) method for large language models (LLMs) by constraining\nweight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and\nVB-LoRA push efficiency further by introducing additional constraints to reduce\nthe trainable parameter space. In this paper, we show that the parameter space\nreduction strategies employed by these LoRA variants can be formulated within a\nunified framework, Uni-LoRA, where the LoRA parameter space, flattened as a\nhigh-dimensional vector space $R^D$, can be reconstructed through a projection\nfrom a subspace R^d, with $d \\ll D$. We demonstrate that the fundamental\ndifference among various LoRA methods lies in the choice of the projection\nmatrix, $P \\in R^{D \\times d}$.Most existing LoRA variants rely on layer-wise\nor structure-specific projections that limit cross-layer parameter sharing,\nthereby compromising parameter efficiency. In light of this, we introduce an\nefficient and theoretically grounded projection matrix that is isometric,\nenabling global parameter sharing and reducing computation overhead.\nFurthermore, under the unified view of Uni-LoRA, this design requires only a\nsingle trainable vector to reconstruct LoRA parameters for the entire LLM -\nmaking Uni-LoRA both a unified framework and a \"one-vector-only\" solution.\nExtensive experiments on GLUE, mathematical reasoning, and instruction tuning\nbenchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter\nefficiency while outperforming or matching prior approaches in predictive\nperformance. Our code is available at\nhttps://github.com/KaiyangLi1992/Uni-LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient\nfine-tuning (PEFT) method for large language models (LLMs) by constraining\nweight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and\nVB-LoRA push efficiency further by introducing additional constraints to reduce\nthe trainable parameter space. In this paper, we show that the parameter space\nreduction strategies employed by these LoRA variants can be formulated within a\nunified framework, Uni-LoRA, where the LoRA parameter space, flattened as a\nhigh-dimensional vector space $R^D$, can be reconstructed through a projection\nfrom a subspace R^d, with $d \\ll D$. We demonstrate that the fundamental\ndifference among various LoRA methods lies in the choice of the projection\nmatrix, $P \\in R^{D \\times d}$.Most existing LoRA variants rely on layer-wise\nor structure-specific projections that limit cross-layer parameter sharing,\nthereby compromising parameter efficiency. In light of this, we introduce an\nefficient and theoretically grounded projection matrix that is isometric,\nenabling global parameter sharing and reducing computation overhead.\nFurthermore, under the unified view of Uni-LoRA, this design requires only a\nsingle trainable vector to reconstruct LoRA parameters for the entire LLM -\nmaking Uni-LoRA both a unified framework and a \"one-vector-only\" solution.\nExtensive experiments on GLUE, mathematical reasoning, and instruction tuning\nbenchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter\nefficiency while outperforming or matching prior approaches in predictive\nperformance. Our code is available at\nhttps://github.com/KaiyangLi1992/Uni-LoRA."
                },
                "authors": [
                    {
                        "name": "Kaiyang Li"
                    },
                    {
                        "name": "Shaobo Han"
                    },
                    {
                        "name": "Qing Su"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Zhipeng Cai"
                    },
                    {
                        "name": "Shihao Ji"
                    }
                ],
                "author_detail": {
                    "name": "Shihao Ji"
                },
                "author": "Shihao Ji",
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00799v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00799v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24509v1",
                "updated": "2025-10-28T15:20:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    20,
                    38,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:20:38Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    20,
                    38,
                    1,
                    301,
                    0
                ],
                "title": "Quantum Combinatorial Reasoning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Combinatorial Reasoning for Large Language Models"
                },
                "summary": "We design and implement a quantum combinatorial reasoning framework for large\nlanguage models (QCR-LLM), integrating a real quantum computer in the hybrid\nworkflow. QCR-LLM reformulates reasoning aggregation as a higher-order\nunconstrained binary optimization (HUBO) problem. In this sense, reasoning\nfragments are represented as binary variables and their interactions encode\nstatistical relevance, logical coherence, and semantic redundancy. We tackle\nthe resulting high-order optimization problem both classically, via simulated\nannealing, and quantumly through the bias-field digitized counterdiabatic\nquantum optimizer (BF-DCQO) executed on IBM's superconducting digital quantum\nprocessors. Experiments on BIG-Bench Extra Hard (BBEH) benchmarks demonstrate\nthat our QCR-LLM consistently improves reasoning accuracy across multiple LLM\nbackbones, surpassing reasoning-native systems such as o3-high and DeepSeek R1\nby up to $+9\\,$pp. Despite requiring multiple reasoning samples per query, our\nQCR-LLM remains approximately five times more energy-efficient than o3-high,\nowing to the low per-token energy footprint of its GPT-4o backbone. These\nresults constitute the first experimental evidence of quantum-assisted\nreasoning, showing that hybrid quantum-classical optimization can efficiently\nenhance reasoning coherence, interpretability, and sustainability in\nlarge-scale language models. We have opened the doors to the emergence of\nquantum intelligence, where harder prompts require quantum optimizers at\nquantum-advantage level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We design and implement a quantum combinatorial reasoning framework for large\nlanguage models (QCR-LLM), integrating a real quantum computer in the hybrid\nworkflow. QCR-LLM reformulates reasoning aggregation as a higher-order\nunconstrained binary optimization (HUBO) problem. In this sense, reasoning\nfragments are represented as binary variables and their interactions encode\nstatistical relevance, logical coherence, and semantic redundancy. We tackle\nthe resulting high-order optimization problem both classically, via simulated\nannealing, and quantumly through the bias-field digitized counterdiabatic\nquantum optimizer (BF-DCQO) executed on IBM's superconducting digital quantum\nprocessors. Experiments on BIG-Bench Extra Hard (BBEH) benchmarks demonstrate\nthat our QCR-LLM consistently improves reasoning accuracy across multiple LLM\nbackbones, surpassing reasoning-native systems such as o3-high and DeepSeek R1\nby up to $+9\\,$pp. Despite requiring multiple reasoning samples per query, our\nQCR-LLM remains approximately five times more energy-efficient than o3-high,\nowing to the low per-token energy footprint of its GPT-4o backbone. These\nresults constitute the first experimental evidence of quantum-assisted\nreasoning, showing that hybrid quantum-classical optimization can efficiently\nenhance reasoning coherence, interpretability, and sustainability in\nlarge-scale language models. We have opened the doors to the emergence of\nquantum intelligence, where harder prompts require quantum optimizers at\nquantum-advantage level."
                },
                "authors": [
                    {
                        "name": "Carlos Flores-Garrigos"
                    },
                    {
                        "name": "Gaurav Dev"
                    },
                    {
                        "name": "Michael Falkenthal"
                    },
                    {
                        "name": "Alejandro Gomez Cadavid"
                    },
                    {
                        "name": "Anton Simen"
                    },
                    {
                        "name": "Shubham Kumar"
                    },
                    {
                        "name": "Enrique Solano"
                    },
                    {
                        "name": "Narendra N. Hegade"
                    }
                ],
                "author_detail": {
                    "name": "Narendra N. Hegade"
                },
                "author": "Narendra N. Hegade",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24505v1",
                "updated": "2025-10-28T15:16:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    16,
                    6,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:16:06Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    16,
                    6,
                    1,
                    301,
                    0
                ],
                "title": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?"
                },
                "summary": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate confidence calibration in Large Language Models (LLMs) is critical\nfor safe use in high-stakes domains, where clear verbalized confidence enhances\nuser trust. Traditional methods that mimic reference confidence expressions\noften fail to capture the reasoning needed for accurate confidence assessment.\nWe propose natural language critiques as a solution, ideally suited for\nconfidence calibration, as precise gold confidence labels are hard to obtain\nand often require multiple generations. This paper studies how natural language\ncritiques can enhance verbalized confidence, addressing: (1) What to critique:\nuncertainty (question-focused) or confidence (answer-specific)? Analysis shows\nconfidence suits multiple-choice tasks, while uncertainty excels in open-ended\nscenarios. (2) How to critique: self-critique or critique calibration training?\nWe propose Self-Critique, enabling LLMs to critique and optimize their\nconfidence beyond mere accuracy, and CritiCal, a novel Critique Calibration\ntraining method that leverages natural language critiques to improve confidence\ncalibration, moving beyond direct numerical optimization. Experiments show that\nCritiCal significantly outperforms Self-Critique and other competitive\nbaselines, even surpassing its teacher model, GPT-4o, in complex reasoning\ntasks. CritiCal also shows robust generalization in out-of-distribution\nsettings, advancing LLM's reliability."
                },
                "authors": [
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Baixuan Xu"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10978v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10978v3",
                "updated": "2025-10-28T15:11:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    11,
                    36,
                    1,
                    301,
                    0
                ],
                "published": "2025-05-16T08:26:59Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    8,
                    26,
                    59,
                    4,
                    136,
                    0
                ],
                "title": "Group-in-Group Policy Optimization for LLM Agent Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group-in-Group Policy Optimization for LLM Agent Training"
                },
                "summary": "Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to multi-turn LLM agent training remains\nlimited. Unlike static tasks, agent-environment interactions unfold over many\nsteps and often yield sparse or delayed rewards, making credit assignment\nacross individual steps significantly more challenging. In this work, we\npropose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that\nachieves fine-grained credit assignment for LLM agents while preserving the\nappealing properties of group-based RL: critic-free, low memory, and stable\nconvergence. GiGPO introduces a two-level structure for estimating relative\nadvantage: (i) At the episode-level, GiGPO computes macro relative advantages\nbased on groups of complete trajectories; (ii) At the step-level, GiGPO\nintroduces an anchor state grouping mechanism that retroactively constructs\nstep-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop,\nas well as tool-integrated reasoning on search-augmented QA tasks, using\nQwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step\ncredit signals, achieves performance gains of > 12% on ALFWorld and > 9% on\nWebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B\nand 47.2% on 7B): all while maintaining the same GPU memory overhead, identical\nLLM rollout, and incurring little to no additional time cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in group-based reinforcement learning (RL) have driven\nfrontier large language models (LLMs) in single-turn tasks like mathematical\nreasoning. However, their scalability to multi-turn LLM agent training remains\nlimited. Unlike static tasks, agent-environment interactions unfold over many\nsteps and often yield sparse or delayed rewards, making credit assignment\nacross individual steps significantly more challenging. In this work, we\npropose Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that\nachieves fine-grained credit assignment for LLM agents while preserving the\nappealing properties of group-based RL: critic-free, low memory, and stable\nconvergence. GiGPO introduces a two-level structure for estimating relative\nadvantage: (i) At the episode-level, GiGPO computes macro relative advantages\nbased on groups of complete trajectories; (ii) At the step-level, GiGPO\nintroduces an anchor state grouping mechanism that retroactively constructs\nstep-level groups by identifying repeated environment states across\ntrajectories. Actions stemming from the same state are grouped together,\nenabling micro relative advantage estimation. This hierarchical structure\neffectively captures both global trajectory quality and local step\neffectiveness without relying on auxiliary models or additional rollouts. We\nevaluate GiGPO on challenging agent benchmarks, including ALFWorld and WebShop,\nas well as tool-integrated reasoning on search-augmented QA tasks, using\nQwen2.5-1.5B/3B/7B-Instruct. Crucially, GiGPO delivers fine-grained per-step\ncredit signals, achieves performance gains of > 12% on ALFWorld and > 9% on\nWebShop over GRPO, and obtains superior performance on QA tasks (42.1% on 3B\nand 47.2% on 7B): all while maintaining the same GPU memory overhead, identical\nLLM rollout, and incurring little to no additional time cost."
                },
                "authors": [
                    {
                        "name": "Lang Feng"
                    },
                    {
                        "name": "Zhenghai Xue"
                    },
                    {
                        "name": "Tingcong Liu"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10978v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10978v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24488v1",
                "updated": "2025-10-28T15:03:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    3,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:03:18Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    3,
                    18,
                    1,
                    301,
                    0
                ],
                "title": "A word association network methodology for evaluating implicit biases in\n  LLMs compared to humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A word association network methodology for evaluating implicit biases in\n  LLMs compared to humans"
                },
                "summary": "As Large language models (LLMs) become increasingly integrated into our\nlives, their inherent social biases remain a pressing concern. Detecting and\nevaluating these biases can be challenging because they are often implicit\nrather than explicit in nature, so developing evaluation methods that assess\nthe implicit knowledge representations of LLMs is essential. We present a novel\nword association network methodology for evaluating implicit biases in LLMs\nbased on simulating semantic priming within LLM-generated word association\nnetworks. Our prompt-based approach taps into the implicit relational\nstructures encoded in LLMs, providing both quantitative and qualitative\nassessments of bias. Unlike most prompt-based evaluation methods, our method\nenables direct comparisons between various LLMs and humans, providing a\nvaluable point of reference and offering new insights into the alignment of\nLLMs with human cognition. To demonstrate the utility of our methodology, we\napply it to both humans and several widely used LLMs to investigate social\nbiases related to gender, religion, ethnicity, sexual orientation, and\npolitical party. Our results reveal both convergences and divergences between\nLLM and human biases, providing new perspectives on the potential risks of\nusing LLMs. Our methodology contributes to a systematic, scalable, and\ngeneralizable framework for evaluating and comparing biases across multiple\nLLMs and humans, advancing the goal of transparent and socially responsible\nlanguage technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large language models (LLMs) become increasingly integrated into our\nlives, their inherent social biases remain a pressing concern. Detecting and\nevaluating these biases can be challenging because they are often implicit\nrather than explicit in nature, so developing evaluation methods that assess\nthe implicit knowledge representations of LLMs is essential. We present a novel\nword association network methodology for evaluating implicit biases in LLMs\nbased on simulating semantic priming within LLM-generated word association\nnetworks. Our prompt-based approach taps into the implicit relational\nstructures encoded in LLMs, providing both quantitative and qualitative\nassessments of bias. Unlike most prompt-based evaluation methods, our method\nenables direct comparisons between various LLMs and humans, providing a\nvaluable point of reference and offering new insights into the alignment of\nLLMs with human cognition. To demonstrate the utility of our methodology, we\napply it to both humans and several widely used LLMs to investigate social\nbiases related to gender, religion, ethnicity, sexual orientation, and\npolitical party. Our results reveal both convergences and divergences between\nLLM and human biases, providing new perspectives on the potential risks of\nusing LLMs. Our methodology contributes to a systematic, scalable, and\ngeneralizable framework for evaluating and comparing biases across multiple\nLLMs and humans, advancing the goal of transparent and socially responsible\nlanguage technologies."
                },
                "authors": [
                    {
                        "name": "Katherine Abramski"
                    },
                    {
                        "name": "Giulio Rossetti"
                    },
                    {
                        "name": "Massimo Stella"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Stella"
                },
                "author": "Massimo Stella",
                "arxiv_comment": "24 pages, 13 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24476v1",
                "updated": "2025-10-28T14:48:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    48,
                    57,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:48:57Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    48,
                    57,
                    1,
                    301,
                    0
                ],
                "title": "Mitigating Hallucination in Large Language Models (LLMs): An\n  Application-Oriented Survey on RAG, Reasoning, and Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Hallucination in Large Language Models (LLMs): An\n  Application-Oriented Survey on RAG, Reasoning, and Agentic Systems"
                },
                "summary": "Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination remains one of the key obstacles to the reliable deployment of\nlarge language models (LLMs), particularly in real-world applications. Among\nvarious mitigation strategies, Retrieval-Augmented Generation (RAG) and\nreasoning enhancement have emerged as two of the most effective and widely\nadopted approaches, marking a shift from merely suppressing hallucinations to\nbalancing creativity and reliability. However, their synergistic potential and\nunderlying mechanisms for hallucination mitigation have not yet been\nsystematically examined. This survey adopts an application-oriented perspective\nof capability enhancement to analyze how RAG, reasoning enhancement, and their\nintegration in Agentic Systems mitigate hallucinations. We propose a taxonomy\ndistinguishing knowledge-based and logic-based hallucinations, systematically\nexamine how RAG and reasoning address each, and present a unified framework\nsupported by real-world applications, evaluations, and benchmarks."
                },
                "authors": [
                    {
                        "name": "Yihan Li"
                    },
                    {
                        "name": "Xiyuan Fu"
                    },
                    {
                        "name": "Ghanshyam Verma"
                    },
                    {
                        "name": "Paul Buitelaar"
                    },
                    {
                        "name": "Mingming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Liu"
                },
                "author": "Mingming Liu",
                "arxiv_comment": "25 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24469v1",
                "updated": "2025-10-28T14:36:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    36,
                    22,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:36:22Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    36,
                    22,
                    1,
                    301,
                    0
                ],
                "title": "Iterative Critique-Refine Framework for Enhancing LLM Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Critique-Refine Framework for Enhancing LLM Personalization"
                },
                "summary": "Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic."
                },
                "authors": [
                    {
                        "name": "Durga Prasad Maram"
                    },
                    {
                        "name": "Dhruvin Gandhi"
                    },
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Gayathri Akkinapalli"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Nesreen K. Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Nesreen K. Ahmed"
                },
                "author": "Nesreen K. Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20039v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20039v3",
                "updated": "2025-10-28T14:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    35,
                    32,
                    1,
                    301,
                    0
                ],
                "published": "2025-04-28T17:59:28Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    59,
                    28,
                    0,
                    118,
                    0
                ],
                "title": "AutoJudge: Judge Decoding Without Manual Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoJudge: Judge Decoding Without Manual Annotation"
                },
                "summary": "We introduce AutoJudge, a method that accelerates large language model (LLM)\ninference with task-specific lossy speculative decoding. Instead of matching\nthe original model output distribution token-by-token, we identify which of the\ngenerated tokens affect the downstream quality of the response, relaxing the\ndistribution match guarantee so that the \"unimportant\" tokens can be generated\nfaster. Our approach relies on a semi-greedy search algorithm to test which of\nthe mismatches between target and draft models should be corrected to preserve\nquality and which ones may be skipped. We then train a lightweight classifier\nbased on existing LLM embeddings to predict, at inference time, which\nmismatching tokens can be safely accepted without compromising the final answer\nquality. We evaluate the effectiveness of AutoJudge with multiple draft/target\nmodel pairs on mathematical reasoning and programming benchmarks, achieving\nsignificant speedups at the cost of a minor accuracy reduction. Notably, on\nGSM8k with the Llama 3.1 70B target model, our approach achieves up to\n$\\approx2\\times$ speedup over speculative decoding at the cost of $\\le 1\\%$\ndrop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge\nautomatically detects programming-specific important tokens, accepting $\\ge 25$\ntokens per speculation cycle at $2\\%$ drop in Pass@1. Our approach requires no\nhuman annotation and is easy to integrate with modern LLM inference frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AutoJudge, a method that accelerates large language model (LLM)\ninference with task-specific lossy speculative decoding. Instead of matching\nthe original model output distribution token-by-token, we identify which of the\ngenerated tokens affect the downstream quality of the response, relaxing the\ndistribution match guarantee so that the \"unimportant\" tokens can be generated\nfaster. Our approach relies on a semi-greedy search algorithm to test which of\nthe mismatches between target and draft models should be corrected to preserve\nquality and which ones may be skipped. We then train a lightweight classifier\nbased on existing LLM embeddings to predict, at inference time, which\nmismatching tokens can be safely accepted without compromising the final answer\nquality. We evaluate the effectiveness of AutoJudge with multiple draft/target\nmodel pairs on mathematical reasoning and programming benchmarks, achieving\nsignificant speedups at the cost of a minor accuracy reduction. Notably, on\nGSM8k with the Llama 3.1 70B target model, our approach achieves up to\n$\\approx2\\times$ speedup over speculative decoding at the cost of $\\le 1\\%$\ndrop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge\nautomatically detects programming-specific important tokens, accepting $\\ge 25$\ntokens per speculation cycle at $2\\%$ drop in Pass@1. Our approach requires no\nhuman annotation and is easy to integrate with modern LLM inference frameworks."
                },
                "authors": [
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Fedor Velikonivtsev"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20039v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20039v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17336v2",
                "updated": "2025-10-28T14:31:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    31,
                    14,
                    1,
                    301,
                    0
                ],
                "published": "2025-09-22T03:13:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    3,
                    13,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Mano Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mano Technical Report"
                },
                "summary": "Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Anyang Su"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Hanning Wang"
                    },
                    {
                        "name": "Minghui Wu"
                    },
                    {
                        "name": "Zhe Yu"
                    },
                    {
                        "name": "Fei Hu"
                    },
                    {
                        "name": "Mingjia Shi"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Jiayao Wang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Ruiyang Yu"
                    },
                    {
                        "name": "Siran Peng"
                    },
                    {
                        "name": "Menglin Li"
                    },
                    {
                        "name": "Nan Huang"
                    },
                    {
                        "name": "Haitian Wei"
                    },
                    {
                        "name": "Jiawei Yu"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Xilin Zhao"
                    },
                    {
                        "name": "Kai Gu"
                    },
                    {
                        "name": "Ping Jiang"
                    },
                    {
                        "name": "Sifan Zhou"
                    },
                    {
                        "name": "Shuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Wang"
                },
                "author": "Shuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24460v1",
                "updated": "2025-10-28T14:27:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    27,
                    33,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:27:33Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    27,
                    33,
                    1,
                    301,
                    0
                ],
                "title": "Collaborating Unmanned Aerial Vehicle and Ground Sensors for Urban\n  Signalized Network Traffic Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborating Unmanned Aerial Vehicle and Ground Sensors for Urban\n  Signalized Network Traffic Monitoring"
                },
                "summary": "Reliable estimation of network-wide traffic states is essential for urban\ntraffic management. Unmanned Aerial Vehicles (UAVs), with their airborne\nfull-sample continuous trajectory observation, bring new opportunities for\ntraffic state estimation. In this study, we will explore the optimal UAV\ndeployment problem in road networks in conjunction with ground sensors,\nincluding connected vehicle (CV) and loop detectors, to achieve more reliable\nestimation of vehicle path reconstruction as well as movement-based arrival\nrates and queue lengths. Oriented towards reliable estimation of traffic\nstates, we propose an index, feasible domain size, as the uncertainty\nmeasurement, and transform the optimal UAV deployment problem into minimizing\nthe observation uncertainty of network-wide traffic states. Given the\nlarge-scale and nonlinear nature of the problem, an improved quantum genetic\nalgorithm (IQGA) that integrates two customized operators is proposed to\nenhance neighbor searching and solution refinement, thereby improving the\nobservability of UAV pairs. Evaluation was conducted on an empirical network\nwith 18 intersections. Results demonstrated that a UAV fleet size of 7 is\nsufficient for traffic monitoring, with more than 60\\% of network-wide\nobservation uncertainty reduced. Through horizontal comparison with three\nbaselines, the optimal UAV location scheme obtained by the proposed method can\nreach an improvement of up to 7.23\\% and 5.02\\% in the estimation accuracy of\narrival rate and queue length, respectively. The proposed IQGA is also shown to\nbe faster in solution convergence than the classic QGA by about 9.22\\% with\nbetter exploration ability in optimum searching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable estimation of network-wide traffic states is essential for urban\ntraffic management. Unmanned Aerial Vehicles (UAVs), with their airborne\nfull-sample continuous trajectory observation, bring new opportunities for\ntraffic state estimation. In this study, we will explore the optimal UAV\ndeployment problem in road networks in conjunction with ground sensors,\nincluding connected vehicle (CV) and loop detectors, to achieve more reliable\nestimation of vehicle path reconstruction as well as movement-based arrival\nrates and queue lengths. Oriented towards reliable estimation of traffic\nstates, we propose an index, feasible domain size, as the uncertainty\nmeasurement, and transform the optimal UAV deployment problem into minimizing\nthe observation uncertainty of network-wide traffic states. Given the\nlarge-scale and nonlinear nature of the problem, an improved quantum genetic\nalgorithm (IQGA) that integrates two customized operators is proposed to\nenhance neighbor searching and solution refinement, thereby improving the\nobservability of UAV pairs. Evaluation was conducted on an empirical network\nwith 18 intersections. Results demonstrated that a UAV fleet size of 7 is\nsufficient for traffic monitoring, with more than 60\\% of network-wide\nobservation uncertainty reduced. Through horizontal comparison with three\nbaselines, the optimal UAV location scheme obtained by the proposed method can\nreach an improvement of up to 7.23\\% and 5.02\\% in the estimation accuracy of\narrival rate and queue length, respectively. The proposed IQGA is also shown to\nbe faster in solution convergence than the classic QGA by about 9.22\\% with\nbetter exploration ability in optimum searching."
                },
                "authors": [
                    {
                        "name": "Jiarong Yao"
                    },
                    {
                        "name": "Chaopeng Tan"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Wei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ma"
                },
                "author": "Wei Ma",
                "arxiv_comment": "22 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21724v2",
                "updated": "2025-10-28T14:26:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    26,
                    23,
                    1,
                    301,
                    0
                ],
                "published": "2025-05-27T20:12:46Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    20,
                    12,
                    46,
                    1,
                    147,
                    0
                ],
                "title": "OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions"
                },
                "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task designed to produce synchronized verbal and\nnon-verbal listener feedback online, based on the speaker's multimodal inputs.\nOMCRG captures natural dyadic interactions and introduces new challenges in\naligning generated audio with listeners' facial responses. To tackle these\nchallenges, we incorporate text as an intermediate modality to connect audio\nand facial responses. We propose OmniResponse, a Multimodal Large Language\nModel (MLLM) that autoregressively generates accurate multimodal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two core\ncomponents: Chrono-Text Markup, which precisely timestamps generated text\ntokens, and TempoVoice, a controllable online text-to-speech (TTS) module that\noutputs speech synchronized with facial responses. To advance OMCRG research,\nwe offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring\nsynchronized split-screen videos, multichannel audio, transcripts, and\nannotated facial behaviors. Comprehensive evaluations on ResponseNet\ndemonstrate that OmniResponse outperforms baseline models in terms of semantic\nspeech content, audio-visual synchronization, and generation quality. Our\ndataset, code, and models are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task designed to produce synchronized verbal and\nnon-verbal listener feedback online, based on the speaker's multimodal inputs.\nOMCRG captures natural dyadic interactions and introduces new challenges in\naligning generated audio with listeners' facial responses. To tackle these\nchallenges, we incorporate text as an intermediate modality to connect audio\nand facial responses. We propose OmniResponse, a Multimodal Large Language\nModel (MLLM) that autoregressively generates accurate multimodal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two core\ncomponents: Chrono-Text Markup, which precisely timestamps generated text\ntokens, and TempoVoice, a controllable online text-to-speech (TTS) module that\noutputs speech synchronized with facial responses. To advance OMCRG research,\nwe offer ResponseNet, a dataset of 696 detailed dyadic interactions featuring\nsynchronized split-screen videos, multichannel audio, transcripts, and\nannotated facial behaviors. Comprehensive evaluations on ResponseNet\ndemonstrate that OmniResponse outperforms baseline models in terms of semantic\nspeech content, audio-visual synchronization, and generation quality. Our\ndataset, code, and models are publicly available."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Jianghui Wang"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Siyang Song"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24450v1",
                "updated": "2025-10-28T14:13:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    13,
                    44,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:13:44Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    13,
                    44,
                    1,
                    301,
                    0
                ],
                "title": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a\n  Set of Best Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a\n  Set of Best Practices"
                },
                "summary": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods."
                },
                "authors": [
                    {
                        "name": "Špela Vintar"
                    },
                    {
                        "name": "Taja Kuzman Pungeršek"
                    },
                    {
                        "name": "Mojca Brglez"
                    },
                    {
                        "name": "Nikola Ljubešić"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Ljubešić"
                },
                "author": "Nikola Ljubešić",
                "arxiv_comment": "12 pages, 1 figure. Submitted to the LREC 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24448v1",
                "updated": "2025-10-28T14:12:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    12,
                    11,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:12:11Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    12,
                    11,
                    1,
                    301,
                    0
                ],
                "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Visual Intelligence: Insights from Video Pretraining"
                },
                "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models."
                },
                "authors": [
                    {
                        "name": "Pablo Acuaviva"
                    },
                    {
                        "name": "Aram Davtyan"
                    },
                    {
                        "name": "Mariam Hassan"
                    },
                    {
                        "name": "Sebastian Stapf"
                    },
                    {
                        "name": "Ahmad Rahimi"
                    },
                    {
                        "name": "Alexandre Alahi"
                    },
                    {
                        "name": "Paolo Favaro"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Favaro"
                },
                "author": "Paolo Favaro",
                "arxiv_comment": "Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T45, 68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8; I.5.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24442v1",
                "updated": "2025-10-28T14:07:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    7,
                    10,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:07:10Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    7,
                    10,
                    1,
                    301,
                    0
                ],
                "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Law in Silico: Simulating Legal Society with LLM-Based Agents"
                },
                "summary": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals."
                },
                "authors": [
                    {
                        "name": "Yiding Wang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Xifan Chen"
                    },
                    {
                        "name": "Xiaolei Yang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16175v2",
                "updated": "2025-10-28T14:06:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    6,
                    41,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-17T19:35:54Z",
                "published_parsed": [
                    2025,
                    10,
                    17,
                    19,
                    35,
                    54,
                    4,
                    290,
                    0
                ],
                "title": "The Formalism-Implementation Gap in Reinforcement Learning Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Formalism-Implementation Gap in Reinforcement Learning Research"
                },
                "summary": "The last decade has seen an upswing in interest and adoption of reinforcement\nlearning (RL) techniques, in large part due to its demonstrated capabilities at\nperforming certain tasks at \"super-human levels\". This has incentivized the\ncommunity to prioritize research that demonstrates RL agent performance, often\nat the expense of research aimed at understanding their learning dynamics.\nPerformance-focused research runs the risk of overfitting on academic\nbenchmarks -- thereby rendering them less useful -- which can make it difficult\nto transfer proposed techniques to novel problems. Further, it implicitly\ndiminishes work that does not push the performance-frontier, but aims at\nimproving our understanding of these techniques. This paper argues two points:\n(i) RL research should stop focusing solely on demonstrating agent\ncapabilities, and focus more on advancing the science and understanding of\nreinforcement learning; and (ii) we need to be more precise on how our\nbenchmarks map to the underlying mathematical formalisms. We use the popular\nArcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a\nbenchmark that, despite being increasingly considered \"saturated\", can be\neffectively used for developing this understanding, and facilitating the\ndeployment of RL techniques in impactful real-world problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The last decade has seen an upswing in interest and adoption of reinforcement\nlearning (RL) techniques, in large part due to its demonstrated capabilities at\nperforming certain tasks at \"super-human levels\". This has incentivized the\ncommunity to prioritize research that demonstrates RL agent performance, often\nat the expense of research aimed at understanding their learning dynamics.\nPerformance-focused research runs the risk of overfitting on academic\nbenchmarks -- thereby rendering them less useful -- which can make it difficult\nto transfer proposed techniques to novel problems. Further, it implicitly\ndiminishes work that does not push the performance-frontier, but aims at\nimproving our understanding of these techniques. This paper argues two points:\n(i) RL research should stop focusing solely on demonstrating agent\ncapabilities, and focus more on advancing the science and understanding of\nreinforcement learning; and (ii) we need to be more precise on how our\nbenchmarks map to the underlying mathematical formalisms. We use the popular\nArcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a\nbenchmark that, despite being increasingly considered \"saturated\", can be\neffectively used for developing this understanding, and facilitating the\ndeployment of RL techniques in impactful real-world problems."
                },
                "authors": [
                    {
                        "name": "Pablo Samuel Castro"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Samuel Castro"
                },
                "author": "Pablo Samuel Castro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24438v1",
                "updated": "2025-10-28T14:05:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    5,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:05:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    5,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated\n  Islamic Content",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated\n  Islamic Content"
                },
                "summary": "Large language models are increasingly used for Islamic guidance, but risk\nmisquoting texts, misapplying jurisprudence, or producing culturally\ninconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar\non prompts from authentic Islamic blogs. Our dual-agent framework uses a\nquantitative agent for citation verification and six-dimensional scoring (e.g.,\nStructure, Islamic Consistency, Citations) and a qualitative agent for\nfive-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).\nGPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI\nfollowed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong\nperformance, models still fall short in reliably producing accurate Islamic\ncontent and citations -- a paramount requirement in faith-sensitive writing.\nGPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led\nqualitative pairwise wins (116/200). Fanar, though trailing, introduces\ninnovations for Islamic and Arabic contexts. This study underscores the need\nfor community-driven benchmarks centering Muslim perspectives, offering an\nearly step toward more reliable AI in Islamic knowledge and other high-stakes\ndomains such as medicine, law, and journalism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly used for Islamic guidance, but risk\nmisquoting texts, misapplying jurisprudence, or producing culturally\ninconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar\non prompts from authentic Islamic blogs. Our dual-agent framework uses a\nquantitative agent for citation verification and six-dimensional scoring (e.g.,\nStructure, Islamic Consistency, Citations) and a qualitative agent for\nfive-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).\nGPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI\nfollowed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong\nperformance, models still fall short in reliably producing accurate Islamic\ncontent and citations -- a paramount requirement in faith-sensitive writing.\nGPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led\nqualitative pairwise wins (116/200). Fanar, though trailing, introduces\ninnovations for Islamic and Arabic contexts. This study underscores the need\nfor community-driven benchmarks centering Muslim perspectives, offering an\nearly step toward more reliable AI in Islamic knowledge and other high-stakes\ndomains such as medicine, law, and journalism."
                },
                "authors": [
                    {
                        "name": "Abdullah Mushtaq"
                    },
                    {
                        "name": "Rafay Naeem"
                    },
                    {
                        "name": "Ezieddin Elmahjub"
                    },
                    {
                        "name": "Ibrahim Ghaznavi"
                    },
                    {
                        "name": "Shawqi Al-Maliki"
                    },
                    {
                        "name": "Mohamed Abdallah"
                    },
                    {
                        "name": "Ala Al-Fuqaha"
                    },
                    {
                        "name": "Junaid Qadir"
                    }
                ],
                "author_detail": {
                    "name": "Junaid Qadir"
                },
                "author": "Junaid Qadir",
                "arxiv_comment": "Accepted at 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Workshop: 5th Muslims in Machine Learning (MusIML) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24435v1",
                "updated": "2025-10-28T14:02:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    2,
                    58,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:02:58Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    2,
                    58,
                    1,
                    301,
                    0
                ],
                "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on\n  Logical and Abstract Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Level Reasoning: A Comparative Study of Large Language Models on\n  Logical and Abstract Reasoning"
                },
                "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction."
                },
                "authors": [
                    {
                        "name": "Benjamin Grando Moreira"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Grando Moreira"
                },
                "author": "Benjamin Grando Moreira",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24434v1",
                "updated": "2025-10-28T14:02:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    2,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T14:02:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    2,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed\n  Data"
                },
                "summary": "The effectiveness of instruction-tuned Large Language Models (LLMs) is often\nlimited in low-resource linguistic settings due to a lack of high-quality\ntraining data. We introduce LuxIT, a novel, monolingual instruction tuning\ndataset for Luxembourgish developed to mitigate this challenge. We synthesize\nthe dataset from a corpus of native Luxembourgish texts, utilizing\nDeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following\ngeneration, we apply a quality assurance process, employing an LLM-as-a-judge\napproach. To investigate the practical utility of the dataset, we fine-tune\nseveral smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base\nmodels on Luxembourgish language proficiency examinations, however, yields\nmixed results, with performance varying significantly across different models.\nLuxIT represents a critical contribution to Luxembourgish natural language\nprocessing and offers a replicable monolingual methodology, though our findings\nhighlight the need for further research to optimize its application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of instruction-tuned Large Language Models (LLMs) is often\nlimited in low-resource linguistic settings due to a lack of high-quality\ntraining data. We introduce LuxIT, a novel, monolingual instruction tuning\ndataset for Luxembourgish developed to mitigate this challenge. We synthesize\nthe dataset from a corpus of native Luxembourgish texts, utilizing\nDeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following\ngeneration, we apply a quality assurance process, employing an LLM-as-a-judge\napproach. To investigate the practical utility of the dataset, we fine-tune\nseveral smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base\nmodels on Luxembourgish language proficiency examinations, however, yields\nmixed results, with performance varying significantly across different models.\nLuxIT represents a critical contribution to Luxembourgish natural language\nprocessing and offers a replicable monolingual methodology, though our findings\nhighlight the need for further research to optimize its application."
                },
                "authors": [
                    {
                        "name": "Julian Valline"
                    },
                    {
                        "name": "Cedric Lothritz"
                    },
                    {
                        "name": "Jordi Cabot"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Cabot"
                },
                "author": "Jordi Cabot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24431v1",
                "updated": "2025-10-28T13:58:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    58,
                    36,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:58:36Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    58,
                    36,
                    1,
                    301,
                    0
                ],
                "title": "MiniOneRec: An Open-Source Framework for Scaling Generative\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniOneRec: An Open-Source Framework for Scaling Generative\n  Recommendation"
                },
                "summary": "The recent success of large language models (LLMs) has renewed interest in\nwhether recommender systems can achieve similar scaling benefits. Conventional\nrecommenders, dominated by massive embedding tables, tend to plateau as\nembedding dimensions grow. In contrast, the emerging generative paradigm\nreplaces embeddings with compact Semantic ID (SID) sequences produced by\nautoregressive Transformers. Yet most industrial deployments remain\nproprietary, leaving two fundamental questions open: (1) Do the expected\nscaling laws hold on public benchmarks? (2) What is the minimal post-training\nrecipe that enables competitive performance?\n  We present MiniOneRec, to the best of our knowledge, the first fully\nopen-source generative recommendation framework, which provides an end-to-end\nworkflow spanning SID construction, supervised fine-tuning, and\nrecommendation-oriented reinforcement learning. We generate SIDs via a Residual\nQuantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters\non the Amazon Review dataset. Our experiments reveal a consistent downward\ntrend in both training and evaluation losses with increasing model size,\nvalidating the parameter efficiency of the generative approach. To further\nenhance performance, we propose a lightweight yet effective post-training\npipeline that (1) enforces full-process SID alignment and (2) applies\nreinforcement learning with constrained decoding and hybrid rewards. Together,\nthese techniques yield significant improvements in both ranking accuracy and\ncandidate diversity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent success of large language models (LLMs) has renewed interest in\nwhether recommender systems can achieve similar scaling benefits. Conventional\nrecommenders, dominated by massive embedding tables, tend to plateau as\nembedding dimensions grow. In contrast, the emerging generative paradigm\nreplaces embeddings with compact Semantic ID (SID) sequences produced by\nautoregressive Transformers. Yet most industrial deployments remain\nproprietary, leaving two fundamental questions open: (1) Do the expected\nscaling laws hold on public benchmarks? (2) What is the minimal post-training\nrecipe that enables competitive performance?\n  We present MiniOneRec, to the best of our knowledge, the first fully\nopen-source generative recommendation framework, which provides an end-to-end\nworkflow spanning SID construction, supervised fine-tuning, and\nrecommendation-oriented reinforcement learning. We generate SIDs via a Residual\nQuantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters\non the Amazon Review dataset. Our experiments reveal a consistent downward\ntrend in both training and evaluation losses with increasing model size,\nvalidating the parameter efficiency of the generative approach. To further\nenhance performance, we propose a lightweight yet effective post-training\npipeline that (1) enforces full-process SID alignment and (2) applies\nreinforcement learning with constrained decoding and hybrid rewards. Together,\nthese techniques yield significant improvements in both ranking accuracy and\ncandidate diversity."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Kong"
                    },
                    {
                        "name": "Leheng Sheng"
                    },
                    {
                        "name": "Junfei Tan"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24430v1",
                "updated": "2025-10-28T13:57:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    57,
                    23,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:57:23Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    57,
                    23,
                    1,
                    301,
                    0
                ],
                "title": "From Time and Place to Preference: LLM-Driven Geo-Temporal Context in\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Time and Place to Preference: LLM-Driven Geo-Temporal Context in\n  Recommendations"
                },
                "summary": "Most recommender systems treat timestamps as numeric or cyclical values,\noverlooking real-world context such as holidays, events, and seasonal patterns.\nWe propose a scalable framework that uses large language models (LLMs) to\ngenerate geo-temporal embeddings from only a timestamp and coarse location,\ncapturing holidays, seasonal trends, and local/global events. We then introduce\na geo-temporal embedding informativeness test as a lightweight diagnostic,\ndemonstrating on MovieLens, LastFM, and a production dataset that these\nembeddings provide predictive signal consistent with the outcomes of full model\nintegrations. Geo-temporal embeddings are incorporated into sequential models\nthrough (1) direct feature fusion with metadata embeddings or (2) an auxiliary\nloss that enforces semantic and geo-temporal alignment. Our findings highlight\nthe need for adaptive or hybrid recommendation strategies, and we release a\ncontext-enriched MovieLens dataset to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most recommender systems treat timestamps as numeric or cyclical values,\noverlooking real-world context such as holidays, events, and seasonal patterns.\nWe propose a scalable framework that uses large language models (LLMs) to\ngenerate geo-temporal embeddings from only a timestamp and coarse location,\ncapturing holidays, seasonal trends, and local/global events. We then introduce\na geo-temporal embedding informativeness test as a lightweight diagnostic,\ndemonstrating on MovieLens, LastFM, and a production dataset that these\nembeddings provide predictive signal consistent with the outcomes of full model\nintegrations. Geo-temporal embeddings are incorporated into sequential models\nthrough (1) direct feature fusion with metadata embeddings or (2) an auxiliary\nloss that enforces semantic and geo-temporal alignment. Our findings highlight\nthe need for adaptive or hybrid recommendation strategies, and we release a\ncontext-enriched MovieLens dataset to support future research."
                },
                "authors": [
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Shaghayegh Agah"
                    },
                    {
                        "name": "Mayur Nankani"
                    },
                    {
                        "name": "Neeraj Sharma"
                    },
                    {
                        "name": "Feifei Peng"
                    },
                    {
                        "name": "Maria Peifer"
                    },
                    {
                        "name": "Sardar Hamidian"
                    },
                    {
                        "name": "H Howie Huang"
                    }
                ],
                "author_detail": {
                    "name": "H Howie Huang"
                },
                "author": "H Howie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24428v1",
                "updated": "2025-10-28T13:52:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    52,
                    46,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:52:46Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    52,
                    46,
                    1,
                    301,
                    0
                ],
                "title": "CodeWiki: Automated Repository-Level Documentation at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeWiki: Automated Repository-Level Documentation at Scale"
                },
                "summary": "Developers spend nearly 58% of their time understanding codebases, yet\nmaintaining comprehensive documentation remains challenging due to complexity\nand manual effort. While recent Large Language Models (LLMs) show promise for\nfunction-level documentation, they fail at the repository level, where\ncapturing architectural patterns and cross-module interactions is essential. We\nintroduce CodeWiki, the first open-source framework for holistic\nrepository-level documentation across seven programming languages. CodeWiki\nemploys three innovations: (i) hierarchical decomposition that preserves\narchitectural context, (ii) recursive agentic processing with dynamic\ndelegation, and (iii) synthesis of textual and visual artifacts including\narchitecture diagrams and data flows. We also present CodeWikiBench, the first\nrepository-level documentation benchmark with multi-level rubrics and agentic\nassessment. CodeWiki achieves 68.79% quality score with proprietary models and\n64.80% with open-source alternatives, outperforming existing closed-source\nsystems and demonstrating scalable, accurate documentation for real-world\nrepositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developers spend nearly 58% of their time understanding codebases, yet\nmaintaining comprehensive documentation remains challenging due to complexity\nand manual effort. While recent Large Language Models (LLMs) show promise for\nfunction-level documentation, they fail at the repository level, where\ncapturing architectural patterns and cross-module interactions is essential. We\nintroduce CodeWiki, the first open-source framework for holistic\nrepository-level documentation across seven programming languages. CodeWiki\nemploys three innovations: (i) hierarchical decomposition that preserves\narchitectural context, (ii) recursive agentic processing with dynamic\ndelegation, and (iii) synthesis of textual and visual artifacts including\narchitecture diagrams and data flows. We also present CodeWikiBench, the first\nrepository-level documentation benchmark with multi-level rubrics and agentic\nassessment. CodeWiki achieves 68.79% quality score with proprietary models and\n64.80% with open-source alternatives, outperforming existing closed-source\nsystems and demonstrating scalable, accurate documentation for real-world\nrepositories."
                },
                "authors": [
                    {
                        "name": "Nguyen Hoang Anh"
                    },
                    {
                        "name": "Minh Le-Anh"
                    },
                    {
                        "name": "Bach Le"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17281v2",
                "updated": "2025-10-28T13:52:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    52,
                    29,
                    1,
                    301,
                    0
                ],
                "published": "2025-08-24T10:02:51Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    10,
                    2,
                    51,
                    6,
                    236,
                    0
                ],
                "title": "From Language to Action: A Review of Large Language Models as Autonomous\n  Agents and Tool Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Language to Action: A Review of Large Language Models as Autonomous\n  Agents and Tool Users"
                },
                "summary": "The pursuit of human-level artificial intelligence (AI) has significantly\nadvanced the development of autonomous agents and Large Language Models (LLMs).\nLLMs are now widely utilized as decision-making agents for their ability to\ninterpret instructions, manage sequential tasks, and adapt through feedback.\nThis review examines recent developments in employing LLMs as autonomous agents\nand tool users and comprises seven research questions. We only used the papers\npublished between 2023 and 2025 in conferences of the A* and A rank and Q1\njournals. A structured analysis of the LLM agents' architectural design\nprinciples, dividing their applications into single-agent and multi-agent\nsystems, and strategies for integrating external tools is presented. In\naddition, the cognitive mechanisms of LLM, including reasoning, planning, and\nmemory, and the impact of prompting methods and fine-tuning procedures on agent\nperformance are also investigated. Furthermore, we evaluated current benchmarks\nand assessment protocols and have provided an analysis of 68 publicly available\ndatasets to assess the performance of LLM-based agents in various tasks. In\nconducting this review, we have identified critical findings on verifiable\nreasoning of LLMs, the capacity for self-improvement, and the personalization\nof LLM-based agents. Finally, we have discussed ten future research directions\nto overcome these gaps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of human-level artificial intelligence (AI) has significantly\nadvanced the development of autonomous agents and Large Language Models (LLMs).\nLLMs are now widely utilized as decision-making agents for their ability to\ninterpret instructions, manage sequential tasks, and adapt through feedback.\nThis review examines recent developments in employing LLMs as autonomous agents\nand tool users and comprises seven research questions. We only used the papers\npublished between 2023 and 2025 in conferences of the A* and A rank and Q1\njournals. A structured analysis of the LLM agents' architectural design\nprinciples, dividing their applications into single-agent and multi-agent\nsystems, and strategies for integrating external tools is presented. In\naddition, the cognitive mechanisms of LLM, including reasoning, planning, and\nmemory, and the impact of prompting methods and fine-tuning procedures on agent\nperformance are also investigated. Furthermore, we evaluated current benchmarks\nand assessment protocols and have provided an analysis of 68 publicly available\ndatasets to assess the performance of LLM-based agents in various tasks. In\nconducting this review, we have identified critical findings on verifiable\nreasoning of LLMs, the capacity for self-improvement, and the personalization\nof LLM-based agents. Finally, we have discussed ten future research directions\nto overcome these gaps."
                },
                "authors": [
                    {
                        "name": "Sadia Sultana Chowa"
                    },
                    {
                        "name": "Riasad Alvi"
                    },
                    {
                        "name": "Subhey Sadi Rahman"
                    },
                    {
                        "name": "Md Abdur Rahman"
                    },
                    {
                        "name": "Mohaimenul Azam Khan Raiaan"
                    },
                    {
                        "name": "Md Rafiqul Islam"
                    },
                    {
                        "name": "Mukhtar Hussain"
                    },
                    {
                        "name": "Sami Azam"
                    }
                ],
                "author_detail": {
                    "name": "Sami Azam"
                },
                "author": "Sami Azam",
                "arxiv_comment": "Submitted to Artificial Intelligence Review for peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10856v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10856v4",
                "updated": "2025-10-28T13:45:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    45,
                    25,
                    1,
                    301,
                    0
                ],
                "published": "2024-12-14T15:11:07Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    15,
                    11,
                    7,
                    5,
                    349,
                    0
                ],
                "title": "RWKV-edge: Deeply Compressed RWKV for Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RWKV-edge: Deeply Compressed RWKV for Resource-Constrained Devices"
                },
                "summary": "To deploy LLMs on resource-contained platforms such as mobile robots and\nsmartphones, non-transformers LLMs have achieved major breakthroughs. Recently,\na novel RNN-based LLM family, Repentance Weighted Key Value (RWKV) has shown\nstrong computational efficiency; nevertheless, RWKV models still have high\nparameter counts which limited their deployment. In this paper, we propose a\nsuite of compression techniques, ranging from model architecture optimizations\nto post-training compression, tailored to the RWKV architecture. Combined, our\ntechniques reduce the memory footprint of RWKV models by 3.4x -- 5x with only\nnegligible degradation in accuracy; compared to transformer LLMs with similar\naccuracy, our models require 4x less memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To deploy LLMs on resource-contained platforms such as mobile robots and\nsmartphones, non-transformers LLMs have achieved major breakthroughs. Recently,\na novel RNN-based LLM family, Repentance Weighted Key Value (RWKV) has shown\nstrong computational efficiency; nevertheless, RWKV models still have high\nparameter counts which limited their deployment. In this paper, we propose a\nsuite of compression techniques, ranging from model architecture optimizations\nto post-training compression, tailored to the RWKV architecture. Combined, our\ntechniques reduce the memory footprint of RWKV models by 3.4x -- 5x with only\nnegligible degradation in accuracy; compared to transformer LLMs with similar\naccuracy, our models require 4x less memory footprint."
                },
                "authors": [
                    {
                        "name": "Wonkyo Choe"
                    },
                    {
                        "name": "Yangfeng Ji"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10856v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10856v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24408v1",
                "updated": "2025-10-28T13:19:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    19,
                    46,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:19:46Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    19,
                    46,
                    1,
                    301,
                    0
                ],
                "title": "Uncovering Gaps Between RFC Updates and TCP/IP Implementations:\n  LLM-Facilitated Differential Checks on Intermediate Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Gaps Between RFC Updates and TCP/IP Implementations:\n  LLM-Facilitated Differential Checks on Intermediate Representations"
                },
                "summary": "As the core of the Internet infrastructure, the TCP/IP protocol stack\nundertakes the task of network data transmission. However, due to the\ncomplexity of the protocol and the uncertainty of cross-layer interaction,\nthere are often inconsistencies between the implementation of the protocol\nstack code and the RFC standard. This inconsistency may not only lead to\ndifferences in protocol functions but also cause serious security\nvulnerabilities. At present, with the continuous expansion of protocol stack\nfunctions and the rapid iteration of RFC documents, it is increasingly\nimportant to detect and fix these inconsistencies. With the rise of large\nlanguage models, researchers have begun to explore how to extract protocol\nspecifications from RFC documents through these models, including protocol\nstack modeling, state machine extraction, text ambiguity analysis, and other\nrelated content. However, existing methods rely on predefined patterns or\nrule-based approaches that fail to generalize across different protocol\nspecifications. Automated and scalable detection of these inconsistencies\nremains a significant challenge. In this study, we propose an automated\nanalysis framework based on LLM and differential models. By modeling the\niterative relationship of the protocol and based on the iterative update\nrelationship of the RFC standard, we perform incremental code function analysis\non different versions of kernel code implementations to automatically perform\ncode detection and vulnerability analysis. We conduct extensive evaluations to\nvalidate the effectiveness of our framework, demonstrating its effectiveness in\nidentifying potential vulnerabilities caused by RFC code inconsistencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the core of the Internet infrastructure, the TCP/IP protocol stack\nundertakes the task of network data transmission. However, due to the\ncomplexity of the protocol and the uncertainty of cross-layer interaction,\nthere are often inconsistencies between the implementation of the protocol\nstack code and the RFC standard. This inconsistency may not only lead to\ndifferences in protocol functions but also cause serious security\nvulnerabilities. At present, with the continuous expansion of protocol stack\nfunctions and the rapid iteration of RFC documents, it is increasingly\nimportant to detect and fix these inconsistencies. With the rise of large\nlanguage models, researchers have begun to explore how to extract protocol\nspecifications from RFC documents through these models, including protocol\nstack modeling, state machine extraction, text ambiguity analysis, and other\nrelated content. However, existing methods rely on predefined patterns or\nrule-based approaches that fail to generalize across different protocol\nspecifications. Automated and scalable detection of these inconsistencies\nremains a significant challenge. In this study, we propose an automated\nanalysis framework based on LLM and differential models. By modeling the\niterative relationship of the protocol and based on the iterative update\nrelationship of the RFC standard, we perform incremental code function analysis\non different versions of kernel code implementations to automatically perform\ncode detection and vulnerability analysis. We conduct extensive evaluations to\nvalidate the effectiveness of our framework, demonstrating its effectiveness in\nidentifying potential vulnerabilities caused by RFC code inconsistencies."
                },
                "authors": [
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Xuewei Feng"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Ke Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ke Xu"
                },
                "author": "Ke Xu",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24402v1",
                "updated": "2025-10-28T13:16:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    16,
                    36,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:16:36Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    16,
                    36,
                    1,
                    301,
                    0
                ],
                "title": "Metadata-Driven Retrieval-Augmented Generation for Financial Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metadata-Driven Retrieval-Augmented Generation for Financial Question\n  Answering"
                },
                "summary": "Retrieval-Augmented Generation (RAG) struggles on long, structured financial\nfilings where relevant evidence is sparse and cross-referenced. This paper\npresents a systematic investigation of advanced metadata-driven\nRetrieval-Augmented Generation (RAG) techniques, proposing and evaluating a\nnovel, multi-stage RAG architecture that leverages LLM-generated metadata. We\nintroduce a sophisticated indexing pipeline to create contextually rich\ndocument chunks and benchmark a spectrum of enhancements, including\npre-retrieval filtering, post-retrieval reranking, and enriched embeddings,\nbenchmarked on the FinanceBench dataset. Our results reveal that while a\npowerful reranker is essential for precision, the most significant performance\ngains come from embedding chunk metadata directly with text (\"contextual\nchunks\"). Our proposed optimal architecture combines LLM-driven pre-retrieval\noptimizations with these contextual embeddings to achieve superior performance.\nAdditionally, we present a custom metadata reranker that offers a compelling,\ncost-effective alternative to commercial solutions, highlighting a practical\ntrade-off between peak performance and operational efficiency. This study\nprovides a blueprint for building robust, metadata-aware RAG systems for\nfinancial document analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) struggles on long, structured financial\nfilings where relevant evidence is sparse and cross-referenced. This paper\npresents a systematic investigation of advanced metadata-driven\nRetrieval-Augmented Generation (RAG) techniques, proposing and evaluating a\nnovel, multi-stage RAG architecture that leverages LLM-generated metadata. We\nintroduce a sophisticated indexing pipeline to create contextually rich\ndocument chunks and benchmark a spectrum of enhancements, including\npre-retrieval filtering, post-retrieval reranking, and enriched embeddings,\nbenchmarked on the FinanceBench dataset. Our results reveal that while a\npowerful reranker is essential for precision, the most significant performance\ngains come from embedding chunk metadata directly with text (\"contextual\nchunks\"). Our proposed optimal architecture combines LLM-driven pre-retrieval\noptimizations with these contextual embeddings to achieve superior performance.\nAdditionally, we present a custom metadata reranker that offers a compelling,\ncost-effective alternative to commercial solutions, highlighting a practical\ntrade-off between peak performance and operational efficiency. This study\nprovides a blueprint for building robust, metadata-aware RAG systems for\nfinancial document analysis."
                },
                "authors": [
                    {
                        "name": "Michail Dadopoulos"
                    },
                    {
                        "name": "Anestis Ladas"
                    },
                    {
                        "name": "Stratos Moschidis"
                    },
                    {
                        "name": "Ioannis Negkakis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Negkakis"
                },
                "author": "Ioannis Negkakis",
                "arxiv_comment": "Preprint version submitted to the International Journal of Accounting\n  Information Systems; currently under major revision. 20 pages, 1 figure, 1\n  table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24397v1",
                "updated": "2025-10-28T13:11:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    11,
                    22,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:11:22Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    11,
                    22,
                    1,
                    301,
                    0
                ],
                "title": "APTBench: Benchmarking Agentic Potential of Base LLMs During\n  Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APTBench: Benchmarking Agentic Potential of Base LLMs During\n  Pre-Training"
                },
                "summary": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training."
                },
                "authors": [
                    {
                        "name": "Jiarui Qin"
                    },
                    {
                        "name": "Yunjia Xi"
                    },
                    {
                        "name": "Junjie Huang"
                    },
                    {
                        "name": "Renting Rui"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "46 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.06328v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.06328v5",
                "updated": "2025-10-28T13:10:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    10,
                    33,
                    1,
                    301,
                    0
                ],
                "published": "2023-10-10T05:54:00Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    5,
                    54,
                    0,
                    1,
                    283,
                    0
                ],
                "title": "UniCrossFi: A Unified Framework For Cross-Domain Wi-Fi-based Gesture\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCrossFi: A Unified Framework For Cross-Domain Wi-Fi-based Gesture\n  Recognition"
                },
                "summary": "Wi-Fi sensing systems are severely hindered by cross domain problem when\ndeployed in unseen real-world environments. Existing methods typically design\nseparate frameworks for either domain adaptation or domain generalization,\noften relying on extensive labeled data. Existing methods that designed for\ndomain generalization is often relying on extensive labeled data. However,\nreal-world scenarios are far more complex, where the deployed model must be\ncapable of handling generalization under limited labeled source data. To this\nend, we propose UniCrossFi, a unified framework designed to mitigate\nperformance drop in CSI-based sensing across diverse deployment settings. Our\nframework not only extends conventional Domain Generalization (DG) to a more\npractical Semi-Supervised Domain Generalization (SSDG) setting, where only\npartially labeled source data are available, but also introduces a\nphysics-informed data augmentation strategy, Antenna Response Consistency\n(ARC). ARC mitigates the risk of learning superficial shortcuts by exploiting\nthe intrinsic spatial diversity of multi-antenna systems, treating signals from\ndifferent antennas as naturally augmented views of the same event. In addition,\nwe design a Unified Contrastive Objective to prevent conventional contrastive\nlearning from pushing apart samples from different domains that share the same\nclass. We conduct extensive experiments on the public Widar and CSIDA datasets.\nThe results demonstrate that UniCrossFi consistently establishes a new\nstate-of-the-art, significantly outperforming existing methods across all\nunsupervised domain adaptation, DG, and SSDG benchmarks. UniCrossFi provides a\nprincipled and practical solution to the domain shift challenge, advancing the\nfeasibility of robust, real-world Wi-Fi sensing systems that can operate\neffectively with limited labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wi-Fi sensing systems are severely hindered by cross domain problem when\ndeployed in unseen real-world environments. Existing methods typically design\nseparate frameworks for either domain adaptation or domain generalization,\noften relying on extensive labeled data. Existing methods that designed for\ndomain generalization is often relying on extensive labeled data. However,\nreal-world scenarios are far more complex, where the deployed model must be\ncapable of handling generalization under limited labeled source data. To this\nend, we propose UniCrossFi, a unified framework designed to mitigate\nperformance drop in CSI-based sensing across diverse deployment settings. Our\nframework not only extends conventional Domain Generalization (DG) to a more\npractical Semi-Supervised Domain Generalization (SSDG) setting, where only\npartially labeled source data are available, but also introduces a\nphysics-informed data augmentation strategy, Antenna Response Consistency\n(ARC). ARC mitigates the risk of learning superficial shortcuts by exploiting\nthe intrinsic spatial diversity of multi-antenna systems, treating signals from\ndifferent antennas as naturally augmented views of the same event. In addition,\nwe design a Unified Contrastive Objective to prevent conventional contrastive\nlearning from pushing apart samples from different domains that share the same\nclass. We conduct extensive experiments on the public Widar and CSIDA datasets.\nThe results demonstrate that UniCrossFi consistently establishes a new\nstate-of-the-art, significantly outperforming existing methods across all\nunsupervised domain adaptation, DG, and SSDG benchmarks. UniCrossFi provides a\nprincipled and practical solution to the domain shift challenge, advancing the\nfeasibility of robust, real-world Wi-Fi sensing systems that can operate\neffectively with limited labeled data."
                },
                "authors": [
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhiyong Zheng"
                    },
                    {
                        "name": "Hongyuan Zhu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jiangtao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangtao Wang"
                },
                "author": "Jiangtao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.06328v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.06328v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24395v1",
                "updated": "2025-10-28T13:09:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    9,
                    8,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:09:08Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    9,
                    8,
                    1,
                    301,
                    0
                ],
                "title": "Optimising Underwater Neutrino Telescopes for All-Flavour Point Source\n  Sensitivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimising Underwater Neutrino Telescopes for All-Flavour Point Source\n  Sensitivity"
                },
                "summary": "High-energy neutrino astronomy has advanced rapidly in recent years, with\nIceCube, KM3NeT, and Baikal-GVD establishing a diffuse astrophysical flux and\npointing to promising source candidates. These achievements mark the transition\nfrom first detections to detailed source studies, motivating next-generation\ndetectors with larger volumes, improved angular resolution, and full\nneutrino-flavour sensitivity. We present a performance study of large\nunderwater neutrino telescopes, taking the proposed TRIDENT array in the South\nChina Sea as a case study, with a focus on comparing the performance of various\ndetector configurations against the TRIDENT baseline design. Both track-like\nevents primarily from muon neutrinos, which provide precise directional\ninformation, and cascade events from all flavours, which offer superior energy\nresolution, diffuse-source sensitivity, and all-sky flavour coverage, are\nincluded to achieve a balanced performance across source types. The time to\ndiscover potential astrophysical sources with both track- and cascade-like\nevents is used as the figure of merit to compare a variety of detector design\nchoices. Our results show that, for a fixed number of optical modules, simply\nenlarging the instrumented volume does not inherently lead to improved\nperformance, while taller strings can provide modest gains across all detector\nchannels, within engineering constraints. Distributing dense clusters of\nstrings over a large volume is found to generally worsen discovery potential\ncompared to the baseline layout. Finally, the optical properties of the\nsea-water emerge as the key factor dictating the optimisation of detector\nlayout, highlighting the need for in-situ measurements and early deployment of\noptical modules to guide the final array configuration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-energy neutrino astronomy has advanced rapidly in recent years, with\nIceCube, KM3NeT, and Baikal-GVD establishing a diffuse astrophysical flux and\npointing to promising source candidates. These achievements mark the transition\nfrom first detections to detailed source studies, motivating next-generation\ndetectors with larger volumes, improved angular resolution, and full\nneutrino-flavour sensitivity. We present a performance study of large\nunderwater neutrino telescopes, taking the proposed TRIDENT array in the South\nChina Sea as a case study, with a focus on comparing the performance of various\ndetector configurations against the TRIDENT baseline design. Both track-like\nevents primarily from muon neutrinos, which provide precise directional\ninformation, and cascade events from all flavours, which offer superior energy\nresolution, diffuse-source sensitivity, and all-sky flavour coverage, are\nincluded to achieve a balanced performance across source types. The time to\ndiscover potential astrophysical sources with both track- and cascade-like\nevents is used as the figure of merit to compare a variety of detector design\nchoices. Our results show that, for a fixed number of optical modules, simply\nenlarging the instrumented volume does not inherently lead to improved\nperformance, while taller strings can provide modest gains across all detector\nchannels, within engineering constraints. Distributing dense clusters of\nstrings over a large volume is found to generally worsen discovery potential\ncompared to the baseline layout. Finally, the optical properties of the\nsea-water emerge as the key factor dictating the optimisation of detector\nlayout, highlighting the need for in-situ measurements and early deployment of\noptical modules to guide the final array configuration."
                },
                "authors": [
                    {
                        "name": "Iwan Morton-Blake"
                    },
                    {
                        "name": "Fuyudi Zhang"
                    },
                    {
                        "name": "Qichao Chang"
                    },
                    {
                        "name": "Shuhua Hao"
                    },
                    {
                        "name": "Weilun Huang"
                    },
                    {
                        "name": "Hualin Mei"
                    },
                    {
                        "name": "Wei Tian"
                    },
                    {
                        "name": "Yingwei Wang"
                    },
                    {
                        "name": "Xin Xiang"
                    },
                    {
                        "name": "Donglian Xu"
                    }
                ],
                "author_detail": {
                    "name": "Donglian Xu"
                },
                "author": "Donglian Xu",
                "arxiv_comment": "16 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24390v1",
                "updated": "2025-10-28T13:05:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    5,
                    23,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:05:23Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    5,
                    23,
                    1,
                    301,
                    0
                ],
                "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and\n  Logic-Parallel Content Expansion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and\n  Logic-Parallel Content Expansion"
                },
                "summary": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies."
                },
                "authors": [
                    {
                        "name": "Xianjun Gao"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24384v1",
                "updated": "2025-10-28T13:01:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    1,
                    7,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T13:01:07Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    1,
                    7,
                    1,
                    301,
                    0
                ],
                "title": "Optimal Unmanned Aerial Vehicle Deployment for Macro-Micro Traffic\n  Monitoring Fused with Connected Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Unmanned Aerial Vehicle Deployment for Macro-Micro Traffic\n  Monitoring Fused with Connected Vehicles"
                },
                "summary": "Reliable estimation of macro and micro traffic states is essential for urban\ntraffic management. Unmanned Aerial Vehicles, with their airborne full-sample\ncontinuous trajectory observation, bring new opportunities for macro- and\nmicro-traffic state estimation. In this study, we will explore the optimal UAV\ndeployment problem in road networks in conjunction with sampled connected\nvehicle data to achieve more reliable estimation of macroscopic path flow as\nwell as microscopic arrival rates and queue lengths. Oriented towards\nmacro-micro traffic states, we propose entropy-based and area-based uncertainty\nmeasures, respectively, and transform the optimal UAV deployment problem into\nminimizing the uncertainty of macro-micro traffic states. A quantum genetic\nalgorithm that integrates the thoughts of metaheuristic algorithms and quantum\ncomputation is then proposed to solve the large-scale nonlinear problem\nefficiently. Evaluation results on a network with 18 intersections have\ndemonstrated that by deploying UAV detection at specific locations, the\nuncertainty reduction of macro-micro traffic state estimation ranges from\n15.28\\% to 75.69\\%. A total of 5 UAVs with optimal location schemes would be\nsufficient to detect over 95\\% of the paths in the network considering both\nmicroscopic uncertainty regarding the intersection operation efficiency and the\nmacroscopic uncertainty regarding the route choice of road users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable estimation of macro and micro traffic states is essential for urban\ntraffic management. Unmanned Aerial Vehicles, with their airborne full-sample\ncontinuous trajectory observation, bring new opportunities for macro- and\nmicro-traffic state estimation. In this study, we will explore the optimal UAV\ndeployment problem in road networks in conjunction with sampled connected\nvehicle data to achieve more reliable estimation of macroscopic path flow as\nwell as microscopic arrival rates and queue lengths. Oriented towards\nmacro-micro traffic states, we propose entropy-based and area-based uncertainty\nmeasures, respectively, and transform the optimal UAV deployment problem into\nminimizing the uncertainty of macro-micro traffic states. A quantum genetic\nalgorithm that integrates the thoughts of metaheuristic algorithms and quantum\ncomputation is then proposed to solve the large-scale nonlinear problem\nefficiently. Evaluation results on a network with 18 intersections have\ndemonstrated that by deploying UAV detection at specific locations, the\nuncertainty reduction of macro-micro traffic state estimation ranges from\n15.28\\% to 75.69\\%. A total of 5 UAVs with optimal location schemes would be\nsufficient to detect over 95\\% of the paths in the network considering both\nmicroscopic uncertainty regarding the intersection operation efficiency and the\nmacroscopic uncertainty regarding the route choice of road users."
                },
                "authors": [
                    {
                        "name": "Chaopeng Tan"
                    },
                    {
                        "name": "Jiarong Yao"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26080v2",
                "updated": "2025-10-28T13:00:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    13,
                    0,
                    46,
                    1,
                    301,
                    0
                ],
                "published": "2025-09-30T10:53:54Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    10,
                    53,
                    54,
                    1,
                    273,
                    0
                ],
                "title": "Evaluating the Use of Large Language Models as Synthetic Social Agents\n  in Social Science Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Use of Large Language Models as Synthetic Social Agents\n  in Social Science Research"
                },
                "summary": "Large Language Models (LLMs) are being increasingly used as synthetic agents\nin social science, in applications ranging from augmenting survey responses to\npowering multi-agent simulations. This paper outlines cautions that should be\ntaken when interpreting LLM outputs and proposes a pragmatic reframing for the\nsocial sciences in which LLMs are used as high-capacity pattern matchers for\nquasi-predictive interpolation under explicit scope conditions and not as\nsubstitutes for probabilistic inference. Practical guardrails such as\nindependent draws, preregistered human baselines, reliability-aware validation,\nand subgroup calibration, are introduced so that researchers may engage in\nuseful prototyping and forecasting while avoiding category errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being increasingly used as synthetic agents\nin social science, in applications ranging from augmenting survey responses to\npowering multi-agent simulations. This paper outlines cautions that should be\ntaken when interpreting LLM outputs and proposes a pragmatic reframing for the\nsocial sciences in which LLMs are used as high-capacity pattern matchers for\nquasi-predictive interpolation under explicit scope conditions and not as\nsubstitutes for probabilistic inference. Practical guardrails such as\nindependent draws, preregistered human baselines, reliability-aware validation,\nand subgroup calibration, are introduced so that researchers may engage in\nuseful prototyping and forecasting while avoiding category errors."
                },
                "authors": [
                    {
                        "name": "Emma Rose Madden"
                    }
                ],
                "author_detail": {
                    "name": "Emma Rose Madden"
                },
                "author": "Emma Rose Madden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24383v1",
                "updated": "2025-10-28T12:59:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    59,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    59,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Policy Cards: Machine-Readable Runtime Governance for Autonomous AI\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Cards: Machine-Readable Runtime Governance for Autonomous AI\n  Agents"
                },
                "summary": "Policy Cards are introduced as a machine-readable, deployment-layer standard\nfor expressing operational, regulatory, and ethical constraints for AI agents.\nThe Policy Card sits with the agent and enables it to follow required\nconstraints at runtime. It tells the agent what it must and must not do. As\nsuch, it becomes an integral part of the deployed agent. Policy Cards extend\nexisting transparency artifacts such as Model, Data, and System Cards by\ndefining a normative layer that encodes allow/deny rules, obligations,\nevidentiary requirements, and crosswalk mappings to assurance frameworks\nincluding NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can\nbe validated automatically, version-controlled, and linked to runtime\nenforcement or continuous-audit pipelines. The framework enables verifiable\ncompliance for autonomous agents, forming a foundation for distributed\nassurance in multi-agent ecosystems. Policy Cards provide a practical mechanism\nfor integrating high-level governance with hands-on engineering practice and\nenabling accountable autonomy at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Cards are introduced as a machine-readable, deployment-layer standard\nfor expressing operational, regulatory, and ethical constraints for AI agents.\nThe Policy Card sits with the agent and enables it to follow required\nconstraints at runtime. It tells the agent what it must and must not do. As\nsuch, it becomes an integral part of the deployed agent. Policy Cards extend\nexisting transparency artifacts such as Model, Data, and System Cards by\ndefining a normative layer that encodes allow/deny rules, obligations,\nevidentiary requirements, and crosswalk mappings to assurance frameworks\nincluding NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can\nbe validated automatically, version-controlled, and linked to runtime\nenforcement or continuous-audit pipelines. The framework enables verifiable\ncompliance for autonomous agents, forming a foundation for distributed\nassurance in multi-agent ecosystems. Policy Cards provide a practical mechanism\nfor integrating high-level governance with hands-on engineering practice and\nenabling accountable autonomy at scale."
                },
                "authors": [
                    {
                        "name": "Juraj Mavračić"
                    }
                ],
                "author_detail": {
                    "name": "Juraj Mavračić"
                },
                "author": "Juraj Mavračić",
                "arxiv_doi": "10.5281/zenodo.17464706",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.17464706",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.24383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "First published on 19/10/2025. Canonical archived record and DOI:\n  10.5281/zenodo.17391796",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.1; I.2.4; K.4.1; K.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14788v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14788v2",
                "updated": "2025-10-28T12:58:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    58,
                    38,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-16T15:20:49Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    20,
                    49,
                    3,
                    289,
                    0
                ],
                "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Scenario Unified Modeling of User Interests at Billion Scale"
                },
                "summary": "User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms."
                },
                "authors": [
                    {
                        "name": "Manjie Xu"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Jingyi Zhou"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Zejian Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Kai Zuo"
                    },
                    {
                        "name": "Yibo Chen"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Yixin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Zhu"
                },
                "author": "Yixin Zhu",
                "arxiv_comment": "https://github.com/ariesssxu/RedSeqRec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14788v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14788v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24378v1",
                "updated": "2025-10-28T12:56:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    56,
                    48,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:56:48Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    56,
                    48,
                    1,
                    301,
                    0
                ],
                "title": "Stroke Lesion Segmentation in Clinical Workflows: A Modular,\n  Lightweight, and Deployment-Ready Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stroke Lesion Segmentation in Clinical Workflows: A Modular,\n  Lightweight, and Deployment-Ready Tool"
                },
                "summary": "Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools."
                },
                "authors": [
                    {
                        "name": "Yann Kerverdo"
                    },
                    {
                        "name": "Florent Leray"
                    },
                    {
                        "name": "Youwan Mahé"
                    },
                    {
                        "name": "Stéphanie Leplaideur"
                    },
                    {
                        "name": "Francesca Galassi"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Galassi"
                },
                "author": "Francesca Galassi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24367v1",
                "updated": "2025-10-28T12:44:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    44,
                    54,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:44:54Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    44,
                    54,
                    1,
                    301,
                    0
                ],
                "title": "LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and\n  the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and\n  the Road Ahead"
                },
                "summary": "The rapid integration of Large Language Models (LLMs) into software\nengineering (SE) has revolutionized tasks like code generation, producing a\nmassive volume of software artifacts. This surge has exposed a critical\nbottleneck: the lack of scalable, reliable methods to evaluate these outputs.\nHuman evaluation is costly and time-consuming, while traditional automated\nmetrics like BLEU fail to capture nuanced quality aspects. In response, the\nLLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.\nThis approach leverages the advanced reasoning of LLMs, offering a path toward\nhuman-like nuance at automated scale. However, LLM-as-a-Judge research in SE is\nstill in its early stages. This forward-looking SE 2030 paper aims to steer the\ncommunity toward advancing LLM-as-a-Judge for evaluating LLM-generated software\nartifacts. We provide a literature review of existing SE studies, analyze their\nlimitations, identify key research gaps, and outline a detailed roadmap. We\nenvision these frameworks as reliable, robust, and scalable human surrogates\ncapable of consistent, multi-faceted artifact evaluation by 2030. Our work aims\nto foster research and adoption of LLM-as-a-Judge frameworks, ultimately\nimproving the scalability of software artifact evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid integration of Large Language Models (LLMs) into software\nengineering (SE) has revolutionized tasks like code generation, producing a\nmassive volume of software artifacts. This surge has exposed a critical\nbottleneck: the lack of scalable, reliable methods to evaluate these outputs.\nHuman evaluation is costly and time-consuming, while traditional automated\nmetrics like BLEU fail to capture nuanced quality aspects. In response, the\nLLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.\nThis approach leverages the advanced reasoning of LLMs, offering a path toward\nhuman-like nuance at automated scale. However, LLM-as-a-Judge research in SE is\nstill in its early stages. This forward-looking SE 2030 paper aims to steer the\ncommunity toward advancing LLM-as-a-Judge for evaluating LLM-generated software\nartifacts. We provide a literature review of existing SE studies, analyze their\nlimitations, identify key research gaps, and outline a detailed roadmap. We\nenvision these frameworks as reliable, robust, and scalable human surrogates\ncapable of consistent, multi-faceted artifact evaluation by 2030. Our work aims\nto foster research and adoption of LLM-as-a-Judge frameworks, ultimately\nimproving the scalability of software artifact evaluation."
                },
                "authors": [
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Terry Yue Zhuo"
                    },
                    {
                        "name": "Christoph Treude"
                    },
                    {
                        "name": "Jiamou Sun"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Xiaoning Du"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11842v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11842v3",
                "updated": "2025-10-28T12:44:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    44,
                    7,
                    1,
                    301,
                    0
                ],
                "published": "2025-05-17T05:06:38Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    5,
                    6,
                    38,
                    5,
                    137,
                    0
                ],
                "title": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs"
                },
                "summary": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies."
                },
                "authors": [
                    {
                        "name": "Xuannan Liu"
                    },
                    {
                        "name": "Zekun Li"
                    },
                    {
                        "name": "Zheqi He"
                    },
                    {
                        "name": "Peipei Li"
                    },
                    {
                        "name": "Shuhan Xia"
                    },
                    {
                        "name": "Xing Cui"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "Accepted by NeurIPS 2025 Dataset and Benchmark Track, Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11842v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11842v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24365v1",
                "updated": "2025-10-28T12:41:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    41,
                    10,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:41:10Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    41,
                    10,
                    1,
                    301,
                    0
                ],
                "title": "Text Simplification with Sentence Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Simplification with Sentence Embeddings"
                },
                "summary": "Sentence embeddings can be decoded to give approximations of the original\ntexts used to create them. We explore this effect in the context of text\nsimplification, demonstrating that reconstructed text embeddings preserve\ncomplexity levels. We experiment with a small feed forward neural network to\neffectively learn a transformation between sentence embeddings representing\nhigh-complexity and low-complexity texts. We provide comparison to a Seq2Seq\nand LLM-based approach, showing encouraging results in our much smaller\nlearning setting. Finally, we demonstrate the applicability of our\ntransformation to an unseen simplification dataset (MedEASI), as well as\ndatasets from languages outside the training data (ES,DE). We conclude that\nlearning transformations in sentence embedding space is a promising direction\nfor future research and has potential to unlock the ability to develop small,\nbut powerful models for text simplification and other natural language\ngeneration tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentence embeddings can be decoded to give approximations of the original\ntexts used to create them. We explore this effect in the context of text\nsimplification, demonstrating that reconstructed text embeddings preserve\ncomplexity levels. We experiment with a small feed forward neural network to\neffectively learn a transformation between sentence embeddings representing\nhigh-complexity and low-complexity texts. We provide comparison to a Seq2Seq\nand LLM-based approach, showing encouraging results in our much smaller\nlearning setting. Finally, we demonstrate the applicability of our\ntransformation to an unseen simplification dataset (MedEASI), as well as\ndatasets from languages outside the training data (ES,DE). We conclude that\nlearning transformations in sentence embedding space is a promising direction\nfor future research and has potential to unlock the ability to develop small,\nbut powerful models for text simplification and other natural language\ngeneration tasks."
                },
                "authors": [
                    {
                        "name": "Matthew Shardlow"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Shardlow"
                },
                "author": "Matthew Shardlow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09536v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09536v3",
                "updated": "2025-10-28T12:34:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    34,
                    26,
                    1,
                    301,
                    0
                ],
                "published": "2024-08-18T16:44:01Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    16,
                    44,
                    1,
                    6,
                    231,
                    0
                ],
                "title": "Galapagos: Automated N-Version Programming with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galapagos: Automated N-Version Programming with LLMs"
                },
                "summary": "N-Version Programming is a well-known methodology for developing\nfault-tolerant systems. It achieves fault detection and correction at runtime\nby adding diverse redundancy into programs, minimizing fault mode overlap\nbetween redundant program variants. In this work, we propose the automated\ngeneration of program variants using large language models. We design, develop\nand evaluate Gal\\'apagos: a tool for generating program variants using LLMs,\nvalidating their correctness and equivalence, and using them to assemble\nN-Version binaries. We evaluate Gal\\'apagos by creating N-Version components of\nreal-world C code. Our original results show that Gal\\'apagos can produce\nprogram variants that are proven to be functionally equivalent, even when the\nvariants are written in a different programming language. Our systematic\ndiversity measurement indicates that functionally equivalent variants produced\nby Gal\\'apagos, are statically different after compilation, and present\ndiverging internal behavior at runtime. We demonstrate that the variants\nproduced by Gal\\'apagos can protect C code against real miscompilation bugs\nwhich affect the Clang compiler. Overall, our paper shows that producing\nN-Version software can be drastically automated by advanced usage of practical\nformal verification and generative language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-Version Programming is a well-known methodology for developing\nfault-tolerant systems. It achieves fault detection and correction at runtime\nby adding diverse redundancy into programs, minimizing fault mode overlap\nbetween redundant program variants. In this work, we propose the automated\ngeneration of program variants using large language models. We design, develop\nand evaluate Gal\\'apagos: a tool for generating program variants using LLMs,\nvalidating their correctness and equivalence, and using them to assemble\nN-Version binaries. We evaluate Gal\\'apagos by creating N-Version components of\nreal-world C code. Our original results show that Gal\\'apagos can produce\nprogram variants that are proven to be functionally equivalent, even when the\nvariants are written in a different programming language. Our systematic\ndiversity measurement indicates that functionally equivalent variants produced\nby Gal\\'apagos, are statically different after compilation, and present\ndiverging internal behavior at runtime. We demonstrate that the variants\nproduced by Gal\\'apagos can protect C code against real miscompilation bugs\nwhich affect the Clang compiler. Overall, our paper shows that producing\nN-Version software can be drastically automated by advanced usage of practical\nformal verification and generative language models."
                },
                "authors": [
                    {
                        "name": "Javier Ron"
                    },
                    {
                        "name": "Diogo Gaspar"
                    },
                    {
                        "name": "Javier Cabrera-Arteaga"
                    },
                    {
                        "name": "Benoit Baudry"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09536v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09536v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07883v2",
                "updated": "2025-10-28T12:30:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    30,
                    22,
                    1,
                    301,
                    0
                ],
                "published": "2024-05-13T16:17:10Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    16,
                    17,
                    10,
                    0,
                    134,
                    0
                ],
                "title": "Zero-Shot Tokenizer Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Tokenizer Transfer"
                },
                "summary": "Language models (LMs) are bound to their tokenizer, which maps raw text to a\nsequence of vocabulary items (tokens). This restricts their flexibility: for\nexample, LMs trained primarily on English may still perform well in other\nnatural and programming languages, but have vastly decreased efficiency due to\ntheir English-centric tokenizer. To mitigate this, we should be able to swap\nthe original LM tokenizer with an arbitrary one, on the fly, without degrading\nperformance. Hence, in this work we define a new problem: Zero-Shot Tokenizer\nTransfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for\nthe tokens in the vocabulary of the new tokenizer. Since prior heuristics for\ninitializing embeddings often perform at chance level in a ZeTT setting, we\npropose a new solution: we train a hypernetwork taking a tokenizer as input and\npredicting the corresponding embeddings. We empirically demonstrate that the\nhypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and\ndecoder LLMs (e.g., Mistral-7B). Our method comes close to the original models'\nperformance in cross-lingual and coding tasks while markedly reducing the\nlength of the tokenized sequence. We also find that the remaining gap can be\nquickly closed by continued training on less than 1B tokens. Finally, we show\nthat a ZeTT hypernetwork trained for a base (L)LM can also be applied to\nfine-tuned variants without extra training. Overall, our results make\nsubstantial strides toward detaching LMs from their tokenizer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) are bound to their tokenizer, which maps raw text to a\nsequence of vocabulary items (tokens). This restricts their flexibility: for\nexample, LMs trained primarily on English may still perform well in other\nnatural and programming languages, but have vastly decreased efficiency due to\ntheir English-centric tokenizer. To mitigate this, we should be able to swap\nthe original LM tokenizer with an arbitrary one, on the fly, without degrading\nperformance. Hence, in this work we define a new problem: Zero-Shot Tokenizer\nTransfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for\nthe tokens in the vocabulary of the new tokenizer. Since prior heuristics for\ninitializing embeddings often perform at chance level in a ZeTT setting, we\npropose a new solution: we train a hypernetwork taking a tokenizer as input and\npredicting the corresponding embeddings. We empirically demonstrate that the\nhypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and\ndecoder LLMs (e.g., Mistral-7B). Our method comes close to the original models'\nperformance in cross-lingual and coding tasks while markedly reducing the\nlength of the tokenized sequence. We also find that the remaining gap can be\nquickly closed by continued training on less than 1B tokens. Finally, we show\nthat a ZeTT hypernetwork trained for a base (L)LM can also be applied to\nfine-tuned variants without extra training. Overall, our results make\nsubstantial strides toward detaching LMs from their tokenizer."
                },
                "authors": [
                    {
                        "name": "Benjamin Minixhofer"
                    },
                    {
                        "name": "Edoardo Maria Ponti"
                    },
                    {
                        "name": "Ivan Vulić"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Vulić"
                },
                "author": "Ivan Vulić",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24358v1",
                "updated": "2025-10-28T12:26:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    26,
                    45,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:26:45Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    26,
                    45,
                    1,
                    301,
                    0
                ],
                "title": "Automatically Benchmarking LLM Code Agents through Agent-Driven\n  Annotation and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Benchmarking LLM Code Agents through Agent-Driven\n  Annotation and Evaluation"
                },
                "summary": "Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation."
                },
                "authors": [
                    {
                        "name": "Lingyue Fu"
                    },
                    {
                        "name": "Bolun Zhang"
                    },
                    {
                        "name": "Hao Guan"
                    },
                    {
                        "name": "Yaoming Zhu"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Yong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Yu"
                },
                "author": "Yong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24345v1",
                "updated": "2025-10-28T12:11:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    11,
                    12,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:11:12Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    11,
                    12,
                    1,
                    301,
                    0
                ],
                "title": "LongWeave: A Long-Form Generation Benchmark Bridging Real-World\n  Relevance and Verifiability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongWeave: A Long-Form Generation Benchmark Bridging Real-World\n  Relevance and Verifiability"
                },
                "summary": "Generating long, informative, and factual outputs remains a major challenge\nfor Large Language Models (LLMs). Existing benchmarks for long-form generation\ntypically assess real-world queries with hard-to-verify metrics or use\nsynthetic setups that ease evaluation but overlook real-world intricacies. In\nthis paper, we introduce \\textbf{LongWeave}, which balances real-world and\nverifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval\nconstructs tasks by first defining verifiable targets within real-world\nscenarios, then systematically generating corresponding queries, textual\nmaterials, and constraints based on these targets. This ensures that tasks are\nboth realistic and objectively assessable, enabling rigorous assessment of\nmodel capabilities in meeting complex real-world constraints. LongWeave\nsupports customizable input/output lengths (up to 64K/8K tokens) across seven\ndistinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models\nencounter significant challenges in long-form generation as real-world\ncomplexity and output length increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, informative, and factual outputs remains a major challenge\nfor Large Language Models (LLMs). Existing benchmarks for long-form generation\ntypically assess real-world queries with hard-to-verify metrics or use\nsynthetic setups that ease evaluation but overlook real-world intricacies. In\nthis paper, we introduce \\textbf{LongWeave}, which balances real-world and\nverifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval\nconstructs tasks by first defining verifiable targets within real-world\nscenarios, then systematically generating corresponding queries, textual\nmaterials, and constraints based on these targets. This ensures that tasks are\nboth realistic and objectively assessable, enabling rigorous assessment of\nmodel capabilities in meeting complex real-world constraints. LongWeave\nsupports customizable input/output lengths (up to 64K/8K tokens) across seven\ndistinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models\nencounter significant challenges in long-form generation as real-world\ncomplexity and output length increase."
                },
                "authors": [
                    {
                        "name": "Zikai Xiao"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Jianhui Wei"
                    },
                    {
                        "name": "Wen Ma"
                    },
                    {
                        "name": "Yuxuan Zhou"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Zuozhu Liu"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "arxiv_comment": "EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24339v2",
                "updated": "2025-10-29T04:05:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    5,
                    13,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-28T12:07:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    7,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science\n  Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science\n  Automation"
                },
                "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation."
                },
                "authors": [
                    {
                        "name": "Yunxuan Jiang"
                    },
                    {
                        "name": "Silan Hu"
                    },
                    {
                        "name": "Xiaoning Wang"
                    },
                    {
                        "name": "Yuanyuan Zhang"
                    },
                    {
                        "name": "Xiangyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Chang"
                },
                "arxiv_affiliation": "School of Management, Xi'an Jiaotong University",
                "author": "Xiangyu Chang",
                "arxiv_comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents . Submitted to Stat\n  (manuscript ID: STAT-25-0222.R1, under review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15189v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15189v3",
                "updated": "2025-10-29T05:47:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    5,
                    47,
                    44,
                    2,
                    302,
                    0
                ],
                "published": "2024-12-19T18:57:11Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    57,
                    11,
                    3,
                    354,
                    0
                ],
                "title": "Face the Facts! Evaluating RAG-based Pipelines for Professional\n  Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face the Facts! Evaluating RAG-based Pipelines for Professional\n  Fact-Checking"
                },
                "summary": "Natural Language Processing and Generation systems have recently shown the\npotential to complement and streamline the costly and time-consuming job of\nprofessional fact-checkers. In this work, we lift several constraints of\ncurrent state-of-the-art pipelines for automated fact-checking based on the\nRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark,\nfollowing professional fact-checking practices, RAG-based methods for the\ngeneration of verdicts - i.e., short texts discussing the veracity of a claim -\nevaluating them on stylistically complex claims and heterogeneous, yet\nreliable, knowledge bases. Our findings show a complex landscape, where, for\nexample, LLM-based retrievers outperform other retrieval techniques, though\nthey still struggle with heterogeneous knowledge bases; larger models excel in\nverdict faithfulness, while smaller models provide better context adherence,\nwith human evaluations favouring zero-shot and one-shot approaches for\ninformativeness, and fine-tuned models for emotional alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing and Generation systems have recently shown the\npotential to complement and streamline the costly and time-consuming job of\nprofessional fact-checkers. In this work, we lift several constraints of\ncurrent state-of-the-art pipelines for automated fact-checking based on the\nRetrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark,\nfollowing professional fact-checking practices, RAG-based methods for the\ngeneration of verdicts - i.e., short texts discussing the veracity of a claim -\nevaluating them on stylistically complex claims and heterogeneous, yet\nreliable, knowledge bases. Our findings show a complex landscape, where, for\nexample, LLM-based retrievers outperform other retrieval techniques, though\nthey still struggle with heterogeneous knowledge bases; larger models excel in\nverdict faithfulness, while smaller models provide better context adherence,\nwith human evaluations favouring zero-shot and one-shot approaches for\ninformativeness, and fine-tuned models for emotional alignment."
                },
                "authors": [
                    {
                        "name": "Daniel Russo"
                    },
                    {
                        "name": "Stefano Menini"
                    },
                    {
                        "name": "Jacopo Staiano"
                    },
                    {
                        "name": "Marco Guerini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Guerini"
                },
                "author": "Marco Guerini",
                "arxiv_comment": "Code and data at https://github.com/drusso98/face-the-facts -\n  Accepted for publication at INLG 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15189v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15189v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19477v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19477v5",
                "updated": "2025-10-28T11:59:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    59,
                    43,
                    1,
                    301,
                    0
                ],
                "published": "2024-11-29T05:29:47Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    29,
                    47,
                    4,
                    334,
                    0
                ],
                "title": "Provable Scaling Laws for the Test-Time Compute of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provable Scaling Laws for the Test-Time Compute of Large Language Models"
                },
                "summary": "We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms."
                },
                "authors": [
                    {
                        "name": "Yanxi Chen"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "NeurIPS 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19477v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19477v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24331v1",
                "updated": "2025-10-28T11:55:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    55,
                    24,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T11:55:24Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    55,
                    24,
                    1,
                    301,
                    0
                ],
                "title": "What do vision-language models see in the context? Investigating\n  multimodal in-context learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What do vision-language models see in the context? Investigating\n  multimodal in-context learning"
                },
                "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples."
                },
                "authors": [
                    {
                        "name": "Gabriel O. dos Santos"
                    },
                    {
                        "name": "Esther Colombini"
                    },
                    {
                        "name": "Sandra Avila"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Avila"
                },
                "author": "Sandra Avila",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24328v1",
                "updated": "2025-10-28T11:52:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    52,
                    51,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T11:52:51Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    52,
                    51,
                    1,
                    301,
                    0
                ],
                "title": "Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect\n  Variants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect\n  Variants"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to answer everyday\nquestions, yet their performance on culturally grounded and dialectal content\nremains uneven across languages. We propose a comprehensive method that (i)\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\nEnglish and several Arabic dialects, (ii) converts them into open-ended\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\nrationales to fine-tune models for step-by-step reasoning. Using this method,\nwe extend an existing dataset in which QAs are parallelly aligned across\nmultiple language varieties, making it, to our knowledge, the first of its\nkind. We conduct extensive experiments with both open and closed models. Our\nfindings show that (i) models underperform on Arabic dialects, revealing\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\ndeveloped dataset will be publicly released to support further research on\nculturally and linguistically inclusive evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to answer everyday\nquestions, yet their performance on culturally grounded and dialectal content\nremains uneven across languages. We propose a comprehensive method that (i)\ntranslates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into\nEnglish and several Arabic dialects, (ii) converts them into open-ended\nquestions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs\nunder both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)\nrationales to fine-tune models for step-by-step reasoning. Using this method,\nwe extend an existing dataset in which QAs are parallelly aligned across\nmultiple language varieties, making it, to our knowledge, the first of its\nkind. We conduct extensive experiments with both open and closed models. Our\nfindings show that (i) models underperform on Arabic dialects, revealing\npersistent gaps in culturally grounded and dialect-specific knowledge; (ii)\nArabic-centric models perform well on MCQs but struggle with OEQs; and (iii)\nCoT improves judged correctness while yielding mixed n-gram-based metrics. The\ndeveloped dataset will be publicly released to support further research on\nculturally and linguistically inclusive evaluation."
                },
                "authors": [
                    {
                        "name": "Hunzalah Hassan Bhatti"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "Cultural Knowledge, Everyday Knowledge, Open-Ended Question,\n  Chain-of-Thought, Large Language Models, Native, Multilingual, Language\n  Diversity",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24320v1",
                "updated": "2025-10-28T11:37:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    37,
                    1,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T11:37:01Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    37,
                    1,
                    1,
                    301,
                    0
                ],
                "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning"
                },
                "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential."
                },
                "authors": [
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Jixuan Huang"
                    },
                    {
                        "name": "Xin Guo"
                    },
                    {
                        "name": "Boyang Hong"
                    },
                    {
                        "name": "Dingwen Yang"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Jiecao Chen"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Preprint, 25 pages, 9 figures. Code:\n  https://github.com/WooooDyy/Critique-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24317v1",
                "updated": "2025-10-28T11:36:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    36,
                    20,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T11:36:20Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    36,
                    20,
                    1,
                    301,
                    0
                ],
                "title": "Cybersecurity AI Benchmark (CAIBench): A Meta-Benchmark for Evaluating\n  Cybersecurity AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersecurity AI Benchmark (CAIBench): A Meta-Benchmark for Evaluating\n  Cybersecurity AI Agents"
                },
                "summary": "Cybersecurity spans multiple interconnected domains, complicating the\ndevelopment of meaningful, labor-relevant benchmarks. Existing benchmarks\nassess isolated skills rather than integrated performance. We find that\npre-trained knowledge of cybersecurity in LLMs does not imply attack and\ndefense abilities, revealing a gap between knowledge and capability. To address\nthis limitation, we present the Cybersecurity AI Benchmark (CAIBench), a\nmodular meta-benchmark framework that allows evaluating LLM models and agents\nacross offensive and defensive cybersecurity domains, taking a step towards\nmeaningfully measuring their labor-relevance. CAIBench integrates five\nevaluation categories, covering over 10,000 instances: Jeopardy-style CTFs,\nAttack and Defense CTFs, Cyber Range exercises, knowledge benchmarks, and\nprivacy assessments. Key novel contributions include systematic simultaneous\noffensive-defensive evaluation, robotics-focused cybersecurity challenges\n(RCTF2), and privacy-preserving performance assessment (CyberPII-Bench).\nEvaluation of state-of-the-art AI models reveals saturation on security\nknowledge metrics (~70\\% success) but substantial degradation in multi-step\nadversarial (A\\&D) scenarios (20-40\\% success), or worse in robotic targets\n(22\\% success). The combination of framework scaffolding and LLM model choice\nsignificantly impacts performance; we find that proper matches improve up to\n2.6$\\times$ variance in Attack and Defense CTFs. These results demonstrate a\npronounced gap between conceptual knowledge and adaptive capability,\nemphasizing the need for a meta-benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersecurity spans multiple interconnected domains, complicating the\ndevelopment of meaningful, labor-relevant benchmarks. Existing benchmarks\nassess isolated skills rather than integrated performance. We find that\npre-trained knowledge of cybersecurity in LLMs does not imply attack and\ndefense abilities, revealing a gap between knowledge and capability. To address\nthis limitation, we present the Cybersecurity AI Benchmark (CAIBench), a\nmodular meta-benchmark framework that allows evaluating LLM models and agents\nacross offensive and defensive cybersecurity domains, taking a step towards\nmeaningfully measuring their labor-relevance. CAIBench integrates five\nevaluation categories, covering over 10,000 instances: Jeopardy-style CTFs,\nAttack and Defense CTFs, Cyber Range exercises, knowledge benchmarks, and\nprivacy assessments. Key novel contributions include systematic simultaneous\noffensive-defensive evaluation, robotics-focused cybersecurity challenges\n(RCTF2), and privacy-preserving performance assessment (CyberPII-Bench).\nEvaluation of state-of-the-art AI models reveals saturation on security\nknowledge metrics (~70\\% success) but substantial degradation in multi-step\nadversarial (A\\&D) scenarios (20-40\\% success), or worse in robotic targets\n(22\\% success). The combination of framework scaffolding and LLM model choice\nsignificantly impacts performance; we find that proper matches improve up to\n2.6$\\times$ variance in Attack and Defense CTFs. These results demonstrate a\npronounced gap between conceptual knowledge and adaptive capability,\nemphasizing the need for a meta-benchmark."
                },
                "authors": [
                    {
                        "name": "María Sanz-Gómez"
                    },
                    {
                        "name": "Víctor Mayoral-Vilches"
                    },
                    {
                        "name": "Francesco Balassone"
                    },
                    {
                        "name": "Luis Javier Navarrete-Lozano"
                    },
                    {
                        "name": "Cristóbal R. J. Veas Chavez"
                    },
                    {
                        "name": "Maite del Mundo de Torres"
                    }
                ],
                "author_detail": {
                    "name": "Maite del Mundo de Torres"
                },
                "author": "Maite del Mundo de Torres",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24303v1",
                "updated": "2025-10-28T11:12:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    12,
                    43,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T11:12:43Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    12,
                    43,
                    1,
                    301,
                    0
                ],
                "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental\n  Forecasting"
                },
                "summary": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification."
                },
                "authors": [
                    {
                        "name": "Deniz Gorur"
                    },
                    {
                        "name": "Antoni Rago"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23497v2",
                "updated": "2025-10-28T11:09:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    9,
                    37,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-27T16:32:12Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    16,
                    32,
                    12,
                    0,
                    300,
                    0
                ],
                "title": "VOLD: Reasoning Transfer from LLMs to Vision-Language Models via\n  On-Policy Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VOLD: Reasoning Transfer from LLMs to Vision-Language Models via\n  On-Policy Distillation"
                },
                "summary": "Training vision-language models (VLMs) for complex reasoning remains a\nchallenging task, i.a. due to the scarcity of high-quality image-text reasoning\ndata. Conversely, text-based reasoning resources are abundant and scalable, but\nit is still an open question how to leveraging them for VLM reasoning. To\naddress this problem, we propose VOLD, a framework to transfer reasoning\ncapabilities from text-only teacher models to VLM student models. To this end,\nVOLD combines reinforcement learning via Group Relative Policy Optimization\n(GRPO) with on-policy distillation, which allows the student reasoning traces\nto be guided by the teacher model, resulting in a significant gain over using\nGRPO alone. We further show that a cold-start alignment is essential for an\neffective transfer during the online training phase in this scenario and that\nwithout sufficient distributional alignment between teacher and student,\non-policy distillation fails to provide meaningful guidance. We evaluate VOLD\nacross diverse benchmarks including MMMU-Pro, MathVision, MathVista, and\nLogicVista, showing that VOLD outperforms the baseline model significantly and\nimproves over the state of the art by a margin. Our ablation shows the\nimportance of a cold-start alignment via SFT for on-policy distillation with a\ntext-only teacher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training vision-language models (VLMs) for complex reasoning remains a\nchallenging task, i.a. due to the scarcity of high-quality image-text reasoning\ndata. Conversely, text-based reasoning resources are abundant and scalable, but\nit is still an open question how to leveraging them for VLM reasoning. To\naddress this problem, we propose VOLD, a framework to transfer reasoning\ncapabilities from text-only teacher models to VLM student models. To this end,\nVOLD combines reinforcement learning via Group Relative Policy Optimization\n(GRPO) with on-policy distillation, which allows the student reasoning traces\nto be guided by the teacher model, resulting in a significant gain over using\nGRPO alone. We further show that a cold-start alignment is essential for an\neffective transfer during the online training phase in this scenario and that\nwithout sufficient distributional alignment between teacher and student,\non-policy distillation fails to provide meaningful guidance. We evaluate VOLD\nacross diverse benchmarks including MMMU-Pro, MathVision, MathVista, and\nLogicVista, showing that VOLD outperforms the baseline model significantly and\nimproves over the state of the art by a margin. Our ablation shows the\nimportance of a cold-start alignment via SFT for on-policy distillation with a\ntext-only teacher."
                },
                "authors": [
                    {
                        "name": "Walid Bousselham"
                    },
                    {
                        "name": "Hilde Kuehne"
                    },
                    {
                        "name": "Cordelia Schmid"
                    }
                ],
                "author_detail": {
                    "name": "Cordelia Schmid"
                },
                "author": "Cordelia Schmid",
                "arxiv_comment": "www.walidbousselham.com/VOLD/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24299v1",
                "updated": "2025-10-28T11:01:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    1,
                    10,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T11:01:10Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    1,
                    10,
                    1,
                    301,
                    0
                ],
                "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix\n  Rank",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix\n  Rank"
                },
                "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%."
                },
                "authors": [
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Ning Miao"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08146v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08146v3",
                "updated": "2025-10-28T10:58:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    58,
                    14,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-09T12:33:16Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    33,
                    16,
                    3,
                    282,
                    0
                ],
                "title": "Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM\n  Reasoning"
                },
                "summary": "We introduce a simple, yet novel entropy-based framework to drive token\nefficiency in large language models during reasoning tasks. Our approach uses\nShannon entropy from token-level logprobs as a confidence signal to enable\nearly stopping, achieving 25-50% computational savings while maintaining task\naccuracy. Crucially, we demonstrate that entropy-based confidence calibration\nrepresents an emergent property of advanced post-training optimization present\nin modern reasoning models but notably absent in standard instruction-tuned and\npre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop\nreasoning varies from model to model but can be calculated easily in one shot\nusing only a few examples from existing reasoning datasets. Our results\nindicate that advanced reasoning models often know that they've gotten a\ncorrect answer early on, and that this emergent confidence awareness can be\nexploited to save tokens and reduce latency. The framework demonstrates\nconsistent performance across reasoning-optimized model families with 25-50%\ncomputational cost reduction while preserving accuracy, revealing that\nconfidence mechanisms represent a distinguishing characteristic of modern\npost-trained reasoning systems versus their predecessors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a simple, yet novel entropy-based framework to drive token\nefficiency in large language models during reasoning tasks. Our approach uses\nShannon entropy from token-level logprobs as a confidence signal to enable\nearly stopping, achieving 25-50% computational savings while maintaining task\naccuracy. Crucially, we demonstrate that entropy-based confidence calibration\nrepresents an emergent property of advanced post-training optimization present\nin modern reasoning models but notably absent in standard instruction-tuned and\npre-trained models (Llama 3.3 70B). We show that the entropy threshold to stop\nreasoning varies from model to model but can be calculated easily in one shot\nusing only a few examples from existing reasoning datasets. Our results\nindicate that advanced reasoning models often know that they've gotten a\ncorrect answer early on, and that this emergent confidence awareness can be\nexploited to save tokens and reduce latency. The framework demonstrates\nconsistent performance across reasoning-optimized model families with 25-50%\ncomputational cost reduction while preserving accuracy, revealing that\nconfidence mechanisms represent a distinguishing characteristic of modern\npost-trained reasoning systems versus their predecessors."
                },
                "authors": [
                    {
                        "name": "Aman Sharma"
                    },
                    {
                        "name": "Paras Chopra"
                    }
                ],
                "author_detail": {
                    "name": "Paras Chopra"
                },
                "author": "Paras Chopra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08146v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08146v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13771v2",
                "updated": "2025-10-28T10:57:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    57,
                    14,
                    1,
                    301,
                    0
                ],
                "published": "2025-05-30T06:43:03Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    6,
                    43,
                    3,
                    4,
                    150,
                    0
                ],
                "title": "LittleBit: Ultra Low-Bit Quantization via Latent Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LittleBit: Ultra Low-Bit Quantization via Latent Factorization"
                },
                "summary": "Deploying large language models (LLMs) often faces challenges from\nsubstantial memory and computational costs. Quantization offers a solution, yet\nperformance degradation in the sub-1-bit regime remains particularly difficult.\nThis paper introduces LittleBit, a novel method for extreme LLM compression. It\ntargets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$\nmemory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents\nweights in a low-rank form using latent matrix factorization, subsequently\nbinarizing these factors. To counteract information loss from this extreme\nprecision, it integrates a multi-scale compensation mechanism. This includes\nrow, column, and an additional latent dimension that learns per-rank\nimportance. Two key contributions enable effective training: Dual\nSign-Value-Independent Decomposition (Dual-SVID) for quantization-aware\ntraining (QAT) initialization, and integrated Residual Compensation to mitigate\nerrors. Extensive experiments confirm LittleBit's superiority in sub-1-bit\nquantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading\nmethod's 0.7 BPW. LittleBit establishes a new, viable size-performance\ntrade-off--unlocking a potential 11.6$\\times$ speedup over FP16 at the kernel\nlevel--and makes powerful LLMs practical for resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) often faces challenges from\nsubstantial memory and computational costs. Quantization offers a solution, yet\nperformance degradation in the sub-1-bit regime remains particularly difficult.\nThis paper introduces LittleBit, a novel method for extreme LLM compression. It\ntargets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$\nmemory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents\nweights in a low-rank form using latent matrix factorization, subsequently\nbinarizing these factors. To counteract information loss from this extreme\nprecision, it integrates a multi-scale compensation mechanism. This includes\nrow, column, and an additional latent dimension that learns per-rank\nimportance. Two key contributions enable effective training: Dual\nSign-Value-Independent Decomposition (Dual-SVID) for quantization-aware\ntraining (QAT) initialization, and integrated Residual Compensation to mitigate\nerrors. Extensive experiments confirm LittleBit's superiority in sub-1-bit\nquantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading\nmethod's 0.7 BPW. LittleBit establishes a new, viable size-performance\ntrade-off--unlocking a potential 11.6$\\times$ speedup over FP16 at the kernel\nlevel--and makes powerful LLMs practical for resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Banseok Lee"
                    },
                    {
                        "name": "Dongkyu Kim"
                    },
                    {
                        "name": "Youngcheon You"
                    },
                    {
                        "name": "Youngmin Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngmin Kim"
                },
                "author": "Youngmin Kim",
                "arxiv_comment": "Accepted to NeurIPS 2025. Banseok Lee and Dongkyu Kim contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24284v1",
                "updated": "2025-10-28T10:42:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    42,
                    17,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:42:17Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    42,
                    17,
                    1,
                    301,
                    0
                ],
                "title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and\n  Scaling MCP Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and\n  Scaling MCP Tools"
                },
                "summary": "Large Language Models (LLMs) increasingly rely on external tools to perform\ncomplex, realistic tasks, yet their ability to utilize the rapidly expanding\nModel Contextual Protocol (MCP) ecosystem remains limited. Existing MCP\nresearch covers few servers, depends on costly manual curation, and lacks\ntraining support, hindering progress toward real-world deployment. To overcome\nthese limitations, we introduce MCP-Flow, an automated web-agent-driven\npipeline for large-scale server discovery, data synthesis, and model training.\nMCP-Flow collects and filters data from 1166 servers and 11536 tools, producing\n68733 high-quality instruction-function call pairs and 6439 trajectories, far\nexceeding prior work in scale and diversity. Extensive experiments demonstrate\nMCP-Flow's effectiveness in driving superior MCP tool selection, function-call\ngeneration, and enhanced agentic task performance. MCP-Flow thus provides a\nscalable foundation for advancing LLM agents' proficiency in real-world MCP\nenvironments. MCP-Flow is publicly available at\n\\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly rely on external tools to perform\ncomplex, realistic tasks, yet their ability to utilize the rapidly expanding\nModel Contextual Protocol (MCP) ecosystem remains limited. Existing MCP\nresearch covers few servers, depends on costly manual curation, and lacks\ntraining support, hindering progress toward real-world deployment. To overcome\nthese limitations, we introduce MCP-Flow, an automated web-agent-driven\npipeline for large-scale server discovery, data synthesis, and model training.\nMCP-Flow collects and filters data from 1166 servers and 11536 tools, producing\n68733 high-quality instruction-function call pairs and 6439 trajectories, far\nexceeding prior work in scale and diversity. Extensive experiments demonstrate\nMCP-Flow's effectiveness in driving superior MCP tool selection, function-call\ngeneration, and enhanced agentic task performance. MCP-Flow thus provides a\nscalable foundation for advancing LLM agents' proficiency in real-world MCP\nenvironments. MCP-Flow is publicly available at\n\\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}."
                },
                "authors": [
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Peizhi Niu"
                    },
                    {
                        "name": "Zhao Xu"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Jian Du"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Keduan Huang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Qiang Yan"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24273v1",
                "updated": "2025-10-28T10:32:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:32:52Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "title": "SALS: Sparse Attention in Latent Space for KV cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALS: Sparse Attention in Latent Space for KV cache Compression"
                },
                "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively."
                },
                "authors": [
                    {
                        "name": "Junlin Mu"
                    },
                    {
                        "name": "Hantao Huang"
                    },
                    {
                        "name": "Jihang Zhang"
                    },
                    {
                        "name": "Minghui Yu"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Yidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Li"
                },
                "author": "Yidong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23464v2",
                "updated": "2025-10-28T10:19:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    19,
                    32,
                    1,
                    301,
                    0
                ],
                "published": "2025-06-30T02:06:54Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    2,
                    6,
                    54,
                    0,
                    181,
                    0
                ],
                "title": "The Confidence Paradox: Can LLM Know When It's Wrong",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Confidence Paradox: Can LLM Know When It's Wrong"
                },
                "summary": "Document Visual Question Answering (DocVQA) models often produce\noverconfident or ethically misaligned responses, especially under uncertainty.\nExisting models like LayoutLMv3, UDOP, and DONUT focus on accuracy but lack\nethical calibration. We propose HonestVQA, a model-agnostic, self-supervised\nframework that aligns model confidence with correctness using weighted loss and\ncontrastive learning. We introduce two new metrics Honesty Score (H-Score) and\nEthical Confidence Index (ECI)-to evaluate ethical alignment. HonestVQA\nimproves accuracy and F1 by up to 4.3% across SpDocVQA, InfographicsVQA, and\nSROIE datasets, while reducing overconfidence. It also generalizes well across\ndomains, achieving 78.9% accuracy and 76.1% F1-score.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Visual Question Answering (DocVQA) models often produce\noverconfident or ethically misaligned responses, especially under uncertainty.\nExisting models like LayoutLMv3, UDOP, and DONUT focus on accuracy but lack\nethical calibration. We propose HonestVQA, a model-agnostic, self-supervised\nframework that aligns model confidence with correctness using weighted loss and\ncontrastive learning. We introduce two new metrics Honesty Score (H-Score) and\nEthical Confidence Index (ECI)-to evaluate ethical alignment. HonestVQA\nimproves accuracy and F1 by up to 4.3% across SpDocVQA, InfographicsVQA, and\nSROIE datasets, while reducing overconfidence. It also generalizes well across\ndomains, achieving 78.9% accuracy and 76.1% F1-score."
                },
                "authors": [
                    {
                        "name": "Sahil Tripathi"
                    },
                    {
                        "name": "Md Tabrez Nafis"
                    },
                    {
                        "name": "Imran Hussain"
                    },
                    {
                        "name": "Jiechao Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jiechao Gao"
                },
                "author": "Jiechao Gao",
                "arxiv_comment": "Accepted at the 14th IJCNLP & 4th AACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24259v1",
                "updated": "2025-10-28T10:13:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    13,
                    43,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:13:43Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    13,
                    43,
                    1,
                    301,
                    0
                ],
                "title": "Can LLMs Translate Human Instructions into a Reinforcement Learning\n  Agent's Internal Emergent Symbolic Representation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Translate Human Instructions into a Reinforcement Learning\n  Agent's Internal Emergent Symbolic Representation?"
                },
                "summary": "Emergent symbolic representations are critical for enabling developmental\nlearning agents to plan and generalize across tasks. In this work, we\ninvestigate whether large language models (LLMs) can translate human natural\nlanguage instructions into the internal symbolic representations that emerge\nduring hierarchical reinforcement learning. We apply a structured evaluation\nframework to measure the translation performance of commonly seen LLMs -- GPT,\nClaude, Deepseek and Grok -- across different internal symbolic partitions\ngenerated by a hierarchical reinforcement learning algorithm in the Ant Maze\nand Ant Fall environments. Our findings reveal that although LLMs demonstrate\nsome ability to translate natural language into a symbolic representation of\nthe environment dynamics, their performance is highly sensitive to partition\ngranularity and task complexity. The results expose limitations in current LLMs\ncapacity for representation alignment, highlighting the need for further\nresearch on robust alignment between language and internal agent\nrepresentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent symbolic representations are critical for enabling developmental\nlearning agents to plan and generalize across tasks. In this work, we\ninvestigate whether large language models (LLMs) can translate human natural\nlanguage instructions into the internal symbolic representations that emerge\nduring hierarchical reinforcement learning. We apply a structured evaluation\nframework to measure the translation performance of commonly seen LLMs -- GPT,\nClaude, Deepseek and Grok -- across different internal symbolic partitions\ngenerated by a hierarchical reinforcement learning algorithm in the Ant Maze\nand Ant Fall environments. Our findings reveal that although LLMs demonstrate\nsome ability to translate natural language into a symbolic representation of\nthe environment dynamics, their performance is highly sensitive to partition\ngranularity and task complexity. The results expose limitations in current LLMs\ncapacity for representation alignment, highlighting the need for further\nresearch on robust alignment between language and internal agent\nrepresentations."
                },
                "authors": [
                    {
                        "name": "Ziqi Ma"
                    },
                    {
                        "name": "Sao Mai Nguyen"
                    },
                    {
                        "name": "Philippe Xu"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Xu"
                },
                "author": "Philippe Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24255v1",
                "updated": "2025-10-28T10:05:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    5,
                    53,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:05:53Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    5,
                    53,
                    1,
                    301,
                    0
                ],
                "title": "Trajectory Design for UAV-Based Low-Altitude Wireless Networks in\n  Unknown Environments: A Digital Twin-Assisted TD3 Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Design for UAV-Based Low-Altitude Wireless Networks in\n  Unknown Environments: A Digital Twin-Assisted TD3 Approach"
                },
                "summary": "Unmanned aerial vehicles (UAVs) are emerging as key enablers for low-altitude\nwireless network (LAWN), particularly when terrestrial networks are\nunavailable. In such scenarios, the environmental topology is typically\nunknown; hence, designing efficient and safe UAV trajectories is essential yet\nchallenging. To address this, we propose a digital twin (DT)-assisted training\nand deployment framework. In this framework, the UAV transmits integrated\nsensing and communication signals to provide communication services to ground\nusers, while simultaneously collecting echoes that are uploaded to the DT\nserver to progressively construct virtual environments (VEs). These VEs\naccelerate model training and are continuously updated with real-time UAV\nsensing data during deployment, supporting decision-making and enhancing flight\nsafety. Based on this framework, we further develop a trajectory design scheme\nthat integrates simulated annealing for efficient user scheduling with the\ntwin-delayed deep deterministic policy gradient algorithm for continuous\ntrajectory design, aiming to minimize mission completion time while ensuring\nobstacle avoidance. Simulation results demonstrate that the proposed approach\nachieves faster convergence, higher flight safety, and shorter mission\ncompletion time compared with baseline methods, providing a robust and\nefficient solution for LAWN deployment in unknown environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) are emerging as key enablers for low-altitude\nwireless network (LAWN), particularly when terrestrial networks are\nunavailable. In such scenarios, the environmental topology is typically\nunknown; hence, designing efficient and safe UAV trajectories is essential yet\nchallenging. To address this, we propose a digital twin (DT)-assisted training\nand deployment framework. In this framework, the UAV transmits integrated\nsensing and communication signals to provide communication services to ground\nusers, while simultaneously collecting echoes that are uploaded to the DT\nserver to progressively construct virtual environments (VEs). These VEs\naccelerate model training and are continuously updated with real-time UAV\nsensing data during deployment, supporting decision-making and enhancing flight\nsafety. Based on this framework, we further develop a trajectory design scheme\nthat integrates simulated annealing for efficient user scheduling with the\ntwin-delayed deep deterministic policy gradient algorithm for continuous\ntrajectory design, aiming to minimize mission completion time while ensuring\nobstacle avoidance. Simulation results demonstrate that the proposed approach\nachieves faster convergence, higher flight safety, and shorter mission\ncompletion time compared with baseline methods, providing a robust and\nefficient solution for LAWN deployment in unknown environments."
                },
                "authors": [
                    {
                        "name": "Jihao Luo"
                    },
                    {
                        "name": "Zesong Fei"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Le Zhao"
                    },
                    {
                        "name": "Yuanhao Cui"
                    },
                    {
                        "name": "Guangxu Zhu"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "13 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24251v1",
                "updated": "2025-10-28T10:01:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    1,
                    43,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:01:43Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    1,
                    43,
                    1,
                    301,
                    0
                ],
                "title": "GRAPHIA: Harnessing Social Graph Data to Enhance LLM-Based Social\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRAPHIA: Harnessing Social Graph Data to Enhance LLM-Based Social\n  Simulation"
                },
                "summary": "Large language models (LLMs) have shown promise in simulating human-like\nsocial behaviors. Social graphs provide high-quality supervision signals that\nencode both local interactions and global network structure, yet they remain\nunderutilized for LLM training. To address this gap, we propose Graphia, the\nfirst general LLM-based social graph simulation framework that leverages graph\ndata as supervision for LLM post-training via reinforcement learning. With\nGNN-based structural rewards, Graphia trains specialized agents to predict whom\nto interact with (destination selection) and how to interact (edge generation),\nfollowed by designed graph generation pipelines. We evaluate Graphia under two\nsettings: Transductive Dynamic Graph Generation (TDGG), a micro-level task with\nour proposed node-wise interaction alignment metrics; and Inductive Dynamic\nGraph Generation (IDGG), a macro-level task with our proposed metrics for\naligning emergent network properties. On three real-world networks, Graphia\nimproves micro-level alignment by 6.1% in the composite destination selection\nscore, 12% in edge classification accuracy, and 27.9% in edge content BERTScore\nover the strongest baseline. For macro-level alignment, it achieves 41.11%\nhigher structural similarity and 32.98% better replication of social phenomena\nsuch as power laws and echo chambers. Graphia also supports counterfactual\nsimulation, generating plausible behavioral shifts under platform incentives.\nOur results show that social graphs can serve as high-quality supervision\nsignals for LLM post-training, closing the gap between agent behaviors and\nnetwork dynamics for LLM-based simulation. Code is available at\nhttps://github.com/Ji-Cather/Graphia.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in simulating human-like\nsocial behaviors. Social graphs provide high-quality supervision signals that\nencode both local interactions and global network structure, yet they remain\nunderutilized for LLM training. To address this gap, we propose Graphia, the\nfirst general LLM-based social graph simulation framework that leverages graph\ndata as supervision for LLM post-training via reinforcement learning. With\nGNN-based structural rewards, Graphia trains specialized agents to predict whom\nto interact with (destination selection) and how to interact (edge generation),\nfollowed by designed graph generation pipelines. We evaluate Graphia under two\nsettings: Transductive Dynamic Graph Generation (TDGG), a micro-level task with\nour proposed node-wise interaction alignment metrics; and Inductive Dynamic\nGraph Generation (IDGG), a macro-level task with our proposed metrics for\naligning emergent network properties. On three real-world networks, Graphia\nimproves micro-level alignment by 6.1% in the composite destination selection\nscore, 12% in edge classification accuracy, and 27.9% in edge content BERTScore\nover the strongest baseline. For macro-level alignment, it achieves 41.11%\nhigher structural similarity and 32.98% better replication of social phenomena\nsuch as power laws and echo chambers. Graphia also supports counterfactual\nsimulation, generating plausible behavioral shifts under platform incentives.\nOur results show that social graphs can serve as high-quality supervision\nsignals for LLM post-training, closing the gap between agent behaviors and\nnetwork dynamics for LLM-based simulation. Code is available at\nhttps://github.com/Ji-Cather/Graphia.git."
                },
                "authors": [
                    {
                        "name": "Jiarui Ji"
                    },
                    {
                        "name": "Zehua Zhang"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Bin Tong"
                    },
                    {
                        "name": "Guan Wang"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24250v1",
                "updated": "2025-10-28T10:00:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    0,
                    52,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:00:52Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    0,
                    52,
                    1,
                    301,
                    0
                ],
                "title": "Evaluating LLMs on Generating Age-Appropriate Child-Like Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs on Generating Age-Appropriate Child-Like Conversations"
                },
                "summary": "Large Language Models (LLMs), predominantly trained on adult conversational\ndata, face significant challenges when generating authentic, child-like\ndialogue for specialized applications. We present a comparative study\nevaluating five different LLMs (GPT-4, RUTER-LLAMA-2-13b, GPTSW, NorMistral-7b,\nand NorBloom-7b) to generate age-appropriate Norwegian conversations for\nchildren aged 5 and 9 years. Through a blind evaluation by eleven education\nprofessionals using both real child interview data and LLM-generated text\nsamples, we assessed authenticity and developmental appropriateness. Our\nresults show that evaluators achieved strong inter-rater reliability (ICC=0.75)\nand demonstrated higher accuracy in age prediction for younger children\n(5-year-olds) compared to older children (9-year-olds). While GPT-4 and\nNorBloom-7b performed relatively well, most models generated language perceived\nas more linguistically advanced than the target age groups. These findings\nhighlight critical data-related challenges in developing LLM systems for\nspecialized applications involving children, particularly in low-resource\nlanguages where comprehensive age-appropriate lexical resources are scarce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), predominantly trained on adult conversational\ndata, face significant challenges when generating authentic, child-like\ndialogue for specialized applications. We present a comparative study\nevaluating five different LLMs (GPT-4, RUTER-LLAMA-2-13b, GPTSW, NorMistral-7b,\nand NorBloom-7b) to generate age-appropriate Norwegian conversations for\nchildren aged 5 and 9 years. Through a blind evaluation by eleven education\nprofessionals using both real child interview data and LLM-generated text\nsamples, we assessed authenticity and developmental appropriateness. Our\nresults show that evaluators achieved strong inter-rater reliability (ICC=0.75)\nand demonstrated higher accuracy in age prediction for younger children\n(5-year-olds) compared to older children (9-year-olds). While GPT-4 and\nNorBloom-7b performed relatively well, most models generated language perceived\nas more linguistically advanced than the target age groups. These findings\nhighlight critical data-related challenges in developing LLM systems for\nspecialized applications involving children, particularly in low-resource\nlanguages where comprehensive age-appropriate lexical resources are scarce."
                },
                "authors": [
                    {
                        "name": "Syed Zohaib Hassan"
                    },
                    {
                        "name": "Pål Halvorsen"
                    },
                    {
                        "name": "Miriam S. Johnson"
                    },
                    {
                        "name": "Pierre Lison"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Lison"
                },
                "author": "Pierre Lison",
                "arxiv_comment": "11 pages excluding references and appendix. 3 figures and 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]