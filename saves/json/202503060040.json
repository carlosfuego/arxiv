[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Éric de la Clergerie"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02758v1",
                "updated": "2025-03-04T16:21:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:21:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Efficient and Optimal No-Regret Caching under Partial Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Optimal No-Regret Caching under Partial Observation"
                },
                "summary": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces."
                },
                "authors": [
                    {
                        "name": "Younes Ben Mazziane"
                    },
                    {
                        "name": "Francescomaria Faticanti"
                    },
                    {
                        "name": "Sara Alouf"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02508v1",
                "updated": "2025-03-04T11:19:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:19:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q&C: When Quantization Meets Cache in Efficient Image Generation"
                },
                "summary": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02504v1",
                "updated": "2025-03-04T11:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects"
                },
                "summary": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
                },
                "authors": [
                    {
                        "name": "Emese Sziklay"
                    },
                    {
                        "name": "Tamás Jursonovics"
                    }
                ],
                "author_detail": {
                    "name": "Tamás Jursonovics"
                },
                "author": "Tamás Jursonovics",
                "arxiv_comment": "13 pages, 7 figures, ICRIC 2023, Volume 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v1",
                "updated": "2025-03-04T03:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v1",
                "updated": "2025-03-03T18:32:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding\n\\emph{unstable} configurations. As many as $63.3\\%$ of the configurations\nselected as \"best\" during tuning can have their performance degrade by $30\\%$\nor more when deployed. Using this as motivation, we propose a novel approach to\nimprove the efficiency of autotuning systems by (a) detecting and removing\noutlier configurations and (b) using ML-based approaches to provide a more\nstable \\emph{true} signal of de-noised experiment results to the optimizer. The\nresulting system, TUNA (\\underline{T}uning \\underline{U}nstable and\n\\underline{N}oisy Cloud \\underline{A}pplications) enables faster convergence\nand robust configurations. Tuning postgres running \\texttt{mssales}, an\nenterprise production workload, we find that TUNA can lead to $1.88$x lower\nrunning time on average with $2.58x$ lower standard deviation compared to\ntraditional sampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding\n\\emph{unstable} configurations. As many as $63.3\\%$ of the configurations\nselected as \"best\" during tuning can have their performance degrade by $30\\%$\nor more when deployed. Using this as motivation, we propose a novel approach to\nimprove the efficiency of autotuning systems by (a) detecting and removing\noutlier configurations and (b) using ML-based approaches to provide a more\nstable \\emph{true} signal of de-noised experiment results to the optimizer. The\nresulting system, TUNA (\\underline{T}uning \\underline{U}nstable and\n\\underline{N}oisy Cloud \\underline{A}pplications) enables faster convergence\nand robust configurations. Tuning postgres running \\texttt{mssales}, an\nenterprise production workload, we find that TUNA can lead to $1.88$x lower\nrunning time on average with $2.58x$ lower standard deviation compared to\ntraditional sampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v2",
                "updated": "2025-03-03T18:23:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    23,
                    47,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "RefreshKV: Updating Small KV Cache During Long-form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefreshKV: Updating Small KV Cache During Long-form Generation"
                },
                "summary": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01586v1",
                "updated": "2025-03-03T14:26:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T14:26:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection"
                },
                "summary": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Sirui Song"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01483v1",
                "updated": "2025-03-03T12:43:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T12:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "title": "KurTail : Kurtosis-based LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KurTail : Kurtosis-based LLM Quantization"
                },
                "summary": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Akhondzadeh"
                    },
                    {
                        "name": "Aleksandar Bojchevski"
                    },
                    {
                        "name": "Evangelos Eleftheriou"
                    },
                    {
                        "name": "Martino Dazzi"
                    }
                ],
                "author_detail": {
                    "name": "Martino Dazzi"
                },
                "author": "Martino Dazzi",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01348v1",
                "updated": "2025-03-03T09:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension"
                },
                "summary": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems."
                },
                "authors": [
                    {
                        "name": "Hongguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Chen"
                },
                "author": "Hongguang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01330v1",
                "updated": "2025-03-03T09:12:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio."
                },
                "authors": [
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01323v1",
                "updated": "2025-03-03T09:04:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:04:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "CacheQuant: Comprehensively Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheQuant: Comprehensively Accelerated Diffusion Models"
                },
                "summary": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant ."
                },
                "authors": [
                    {
                        "name": "Xuewen Liu"
                    },
                    {
                        "name": "Zhikai Li"
                    },
                    {
                        "name": "Qingyi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyi Gu"
                },
                "author": "Qingyi Gu",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01281v1",
                "updated": "2025-03-03T08:06:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T08:06:55Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "title": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System"
                },
                "summary": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Yaobin Wang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yingchen Song"
                    },
                    {
                        "name": "Huan Wu"
                    },
                    {
                        "name": "Qingfeng Wang"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v2",
                "updated": "2025-03-03T05:49:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    49,
                    41,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v1",
                "updated": "2025-03-02T18:12:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v2",
                "updated": "2025-03-02T14:37:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    14,
                    37,
                    53,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00695v1",
                "updated": "2025-03-02T02:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T02:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "title": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis."
                },
                "authors": [
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Xu Lian"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07295v2",
                "updated": "2025-03-02T01:39:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    1,
                    39,
                    57,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-09T16:21:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    21,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking"
                },
                "summary": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com."
                },
                "authors": [
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Rohan Gumaste"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Gagandeep Singh"
                    },
                    {
                        "name": "Sasa Misailovic"
                    }
                ],
                "author_detail": {
                    "name": "Sasa Misailovic"
                },
                "author": "Sasa Misailovic",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00540v1",
                "updated": "2025-03-01T15:53:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T15:53:33Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "title": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval"
                },
                "summary": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models."
                },
                "authors": [
                    {
                        "name": "Shangzhe Di"
                    },
                    {
                        "name": "Zhelun Yu"
                    },
                    {
                        "name": "Guanghao Zhang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Bolin Li"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Fangxun Shu"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "arxiv_comment": "Accepted to ICLR 2025. Code: https://github.com/Becomebright/ReKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00392v1",
                "updated": "2025-03-01T07:56:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T07:56:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving"
                },
                "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v6",
                "updated": "2025-03-01T05:43:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    5,
                    43,
                    19,
                    5,
                    60,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3706628.3708873",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706628.3708873",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03058v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00323v1",
                "updated": "2025-03-01T03:20:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T03:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "title": "FLStore: Efficient Federated Learning Storage for non-training workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLStore: Efficient Federated Learning Storage for non-training workloads"
                },
                "summary": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable."
                },
                "authors": [
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Samuel Fountain"
                    },
                    {
                        "name": "Ahmed M. Abdelmoniem"
                    },
                    {
                        "name": "Ali R. Butt"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "arxiv_comment": "11 pages, 19 figures, 2 tables This paper has been accepted at the\n  The Eighth Annual Conference on Machine Learning and Systems (MLSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v4",
                "updated": "2025-02-28T18:04:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    4,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21117v1",
                "updated": "2025-02-28T14:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "title": "Distributed Data Access in Industrial Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Data Access in Industrial Edge Networks"
                },
                "summary": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication."
                },
                "authors": [
                    {
                        "name": "Theofanis P. Raptis"
                    },
                    {
                        "name": "Andrea Passarella"
                    },
                    {
                        "name": "Marco Conti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Conti"
                },
                "author": "Marco Conti",
                "arxiv_doi": "10.1109/JSAC.2020.2980917",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2020.2980917",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work was funded by the EC through the FoF-RIA Project AUTOWARE\n  (No. 723909)",
                "arxiv_journal_ref": "IEEE Journal on Selected Areas in Communications, vol. 38, no. 5,\n  pp. 915-927, May 2020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21079v1",
                "updated": "2025-02-28T14:11:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:11:20Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation"
                },
                "summary": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation."
                },
                "authors": [
                    {
                        "name": "Yifei Xia"
                    },
                    {
                        "name": "Suhan Ling"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v3",
                "updated": "2025-02-28T13:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    23,
                    56,
                    4,
                    59,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v3",
                "updated": "2025-02-28T13:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    8,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20812v1",
                "updated": "2025-02-28T07:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T07:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "title": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Yinglin Xie"
                    },
                    {
                        "name": "Zhao Liu"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v1",
                "updated": "2025-02-27T23:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%."
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "Mingyuan, Jize, and Haozhen contributed equally, while Minjia,\n  Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15896v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15896v3",
                "updated": "2025-02-27T21:50:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    50,
                    48,
                    3,
                    58,
                    0
                ],
                "published": "2023-12-26T06:16:12Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    6,
                    16,
                    12,
                    1,
                    360,
                    0
                ],
                "title": "WWW: What, When, Where to Compute-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WWW: What, When, Where to Compute-in-Memory"
                },
                "summary": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Tanvi Sharma"
                    },
                    {
                        "name": "Mustafa Ali"
                    },
                    {
                        "name": "Indranil Chakraborty"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "added supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15896v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15896v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20547v1",
                "updated": "2025-02-27T21:42:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T21:42:49Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "title": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches"
                },
                "summary": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers."
                },
                "authors": [
                    {
                        "name": "Aurore Poirier"
                    },
                    {
                        "name": "Erven Rohou"
                    },
                    {
                        "name": "Manuel Serrano"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Serrano"
                },
                "arxiv_affiliation": "Inria - University of Côte d'Azur, France",
                "author": "Manuel Serrano",
                "arxiv_doi": "10.22152/programming-journal.org/2026/10/6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.22152/programming-journal.org/2026/10/6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The Art, Science, and Engineering of Programming, 2025, Vol. 10,\n  Issue 1, Article 6",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v2",
                "updated": "2025-02-27T15:29:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    29,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v3",
                "updated": "2025-02-27T12:30:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    30,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v2",
                "updated": "2025-02-27T12:15:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    15,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "36 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v2",
                "updated": "2025-02-27T06:39:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    6,
                    39,
                    6,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v4",
                "updated": "2025-02-27T03:22:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    3,
                    22,
                    41,
                    3,
                    58,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v3",
                "updated": "2025-02-26T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    47,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v3",
                "updated": "2025-02-26T10:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    49,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "arxiv_comment": "Accepted by TMLR: https://openreview.net/forum?id=xXs2GKXPnH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v1",
                "updated": "2025-02-26T07:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v2",
                "updated": "2025-02-26T02:48:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    48,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18755v1",
                "updated": "2025-02-26T02:16:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T02:16:46Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "title": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type"
                },
                "summary": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator."
                },
                "authors": [
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Haoyan Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Renyang Guan"
                    },
                    {
                        "name": "Zhendong Hua"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.02550v3",
                "updated": "2025-02-25T13:03:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    3,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2022-03-04T19:56:56Z",
                "published_parsed": [
                    2022,
                    3,
                    4,
                    19,
                    56,
                    56,
                    4,
                    63,
                    0
                ],
                "title": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications"
                },
                "summary": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation."
                },
                "authors": [
                    {
                        "name": "Jawad Haj Yahya"
                    },
                    {
                        "name": "Haris Volos"
                    },
                    {
                        "name": "Davide B. Bartolini"
                    },
                    {
                        "name": "Georgia Antoniou"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Kleovoulos Kalaitzidis"
                    },
                    {
                        "name": "Tom Rollet"
                    },
                    {
                        "name": "Zhirui Chen"
                    },
                    {
                        "name": "Ye Geng"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Yiannakis Sazeides"
                    }
                ],
                "author_detail": {
                    "name": "Yiannakis Sazeides"
                },
                "author": "Yiannakis Sazeides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18113v1",
                "updated": "2025-02-25T11:36:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:36:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "Accelerating Graph Indexing for ANNS on Modern CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Graph Indexing for ANNS on Modern CPUs"
                },
                "summary": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance."
                },
                "authors": [
                    {
                        "name": "Mengzhao Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Wenchao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Zhou"
                },
                "author": "Wenchao Zhou",
                "arxiv_comment": "SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v2",
                "updated": "2025-02-25T09:42:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    42,
                    11,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v3",
                "updated": "2025-02-25T03:42:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    3,
                    42,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17606v1",
                "updated": "2025-02-24T19:48:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:48:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores"
                },
                "summary": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively."
                },
                "authors": [
                    {
                        "name": "Viraj Thakkar"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Kenanya Keandra Adriel Prasetyo"
                    },
                    {
                        "name": "Raden Haryosatyo Wisjnunandono"
                    },
                    {
                        "name": "Achmad Imam Kistijantoro"
                    },
                    {
                        "name": "Reza Fuad Rachmadi"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v1",
                "updated": "2025-02-24T19:34:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01418v2",
                "updated": "2025-02-24T18:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    51,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-02T16:08:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version"
                },
                "summary": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system."
                },
                "authors": [
                    {
                        "name": "Libin Zhou"
                    },
                    {
                        "name": "Lu Xing"
                    },
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid. G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid. G. Aref"
                },
                "author": "Walid. G. Aref",
                "arxiv_comment": "technical report for our main paper GTX: A Write-Optimized Latch-free\n  Graph Data System with Transactional Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17398v1",
                "updated": "2025-02-24T18:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs"
                },
                "summary": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Cyril Koenig"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v5",
                "updated": "2025-02-24T15:42:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    42,
                    59,
                    0,
                    55,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17535v1",
                "updated": "2025-02-24T15:39:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T15:39:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    15,
                    39,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM\n  Compression Preserve?"
                },
                "summary": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by reducing the computational and storage costs of LLMs, model\ncompression and KV cache compression have attracted much attention from\nresearchers. However, current methods predominantly emphasize maintaining the\nperformance of compressed LLMs, as measured by perplexity or simple accuracy on\ntasks of common sense knowledge QA and basic arithmetic reasoning. In this\nblog, we present a brief review of recent advancements in LLMs related to\nretrieval-augmented generation, multi-step reasoning, external tools, and\ncomputational expressivity, all of which substantially enhance LLM performance.\nThen, we propose a lottery LLM hypothesis suggesting that for a given LLM and\ntask, there exists a smaller lottery LLM capable of producing the same\nperformance as the original LLM with the assistance of multi-step reasoning and\nexternal tools. Based on the review of current progress in LLMs, we discuss and\nsummarize the essential capabilities that the lottery LLM and KV cache\ncompression must possess, which are currently overlooked in existing methods."
                },
                "authors": [
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v2",
                "updated": "2025-02-24T13:35:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    35,
                    18,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users and discovers that the LLM inference manifests a watershed\nlayer, after which the distribution of round-level attention shows notable\nsimilarity. We propose Round Attention, a novel round-level attention mechanism\nthat only recalls and computes the KV cache of the most relevant rounds. The\nexperiments show that our method saves 55\\% memory usage without compromising\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v1",
                "updated": "2025-02-24T13:30:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSwift: Accelerating LLM Inference for Efficient Code Generation"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness, but\nthe autoregressive decoding mechanism of Large Language Models (LLMs) leads to\npoor inference efficiency. Existing LLM inference acceleration methods mainly\nfocus on standalone functions using only built-in components. Moreover, they\ntreat code like natural language sequences, ignoring its unique syntax and\nsemantic characteristics. As a result, the effectiveness of these approaches in\ncode generation tasks remains limited and fails to align with real-world\nprogramming scenarios. To alleviate this issue, we propose CodeSwift, a simple\nyet highly efficient inference acceleration approach specifically designed for\ncode generation, without comprising the quality of the output. CodeSwift\nconstructs a multi-source datastore, providing access to both general and\nproject-specific knowledge, facilitating the retrieval of high-quality draft\nsequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval\ntiming, and enhances efficiency through parallel retrieval and a context- and\nLLM preference-aware cache. Experimental results show that CodeSwift can reach\nup to 2.53x and 2.54x speedup compared to autoregressive decoding in\nrepository-level and standalone code generation tasks, respectively,\noutperforming state-of-the-art inference acceleration approaches by up to 88%."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Borui Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v1",
                "updated": "2025-02-24T06:33:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. However, these techniques often require a\npre-defined cache budget; as the optimal budget varies with different input\nlengths and task types, it limits their practical deployment accepting\nopen-domain instructions. To address this limitation, we propose a new KV cache\ncompression objective: to always ensure the full-cache performance regardless\nof specific inputs, while maximizing KV cache pruning as much as possible. To\nachieve this goal, we introduce a novel KV cache compression method dubbed\nDBudgetKV, which features an attention-based metric to signal when the\nremaining KV cache is unlikely to match the full-cache performance, then\nhalting the pruning process. Empirical evaluation spanning diverse context\nlengths, task types, and model sizes suggests that our method achieves lossless\nKV pruning effectively and robustly, exceeding 25% compression ratio on\naverage. Furthermore, our method is easy to integrate within LLM inference, not\nonly optimizing memory space, but also showing reduced inference time compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00022v1",
                "updated": "2025-02-24T02:57:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T02:57:51Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    2,
                    57,
                    51,
                    0,
                    55,
                    0
                ],
                "title": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCrush: Key value cache size-reduction using similarity in\n  head-behaviour"
                },
                "summary": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization."
                },
                "authors": [
                    {
                        "name": "Gopi Krishna Jha"
                    },
                    {
                        "name": "Sameh Gobriel"
                    },
                    {
                        "name": "Liubov Talamanova"
                    },
                    {
                        "name": "Alexander Kozlov"
                    },
                    {
                        "name": "Nilesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Nilesh Jain"
                },
                "author": "Nilesh Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v2",
                "updated": "2025-02-24T01:28:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    1,
                    28,
                    27,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v2",
                "updated": "2025-02-23T19:48:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    19,
                    48,
                    12,
                    6,
                    54,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_doi": "10.1145/3701716.3715490",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715490",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.15605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, accepted by the Web Conference 2025 (WWW '25) as a short\n  paper",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16632v1",
                "updated": "2025-02-23T16:17:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "published": "2025-02-23T16:17:34Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    16,
                    17,
                    34,
                    6,
                    54,
                    0
                ],
                "title": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously Transmitting And Reflecting Surfaces (STARS) for\n  Multi-Functional 6G"
                },
                "summary": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously transmitting and reflecting surface (STARS) empowered\nmulti-functional 6G wireless networks are investigated. Starting with the\ncommunication functionality, various types of STARS are introduced in terms of\npower amplification capabilities, reciprocity features, and spatial density of\nelements. Then, three STARS-empowered wireless sensing architectures are\nproposed, namely STARS-aided monostatic sensing, STARS-enabled bistatic\nsensing, and sensing with target-mounted STARS, where the representative\nbenefits and application challenges are identified. Furthermore, promising\napplications of STARS for computing and caching functionalities are explored to\nimprove the computation efficiency and reduce the content delivery latency.\nFinally, recent standardization progress for reconfigurable intelligent\nsurfaces is presented for motivating the employment of STARS in\nmulti-functional 6G."
                },
                "authors": [
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "arxiv_doi": "10.1109/MNET.2024.3481293",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MNET.2024.3481293",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.16632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 figures, 8 pages, published in IEEE Network",
                "arxiv_journal_ref": "in IEEE Network, vol. 39, no. 1, pp. 47-55, Jan. 2025",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v3",
                "updated": "2025-02-23T11:52:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    11,
                    52,
                    45,
                    6,
                    54,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v4",
                "updated": "2025-02-23T03:27:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    23,
                    3,
                    27,
                    1,
                    6,
                    54,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol , thereby ensuring\nboth atomicity of data access and cache coherence with sequential consistency.\nSELCC embeds cache-ownership metadata directly into the RDMA latch word,\nenabling efficient cache ownership management via RDMA atomic operations. SELCC\ncan serve as an abstraction layer over disaggregated memory with APIs that\nresemble main-memory accesses. A concurrent B-tree and three transaction\nconcurrency control algorithms are realized using SELCC's abstraction layer.\nExperimental results show that SELCC significantly outperforms\nRemote-Procedure-Call-based protocols for cache coherence under limited remote\ncomputing power. Applications on SELCC achieve comparable or superior\nperformance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v2",
                "updated": "2025-02-22T22:32:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    22,
                    32,
                    8,
                    5,
                    53,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables, more ablation data included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15197v3",
                "updated": "2025-02-22T10:31:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    22,
                    10,
                    31,
                    51,
                    5,
                    53,
                    0
                ],
                "published": "2024-05-24T04:00:04Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    4,
                    0,
                    4,
                    4,
                    145,
                    0
                ],
                "title": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warp-centric GPU meta-meshing and fast triangulation of billion-scale\n  lattice structures"
                },
                "summary": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice structures have been widely used in applications due to their\nsuperior mechanical properties. To fabricate such structures, a geometric\nprocessing step called triangulation is often employed to transform them into\nthe STL format before sending them to 3D printers. Because lattice structures\ntend to have high geometric complexity, this step usually generates a large\namount of triangles, a memory and compute-intensive task. This problem\nmanifests itself clearly through large-scale lattice structures that have\nmillions or billions of struts. To address this problem, this paper proposes to\ntransform a lattice structure into an intermediate model called meta-mesh\nbefore undergoing real triangulation. Compared to triangular meshes,\nmeta-meshes are very lightweight and much less compute-demanding. The meta-mesh\ncan also work as a base mesh reusable for conveniently and efficiently\ntriangulating lattice structures with arbitrary resolutions. A CPU+GPU\nasynchronous meta-meshing pipeline has been developed to efficiently generate\nmeta-meshes from lattice structures. It shifts from the thread-centric GPU\nalgorithm design paradigm commonly used in CAD to the recent warp-centric\ndesign paradigm to achieve high performance. This is achieved by a new data\ncompression method, a GPU cache-aware data structure, and a workload-balanced\nscheduling method that can significantly reduce memory divergence and branch\ndivergence. Experimenting with various billion-scale lattice structures, the\nproposed method is seen to be two orders of magnitude faster than previously\nachievable."
                },
                "authors": [
                    {
                        "name": "Qiang Zou"
                    },
                    {
                        "name": "Yunzhu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunzhu Gao"
                },
                "author": "Yunzhu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v1",
                "updated": "2025-02-21T23:34:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we propose a new\nstrategy to eliminate such inefficiency, where the KV cache of each document is\nprecomputed independently. During inference, the KV caches of retrieved\ndocuments are concatenated, allowing the model to reuse cached representations\ninstead of recomputing them. To mitigate the performance degradation of LLMs\nwhen using KV caches computed independently for each document, KVLink\nintroduces three key components: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments, and applying mixed-data fine-tuning to enhance performance while\npreserving the model's original capabilities. Experiments across 7 datasets\ndemonstrate that KVLink improves question answering accuracy by an average of\n4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV\ncaches, our approach reduces time-to-first-token by up to 90% compared to\nstandard LLM inference, making it a scalable and efficient solution for context\nreuse."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15955v1",
                "updated": "2025-02-21T21:37:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T21:37:52Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    21,
                    37,
                    52,
                    4,
                    52,
                    0
                ],
                "title": "Compression Barriers for Autoregressive Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression Barriers for Autoregressive Transformers"
                },
                "summary": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key limitation of autoregressive Transformers is the large memory needed at\ninference-time to cache all previous key-value (KV) embeddings. Prior works\naddress this by compressing the KV cache, but often assume specific structural\nproperties of the embeddings. This raises the following natural question: Can\ntruly sublinear space utilization be achieved without such assumptions? In this\nwork, we answer this question in the negative. Any algorithm for\nattention-based token generation must use $\\Theta(nd)$ space, where $n$ is the\nnumber of tokens generated so far and $d = \\Omega(\\log n)$ is the dimension of\nthe KV embeddings. Our proof involves a reduction from a classic communication\ncomplexity problem and uses a randomized construction that leverages properties\nof projections in the spirit of the Johnson-Linderstrauss lemma. For the\nlow-dimensional regime $d = o(\\log n)$, we show that any algorithm requires\n$\\Omega(d\\cdot e^d)$ space and prove, using tight bounds on covering numbers,\nthat SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this\nbound. Further, we investigate how sparsity assumptions enable token generation\nin truly sublinear space, presenting impossibility results and proposing a new\nKV cache compression algorithm for sliding window attention when the value\ncache outside the window is unmasked. Finally, we analyze token generation's\ntime complexity, using an indistinguishability argument to prove that no\nnon-adaptive algorithm can compute attention online in sublinear time for all\ntokens."
                },
                "authors": [
                    {
                        "name": "Themistoklis Haris"
                    },
                    {
                        "name": "Krzysztof Onak"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof Onak"
                },
                "author": "Krzysztof Onak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14488v2",
                "updated": "2025-02-21T13:35:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    13,
                    35,
                    43,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-20T12:09:34Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    9,
                    34,
                    3,
                    51,
                    0
                ],
                "title": "U-index: A Universal Indexing Framework for Matching Long Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-index: A Universal Indexing Framework for Matching Long Patterns"
                },
                "summary": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text indexing is a fundamental and well-studied problem. Classic solutions\neither replace the original text with a compressed representation, e.g., the\nFM-index and its variants, or keep it uncompressed but attach some redundancy -\nan index - to accelerate matching. The former solutions thus retain excellent\ncompressed space, but areslow in practice. The latter approaches, like the\nsuffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra\nspace on top of the original text, provided that the query patterns are\nsufficiently long. More specifically, we develop a new indexing paradigm in\nwhich a sketch of a query pattern is first matched against a sketch of the\ntext. Once candidate matches are retrieved, they are verified using the\noriginal text. This paradigm is thus universal in the sense that it allows us\nto use any solution to index the sketched text, like a suffix array, FM-index,\nor r-index.\n  We explore both the theory and the practice of this universal framework. With\nan extensive experimental analysis, we show that, surprisingly, universal\nindexes can be constructed much faster than their unsketched counterparts and\ntake a fraction of the space, as a direct consequence of (i) having a lower\nbound on the length of patterns and (ii) working in sketch space. Furthermore,\nthese data structures have the potential of retaining or even improving query\ntime, because matching against the sketched text is faster and verifying\ncandidates can be theoretically done in constant time per occurrence (or, in\npractice, by short and cache-friendly scans of the text). Finally, we discuss\nsome important applications of this novel indexing paradigm to computational\nbiology. We hypothesize that such indexes will be particularly effective when\nthe queries are sufficiently long, and so demonstrate applications in long-read\nmapping."
                },
                "authors": [
                    {
                        "name": "Lorraine A. K. Ayad"
                    },
                    {
                        "name": "Gabriele Fici"
                    },
                    {
                        "name": "Ragnar Groot Koerkamp"
                    },
                    {
                        "name": "Grigorios Loukides"
                    },
                    {
                        "name": "Rob Patro"
                    },
                    {
                        "name": "Giulio Ermanno Pibiri"
                    },
                    {
                        "name": "Solon P. Pissis"
                    }
                ],
                "author_detail": {
                    "name": "Solon P. Pissis"
                },
                "author": "Solon P. Pissis",
                "arxiv_comment": "18 pages, 6 figures, code available at\n  https://github.com/u-index/u-index-rs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17501v1",
                "updated": "2025-02-21T12:03:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T12:03:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    3,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "CoKV: Optimizing KV Cache Allocation via Cooperative Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoKV: Optimizing KV Cache Allocation via Cooperative Game"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success on various\naspects of human life. However, one of the major challenges in deploying these\nmodels is the substantial memory consumption required to store key-value pairs\n(KV), which imposes significant resource demands. Recent research has focused\non KV cache budget allocation, with several approaches proposing head-level\nbudget distribution by evaluating the importance of individual attention heads.\nThese methods, however, assess the importance of heads independently,\noverlooking their cooperative contributions within the model, which may result\nin a deviation from their true impact on model performance. In light of this\nlimitation, we propose CoKV, a novel method that models the cooperation between\nheads in model inference as a cooperative game. By evaluating the contribution\nof each head within the cooperative game, CoKV can allocate the cache budget\nmore effectively. Extensive experiments show that CoKV achieves\nstate-of-the-art performance on the LongBench benchmark using\nLLama-3-8B-Instruct and Mistral-7B models."
                },
                "authors": [
                    {
                        "name": "Qiheng Sun"
                    },
                    {
                        "name": "Hongwei Zhang"
                    },
                    {
                        "name": "Haocheng Xia"
                    },
                    {
                        "name": "Jiayao Zhang"
                    },
                    {
                        "name": "Jinfei Liu"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15304v1",
                "updated": "2025-02-21T08:55:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T08:55:21Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    55,
                    21,
                    4,
                    52,
                    0
                ],
                "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention"
                },
                "summary": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
                },
                "authors": [
                    {
                        "name": "Hong Yankun"
                    },
                    {
                        "name": "Li Xing"
                    },
                    {
                        "name": "Zhen Hui-Ling"
                    },
                    {
                        "name": "Yu Xianzhi"
                    },
                    {
                        "name": "Liu Wulong"
                    },
                    {
                        "name": "Yuan Mingxuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Mingxuan"
                },
                "author": "Yuan Mingxuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v1",
                "updated": "2025-02-21T04:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAAP: Spatial awareness and Association based Prefetching of Virtual\n  Objects in Augmented Reality at the Edge"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SAAP, a Spatial Awareness and\nAssociation-based Prefetching policy specifically designed for MAR Caches. SAAP\nintelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SAAP significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3\\% to 40\\% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SAAP parameters\nto achieve optimal performance. Our findings demonstrate the potential of SAAP\nto substantially enhance the user experience in MAR applications by ensuring\nthe timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v2",
                "updated": "2025-02-20T23:28:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    23,
                    28,
                    1,
                    3,
                    51,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v1",
                "updated": "2025-02-20T22:24:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "More for Keys, Less for Values: Adaptive KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More for Keys, Less for Values: Adaptive KV Cache Quantization"
                },
                "summary": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an information-aware quantization framework that\nadaptively compresses the key-value (KV) cache in large language models (LLMs).\nAlthough prior work has underscored the distinct roles of key and value cache\nduring inference, our systematic analysis -- examining singular value\ndistributions, spectral norms, and Frobenius norms -- reveals, for the first\ntime, that key matrices consistently exhibit higher norm values and are more\nsensitive to quantization than value matrices. Furthermore, our theoretical\nanalysis shows that matrices with higher spectral norms amplify quantization\nerrors more significantly. Motivated by these insights, we propose a\nmixed-precision quantization strategy, KV-AdaQuant, which allocates more\nbit-width for keys and fewer for values since key matrices have higher norm\nvalues. With the same total KV bit budget, this approach effectively mitigates\nerror propagation across transformer layers while achieving significant memory\nsavings. Our extensive experiments on multiple LLMs (1B--70B) demonstrate that\nour mixed-precision quantization scheme maintains high model accuracy even\nunder aggressive compression. For instance, using 4-bit for Key and 2-bit for\nValue achieves an accuracy of 75.2%, whereas reversing the assignment (2-bit\nfor Key and 4-bit for Value) yields only 54.7% accuracy. The code is available\nat https://tinyurl.com/kv-adaquant"
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Lam Nguyen"
                    },
                    {
                        "name": "Sixu Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v1",
                "updated": "2025-02-20T18:59:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences, yet efficiently serving these long-context models remains\nchallenging due to the quadratic computational complexity of attention in the\nprefilling stage and the large memory footprint of the KV cache in the decoding\nstage. To address these issues, we introduce LServe, an efficient system that\naccelerates long-sequence LLM serving via hybrid sparse attention. This method\nunifies different hardware-friendly, structured sparsity patterns for both\nprefilling and decoding attention into a single framework, where computations\non less important tokens are skipped block-wise. LServe demonstrates the\ncompatibility of static and dynamic sparsity in long-context LLM attention.\nThis design enables multiplicative speedups by combining these optimizations.\nSpecifically, we convert half of the attention heads to nearly free streaming\nheads in both the prefilling and decoding stages. Additionally, we find that\nonly a constant number of KV pages is required to preserve long-context\ncapabilities, irrespective of context length. We then design a hierarchical KV\npage selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9x and\ndecoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is\nreleased at https://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v1",
                "updated": "2025-02-20T18:50:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14938v1",
                "updated": "2025-02-20T14:01:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T14:01:17Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    14,
                    1,
                    17,
                    3,
                    51,
                    0
                ],
                "title": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-Cache: A GS-Cache Inference Framework for Large-scale Gaussian\n  Splatting Models"
                },
                "summary": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering large-scale 3D Gaussian Splatting (3DGS) model faces significant\nchallenges in achieving real-time, high-fidelity performance on consumer-grade\ndevices. Fully realizing the potential of 3DGS in applications such as virtual\nreality (VR) requires addressing critical system-level challenges to support\nreal-time, immersive experiences. We propose GS-Cache, an end-to-end framework\nthat seamlessly integrates 3DGS's advanced representation with a highly\noptimized rendering system. GS-Cache introduces a cache-centric pipeline to\neliminate redundant computations, an efficiency-aware scheduler for elastic\nmulti-GPU rendering, and optimized CUDA kernels to overcome computational\nbottlenecks. This synergy between 3DGS and system design enables GS-Cache to\nachieve up to 5.35x performance improvement, 35% latency reduction, and 42%\nlower GPU memory usage, supporting 2K binocular rendering at over 120 FPS with\nhigh visual quality. By bridging the gap between 3DGS's representation power\nand the demands of VR systems, GS-Cache establishes a scalable and efficient\nframework for real-time neural rendering in immersive environments."
                },
                "authors": [
                    {
                        "name": "Miao Tao"
                    },
                    {
                        "name": "Yuanzhen Zhou"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Zeyu He"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yuchang Zhang"
                    },
                    {
                        "name": "Zhongling Su"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Zhenxiang Ma"
                    },
                    {
                        "name": "Rong Fu"
                    },
                    {
                        "name": "Hengjie Li"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14504v1",
                "updated": "2025-02-20T12:31:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T12:31:31Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    31,
                    31,
                    3,
                    51,
                    0
                ],
                "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large\n  Vision-Language Models"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Chenran Huang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Xiaoping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoping Zhang"
                },
                "author": "Xiaoping Zhang",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v2",
                "updated": "2025-02-20T12:14:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    12,
                    14,
                    49,
                    3,
                    51,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v2",
                "updated": "2025-02-20T09:03:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    3,
                    5,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14347v1",
                "updated": "2025-02-20T08:00:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T08:00:25Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    8,
                    0,
                    25,
                    3,
                    51,
                    0
                ],
                "title": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovery of a new phase in thin flakes of KV$_{3}$Sb$_{5}$ under\n  pressure"
                },
                "summary": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report results of magnetotransport measurements on KV$_3$Sb$_5$ thin\nflakes under pressure. Our zero-field electrical resistance reveals an\nadditional anomaly emerging under pressure ($p$), marking a previously\nunidentified phase boundary $T^{\\rm \\ast}$($p$). Together with the established\n$T_{\\rm CDW}(p)$ and $T_c(p)$, denoting the charge-density-wave transition and\na superconducting transition, respectively, the temperature-pressure phase\ndiagram of KV$_3$Sb$_5$ features a rich interplay among multiple phases. The\nHall coefficient evolves reasonably smoothly when crossing the $T^{\\rm \\ast}$\nphase boundary compared with the variation when crossing $T_{\\rm CDW}$,\nindicating the preservation of the pristine electronic structure. The mobility\nspectrum analysis provides further insights into distinguishing different\nphases. Finally, our high-pressure quantum oscillation studies up to 31 T\ncombined with density functional theory calculations further demonstrate that\nthe new phase does not reconstruct the Fermi surface, confirming that the\ntranslational symmetry of the pristine metallic state is preserved."
                },
                "authors": [
                    {
                        "name": "Zheyu Wang"
                    },
                    {
                        "name": "Lingfei Wang"
                    },
                    {
                        "name": "King Yau Yip"
                    },
                    {
                        "name": "Ying Kit Tsui"
                    },
                    {
                        "name": "Tsz Fung Poon"
                    },
                    {
                        "name": "Wenyan Wang"
                    },
                    {
                        "name": "Chun Wai Tsang"
                    },
                    {
                        "name": "Shanmin Wang"
                    },
                    {
                        "name": "David Graf"
                    },
                    {
                        "name": "Alexandre Pourret"
                    },
                    {
                        "name": "Gabriel Seyfarth"
                    },
                    {
                        "name": "Georg Knebel"
                    },
                    {
                        "name": "Kwing To Lai"
                    },
                    {
                        "name": "Wing Chi Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Swee K. Goh"
                    }
                ],
                "author_detail": {
                    "name": "Swee K. Goh"
                },
                "author": "Swee K. Goh",
                "arxiv_comment": "10 pages, 5 figures. Advanced Science (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v1",
                "updated": "2025-02-20T07:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts is crucial for large language models\n(LLMs). While rotary position embeddings (RoPEs) enhance length generalization,\neffective length extrapolation remains challenging and often requires costly\nfine-tuning. In contrast, recent training-free approaches suffer from the\nattention sink phenomenon, leading to severe performance degradation. In this\npaper, we introduce ParallelComp, a novel training-free method for long-context\nextrapolation that extends LLMs' context length from 4K to 128K while\nmaintaining high throughput and preserving perplexity, and integrates\nseamlessly with Flash Attention. Our analysis offers new insights into\nattention biases in parallel attention mechanisms and provides practical\nsolutions to tackle these challenges. To mitigate the attention sink issue, we\npropose an attention calibration strategy that reduces biases, ensuring more\nstable long-range attention. Additionally, we introduce a chunk eviction\nstrategy to efficiently manage ultra-long contexts on a single A100 80GB GPU.\nTo further enhance efficiency, we propose a parallel KV cache eviction\ntechnique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x\nacceleration in the prefilling stage with negligible performance loss due to\nattention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's\nperformance on long-context tasks using an 8B model trained on 8K-length\ncontext, outperforming powerful closed-source models such as Claude-2 and\nKimi-Chat."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "We will release the code soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14307v1",
                "updated": "2025-02-20T06:42:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T06:42:03Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    42,
                    3,
                    3,
                    51,
                    0
                ],
                "title": "μRL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "μRL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning"
                },
                "summary": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach."
                },
                "authors": [
                    {
                        "name": "M. Caner Tol"
                    },
                    {
                        "name": "Kemal Derya"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v4",
                "updated": "2025-02-20T06:07:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    6,
                    7,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.\nCode is available at https://github.com/facebookresearch/SpinQuant."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14280v1",
                "updated": "2025-02-20T05:41:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T05:41:15Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    5,
                    41,
                    15,
                    3,
                    51,
                    0
                ],
                "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks."
                },
                "authors": [
                    {
                        "name": "Subhajit Chaudhury"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarathkrishna Swaminathan"
                    },
                    {
                        "name": "Georgios Kollias"
                    },
                    {
                        "name": "Elliot Nelson"
                    },
                    {
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Igor Melnyk"
                    },
                    {
                        "name": "Matthew Riemer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Riemer"
                },
                "author": "Matthew Riemer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14220v1",
                "updated": "2025-02-20T03:27:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "published": "2025-02-20T03:27:00Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    3,
                    27,
                    0,
                    3,
                    51,
                    0
                ],
                "title": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NDPage: Efficient Address Translation for Near-Data Processing\n  Architectures via Tailored Page Table"
                },
                "summary": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Data Processing (NDP) has been a promising architectural paradigm to\naddress the memory wall problem for data-intensive applications. Practical\nimplementation of NDP architectures calls for system support for better\nprogrammability, where having virtual memory (VM) is critical. Modern computing\nsystems incorporate a 4-level page table design to support address translation\nin VM. However, simply adopting an existing 4-level page table in NDP systems\ncauses significant address translation overhead because (1) NDP applications\ngenerate a lot of address translations, and (2) the limited L1 cache in NDP\nsystems cannot cover the accesses to page table entries (PTEs). We extensively\nanalyze the 4-level page table design in the NDP scenario and observe that (1)\nthe memory access to page table entries is highly irregular, thus cannot\nbenefit from the L1 cache, and (2) the last two levels of page tables are\nnearly fully occupied. Based on our observations, we propose NDPage, an\nefficient page table design tailored for NDP systems. The key mechanisms of\nNDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates\nthe memory accesses of PTEs but also prevents the pollution of PTEs in the\ncache system, and (2) a flattened page table design that merges the last two\nlevels of page tables, allowing the page table to enjoy the flexibility of a\n4KB page while reducing the number of PTE accesses. We evaluate NDPage using a\nvariety of data-intensive workloads. Our evaluation shows that in a single-core\nNDP system, NDPage improves the end-to-end performance over the\nstate-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core\nNDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Qingcai Jiang"
                    },
                    {
                        "name": "Buxin Tu"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v1",
                "updated": "2025-02-19T19:12:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v4",
                "updated": "2025-02-19T17:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v1",
                "updated": "2025-02-19T16:54:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v4",
                "updated": "2025-02-19T10:39:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    39,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ToCa is honored to be accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v1",
                "updated": "2025-02-19T09:30:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13542v1",
                "updated": "2025-02-19T08:50:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:50:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15804v1",
                "updated": "2025-02-19T06:14:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T06:14:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference"
                },
                "summary": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance."
                },
                "authors": [
                    {
                        "name": "Bingzhe Zhao"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Lian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lian Yu"
                },
                "author": "Lian Yu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v1",
                "updated": "2025-02-18T17:08:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12875v1",
                "updated": "2025-02-18T14:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "title": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations"
                },
                "summary": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Shaoxin Cui"
                    },
                    {
                        "name": "Wen Qiu"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Bomin Mao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v1",
                "updated": "2025-02-18T09:11:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v2",
                "updated": "2025-02-18T07:58:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    58,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective"
                },
                "summary": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12574v1",
                "updated": "2025-02-18T06:26:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T06:26:05Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2209.01235v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.01235v4",
                "updated": "2025-03-04T18:59:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    59,
                    37,
                    1,
                    63,
                    0
                ],
                "published": "2022-09-02T18:46:56Z",
                "published_parsed": [
                    2022,
                    9,
                    2,
                    18,
                    46,
                    56,
                    4,
                    245,
                    0
                ],
                "title": "Smiles in Profiles: Improving Efficiency While Reducing Disparities in\n  Online Marketplaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smiles in Profiles: Improving Efficiency While Reducing Disparities in\n  Online Marketplaces"
                },
                "summary": "Online platforms often have conflicting goals: they face tradeoffs between\nincreasing efficiency and reducing disparities, where the latter may relate to\nobjectives such as the longer-term health of the marketplace or the\norganization's mission. We examine how participants' profile pictures shape\nthis trade-off in the context of a peer-to-peer lending platform. We develop\nand apply an approach to estimate marketplace participants' preferences for\ndifferent profile features, distinguishing between (i) \"type\" (e.g., gender,\nage) and (ii) \"style\" (e.g., smiling in the photo). Relative to type, style\nfeatures are easier to change, and platforms may be more willing to encourage\nsuch changes. Our approach starts by using causal inference methods together\nwith computer vision algorithms applied to observational data to identify type\nand style features of profiles that appear to affect demand for transactions.\nWe further decompose type-based disparities into a component driven by demand\nfor certain types and a component that arises because different types have\ndifferent distributions of style features; we find that style differences\nexacerbate type-based disparities. To improve internal validity, we then carry\nout two randomized survey experiments using generative models to create\nmultiple versions of profile images that differ in one feature at a time. We\nthen evaluate counterfactual platform policies based on the changeable profile\nfeatures and identify approaches that can ameliorate the disparity-efficiency\ntension. We identify marketplace feedback effects, where encouraging certain\nstyle choices attracts participants who value these choices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online platforms often have conflicting goals: they face tradeoffs between\nincreasing efficiency and reducing disparities, where the latter may relate to\nobjectives such as the longer-term health of the marketplace or the\norganization's mission. We examine how participants' profile pictures shape\nthis trade-off in the context of a peer-to-peer lending platform. We develop\nand apply an approach to estimate marketplace participants' preferences for\ndifferent profile features, distinguishing between (i) \"type\" (e.g., gender,\nage) and (ii) \"style\" (e.g., smiling in the photo). Relative to type, style\nfeatures are easier to change, and platforms may be more willing to encourage\nsuch changes. Our approach starts by using causal inference methods together\nwith computer vision algorithms applied to observational data to identify type\nand style features of profiles that appear to affect demand for transactions.\nWe further decompose type-based disparities into a component driven by demand\nfor certain types and a component that arises because different types have\ndifferent distributions of style features; we find that style differences\nexacerbate type-based disparities. To improve internal validity, we then carry\nout two randomized survey experiments using generative models to create\nmultiple versions of profile images that differ in one feature at a time. We\nthen evaluate counterfactual platform policies based on the changeable profile\nfeatures and identify approaches that can ameliorate the disparity-efficiency\ntension. We identify marketplace feedback effects, where encouraging certain\nstyle choices attracts participants who value these choices."
                },
                "authors": [
                    {
                        "name": "Susan Athey"
                    },
                    {
                        "name": "Dean Karlan"
                    },
                    {
                        "name": "Emil Palikot"
                    },
                    {
                        "name": "Yuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Yuan"
                },
                "author": "Yuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.01235v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.01235v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06603v2",
                "updated": "2025-03-04T18:59:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    59,
                    26,
                    1,
                    63,
                    0
                ],
                "published": "2024-12-09T15:53:00Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    53,
                    0,
                    0,
                    344,
                    0
                ],
                "title": "Examining the Use and Impact of an AI Code Assistant on Developer\n  Productivity and Experience in the Enterprise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Use and Impact of an AI Code Assistant on Developer\n  Productivity and Experience in the Enterprise"
                },
                "summary": "AI assistants are being created to help software engineers conduct a variety\nof coding-related tasks, such as writing, documenting, and testing code. We\ndescribe the use of the watsonx Code Assistant (WCA), an LLM-powered coding\nassistant deployed internally within IBM. Through surveys of two user cohorts\n(N=669) and unmoderated usability testing (N=15), we examined developers'\nexperiences with WCA and its impact on their productivity. We learned about\ntheir motivations for using (or not using) WCA, we examined their expectations\nof its speed and quality, and we identified new considerations regarding\nownership of and responsibility for generated code. Our case study\ncharacterizes the impact of an LLM-powered assistant on developers' perceptions\nof productivity and it shows that although such tools do often provide net\nproductivity increases, these benefits may not always be experienced by all\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI assistants are being created to help software engineers conduct a variety\nof coding-related tasks, such as writing, documenting, and testing code. We\ndescribe the use of the watsonx Code Assistant (WCA), an LLM-powered coding\nassistant deployed internally within IBM. Through surveys of two user cohorts\n(N=669) and unmoderated usability testing (N=15), we examined developers'\nexperiences with WCA and its impact on their productivity. We learned about\ntheir motivations for using (or not using) WCA, we examined their expectations\nof its speed and quality, and we identified new considerations regarding\nownership of and responsibility for generated code. Our case study\ncharacterizes the impact of an LLM-powered assistant on developers' perceptions\nof productivity and it shows that although such tools do often provide net\nproductivity increases, these benefits may not always be experienced by all\nusers."
                },
                "authors": [
                    {
                        "name": "Justin D. Weisz"
                    },
                    {
                        "name": "Shraddha Kumar"
                    },
                    {
                        "name": "Michael Muller"
                    },
                    {
                        "name": "Karen-Ellen Browne"
                    },
                    {
                        "name": "Arielle Goldberg"
                    },
                    {
                        "name": "Ellice Heintze"
                    },
                    {
                        "name": "Shagun Bajpai"
                    }
                ],
                "author_detail": {
                    "name": "Shagun Bajpai"
                },
                "author": "Shagun Bajpai",
                "arxiv_comment": "21 pages, 3 figures. CHI EA '25, April 26-May 01, 2025, Yokohama,\n  Japan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02879v1",
                "updated": "2025-03-04T18:58:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    58,
                    13,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:58:13Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    58,
                    13,
                    1,
                    63,
                    0
                ],
                "title": "Wikipedia in the Era of LLMs: Evolution and Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wikipedia in the Era of LLMs: Evolution and Risks"
                },
                "summary": "In this paper, we present a thorough analysis of the impact of Large Language\nModels (LLMs) on Wikipedia, examining the evolution of Wikipedia through\nexisting data and using simulations to explore potential risks. We begin by\nanalyzing page views and article content to study Wikipedia's recent changes\nand assess the impact of LLMs. Subsequently, we evaluate how LLMs affect\nvarious Natural Language Processing (NLP) tasks related to Wikipedia, including\nmachine translation and retrieval-augmented generation (RAG). Our findings and\nsimulation results reveal that Wikipedia articles have been influenced by LLMs,\nwith an impact of approximately 1%-2% in certain categories. If the machine\ntranslation benchmark based on Wikipedia is influenced by LLMs, the scores of\nthe models may become inflated, and the comparative results among models might\nshift as well. Moreover, the effectiveness of RAG might decrease if the\nknowledge base becomes polluted by LLM-generated content. While LLMs have not\nyet fully changed Wikipedia's language and knowledge structures, we believe\nthat our empirical findings signal the need for careful consideration of\npotential future risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a thorough analysis of the impact of Large Language\nModels (LLMs) on Wikipedia, examining the evolution of Wikipedia through\nexisting data and using simulations to explore potential risks. We begin by\nanalyzing page views and article content to study Wikipedia's recent changes\nand assess the impact of LLMs. Subsequently, we evaluate how LLMs affect\nvarious Natural Language Processing (NLP) tasks related to Wikipedia, including\nmachine translation and retrieval-augmented generation (RAG). Our findings and\nsimulation results reveal that Wikipedia articles have been influenced by LLMs,\nwith an impact of approximately 1%-2% in certain categories. If the machine\ntranslation benchmark based on Wikipedia is influenced by LLMs, the scores of\nthe models may become inflated, and the comparative results among models might\nshift as well. Moreover, the effectiveness of RAG might decrease if the\nknowledge base becomes polluted by LLM-generated content. While LLMs have not\nyet fully changed Wikipedia's language and knowledge structures, we believe\nthat our empirical findings signal the need for careful consideration of\npotential future risks."
                },
                "authors": [
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Yuliang Xu"
                    },
                    {
                        "name": "Mingmeng Geng"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Dongping Chen"
                    }
                ],
                "author_detail": {
                    "name": "Dongping Chen"
                },
                "author": "Dongping Chen",
                "arxiv_comment": "We release all the experimental dataset and source code at:\n  https://github.com/HSM316/LLM_Wikipedia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02878v1",
                "updated": "2025-03-04T18:58:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    58,
                    11,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:58:11Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    58,
                    11,
                    1,
                    63,
                    0
                ],
                "title": "Language Models can Self-Improve at State-Value Estimation for Better\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models can Self-Improve at State-Value Estimation for Better\n  Search"
                },
                "summary": "Collecting ground truth task completion rewards or human demonstrations for\nmulti-step reasoning tasks is often cost-prohibitive and time-consuming,\nespecially in interactive domains like web tasks. To address this bottleneck,\nwe present self-taught lookahead, a self-supervised method that leverages\nstate-transition dynamics to train a value model capable of effectively guiding\nlanguage model-controlled search. We find that moderately sized (8 billion\nparameters) open-weight value models improved with self-taught lookahead can\nmatch the performance of using a frontier LLM such as gpt-4o as the value\nmodel. Furthermore, we find that self-taught lookahead improves performance by\n20% while reducing costs 37x compared to previous LLM-based tree search,\nwithout relying on ground truth rewards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collecting ground truth task completion rewards or human demonstrations for\nmulti-step reasoning tasks is often cost-prohibitive and time-consuming,\nespecially in interactive domains like web tasks. To address this bottleneck,\nwe present self-taught lookahead, a self-supervised method that leverages\nstate-transition dynamics to train a value model capable of effectively guiding\nlanguage model-controlled search. We find that moderately sized (8 billion\nparameters) open-weight value models improved with self-taught lookahead can\nmatch the performance of using a frontier LLM such as gpt-4o as the value\nmodel. Furthermore, we find that self-taught lookahead improves performance by\n20% while reducing costs 37x compared to previous LLM-based tree search,\nwithout relying on ground truth rewards."
                },
                "authors": [
                    {
                        "name": "Ethan Mendes"
                    },
                    {
                        "name": "Alan Ritter"
                    }
                ],
                "author_detail": {
                    "name": "Alan Ritter"
                },
                "author": "Alan Ritter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02875v1",
                "updated": "2025-03-04T18:56:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    56,
                    3,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:56:03Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    56,
                    3,
                    1,
                    63,
                    0
                ],
                "title": "The First Few Tokens Are All You Need: An Efficient and Effective\n  Unsupervised Prefix Fine-Tuning Method for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The First Few Tokens Are All You Need: An Efficient and Effective\n  Unsupervised Prefix Fine-Tuning Method for Reasoning Models"
                },
                "summary": "Improving the reasoning capabilities of large language models (LLMs)\ntypically requires supervised fine-tuning with labeled data or computationally\nexpensive sampling. We introduce Unsupervised Prefix Fine-Tuning (UPFT), which\nleverages the observation of Prefix Self-Consistency -- the shared initial\nreasoning steps across diverse solution trajectories -- to enhance LLM\nreasoning efficiency. By training exclusively on the initial prefix substrings\n(as few as 8 tokens), UPFT removes the need for labeled data or exhaustive\nsampling. Experiments on reasoning benchmarks show that UPFT matches the\nperformance of supervised methods such as Rejection Sampling Fine-Tuning, while\nreducing training time by 75% and sampling cost by 99%. Further analysis\nreveals that errors tend to appear in later stages of the reasoning process and\nthat prefix-based training preserves the model's structural knowledge. This\nwork demonstrates how minimal unsupervised fine-tuning can unlock substantial\nreasoning gains in LLMs, offering a scalable and resource-efficient alternative\nto conventional approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the reasoning capabilities of large language models (LLMs)\ntypically requires supervised fine-tuning with labeled data or computationally\nexpensive sampling. We introduce Unsupervised Prefix Fine-Tuning (UPFT), which\nleverages the observation of Prefix Self-Consistency -- the shared initial\nreasoning steps across diverse solution trajectories -- to enhance LLM\nreasoning efficiency. By training exclusively on the initial prefix substrings\n(as few as 8 tokens), UPFT removes the need for labeled data or exhaustive\nsampling. Experiments on reasoning benchmarks show that UPFT matches the\nperformance of supervised methods such as Rejection Sampling Fine-Tuning, while\nreducing training time by 75% and sampling cost by 99%. Further analysis\nreveals that errors tend to appear in later stages of the reasoning process and\nthat prefix-based training preserves the model's structural knowledge. This\nwork demonstrates how minimal unsupervised fine-tuning can unlock substantial\nreasoning gains in LLMs, offering a scalable and resource-efficient alternative\nto conventional approaches."
                },
                "authors": [
                    {
                        "name": "Ke Ji"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Xiaoyuan Liu"
                    },
                    {
                        "name": "Zhijie Wang"
                    },
                    {
                        "name": "Junying Chen"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_doi": "10.13140/RG.2.2.33772.07043",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.33772.07043",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14509v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14509v5",
                "updated": "2025-03-04T18:55:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    55,
                    27,
                    1,
                    63,
                    0
                ],
                "published": "2024-09-22T16:13:00Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    13,
                    0,
                    6,
                    266,
                    0
                ],
                "title": "Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving\n  Human-AI Alignment in the Writing Process through Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving\n  Human-AI Alignment in the Writing Process through Edits"
                },
                "summary": "LLM-based applications are helping people write, and LLM-generated text is\nmaking its way into social media, journalism, and our classrooms. However, the\ndifferences between LLM-generated and human written text remain unclear. To\nexplore this, we hired professional writers to edit paragraphs in several\ncreative domains. We first found these writers agree on undesirable\nidiosyncrasies in LLM generated text, formalizing it into a seven-category\ntaxonomy (e.g. clich\\'es, unnecessary exposition). Second, we curated the LAMP\ncorpus: 1,057 LLM-generated paragraphs edited by professional writers according\nto our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our\nstudy (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms\nof writing quality, revealing common limitations across model families. Third,\nbuilding on existing work in automatic editing we evaluated methods to improve\nLLM-generated text. A large-scale preference annotation confirms that although\nexperts largely prefer text edited by other experts, automatic editing methods\nshow promise in improving alignment between LLM-generated and human-written\ntext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based applications are helping people write, and LLM-generated text is\nmaking its way into social media, journalism, and our classrooms. However, the\ndifferences between LLM-generated and human written text remain unclear. To\nexplore this, we hired professional writers to edit paragraphs in several\ncreative domains. We first found these writers agree on undesirable\nidiosyncrasies in LLM generated text, formalizing it into a seven-category\ntaxonomy (e.g. clich\\'es, unnecessary exposition). Second, we curated the LAMP\ncorpus: 1,057 LLM-generated paragraphs edited by professional writers according\nto our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our\nstudy (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms\nof writing quality, revealing common limitations across model families. Third,\nbuilding on existing work in automatic editing we evaluated methods to improve\nLLM-generated text. A large-scale preference annotation confirms that although\nexperts largely prefer text edited by other experts, automatic editing methods\nshow promise in improving alignment between LLM-generated and human-written\ntext."
                },
                "authors": [
                    {
                        "name": "Tuhin Chakrabarty"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "arxiv_comment": "ACM CHI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14509v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14509v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02866v1",
                "updated": "2025-03-04T18:45:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    45,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:45:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    45,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "Optimal Power Management for Large-Scale Battery Energy Storage Systems\n  via Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Management for Large-Scale Battery Energy Storage Systems\n  via Bayesian Inference"
                },
                "summary": "Large-scale battery energy storage systems (BESS) have found ever-increasing\nuse across industry and society to accelerate clean energy transition and\nimprove energy supply reliability and resilience. However, their optimal power\nmanagement poses significant challenges: the underlying high-dimensional\nnonlinear nonconvex optimization lacks computational tractability in real-world\nimplementation, and the uncertainty of the exogenous power demand makes exact\noptimization difficult. This paper presents a new solution framework to address\nthese bottlenecks. The solution pivots on introducing power-sharing ratios to\nspecify each cell's power quota from the output power demand. To find the\noptimal power-sharing ratios, we formulate a nonlinear model predictive control\n(NMPC) problem to achieve power-loss-minimizing BESS operation while complying\nwith safety, cell balancing, and power supply-demand constraints. We then\npropose a parameterized control policy for the power-sharing ratios, which\nutilizes only three parameters, to reduce the computational demand in solving\nthe NMPC problem. This policy parameterization allows us to translate the NMPC\nproblem into a Bayesian inference problem for the sake of 1) computational\ntractability, and 2) overcoming the nonconvexity of the optimization problem.\nWe leverage the ensemble Kalman inversion technique to solve the parameter\nestimation problem. Concurrently, a low-level control loop is developed to\nseamlessly integrate our proposed approach with the BESS to ensure practical\nimplementation. This low-level controller receives the optimal power-sharing\nratios, generates output power references for the cells, and maintains a\nbalance between power supply and demand despite uncertainty in output power. We\nconduct extensive simulations and experiments on a 20-cell prototype to\nvalidate the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale battery energy storage systems (BESS) have found ever-increasing\nuse across industry and society to accelerate clean energy transition and\nimprove energy supply reliability and resilience. However, their optimal power\nmanagement poses significant challenges: the underlying high-dimensional\nnonlinear nonconvex optimization lacks computational tractability in real-world\nimplementation, and the uncertainty of the exogenous power demand makes exact\noptimization difficult. This paper presents a new solution framework to address\nthese bottlenecks. The solution pivots on introducing power-sharing ratios to\nspecify each cell's power quota from the output power demand. To find the\noptimal power-sharing ratios, we formulate a nonlinear model predictive control\n(NMPC) problem to achieve power-loss-minimizing BESS operation while complying\nwith safety, cell balancing, and power supply-demand constraints. We then\npropose a parameterized control policy for the power-sharing ratios, which\nutilizes only three parameters, to reduce the computational demand in solving\nthe NMPC problem. This policy parameterization allows us to translate the NMPC\nproblem into a Bayesian inference problem for the sake of 1) computational\ntractability, and 2) overcoming the nonconvexity of the optimization problem.\nWe leverage the ensemble Kalman inversion technique to solve the parameter\nestimation problem. Concurrently, a low-level control loop is developed to\nseamlessly integrate our proposed approach with the BESS to ensure practical\nimplementation. This low-level controller receives the optimal power-sharing\nratios, generates output power references for the cells, and maintains a\nbalance between power supply and demand despite uncertainty in output power. We\nconduct extensive simulations and experiments on a 20-cell prototype to\nvalidate the proposed approach."
                },
                "authors": [
                    {
                        "name": "Amir Farakhor"
                    },
                    {
                        "name": "Iman Askari"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Yebin Wang"
                    },
                    {
                        "name": "Huazhen Fang"
                    }
                ],
                "author_detail": {
                    "name": "Huazhen Fang"
                },
                "author": "Huazhen Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02865v1",
                "updated": "2025-03-04T18:43:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    43,
                    57,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:43:57Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    43,
                    57,
                    1,
                    63,
                    0
                ],
                "title": "FairSense-AI: Responsible AI Meets Sustainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairSense-AI: Responsible AI Meets Sustainability"
                },
                "summary": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Mukund Sayeeganesh Chettiar"
                    },
                    {
                        "name": "Matin Yousefabadi"
                    },
                    {
                        "name": "Tahniat Khan"
                    },
                    {
                        "name": "Marcelo Lotif"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo Lotif"
                },
                "author": "Marcelo Lotif",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02863v1",
                "updated": "2025-03-04T18:40:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    40,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:40:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    40,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Calibrating LLM Confidence with Semantic Steering: A Multi-Prompt\n  Aggregation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating LLM Confidence with Semantic Steering: A Multi-Prompt\n  Aggregation Framework"
                },
                "summary": "Large Language Models (LLMs) often exhibit misaligned confidence scores,\nusually overestimating the reliability of their predictions. While verbalized\nconfidence in Large Language Models (LLMs) has gained attention, prior work\nremains divided on whether confidence scores can be systematically steered\nthrough prompting. Recent studies even argue that such prompt-induced\nconfidence shifts are negligible, suggesting LLMs' confidence calibration is\nrigid to linguistic interventions. Contrary to these claims, we first\nrigorously confirm the existence of directional confidence shifts by probing\nthree models (including GPT3.5, LLAMA3-70b, GPT4) across 7 benchmarks,\ndemonstrating that explicit instructions can inflate or deflate confidence\nscores in a regulated manner. Based on this observation, we propose a novel\nframework containing three components: confidence steering, steered confidence\naggregation and steered answers selection, named SteeringConf. Our method,\nSteeringConf, leverages a confidence manipulation mechanism to steer the\nconfidence scores of LLMs in several desired directions, followed by a\nsummarization module that aggregates the steered confidence scores to produce a\nfinal prediction. We evaluate our method on 7 benchmarks and it consistently\noutperforms the baselines in terms of calibration metrics in task of confidence\ncalibration and failure detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often exhibit misaligned confidence scores,\nusually overestimating the reliability of their predictions. While verbalized\nconfidence in Large Language Models (LLMs) has gained attention, prior work\nremains divided on whether confidence scores can be systematically steered\nthrough prompting. Recent studies even argue that such prompt-induced\nconfidence shifts are negligible, suggesting LLMs' confidence calibration is\nrigid to linguistic interventions. Contrary to these claims, we first\nrigorously confirm the existence of directional confidence shifts by probing\nthree models (including GPT3.5, LLAMA3-70b, GPT4) across 7 benchmarks,\ndemonstrating that explicit instructions can inflate or deflate confidence\nscores in a regulated manner. Based on this observation, we propose a novel\nframework containing three components: confidence steering, steered confidence\naggregation and steered answers selection, named SteeringConf. Our method,\nSteeringConf, leverages a confidence manipulation mechanism to steer the\nconfidence scores of LLMs in several desired directions, followed by a\nsummarization module that aggregates the steered confidence scores to produce a\nfinal prediction. We evaluate our method on 7 benchmarks and it consistently\noutperforms the baselines in terms of calibration metrics in task of confidence\ncalibration and failure detection."
                },
                "authors": [
                    {
                        "name": "Ziang Zhou"
                    },
                    {
                        "name": "Tianyuan Jin"
                    },
                    {
                        "name": "Jieming Shi"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02862v1",
                "updated": "2025-03-04T18:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    40,
                    38,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:40:38Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    40,
                    38,
                    1,
                    63,
                    0
                ],
                "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy and Accuracy-Aware AI/ML Model Deduplication"
                },
                "summary": "With the growing adoption of privacy-preserving machine learning algorithms,\nsuch as Differentially Private Stochastic Gradient Descent (DP-SGD), training\nor fine-tuning models on private datasets has become increasingly prevalent.\nThis shift has led to the need for models offering varying privacy guarantees\nand utility levels to satisfy diverse user requirements. However, managing\nnumerous versions of large models introduces significant operational\nchallenges, including increased inference latency, higher resource consumption,\nand elevated costs. Model deduplication is a technique widely used by many\nmodel serving and database systems to support high-performance and low-cost\ninference queries and model diagnosis queries. However, none of the existing\nmodel deduplication works has considered privacy, leading to unbounded\naggregation of privacy costs for certain deduplicated models and inefficiencies\nwhen applied to deduplicate DP-trained models. We formalize the problems of\ndeduplicating DP-trained models for the first time and propose a novel privacy-\nand accuracy-aware deduplication mechanism to address the problems. We\ndeveloped a greedy strategy to select and assign base models to target models\nto minimize storage and privacy costs. When deduplicating a target model, we\ndynamically schedule accuracy validations and apply the Sparse Vector Technique\nto reduce the privacy costs associated with private validation data. Compared\nto baselines that do not provide privacy guarantees, our approach improved the\ncompression ratio by up to $35\\times$ for individual models (including large\nlanguage models and vision transformers). We also observed up to $43\\times$\ninference speedup due to the reduction of I/O operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing adoption of privacy-preserving machine learning algorithms,\nsuch as Differentially Private Stochastic Gradient Descent (DP-SGD), training\nor fine-tuning models on private datasets has become increasingly prevalent.\nThis shift has led to the need for models offering varying privacy guarantees\nand utility levels to satisfy diverse user requirements. However, managing\nnumerous versions of large models introduces significant operational\nchallenges, including increased inference latency, higher resource consumption,\nand elevated costs. Model deduplication is a technique widely used by many\nmodel serving and database systems to support high-performance and low-cost\ninference queries and model diagnosis queries. However, none of the existing\nmodel deduplication works has considered privacy, leading to unbounded\naggregation of privacy costs for certain deduplicated models and inefficiencies\nwhen applied to deduplicate DP-trained models. We formalize the problems of\ndeduplicating DP-trained models for the first time and propose a novel privacy-\nand accuracy-aware deduplication mechanism to address the problems. We\ndeveloped a greedy strategy to select and assign base models to target models\nto minimize storage and privacy costs. When deduplicating a target model, we\ndynamically schedule accuracy validations and apply the Sparse Vector Technique\nto reduce the privacy costs associated with private validation data. Compared\nto baselines that do not provide privacy guarantees, our approach improved the\ncompression ratio by up to $35\\times$ for individual models (including large\nlanguage models and vision transformers). We also observed up to $43\\times$\ninference speedup due to the reduction of I/O operations."
                },
                "authors": [
                    {
                        "name": "Hong Guan"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Lixi Zhou"
                    },
                    {
                        "name": "Li Xiong"
                    },
                    {
                        "name": "Kanchan Chowdhury"
                    },
                    {
                        "name": "Lulu Xie"
                    },
                    {
                        "name": "Xusheng Xiao"
                    },
                    {
                        "name": "Jia Zou"
                    }
                ],
                "author_detail": {
                    "name": "Jia Zou"
                },
                "author": "Jia Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02851v1",
                "updated": "2025-03-04T18:27:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    27,
                    0,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:27:00Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    27,
                    0,
                    1,
                    63,
                    0
                ],
                "title": "Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs'\n  Decoding Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs'\n  Decoding Layers"
                },
                "summary": "Large language models (LLMs) are known to hallucinate, a phenomenon often\nlinked to creativity. While previous research has primarily explored this\nconnection through theoretical or qualitative lenses, our work takes a\nquantitative approach to systematically examine the relationship between\nhallucination and creativity in LLMs. Given the complex nature of creativity,\nwe propose a narrow definition tailored to LLMs and introduce an evaluation\nframework, HCL, which quantifies Hallucination and Creativity across different\nLayers of LLMs during decoding. Our empirical analysis reveals a tradeoff\nbetween hallucination and creativity that is consistent across layer depth,\nmodel type, and model size. Notably, across different model architectures, we\nidentify a specific layer at each model size that optimally balances this\ntradeoff. Additionally, the optimal layer tends to appear in the early layers\nof larger models, and the confidence of the model is also significantly higher\nat this layer. These findings provide a quantitative perspective that offers\nnew insights into the interplay between LLM creativity and hallucination. The\ncode and data for our experiments are available at\nhttps://github.com/ZicongHe2002/HCL-Spark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to hallucinate, a phenomenon often\nlinked to creativity. While previous research has primarily explored this\nconnection through theoretical or qualitative lenses, our work takes a\nquantitative approach to systematically examine the relationship between\nhallucination and creativity in LLMs. Given the complex nature of creativity,\nwe propose a narrow definition tailored to LLMs and introduce an evaluation\nframework, HCL, which quantifies Hallucination and Creativity across different\nLayers of LLMs during decoding. Our empirical analysis reveals a tradeoff\nbetween hallucination and creativity that is consistent across layer depth,\nmodel type, and model size. Notably, across different model architectures, we\nidentify a specific layer at each model size that optimally balances this\ntradeoff. Additionally, the optimal layer tends to appear in the early layers\nof larger models, and the confidence of the model is also significantly higher\nat this layer. These findings provide a quantitative perspective that offers\nnew insights into the interplay between LLM creativity and hallucination. The\ncode and data for our experiments are available at\nhttps://github.com/ZicongHe2002/HCL-Spark."
                },
                "authors": [
                    {
                        "name": "Zicong He"
                    },
                    {
                        "name": "Boxuan Zhang"
                    },
                    {
                        "name": "Lu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Cheng"
                },
                "author": "Lu Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02846v1",
                "updated": "2025-03-04T18:20:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    20,
                    24,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:20:24Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    20,
                    24,
                    1,
                    63,
                    0
                ],
                "title": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs"
                },
                "summary": "Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or\nnonsensical information) when serving as AI assistants in various domains.\nSince hallucinations always come with truthful content in the LLM responses,\nprevious factuality alignment methods that conduct response-level preference\nlearning inevitably introduced noises during training. Therefore, this paper\nproposes a fine-grained factuality alignment method based on Direct Preference\nOptimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as\nmask signals, Mask-DPO only learns from factually correct sentences in the\npreferred samples and prevents the penalty on factual contents in the not\npreferred samples, which resolves the ambiguity in the preference learning.\nExtensive experimental results demonstrate that Mask-DPO can significantly\nimprove the factuality of LLMs responses to questions from both in-domain and\nout-of-domain datasets, although these questions and their corresponding topics\nare unseen during training. Only trained on the ANAH train set, the score of\nLlama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,\neven surpassing the score of Llama3.1-70B-Instruct (53.44%), while its\nFactScore on the out-of-domain Biography dataset is also improved from 30.29%\nto 39.39%. We further study the generalization property of Mask-DPO using\ndifferent training sample scaling strategies and find that scaling the number\nof topics in the dataset is more effective than the number of questions. We\nprovide a hypothesis of what factual alignment is doing with LLMs, on the\nimplication of this phenomenon, and conduct proof-of-concept experiments to\nverify it. We hope the method and the findings pave the way for future research\non scaling factuality alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or\nnonsensical information) when serving as AI assistants in various domains.\nSince hallucinations always come with truthful content in the LLM responses,\nprevious factuality alignment methods that conduct response-level preference\nlearning inevitably introduced noises during training. Therefore, this paper\nproposes a fine-grained factuality alignment method based on Direct Preference\nOptimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as\nmask signals, Mask-DPO only learns from factually correct sentences in the\npreferred samples and prevents the penalty on factual contents in the not\npreferred samples, which resolves the ambiguity in the preference learning.\nExtensive experimental results demonstrate that Mask-DPO can significantly\nimprove the factuality of LLMs responses to questions from both in-domain and\nout-of-domain datasets, although these questions and their corresponding topics\nare unseen during training. Only trained on the ANAH train set, the score of\nLlama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,\neven surpassing the score of Llama3.1-70B-Instruct (53.44%), while its\nFactScore on the out-of-domain Biography dataset is also improved from 30.29%\nto 39.39%. We further study the generalization property of Mask-DPO using\ndifferent training sample scaling strategies and find that scaling the number\nof topics in the dataset is more effective than the number of questions. We\nprovide a hypothesis of what factual alignment is doing with LLMs, on the\nimplication of this phenomenon, and conduct proof-of-concept experiments to\nverify it. We hope the method and the findings pave the way for future research\non scaling factuality alignment."
                },
                "authors": [
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Chengqi Lyu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Accepted by ICLR 2025. Code is available at\n  https://github.com/open-compass/ANAH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16172v2",
                "updated": "2025-03-04T18:15:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    15,
                    36,
                    1,
                    63,
                    0
                ],
                "published": "2024-12-07T00:15:24Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    0,
                    15,
                    24,
                    5,
                    342,
                    0
                ],
                "title": "LABIIUM: AI-Enhanced Zero-configuration Measurement Automation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LABIIUM: AI-Enhanced Zero-configuration Measurement Automation System"
                },
                "summary": "The complexity of laboratory environments requires solutions that simplify\ninstrument interaction and enhance measurement automation. Traditional tools\noften require configuration, software, and programming skills, creating\nbarriers to productivity. Previous approaches, including dedicated software\nsuites and custom scripts, frequently fall short in providing user-friendly\nsolutions that align with programming practices. We present LABIIUM, an\nAI-enhanced, zero-configuration measurement automation system designed to\nstreamline experimental workflows and improve user productivity. LABIIUM\nintegrates an AI assistant powered by Large Language Models (LLMs) to generate\ncode. LABIIUM's Lab-Automation-Measurement Bridges (LAMBs) enable seamless\ninstrument connectivity using standard tools such as VSCode and Python,\neliminating setup overhead. To demonstrate its capabilities, we conducted\nexperiments involving the measurement of the parametric transfer curve of a\nsimple two-transistor inverting amplifier with a current source load. The AI\nassistant was evaluated using different prompt scenarios and compared with\nmultiple models, including Claude Sonnet 3.5, Gemini Pro 1.5, and GPT-4o. An\nexpert solution implementing the Gradient-Weighted Adaptive Stochastic Sampling\n(GWASS) method was used as a baseline. The solutions generated by the AI\nassistant were compared with the expert solution and a uniform linear sweep\nbaseline with 10,000 points. The graph results show that the LLMs were able to\nsuccessfully complete the most basic uniform sweep, but LLMs were unable to\ndevelop adaptive sweeping algorithms to compete with GWASS. The evaluation\nunderscores LABIIUM's ability to enhance laboratory productivity and support\ndigital transformation in research and industry, and emphasizes the future work\nrequired to improve LLM performance in Electronic Measurement Science Tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The complexity of laboratory environments requires solutions that simplify\ninstrument interaction and enhance measurement automation. Traditional tools\noften require configuration, software, and programming skills, creating\nbarriers to productivity. Previous approaches, including dedicated software\nsuites and custom scripts, frequently fall short in providing user-friendly\nsolutions that align with programming practices. We present LABIIUM, an\nAI-enhanced, zero-configuration measurement automation system designed to\nstreamline experimental workflows and improve user productivity. LABIIUM\nintegrates an AI assistant powered by Large Language Models (LLMs) to generate\ncode. LABIIUM's Lab-Automation-Measurement Bridges (LAMBs) enable seamless\ninstrument connectivity using standard tools such as VSCode and Python,\neliminating setup overhead. To demonstrate its capabilities, we conducted\nexperiments involving the measurement of the parametric transfer curve of a\nsimple two-transistor inverting amplifier with a current source load. The AI\nassistant was evaluated using different prompt scenarios and compared with\nmultiple models, including Claude Sonnet 3.5, Gemini Pro 1.5, and GPT-4o. An\nexpert solution implementing the Gradient-Weighted Adaptive Stochastic Sampling\n(GWASS) method was used as a baseline. The solutions generated by the AI\nassistant were compared with the expert solution and a uniform linear sweep\nbaseline with 10,000 points. The graph results show that the LLMs were able to\nsuccessfully complete the most basic uniform sweep, but LLMs were unable to\ndevelop adaptive sweeping algorithms to compete with GWASS. The evaluation\nunderscores LABIIUM's ability to enhance laboratory productivity and support\ndigital transformation in research and industry, and emphasizes the future work\nrequired to improve LLM performance in Electronic Measurement Science Tasks."
                },
                "authors": [
                    {
                        "name": "Emmanuel A. Olowe"
                    },
                    {
                        "name": "Danial Chitnis"
                    }
                ],
                "author_detail": {
                    "name": "Danial Chitnis"
                },
                "author": "Danial Chitnis",
                "arxiv_comment": "accepted for IEEE I2MTC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01335v2",
                "updated": "2025-03-04T18:15:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    15,
                    16,
                    1,
                    63,
                    0
                ],
                "published": "2024-10-02T08:53:07Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    8,
                    53,
                    7,
                    2,
                    276,
                    0
                ],
                "title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language\n  Models"
                },
                "summary": "Model merging, such as model souping, is the practice of combining different\nmodels with the same architecture together without further training. In this\nwork, we present a model merging methodology that addresses the difficulty of\nfine-tuning Large Language Models (LLMs) for target tasks in non-English\nlanguages, where task-specific data is often unavailable. We focus on\nmathematical reasoning and without in-language math data, facilitate\ncross-lingual transfer by composing language and math capabilities. Starting\nfrom the same pretrained model, we fine-tune separate \"experts\" on math\ninstruction data in English and on generic instruction data in the target\nlanguage. We then replace the top and bottom transformer layers of the math\nexpert directly with layers from the language expert, which consequently\nenhances math performance in the target language. The resulting merged models\noutperform the individual experts and other merging methods on the math\nbenchmark, MGSM, by 10% across four major languages where math instruction data\nis scarce. In addition, this layer swapping is simple, inexpensive, and\nintuitive, as it is based on an interpretative analysis of the most important\nparameter changes during the fine-tuning of each expert. The ability to\nsuccessfully re-compose LLMs for cross-lingual transfer in this manner opens up\nfuture possibilities to combine model expertise, create modular solutions, and\ntransfer reasoning capabilities across languages all post hoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging, such as model souping, is the practice of combining different\nmodels with the same architecture together without further training. In this\nwork, we present a model merging methodology that addresses the difficulty of\nfine-tuning Large Language Models (LLMs) for target tasks in non-English\nlanguages, where task-specific data is often unavailable. We focus on\nmathematical reasoning and without in-language math data, facilitate\ncross-lingual transfer by composing language and math capabilities. Starting\nfrom the same pretrained model, we fine-tune separate \"experts\" on math\ninstruction data in English and on generic instruction data in the target\nlanguage. We then replace the top and bottom transformer layers of the math\nexpert directly with layers from the language expert, which consequently\nenhances math performance in the target language. The resulting merged models\noutperform the individual experts and other merging methods on the math\nbenchmark, MGSM, by 10% across four major languages where math instruction data\nis scarce. In addition, this layer swapping is simple, inexpensive, and\nintuitive, as it is based on an interpretative analysis of the most important\nparameter changes during the fine-tuning of each expert. The ability to\nsuccessfully re-compose LLMs for cross-lingual transfer in this manner opens up\nfuture possibilities to combine model expertise, create modular solutions, and\ntransfer reasoning capabilities across languages all post hoc."
                },
                "authors": [
                    {
                        "name": "Lucas Bandarkar"
                    },
                    {
                        "name": "Benjamin Muller"
                    },
                    {
                        "name": "Pritish Yuvraj"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Nayan Singhal"
                    },
                    {
                        "name": "Hongjiang Lv"
                    },
                    {
                        "name": "Bing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Liu"
                },
                "author": "Bing Liu",
                "arxiv_comment": "ICLR 2025, Spotlight Paper, In The Thirteenth International\n  Conference on Learning Representations, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05516v3",
                "updated": "2025-03-04T18:07:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    7,
                    34,
                    1,
                    63,
                    0
                ],
                "published": "2024-06-08T16:35:31Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    16,
                    35,
                    31,
                    5,
                    160,
                    0
                ],
                "title": "Verbalized Probabilistic Graphical Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbalized Probabilistic Graphical Modeling"
                },
                "summary": "Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality."
                },
                "authors": [
                    {
                        "name": "Hengguan Huang"
                    },
                    {
                        "name": "Xing Shen"
                    },
                    {
                        "name": "Songtao Wang"
                    },
                    {
                        "name": "Lingfa Meng"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Samir Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Samir Bhatt"
                },
                "author": "Samir Bhatt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01773v2",
                "updated": "2025-03-04T18:01:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    1,
                    19,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-03T17:57:03Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    17,
                    57,
                    3,
                    0,
                    62,
                    0
                ],
                "title": "Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism\n  Perspective on Focus Areas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism\n  Perspective on Focus Areas"
                },
                "summary": "Large Vision Language Models (VLMs) have long struggled with spatial\nreasoning tasks. Surprisingly, even simple spatial reasoning tasks, such as\nrecognizing \"under\" or \"behind\" relationships between only two objects, pose\nsignificant challenges for current VLMs. In this work, we study the spatial\nreasoning challenge from the lens of mechanistic interpretability, diving into\nthe model's internal states to examine the interactions between image and text\ntokens. By tracing attention distribution over the image through out\nintermediate layers, we observe that successful spatial reasoning correlates\nstrongly with the model's ability to align its attention distribution with\nactual object locations, particularly differing between familiar and unfamiliar\nspatial relationships. Motivated by these findings, we propose ADAPTVIS based\non inference-time confidence scores to sharpen the attention on highly relevant\nregions when confident, while smoothing and broadening the attention window to\nconsider a wider context when confidence is lower. This training-free decoding\nmethod shows significant improvement (e.g., up to a 50 absolute point\nimprovement) on spatial reasoning benchmarks such as WhatsUp and VSR with\nnegligible cost. We make code and data publicly available for research purposes\nat https://github.com/shiqichen17/AdaptVis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision Language Models (VLMs) have long struggled with spatial\nreasoning tasks. Surprisingly, even simple spatial reasoning tasks, such as\nrecognizing \"under\" or \"behind\" relationships between only two objects, pose\nsignificant challenges for current VLMs. In this work, we study the spatial\nreasoning challenge from the lens of mechanistic interpretability, diving into\nthe model's internal states to examine the interactions between image and text\ntokens. By tracing attention distribution over the image through out\nintermediate layers, we observe that successful spatial reasoning correlates\nstrongly with the model's ability to align its attention distribution with\nactual object locations, particularly differing between familiar and unfamiliar\nspatial relationships. Motivated by these findings, we propose ADAPTVIS based\non inference-time confidence scores to sharpen the attention on highly relevant\nregions when confident, while smoothing and broadening the attention window to\nconsider a wider context when confidence is lower. This training-free decoding\nmethod shows significant improvement (e.g., up to a 50 absolute point\nimprovement) on spatial reasoning benchmarks such as WhatsUp and VSR with\nnegligible cost. We make code and data publicly available for research purposes\nat https://github.com/shiqichen17/AdaptVis."
                },
                "authors": [
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Ruochen Zhou"
                    },
                    {
                        "name": "Jinghan Zhang"
                    },
                    {
                        "name": "Siyang Gao"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Mor Geva"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Jiajun Wu"
                    },
                    {
                        "name": "Manling Li"
                    }
                ],
                "author_detail": {
                    "name": "Manling Li"
                },
                "author": "Manling Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02832v1",
                "updated": "2025-03-04T17:57:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    57,
                    9,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:57:09Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    57,
                    9,
                    1,
                    63,
                    0
                ],
                "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy\n  Distillation"
                },
                "summary": "In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization."
                },
                "authors": [
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Bojie Hu"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinan Xu"
                },
                "author": "Jinan Xu",
                "arxiv_comment": "15 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02819v1",
                "updated": "2025-03-04T17:46:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    46,
                    51,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:46:51Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    46,
                    51,
                    1,
                    63,
                    0
                ],
                "title": "Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of\n  Experts"
                },
                "summary": "While score-based generative models are the model of choice across diverse\ndomains, there are limited tools available for controlling inference-time\nbehavior in a principled manner, e.g. for composing multiple pretrained models.\nExisting classifier-free guidance methods use a simple heuristic to mix\nconditional and unconditional scores to approximately sample from conditional\ndistributions. However, such methods do not approximate the intermediate\ndistributions, necessitating additional 'corrector' steps. In this work, we\nprovide an efficient and principled method for sampling from a sequence of\nannealed, geometric-averaged, or product distributions derived from pretrained\nscore-based models. We derive a weighted simulation scheme which we call\nFeynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by\ncarefully accounting for terms in the appropriate partial differential\nequations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo\n(SMC) resampling algorithms that leverage inference-time scaling to improve\nsampling quality. We empirically demonstrate the utility of our methods by\nproposing amortized sampling via inference-time temperature annealing,\nimproving multi-objective molecule generation using pretrained models, and\nimproving classifier-free guidance for text-to-image generation. Our code is\navailable at https://github.com/martaskrt/fkc-diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While score-based generative models are the model of choice across diverse\ndomains, there are limited tools available for controlling inference-time\nbehavior in a principled manner, e.g. for composing multiple pretrained models.\nExisting classifier-free guidance methods use a simple heuristic to mix\nconditional and unconditional scores to approximately sample from conditional\ndistributions. However, such methods do not approximate the intermediate\ndistributions, necessitating additional 'corrector' steps. In this work, we\nprovide an efficient and principled method for sampling from a sequence of\nannealed, geometric-averaged, or product distributions derived from pretrained\nscore-based models. We derive a weighted simulation scheme which we call\nFeynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by\ncarefully accounting for terms in the appropriate partial differential\nequations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo\n(SMC) resampling algorithms that leverage inference-time scaling to improve\nsampling quality. We empirically demonstrate the utility of our methods by\nproposing amortized sampling via inference-time temperature annealing,\nimproving multi-objective molecule generation using pretrained models, and\nimproving classifier-free guidance for text-to-image generation. Our code is\navailable at https://github.com/martaskrt/fkc-diffusion."
                },
                "authors": [
                    {
                        "name": "Marta Skreta"
                    },
                    {
                        "name": "Tara Akhound-Sadegh"
                    },
                    {
                        "name": "Viktor Ohanesian"
                    },
                    {
                        "name": "Roberto Bondesan"
                    },
                    {
                        "name": "Alán Aspuru-Guzik"
                    },
                    {
                        "name": "Arnaud Doucet"
                    },
                    {
                        "name": "Rob Brekelmans"
                    },
                    {
                        "name": "Alexander Tong"
                    },
                    {
                        "name": "Kirill Neklyudov"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Neklyudov"
                },
                "author": "Kirill Neklyudov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Éric de la Clergerie"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21239v2",
                "updated": "2025-03-04T17:31:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    31,
                    25,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-28T17:09:08Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    9,
                    8,
                    4,
                    59,
                    0
                ],
                "title": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring white-box access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring white-box access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses."
                },
                "authors": [
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Ziji Zhang"
                    },
                    {
                        "name": "Yingying Zhuang"
                    },
                    {
                        "name": "Swair Shah"
                    },
                    {
                        "name": "Anurag Beniwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Beniwal"
                },
                "author": "Anurag Beniwal",
                "arxiv_comment": "This paper needs approval from Amazon for open resource release",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02098v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02098v5",
                "updated": "2025-03-04T17:23:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    23,
                    51,
                    1,
                    63,
                    0
                ],
                "published": "2024-10-02T23:39:10Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    23,
                    39,
                    10,
                    2,
                    276,
                    0
                ],
                "title": "EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice\n  Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice\n  Routing"
                },
                "summary": "Diffusion transformers have been widely adopted for text-to-image synthesis.\nWhile scaling these models up to billions of parameters shows promise, the\neffectiveness of scaling beyond current sizes remains underexplored and\nchallenging. By explicitly exploiting the computational heterogeneity of image\ngenerations, we develop a new family of Mixture-of-Experts (MoE) models\n(EC-DIT) for diffusion transformers with expert-choice routing. EC-DIT learns\nto adaptively optimize the compute allocated to understand the input texts and\ngenerate the respective image patches, enabling heterogeneous computation\naligned with varying text-image complexities. This heterogeneity provides an\nefficient way of scaling EC-DIT up to 97 billion parameters and achieving\nsignificant improvements in training convergence, text-to-image alignment, and\noverall generation quality over dense models and conventional MoE models.\nThrough extensive ablations, we show that EC-DIT demonstrates superior\nscalability and adaptive compute allocation by recognizing varying textual\nimportance through end-to-end training. Notably, in text-to-image alignment\nevaluation, our largest models achieve a state-of-the-art GenEval score of\n71.68% and still maintain competitive inference speed with intuitive\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have been widely adopted for text-to-image synthesis.\nWhile scaling these models up to billions of parameters shows promise, the\neffectiveness of scaling beyond current sizes remains underexplored and\nchallenging. By explicitly exploiting the computational heterogeneity of image\ngenerations, we develop a new family of Mixture-of-Experts (MoE) models\n(EC-DIT) for diffusion transformers with expert-choice routing. EC-DIT learns\nto adaptively optimize the compute allocated to understand the input texts and\ngenerate the respective image patches, enabling heterogeneous computation\naligned with varying text-image complexities. This heterogeneity provides an\nefficient way of scaling EC-DIT up to 97 billion parameters and achieving\nsignificant improvements in training convergence, text-to-image alignment, and\noverall generation quality over dense models and conventional MoE models.\nThrough extensive ablations, we show that EC-DIT demonstrates superior\nscalability and adaptive compute allocation by recognizing varying textual\nimportance through end-to-end training. Notably, in text-to-image alignment\nevaluation, our largest models achieve a state-of-the-art GenEval score of\n71.68% and still maintain competitive inference speed with intuitive\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Haotian Sun"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Nan Du"
                    }
                ],
                "author_detail": {
                    "name": "Nan Du"
                },
                "author": "Nan Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02098v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02098v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16600v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16600v3",
                "updated": "2025-03-04T17:23:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    23,
                    23,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-23T15:00:53Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    15,
                    0,
                    53,
                    6,
                    54,
                    0
                ],
                "title": "Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in\n  Language Models"
                },
                "summary": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs."
                },
                "authors": [
                    {
                        "name": "Guangliang Liu"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Xitong Zhang"
                    },
                    {
                        "name": "Kristen Marie Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Marie Johnson"
                },
                "author": "Kristen Marie Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16600v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16600v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02800v1",
                "updated": "2025-03-04T17:20:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    20,
                    43,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:20:43Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    20,
                    43,
                    1,
                    63,
                    0
                ],
                "title": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration"
                },
                "summary": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7 to 89.1 on the real-world dataset. By allowing for the enriching of input\nseries data with semantics, RAAD-LLM incorporates multimodal capabilities that\nfacilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7 to 89.1 on the real-world dataset. By allowing for the enriching of input\nseries data with semantics, RAAD-LLM incorporates multimodal capabilities that\nfacilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries."
                },
                "authors": [
                    {
                        "name": "Alicia Russell-Gilbert"
                    },
                    {
                        "name": "Sudip Mittal"
                    },
                    {
                        "name": "Shahram Rahimi"
                    },
                    {
                        "name": "Maria Seale"
                    },
                    {
                        "name": "Joseph Jabour"
                    },
                    {
                        "name": "Thomas Arnold"
                    },
                    {
                        "name": "Joshua Church"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Church"
                },
                "author": "Joshua Church",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2411.00914",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "1.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02794v1",
                "updated": "2025-03-04T17:11:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    11,
                    16,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:11:16Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    11,
                    16,
                    1,
                    63,
                    0
                ],
                "title": "Social hierarchy shapes foraging decisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social hierarchy shapes foraging decisions"
                },
                "summary": "Social foraging is a widespread form of animal foraging in which groups of\nindividuals coordinate their decisions to exploit resources in the environment.\nAnimals show a variety of social structures from egalitarian to hierarchical.\nIn this study, we examine how different forms of social hierarchy shape\nforaging decisions. We developed a mechanistic analytically tractable model to\nstudy the underlying processes of social foraging, tying the microscopic\nindividual to the macroscopic group levels. Based on a stochastic evidence\naccumulation framework, we developed a model of patch-leaving decisions in a\nlarge hierarchical group with leading and following individuals. Across a\nvariety of information sharing mechanisms, we were able to analytically\nquantify emergent collective dynamics. We found that follower-leader dynamics\nthrough observations of leader movements or through counting the number of\nindividuals in a patch confers, for most conditions, a benefit for the\nfollowing individuals by increasing their accuracy in inferring patch richness.\nOn the other hand, misinformation, through the communication of false beliefs\nabout food rewards or patch quality, shows to be detrimental to following\nindividuals, but paradoxically may lead to increased group cohesion. In an era\nwhere there is a huge amount of animal foraging data collected, our model\nprovides a systematic way to conceptualize and understand those data by\nuncovering hidden mechanisms underlying social foraging decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social foraging is a widespread form of animal foraging in which groups of\nindividuals coordinate their decisions to exploit resources in the environment.\nAnimals show a variety of social structures from egalitarian to hierarchical.\nIn this study, we examine how different forms of social hierarchy shape\nforaging decisions. We developed a mechanistic analytically tractable model to\nstudy the underlying processes of social foraging, tying the microscopic\nindividual to the macroscopic group levels. Based on a stochastic evidence\naccumulation framework, we developed a model of patch-leaving decisions in a\nlarge hierarchical group with leading and following individuals. Across a\nvariety of information sharing mechanisms, we were able to analytically\nquantify emergent collective dynamics. We found that follower-leader dynamics\nthrough observations of leader movements or through counting the number of\nindividuals in a patch confers, for most conditions, a benefit for the\nfollowing individuals by increasing their accuracy in inferring patch richness.\nOn the other hand, misinformation, through the communication of false beliefs\nabout food rewards or patch quality, shows to be detrimental to following\nindividuals, but paradoxically may lead to increased group cohesion. In an era\nwhere there is a huge amount of animal foraging data collected, our model\nprovides a systematic way to conceptualize and understand those data by\nuncovering hidden mechanisms underlying social foraging decisions."
                },
                "authors": [
                    {
                        "name": "Lisa Blum Moyse"
                    },
                    {
                        "name": "Ahmed El Hady"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed El Hady"
                },
                "author": "Ahmed El Hady",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2412.02381",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01711v2",
                "updated": "2025-03-04T17:02:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    2,
                    27,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-03T16:24:36Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    16,
                    24,
                    36,
                    0,
                    62,
                    0
                ],
                "title": "MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation\n  Alignment"
                },
                "summary": "Personalized product search aims to retrieve and rank items that match users'\npreferences and search intent. Despite their effectiveness, existing approaches\ntypically assume that users' query fully captures their real motivation.\nHowever, our analysis of a real-world e-commerce platform reveals that users\noften engage in relevant consultations before searching, indicating they refine\nintents through consultations based on motivation and need. The implied\nmotivation in consultations is a key enhancing factor for personalized search.\nThis unexplored area comes with new challenges including aligning contextual\nmotivations with concise queries, bridging the category-text gap, and filtering\nnoise within sequence history. To address these, we propose a Motivation-Aware\nPersonalized Search (MAPS) method. It embeds queries and consultations into a\nunified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE)\nto prioritize critical semantics, and introduces dual alignment: (1)\ncontrastive learning aligns consultations, reviews, and product features; (2)\nbidirectional attention integrates motivation-aware embeddings with user\npreferences. Extensive experiments on real and synthetic data show MAPS\noutperforms existing methods in both retrieval and ranking tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized product search aims to retrieve and rank items that match users'\npreferences and search intent. Despite their effectiveness, existing approaches\ntypically assume that users' query fully captures their real motivation.\nHowever, our analysis of a real-world e-commerce platform reveals that users\noften engage in relevant consultations before searching, indicating they refine\nintents through consultations based on motivation and need. The implied\nmotivation in consultations is a key enhancing factor for personalized search.\nThis unexplored area comes with new challenges including aligning contextual\nmotivations with concise queries, bridging the category-text gap, and filtering\nnoise within sequence history. To address these, we propose a Motivation-Aware\nPersonalized Search (MAPS) method. It embeds queries and consultations into a\nunified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE)\nto prioritize critical semantics, and introduces dual alignment: (1)\ncontrastive learning aligns consultations, reviews, and product features; (2)\nbidirectional attention integrates motivation-aware embeddings with user\npreferences. Extensive experiments on real and synthetic data show MAPS\noutperforms existing methods in both retrieval and ranking tasks."
                },
                "authors": [
                    {
                        "name": "Weicong Qin"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Chenglei Shen"
                    },
                    {
                        "name": "Ming He"
                    },
                    {
                        "name": "Jianping Fan"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02783v1",
                "updated": "2025-03-04T16:56:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    56,
                    34,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:56:34Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    56,
                    34,
                    1,
                    63,
                    0
                ],
                "title": "IterPref: Focal Preference Learning for Code Generation via Iterative\n  Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterPref: Focal Preference Learning for Code Generation via Iterative\n  Debugging"
                },
                "summary": "Preference learning enhances Code LLMs beyond supervised fine-tuning by\nleveraging relative quality comparisons. Existing methods construct preference\npairs from\n  candidates based on test case success, treating the higher pass rate sample\nas positive and the lower as negative. However, this approach does not pinpoint\nspecific errors in the code, which prevents the model from learning more\ninformative error correction patterns, as aligning failing code as a whole\nlacks the granularity needed to capture meaningful error-resolution\nrelationships. To address these issues, we propose IterPref, a new preference\nalignment framework that mimics human iterative debugging to refine Code LLMs.\nIterPref explicitly locates error regions and aligns the corresponding tokens\nvia a tailored DPO algorithm. To generate informative pairs, we introduce the\nCodeFlow dataset, where samples are iteratively refined until passing tests,\nwith modifications capturing error corrections. Extensive experiments show that\na diverse suite of Code LLMs equipped with IterPref achieves significant\nperformance gains in code generation and improves on challenging tasks like\nBigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our\ncode and data will be made publicaly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference learning enhances Code LLMs beyond supervised fine-tuning by\nleveraging relative quality comparisons. Existing methods construct preference\npairs from\n  candidates based on test case success, treating the higher pass rate sample\nas positive and the lower as negative. However, this approach does not pinpoint\nspecific errors in the code, which prevents the model from learning more\ninformative error correction patterns, as aligning failing code as a whole\nlacks the granularity needed to capture meaningful error-resolution\nrelationships. To address these issues, we propose IterPref, a new preference\nalignment framework that mimics human iterative debugging to refine Code LLMs.\nIterPref explicitly locates error regions and aligns the corresponding tokens\nvia a tailored DPO algorithm. To generate informative pairs, we introduce the\nCodeFlow dataset, where samples are iteratively refined until passing tests,\nwith modifications capturing error corrections. Extensive experiments show that\na diverse suite of Code LLMs equipped with IterPref achieves significant\nperformance gains in code generation and improves on challenging tasks like\nBigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our\ncode and data will be made publicaly available."
                },
                "authors": [
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Haoling Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Jianwen Luo"
                    },
                    {
                        "name": "Yangyu Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Scarlett Li"
                    }
                ],
                "author_detail": {
                    "name": "Scarlett Li"
                },
                "author": "Scarlett Li",
                "arxiv_comment": "The code and data will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02779v1",
                "updated": "2025-03-04T16:52:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    52,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:52:07Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    52,
                    7,
                    1,
                    63,
                    0
                ],
                "title": "Selective electron-phonon coupling strength from nonequilibrium optical\n  spectroscopy: The case of MgB$_2$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective electron-phonon coupling strength from nonequilibrium optical\n  spectroscopy: The case of MgB$_2$"
                },
                "summary": "The coupling between quasiparticles and bosonic excitations rules the energy\ntransfer pathways in condensed matter systems. The possibility of inferring the\nstrength of specific coupling channels from their characteristic time scales\nmeasured in nonequilibrium experiments is still an open question. Here, we\ninvestigate MgB$_2$, in which conventional superconductivity at temperatures as\nhigh as 39 K is mediated by the strong coupling between the conduction\nelectrons and the E$_{2g}$ phonon mode. By means of broadband time-resolved\noptical spectroscopy, we show that this selective electron-phonon coupling\ndictates the nonequilibrium optical response of MgB$_2$ at early times (<100\nfs) after photoexcitation. Furthermore, based on an effective temperature model\nanalysis, we estimate its contribution to the total electron-boson coupling\nfunction extracted from complementary equilibrium spectroscopy approaches,\nnamely optical reflectivity and ARPES. The coupling strength with the E$_{2g}$\nphonon modes is thus estimated to be $\\lambda$ ~ 0.56, which is approximately\nhalf of the total coupling constant, in agreement with ab-initio calculations\nfrom the literature. As a benchmark, broadband time-resolved optical\nspectroscopy is performed also on the isostructural and non-superconducting\ncompound AlB$_2$, showing that the nonequilibrium optical response relaxes on a\nslower timescale due to the lack of strongly-coupled phonon modes. Our findings\ndemonstrate the possibility to resolve and quantify selective electron-phonon\ncoupling from nonequilibrium optical spectroscopy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coupling between quasiparticles and bosonic excitations rules the energy\ntransfer pathways in condensed matter systems. The possibility of inferring the\nstrength of specific coupling channels from their characteristic time scales\nmeasured in nonequilibrium experiments is still an open question. Here, we\ninvestigate MgB$_2$, in which conventional superconductivity at temperatures as\nhigh as 39 K is mediated by the strong coupling between the conduction\nelectrons and the E$_{2g}$ phonon mode. By means of broadband time-resolved\noptical spectroscopy, we show that this selective electron-phonon coupling\ndictates the nonequilibrium optical response of MgB$_2$ at early times (<100\nfs) after photoexcitation. Furthermore, based on an effective temperature model\nanalysis, we estimate its contribution to the total electron-boson coupling\nfunction extracted from complementary equilibrium spectroscopy approaches,\nnamely optical reflectivity and ARPES. The coupling strength with the E$_{2g}$\nphonon modes is thus estimated to be $\\lambda$ ~ 0.56, which is approximately\nhalf of the total coupling constant, in agreement with ab-initio calculations\nfrom the literature. As a benchmark, broadband time-resolved optical\nspectroscopy is performed also on the isostructural and non-superconducting\ncompound AlB$_2$, showing that the nonequilibrium optical response relaxes on a\nslower timescale due to the lack of strongly-coupled phonon modes. Our findings\ndemonstrate the possibility to resolve and quantify selective electron-phonon\ncoupling from nonequilibrium optical spectroscopy."
                },
                "authors": [
                    {
                        "name": "S. Mor"
                    },
                    {
                        "name": "F. Boschini"
                    },
                    {
                        "name": "E. Razzoli"
                    },
                    {
                        "name": "M. Zonno"
                    },
                    {
                        "name": "M. Michiardi"
                    },
                    {
                        "name": "G. Levy"
                    },
                    {
                        "name": "N. D. Zhigadlo"
                    },
                    {
                        "name": "P. C. Canfield"
                    },
                    {
                        "name": "G. Cerullo"
                    },
                    {
                        "name": "A. Damascelli"
                    },
                    {
                        "name": "C. Giannetti"
                    },
                    {
                        "name": "S. Dal Conte"
                    }
                ],
                "author_detail": {
                    "name": "S. Dal Conte"
                },
                "author": "S. Dal Conte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02776v1",
                "updated": "2025-03-04T16:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    49,
                    37,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    49,
                    37,
                    1,
                    63,
                    0
                ],
                "title": "Implicit Bias in LLMs: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Bias in LLMs: A Survey"
                },
                "summary": "Due to the implement of guardrails by developers, Large language models\n(LLMs) have demonstrated exceptional performance in explicit bias tests.\nHowever, bias in LLMs may occur not only explicitly, but also implicitly, much\nlike humans who consciously strive for impartiality yet still harbor implicit\nbias. The unconscious and automatic nature of implicit bias makes it\nparticularly challenging to study. This paper provides a comprehensive review\nof the existing literature on implicit bias in LLMs. We begin by introducing\nkey concepts, theories and methods related to implicit bias in psychology,\nextending them from humans to LLMs. Drawing on the Implicit Association Test\n(IAT) and other psychological frameworks, we categorize detection methods into\nthree primary approaches: word association, task-oriented text generation and\ndecision-making. We divide our taxonomy of evaluation metrics for implicit bias\ninto two categories: single-value-based metrics and comparison-value-based\nmetrics. We classify datasets into two types: sentences with masked tokens and\ncomplete sentences, incorporating datasets from various domains to reflect the\nbroad application of LLMs. Although research on mitigating implicit bias in\nLLMs is still limited, we summarize existing efforts and offer insights on\nfuture challenges. We aim for this work to serve as a clear guide for\nresearchers and inspire innovative ideas to advance exploration in this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the implement of guardrails by developers, Large language models\n(LLMs) have demonstrated exceptional performance in explicit bias tests.\nHowever, bias in LLMs may occur not only explicitly, but also implicitly, much\nlike humans who consciously strive for impartiality yet still harbor implicit\nbias. The unconscious and automatic nature of implicit bias makes it\nparticularly challenging to study. This paper provides a comprehensive review\nof the existing literature on implicit bias in LLMs. We begin by introducing\nkey concepts, theories and methods related to implicit bias in psychology,\nextending them from humans to LLMs. Drawing on the Implicit Association Test\n(IAT) and other psychological frameworks, we categorize detection methods into\nthree primary approaches: word association, task-oriented text generation and\ndecision-making. We divide our taxonomy of evaluation metrics for implicit bias\ninto two categories: single-value-based metrics and comparison-value-based\nmetrics. We classify datasets into two types: sentences with masked tokens and\ncomplete sentences, incorporating datasets from various domains to reflect the\nbroad application of LLMs. Although research on mitigating implicit bias in\nLLMs is still limited, we summarize existing efforts and offer insights on\nfuture challenges. We aim for this work to serve as a clear guide for\nresearchers and inspire innovative ideas to advance exploration in this task."
                },
                "authors": [
                    {
                        "name": "Xinru Lin"
                    },
                    {
                        "name": "Luyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Luyang Li"
                },
                "author": "Luyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02773v1",
                "updated": "2025-03-04T16:42:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    42,
                    46,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:42:46Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    42,
                    46,
                    1,
                    63,
                    0
                ],
                "title": "Prime Convolutional Model: Breaking the Ground for Theoretical\n  Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prime Convolutional Model: Breaking the Ground for Theoretical\n  Explainability"
                },
                "summary": "In this paper, we propose a new theoretical approach to Explainable AI.\nFollowing the Scientific Method, this approach consists in formulating on the\nbasis of empirical evidence, a mathematical model to explain and predict the\nbehaviors of Neural Networks. We apply the method to a case study created in a\ncontrolled environment, which we call Prime Convolutional Model (p-Conv for\nshort). p-Conv operates on a dataset consisting of the first one million\nnatural numbers and is trained to identify the congruence classes modulo a\ngiven integer $m$. Its architecture uses a convolutional-type neural network\nthat contextually processes a sequence of $B$ consecutive numbers to each\ninput. We take an empirical approach and exploit p-Conv to identify the\ncongruence classes of numbers in a validation set using different values for\n$m$ and $B$. The results show that the different behaviors of p-Conv (i.e.,\nwhether it can perform the task or not) can be modeled mathematically in terms\nof $m$ and $B$. The inferred mathematical model reveals interesting patterns\nable to explain when and why p-Conv succeeds in performing task and, if not,\nwhich error pattern it follows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a new theoretical approach to Explainable AI.\nFollowing the Scientific Method, this approach consists in formulating on the\nbasis of empirical evidence, a mathematical model to explain and predict the\nbehaviors of Neural Networks. We apply the method to a case study created in a\ncontrolled environment, which we call Prime Convolutional Model (p-Conv for\nshort). p-Conv operates on a dataset consisting of the first one million\nnatural numbers and is trained to identify the congruence classes modulo a\ngiven integer $m$. Its architecture uses a convolutional-type neural network\nthat contextually processes a sequence of $B$ consecutive numbers to each\ninput. We take an empirical approach and exploit p-Conv to identify the\ncongruence classes of numbers in a validation set using different values for\n$m$ and $B$. The results show that the different behaviors of p-Conv (i.e.,\nwhether it can perform the task or not) can be modeled mathematically in terms\nof $m$ and $B$. The inferred mathematical model reveals interesting patterns\nable to explain when and why p-Conv succeeds in performing task and, if not,\nwhich error pattern it follows."
                },
                "authors": [
                    {
                        "name": "Francesco Panelli"
                    },
                    {
                        "name": "Doaa Almhaithawi"
                    },
                    {
                        "name": "Tania Cerquitelli"
                    },
                    {
                        "name": "Alessandro Bellini"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Bellini"
                },
                "author": "Alessandro Bellini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00320v2",
                "updated": "2025-03-04T16:36:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    36,
                    54,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-01T03:15:13Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    15,
                    13,
                    5,
                    60,
                    0
                ],
                "title": "Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of\n  Bilateral Financial Exchanges, A bond market study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of\n  Bilateral Financial Exchanges, A bond market study"
                },
                "summary": "Bilateral markets, such as those for government bonds, involve decentralized\nand opaque transactions between market makers (MMs) and clients, posing\nsignificant challenges for traditional modeling approaches. To address these\ncomplexities, we introduce TRIBE an agent-based model augmented with a large\nlanguage model (LLM) to simulate human-like decision-making in trading\nenvironments. TRIBE leverages publicly available data and stylized facts to\ncapture realistic trading dynamics, integrating human biases like risk aversion\nand ambiguity sensitivity into the decision-making processes of agents. Our\nresearch yields three key contributions: first, we demonstrate that integrating\nLLMs into agent-based models to enhance client agency is feasible and enriches\nthe simulation of agent behaviors in complex markets; second, we find that even\nslight trade aversion encoded within the LLM leads to a complete cessation of\ntrading activity, highlighting the sensitivity of market dynamics to agents'\nrisk profiles; third, we show that incorporating human-like variability shifts\npower dynamics towards clients and can disproportionately affect the entire\nsystem, often resulting in systemic agent collapse across simulations. These\nfindings underscore the emergent properties that arise when introducing\nstochastic, human-like decision processes, revealing new system behaviors that\nenhance the realism and complexity of artificial societies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bilateral markets, such as those for government bonds, involve decentralized\nand opaque transactions between market makers (MMs) and clients, posing\nsignificant challenges for traditional modeling approaches. To address these\ncomplexities, we introduce TRIBE an agent-based model augmented with a large\nlanguage model (LLM) to simulate human-like decision-making in trading\nenvironments. TRIBE leverages publicly available data and stylized facts to\ncapture realistic trading dynamics, integrating human biases like risk aversion\nand ambiguity sensitivity into the decision-making processes of agents. Our\nresearch yields three key contributions: first, we demonstrate that integrating\nLLMs into agent-based models to enhance client agency is feasible and enriches\nthe simulation of agent behaviors in complex markets; second, we find that even\nslight trade aversion encoded within the LLM leads to a complete cessation of\ntrading activity, highlighting the sensitivity of market dynamics to agents'\nrisk profiles; third, we show that incorporating human-like variability shifts\npower dynamics towards clients and can disproportionately affect the entire\nsystem, often resulting in systemic agent collapse across simulations. These\nfindings underscore the emergent properties that arise when introducing\nstochastic, human-like decision processes, revealing new system behaviors that\nenhance the realism and complexity of artificial societies."
                },
                "authors": [
                    {
                        "name": "Alicia Vidler"
                    },
                    {
                        "name": "Toby Walsh"
                    }
                ],
                "author_detail": {
                    "name": "Toby Walsh"
                },
                "author": "Toby Walsh",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17403v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17403v2",
                "updated": "2025-03-04T16:36:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    36,
                    52,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-24T18:30:36Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    30,
                    36,
                    0,
                    55,
                    0
                ],
                "title": "Large Language Models are Powerful EHR Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Powerful EHR Encoders"
                },
                "summary": "Electronic Health Records (EHRs) offer rich potential for clinical\nprediction, yet their inherent complexity and heterogeneity pose significant\nchallenges for traditional machine learning approaches. Domain-specific EHR\nfoundation models trained on large collections of unlabeled EHR data have\ndemonstrated promising improvements in predictive accuracy and generalization;\nhowever, their training is constrained by limited access to diverse,\nhigh-quality datasets and inconsistencies in coding standards and healthcare\npractices. In this study, we explore the possibility of using general-purpose\nLarge Language Models (LLMs) based embedding methods as EHR encoders. By\nserializing patient records into structured Markdown text, transforming codes\ninto human-readable descriptors, we leverage the extensive generalization\ncapabilities of LLMs pretrained on vast public corpora, thereby bypassing the\nneed for proprietary medical datasets. We systematically evaluate two\nstate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and\nLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from\nthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundation\nmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our results\ndemonstrate that LLM-based embeddings frequently match or exceed the\nperformance of specialized models, even in few-shot settings, and that their\neffectiveness scales with the size of the underlying LLM and the available\ncontext window. Overall, our findings demonstrate that repurposing LLMs for EHR\nencoding offers a scalable and effective approach for clinical prediction,\ncapable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) offer rich potential for clinical\nprediction, yet their inherent complexity and heterogeneity pose significant\nchallenges for traditional machine learning approaches. Domain-specific EHR\nfoundation models trained on large collections of unlabeled EHR data have\ndemonstrated promising improvements in predictive accuracy and generalization;\nhowever, their training is constrained by limited access to diverse,\nhigh-quality datasets and inconsistencies in coding standards and healthcare\npractices. In this study, we explore the possibility of using general-purpose\nLarge Language Models (LLMs) based embedding methods as EHR encoders. By\nserializing patient records into structured Markdown text, transforming codes\ninto human-readable descriptors, we leverage the extensive generalization\ncapabilities of LLMs pretrained on vast public corpora, thereby bypassing the\nneed for proprietary medical datasets. We systematically evaluate two\nstate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and\nLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from\nthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundation\nmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our results\ndemonstrate that LLM-based embeddings frequently match or exceed the\nperformance of specialized models, even in few-shot settings, and that their\neffectiveness scales with the size of the underlying LLM and the available\ncontext window. Overall, our findings demonstrate that repurposing LLMs for EHR\nencoding offers a scalable and effective approach for clinical prediction,\ncapable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications."
                },
                "authors": [
                    {
                        "name": "Stefan Hegselmann"
                    },
                    {
                        "name": "Georg von Arnim"
                    },
                    {
                        "name": "Tillmann Rheude"
                    },
                    {
                        "name": "Noel Kronenberg"
                    },
                    {
                        "name": "David Sontag"
                    },
                    {
                        "name": "Gerhard Hindricks"
                    },
                    {
                        "name": "Roland Eils"
                    },
                    {
                        "name": "Benjamin Wild"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Wild"
                },
                "author": "Benjamin Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17403v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17403v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02488v3",
                "updated": "2025-03-04T16:36:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    36,
                    9,
                    1,
                    63,
                    0
                ],
                "published": "2024-10-03T13:52:22Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    52,
                    22,
                    3,
                    277,
                    0
                ],
                "title": "Evidence for relativistic Sunyaev-Zeldovich effect in Planck CMB maps\n  with an average electron-gas temperature of $T_{\\rm e}\\simeq 5$ keV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence for relativistic Sunyaev-Zeldovich effect in Planck CMB maps\n  with an average electron-gas temperature of $T_{\\rm e}\\simeq 5$ keV"
                },
                "summary": "Stacking the public Planck CMB temperature maps (NILC, SMICA, SEVEM,\nCommander) on galaxy clusters from Planck catalogues reveals substantial\nresidual contamination from thermal Sunyaev-Zeldovich (tSZ) emission.\nUnexpectedly, stacking \"tSZ-free\" CMB maps, like the Planck SMICA-noSZ or\nConstrained ILC (CILC) maps, still shows noticeable residual contamination from\ngalaxy clusters. We demonstrate that this persisting residual stems from\nneglected relativistic SZ (rSZ) corrections in the CMB map estimation.\nEmploying a component-separation method specifically designed for the rSZ\neffect on Planck data, we map the rSZ first-order moment field $y(T_{\\rm\ne}-\\bar{T}_{\\rm e})$ over the sky for different pivot temperatures\n$\\bar{T}_{\\rm e}$ ranging from $2$ to $10$ keV. Stacking these $y(T_{\\rm\ne}-\\bar{T}_{\\rm e})$-maps on Planck clusters exhibits either an intensity\ndecrement or increment at the centre, contingent upon whether $\\bar{T}_{\\rm e}$\nis above or below the ensemble-averaged cluster temperature $T_{\\rm e}$. For\nthe pivot value $\\bar{T}_{\\rm e}=5$ keV, a vanishing intensity is observed in\nthe stacked Planck $y(T_{\\rm e}-\\bar{T}_{\\rm e})$-map, enabling us to infer the\naverage gas temperature of $T_{\\rm e}\\simeq 5$ keV for Planck clusters.\nBuilding upon this finding, we revisit the Planck tSZ-free CMB map by\ndeprojecting the complete rSZ emission using CILC, assuming an rSZ spectrum\nwith $T_{\\rm e} = 5$ keV. Our new, rSZ-free Planck CMB map, when stacked on\nclusters, shows a clear cancellation of residual SZ contamination in contrast\nto prior (non-relativistic) tSZ-free Planck CMB maps. Our map-based approach\nprovides compelling evidence for an average temperature of the Planck galaxy\nclusters of $T_{\\rm e} = 4.9 \\pm 2.6$ keV using the rSZ effect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacking the public Planck CMB temperature maps (NILC, SMICA, SEVEM,\nCommander) on galaxy clusters from Planck catalogues reveals substantial\nresidual contamination from thermal Sunyaev-Zeldovich (tSZ) emission.\nUnexpectedly, stacking \"tSZ-free\" CMB maps, like the Planck SMICA-noSZ or\nConstrained ILC (CILC) maps, still shows noticeable residual contamination from\ngalaxy clusters. We demonstrate that this persisting residual stems from\nneglected relativistic SZ (rSZ) corrections in the CMB map estimation.\nEmploying a component-separation method specifically designed for the rSZ\neffect on Planck data, we map the rSZ first-order moment field $y(T_{\\rm\ne}-\\bar{T}_{\\rm e})$ over the sky for different pivot temperatures\n$\\bar{T}_{\\rm e}$ ranging from $2$ to $10$ keV. Stacking these $y(T_{\\rm\ne}-\\bar{T}_{\\rm e})$-maps on Planck clusters exhibits either an intensity\ndecrement or increment at the centre, contingent upon whether $\\bar{T}_{\\rm e}$\nis above or below the ensemble-averaged cluster temperature $T_{\\rm e}$. For\nthe pivot value $\\bar{T}_{\\rm e}=5$ keV, a vanishing intensity is observed in\nthe stacked Planck $y(T_{\\rm e}-\\bar{T}_{\\rm e})$-map, enabling us to infer the\naverage gas temperature of $T_{\\rm e}\\simeq 5$ keV for Planck clusters.\nBuilding upon this finding, we revisit the Planck tSZ-free CMB map by\ndeprojecting the complete rSZ emission using CILC, assuming an rSZ spectrum\nwith $T_{\\rm e} = 5$ keV. Our new, rSZ-free Planck CMB map, when stacked on\nclusters, shows a clear cancellation of residual SZ contamination in contrast\nto prior (non-relativistic) tSZ-free Planck CMB maps. Our map-based approach\nprovides compelling evidence for an average temperature of the Planck galaxy\nclusters of $T_{\\rm e} = 4.9 \\pm 2.6$ keV using the rSZ effect."
                },
                "authors": [
                    {
                        "name": "Mathieu Remazeilles"
                    },
                    {
                        "name": "Jens Chluba"
                    }
                ],
                "author_detail": {
                    "name": "Jens Chluba"
                },
                "author": "Jens Chluba",
                "arxiv_comment": "11 pages, 10 figures, 4 tables. Includes two new appendices. Updated\n  to match the version accepted by MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02760v1",
                "updated": "2025-03-04T16:22:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    22,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:22:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    22,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "From Metaphor to Mechanism: How LLMs Decode Traditional Chinese Medicine\n  Symbolic Language for Modern Clinical Relevance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Metaphor to Mechanism: How LLMs Decode Traditional Chinese Medicine\n  Symbolic Language for Modern Clinical Relevance"
                },
                "summary": "Metaphorical expressions are abundant in Traditional Chinese Medicine (TCM),\nconveying complex disease mechanisms and holistic health concepts through\nculturally rich and often abstract terminology. Bridging these metaphors to\nanatomically driven Western medical (WM) concepts poses significant challenges\nfor both automated language processing and real-world clinical practice. To\naddress this gap, we propose a novel multi-agent and chain-of-thought (CoT)\nframework designed to interpret TCM metaphors accurately and map them to WM\npathophysiology. Specifically, our approach combines domain-specialized agents\n(TCM Expert, WM Expert) with a Coordinator Agent, leveraging stepwise\nchain-of-thought prompts to ensure transparent reasoning and conflict\nresolution. We detail a methodology for building a metaphor-rich TCM dataset,\ndiscuss strategies for effectively integrating multi-agent collaboration and\nCoT reasoning, and articulate the theoretical underpinnings that guide metaphor\ninterpretation across distinct medical paradigms. We present a comprehensive\nsystem design and highlight both the potential benefits and limitations of our\napproach, while leaving placeholders for future experimental validation. Our\nwork aims to support clinical decision-making, cross-system educational\ninitiatives, and integrated healthcare research, ultimately offering a robust\nscaffold for reconciling TCM's symbolic language with the mechanistic focus of\nWestern medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphorical expressions are abundant in Traditional Chinese Medicine (TCM),\nconveying complex disease mechanisms and holistic health concepts through\nculturally rich and often abstract terminology. Bridging these metaphors to\nanatomically driven Western medical (WM) concepts poses significant challenges\nfor both automated language processing and real-world clinical practice. To\naddress this gap, we propose a novel multi-agent and chain-of-thought (CoT)\nframework designed to interpret TCM metaphors accurately and map them to WM\npathophysiology. Specifically, our approach combines domain-specialized agents\n(TCM Expert, WM Expert) with a Coordinator Agent, leveraging stepwise\nchain-of-thought prompts to ensure transparent reasoning and conflict\nresolution. We detail a methodology for building a metaphor-rich TCM dataset,\ndiscuss strategies for effectively integrating multi-agent collaboration and\nCoT reasoning, and articulate the theoretical underpinnings that guide metaphor\ninterpretation across distinct medical paradigms. We present a comprehensive\nsystem design and highlight both the potential benefits and limitations of our\napproach, while leaving placeholders for future experimental validation. Our\nwork aims to support clinical decision-making, cross-system educational\ninitiatives, and integrated healthcare research, ultimately offering a robust\nscaffold for reconciling TCM's symbolic language with the mechanistic focus of\nWestern medicine."
                },
                "authors": [
                    {
                        "name": "Jiacheng Tang"
                    },
                    {
                        "name": "Nankai Wu"
                    },
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Chengxiao Dai"
                    },
                    {
                        "name": "Mengyao Zhao"
                    },
                    {
                        "name": "Xinjie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xinjie Zhao"
                },
                "author": "Xinjie Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06435v2",
                "updated": "2025-03-04T16:22:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    22,
                    34,
                    1,
                    63,
                    0
                ],
                "published": "2024-12-09T12:21:20Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    21,
                    20,
                    0,
                    344,
                    0
                ],
                "title": "Simulating Human-like Daily Activities with Desire-driven Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Human-like Daily Activities with Desire-driven Autonomy"
                },
                "summary": "Desires motivate humans to interact autonomously with the complex world. In\ncontrast, current AI agents require explicit task specifications, such as\ninstructions or reward functions, which constrain their autonomy and behavioral\ndiversity. In this paper, we introduce a Desire-driven Autonomous Agent (D2A)\nthat can enable a large language model (LLM) to autonomously propose and select\ntasks, motivated by satisfying its multi-dimensional desires. Specifically, the\nmotivational framework of D2A is mainly constructed by a dynamic Value System,\ninspired by the Theory of Needs. It incorporates an understanding of human-like\ndesires, such as the need for social interaction, personal fulfillment, and\nself-care. At each step, the agent evaluates the value of its current state,\nproposes a set of candidate activities, and selects the one that best aligns\nwith its intrinsic motivations. We conduct experiments on Concordia, a\ntext-based simulator, to demonstrate that our agent generates coherent,\ncontextually relevant daily activities while exhibiting variability and\nadaptability similar to human behavior. A comparative analysis with other\nLLM-based agents demonstrates that our approach significantly enhances the\nrationality of the simulated activities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Desires motivate humans to interact autonomously with the complex world. In\ncontrast, current AI agents require explicit task specifications, such as\ninstructions or reward functions, which constrain their autonomy and behavioral\ndiversity. In this paper, we introduce a Desire-driven Autonomous Agent (D2A)\nthat can enable a large language model (LLM) to autonomously propose and select\ntasks, motivated by satisfying its multi-dimensional desires. Specifically, the\nmotivational framework of D2A is mainly constructed by a dynamic Value System,\ninspired by the Theory of Needs. It incorporates an understanding of human-like\ndesires, such as the need for social interaction, personal fulfillment, and\nself-care. At each step, the agent evaluates the value of its current state,\nproposes a set of candidate activities, and selects the one that best aligns\nwith its intrinsic motivations. We conduct experiments on Concordia, a\ntext-based simulator, to demonstrate that our agent generates coherent,\ncontextually relevant daily activities while exhibiting variability and\nadaptability similar to human behavior. A comparative analysis with other\nLLM-based agents demonstrates that our approach significantly enhances the\nrationality of the simulated activities."
                },
                "authors": [
                    {
                        "name": "Yiding Wang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Fangwei Zhong"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05821v2",
                "updated": "2025-03-04T16:21:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    26,
                    1,
                    63,
                    0
                ],
                "published": "2024-10-08T08:51:44Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    8,
                    51,
                    44,
                    1,
                    282,
                    0
                ],
                "title": "Towards Zero-Shot, Controllable Dialog Planning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Zero-Shot, Controllable Dialog Planning with LLMs"
                },
                "summary": "Recently, Large Language Models (LLMs) have emerged as an alternative to\ntraining task-specific dialog agents, due to their broad reasoning capabilities\nand performance in zero-shot learning scenarios. However, many LLM-based dialog\nsystems fall short in planning towards an overarching dialog goal and therefore\ncannot steer the conversation appropriately. Furthermore, these models struggle\nwith hallucination, making them unsuitable for information access in sensitive\ndomains, such as legal or medical domains, where correctness of information\ngiven to users is critical. The recently introduced task Conversational Tree\nSearch (CTS) proposes the use of dialog graphs to avoid hallucination in\nsensitive domains, however, state-of-the-art agents are Reinforcement Learning\n(RL) based and require long training times, despite excelling at dialog\nstrategy. This paper introduces a novel zero-shot method for controllable CTS\nagents, where LLMs guide the dialog planning through domain graphs by searching\nand pruning relevant graph nodes based on user interaction preferences. We show\nthat these agents significantly outperform state-of-the-art CTS agents\n($p<0.0001$; Barnard Exact test) in simulation. This generalizes to all\navailable CTS domains. Finally, we perform user evaluation to test the agent's\nperformance in the wild, showing that our policy significantly ($p<0.05$;\nBarnard Exact) improves task-success compared to the state-of-the-art RL-based\nCTS agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have emerged as an alternative to\ntraining task-specific dialog agents, due to their broad reasoning capabilities\nand performance in zero-shot learning scenarios. However, many LLM-based dialog\nsystems fall short in planning towards an overarching dialog goal and therefore\ncannot steer the conversation appropriately. Furthermore, these models struggle\nwith hallucination, making them unsuitable for information access in sensitive\ndomains, such as legal or medical domains, where correctness of information\ngiven to users is critical. The recently introduced task Conversational Tree\nSearch (CTS) proposes the use of dialog graphs to avoid hallucination in\nsensitive domains, however, state-of-the-art agents are Reinforcement Learning\n(RL) based and require long training times, despite excelling at dialog\nstrategy. This paper introduces a novel zero-shot method for controllable CTS\nagents, where LLMs guide the dialog planning through domain graphs by searching\nand pruning relevant graph nodes based on user interaction preferences. We show\nthat these agents significantly outperform state-of-the-art CTS agents\n($p<0.0001$; Barnard Exact test) in simulation. This generalizes to all\navailable CTS domains. Finally, we perform user evaluation to test the agent's\nperformance in the wild, showing that our policy significantly ($p<0.05$;\nBarnard Exact) improves task-success compared to the state-of-the-art RL-based\nCTS agent."
                },
                "authors": [
                    {
                        "name": "Dirk Väth"
                    },
                    {
                        "name": "Ngoc Thang Vu"
                    }
                ],
                "author_detail": {
                    "name": "Ngoc Thang Vu"
                },
                "author": "Ngoc Thang Vu",
                "arxiv_comment": "This paper has been accepted for publication at the AAAI 2022\n  Workshop on Planning in the Era of LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18632v2",
                "updated": "2025-03-04T16:20:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    20,
                    59,
                    1,
                    63,
                    0
                ],
                "published": "2025-01-27T22:07:52Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    7,
                    52,
                    0,
                    27,
                    0
                ],
                "title": "Towards Safe AI Clinicians: A Comprehensive Study on Large Language\n  Model Jailbreaking in Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Safe AI Clinicians: A Comprehensive Study on Large Language\n  Model Jailbreaking in Healthcare"
                },
                "summary": "Large language models (LLMs) are increasingly utilized in healthcare\napplications. However, their deployment in clinical practice raises significant\nsafety concerns, including the potential spread of harmful information. This\nstudy systematically assesses the vulnerabilities of seven LLMs to three\nadvanced black-box jailbreaking techniques within medical contexts. To quantify\nthe effectiveness of these techniques, we propose an automated and\ndomain-adapted agentic evaluation pipeline. Experiment results indicate that\nleading commercial and open-source LLMs are highly vulnerable to medical\njailbreaking attacks. To bolster model safety and reliability, we further\ninvestigate the effectiveness of Continual Fine-Tuning (CFT) in defending\nagainst medical adversarial attacks. Our findings underscore the necessity for\nevolving attack methods evaluation, domain-specific safety alignment, and LLM\nsafety-utility balancing. This research offers actionable insights for\nadvancing the safety and reliability of AI clinicians, contributing to ethical\nand effective AI deployment in healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized in healthcare\napplications. However, their deployment in clinical practice raises significant\nsafety concerns, including the potential spread of harmful information. This\nstudy systematically assesses the vulnerabilities of seven LLMs to three\nadvanced black-box jailbreaking techniques within medical contexts. To quantify\nthe effectiveness of these techniques, we propose an automated and\ndomain-adapted agentic evaluation pipeline. Experiment results indicate that\nleading commercial and open-source LLMs are highly vulnerable to medical\njailbreaking attacks. To bolster model safety and reliability, we further\ninvestigate the effectiveness of Continual Fine-Tuning (CFT) in defending\nagainst medical adversarial attacks. Our findings underscore the necessity for\nevolving attack methods evaluation, domain-specific safety alignment, and LLM\nsafety-utility balancing. This research offers actionable insights for\nadvancing the safety and reliability of AI clinicians, contributing to ethical\nand effective AI deployment in healthcare."
                },
                "authors": [
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Qian Lou"
                    },
                    {
                        "name": "Yanshan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanshan Wang"
                },
                "author": "Yanshan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02756v1",
                "updated": "2025-03-04T16:20:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    20,
                    52,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:20:52Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    20,
                    52,
                    1,
                    63,
                    0
                ],
                "title": "BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched\n  Prompting and Prompt Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched\n  Prompting and Prompt Compression"
                },
                "summary": "Recent advancements in Large Language Model (LLM)-based Natural Language\nGeneration evaluation have largely focused on single-example prompting,\nresulting in significant token overhead and computational inefficiencies. In\nthis work, we introduce BatchGEMBA-MQM, a framework that integrates batched\nprompting with the GEMBA-MQM metric for machine translation evaluation. Our\napproach aggregates multiple translation examples into a single prompt,\nreducing token usage by 2-4 times (depending on the batch size) relative to\nsingle-example prompting. Furthermore, we propose a batching-aware prompt\ncompression model that achieves an additional token reduction of 13-15% on\naverage while also showing ability to help mitigate batching-induced quality\ndegradation. Evaluations across several LLMs (GPT-4o, GPT-4o-mini, Mistral\nSmall, Phi4, and CommandR7B) and varying batch sizes reveal that while batching\ngenerally negatively affects quality (but sometimes not substantially), prompt\ncompression does not degrade further, and in some cases, recovers quality loss.\nFor instance, GPT-4o retains over 90% of its baseline performance at a batch\nsize of 4 when compression is applied, compared to a 44.6% drop without\ncompression. We plan to release our code and trained models at\nhttps://github.com/NL2G/batchgemba to support future research in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Model (LLM)-based Natural Language\nGeneration evaluation have largely focused on single-example prompting,\nresulting in significant token overhead and computational inefficiencies. In\nthis work, we introduce BatchGEMBA-MQM, a framework that integrates batched\nprompting with the GEMBA-MQM metric for machine translation evaluation. Our\napproach aggregates multiple translation examples into a single prompt,\nreducing token usage by 2-4 times (depending on the batch size) relative to\nsingle-example prompting. Furthermore, we propose a batching-aware prompt\ncompression model that achieves an additional token reduction of 13-15% on\naverage while also showing ability to help mitigate batching-induced quality\ndegradation. Evaluations across several LLMs (GPT-4o, GPT-4o-mini, Mistral\nSmall, Phi4, and CommandR7B) and varying batch sizes reveal that while batching\ngenerally negatively affects quality (but sometimes not substantially), prompt\ncompression does not degrade further, and in some cases, recovers quality loss.\nFor instance, GPT-4o retains over 90% of its baseline performance at a batch\nsize of 4 when compression is applied, compared to a 44.6% drop without\ncompression. We plan to release our code and trained models at\nhttps://github.com/NL2G/batchgemba to support future research in this domain."
                },
                "authors": [
                    {
                        "name": "Daniil Larionov"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02745v1",
                "updated": "2025-03-04T16:10:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    10,
                    42,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:10:42Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    10,
                    42,
                    1,
                    63,
                    0
                ],
                "title": "ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse\n  Points",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse\n  Points"
                },
                "summary": "We introduce ArcPro, a novel learning framework built on architectural\nprograms to recover structured 3D abstractions from highly sparse and\nlow-quality point clouds. Specifically, we design a domain-specific language\n(DSL) to hierarchically represent building structures as a program, which can\nbe efficiently converted into a mesh. We bridge feedforward and inverse\nprocedural modeling by using a feedforward process for training data synthesis,\nallowing the network to make reverse predictions. We train an encoder-decoder\non the points-program pairs to establish a mapping from unstructured point\nclouds to architectural programs, where a 3D convolutional encoder extracts\npoint cloud features and a transformer decoder autoregressively predicts the\nprograms in a tokenized form. Inference by our method is highly efficient and\nproduces plausible and faithful 3D abstractions. Comprehensive experiments\ndemonstrate that ArcPro outperforms both traditional architectural proxy\nreconstruction and learning-based abstraction methods. We further explore its\npotential to work with multi-view image and natural language inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ArcPro, a novel learning framework built on architectural\nprograms to recover structured 3D abstractions from highly sparse and\nlow-quality point clouds. Specifically, we design a domain-specific language\n(DSL) to hierarchically represent building structures as a program, which can\nbe efficiently converted into a mesh. We bridge feedforward and inverse\nprocedural modeling by using a feedforward process for training data synthesis,\nallowing the network to make reverse predictions. We train an encoder-decoder\non the points-program pairs to establish a mapping from unstructured point\nclouds to architectural programs, where a 3D convolutional encoder extracts\npoint cloud features and a transformer decoder autoregressively predicts the\nprograms in a tokenized form. Inference by our method is highly efficient and\nproduces plausible and faithful 3D abstractions. Comprehensive experiments\ndemonstrate that ArcPro outperforms both traditional architectural proxy\nreconstruction and learning-based abstraction methods. We further explore its\npotential to work with multi-view image and natural language inputs."
                },
                "authors": [
                    {
                        "name": "Qirui Huang"
                    },
                    {
                        "name": "Runze Zhang"
                    },
                    {
                        "name": "Kangjun Liu"
                    },
                    {
                        "name": "Minglun Gong"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Hui Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Huang"
                },
                "author": "Hui Huang",
                "arxiv_comment": "CVPR 2025 (Patent Protected); Project page:\n  https://vcc.tech/research/2025/ArcPro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02741v1",
                "updated": "2025-03-04T16:05:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    5,
                    13,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:05:13Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    5,
                    13,
                    1,
                    63,
                    0
                ],
                "title": "Seeded Poisson Factorization: Leveraging domain knowledge to fit topic\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeded Poisson Factorization: Leveraging domain knowledge to fit topic\n  models"
                },
                "summary": "Topic models are widely used for discovering latent thematic structures in\nlarge text corpora, yet traditional unsupervised methods often struggle to\nalign with predefined conceptual domains. This paper introduces Seeded Poisson\nFactorization (SPF), a novel approach that extends the Poisson Factorization\nframework by incorporating domain knowledge through seed words. SPF enables a\nmore interpretable and structured topic discovery by modifying the prior\ndistribution of topic-specific term intensities, assigning higher initial rates\nto predefined seed words. The model is estimated using variational inference\nwith stochastic gradient optimization, ensuring scalability to large datasets.\n  We apply SPF to an Amazon customer feedback dataset, leveraging predefined\nproduct categories as guiding structures. Our evaluation demonstrates that SPF\nachieves superior classification performance compared to alternative guided\ntopic models, particularly in terms of computational efficiency and predictive\nperformance. Furthermore, robustness checks highlight SPF's ability to\nadaptively balance domain knowledge and data-driven topic discovery, even in\ncases of imperfect seed word selection. These results establish SPF as a\npowerful and scalable alternative for integrating expert knowledge into topic\nmodeling, enhancing both interpretability and efficiency in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic models are widely used for discovering latent thematic structures in\nlarge text corpora, yet traditional unsupervised methods often struggle to\nalign with predefined conceptual domains. This paper introduces Seeded Poisson\nFactorization (SPF), a novel approach that extends the Poisson Factorization\nframework by incorporating domain knowledge through seed words. SPF enables a\nmore interpretable and structured topic discovery by modifying the prior\ndistribution of topic-specific term intensities, assigning higher initial rates\nto predefined seed words. The model is estimated using variational inference\nwith stochastic gradient optimization, ensuring scalability to large datasets.\n  We apply SPF to an Amazon customer feedback dataset, leveraging predefined\nproduct categories as guiding structures. Our evaluation demonstrates that SPF\nachieves superior classification performance compared to alternative guided\ntopic models, particularly in terms of computational efficiency and predictive\nperformance. Furthermore, robustness checks highlight SPF's ability to\nadaptively balance domain knowledge and data-driven topic discovery, even in\ncases of imperfect seed word selection. These results establish SPF as a\npowerful and scalable alternative for integrating expert knowledge into topic\nmodeling, enhancing both interpretability and efficiency in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Bernd Prostmaier"
                    },
                    {
                        "name": "Jan Vávra"
                    },
                    {
                        "name": "Bettina Grün"
                    },
                    {
                        "name": "Paul Hofmarcher"
                    }
                ],
                "author_detail": {
                    "name": "Paul Hofmarcher"
                },
                "author": "Paul Hofmarcher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02737v1",
                "updated": "2025-03-04T15:56:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    56,
                    43,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:56:43Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    56,
                    43,
                    1,
                    63,
                    0
                ],
                "title": "Large Language Models for Multilingual Previously Fact-Checked Claim\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Multilingual Previously Fact-Checked Claim\n  Detection"
                },
                "summary": "In our era of widespread false information, human fact-checkers often face\nthe challenge of duplicating efforts when verifying claims that may have\nalready been addressed in other countries or languages. As false information\ntranscends linguistic boundaries, the ability to automatically detect\npreviously fact-checked claims across languages has become an increasingly\nimportant task. This paper presents the first comprehensive evaluation of large\nlanguage models (LLMs) for multilingual previously fact-checked claim\ndetection. We assess seven LLMs across 20 languages in both monolingual and\ncross-lingual settings. Our results show that while LLMs perform well for\nhigh-resource languages, they struggle with low-resource languages. Moreover,\ntranslating original texts into English proved to be beneficial for\nlow-resource languages. These findings highlight the potential of LLMs for\nmultilingual previously fact-checked claim detection and provide a foundation\nfor further research on this promising application of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our era of widespread false information, human fact-checkers often face\nthe challenge of duplicating efforts when verifying claims that may have\nalready been addressed in other countries or languages. As false information\ntranscends linguistic boundaries, the ability to automatically detect\npreviously fact-checked claims across languages has become an increasingly\nimportant task. This paper presents the first comprehensive evaluation of large\nlanguage models (LLMs) for multilingual previously fact-checked claim\ndetection. We assess seven LLMs across 20 languages in both monolingual and\ncross-lingual settings. Our results show that while LLMs perform well for\nhigh-resource languages, they struggle with low-resource languages. Moreover,\ntranslating original texts into English proved to be beneficial for\nlow-resource languages. These findings highlight the potential of LLMs for\nmultilingual previously fact-checked claim detection and provide a foundation\nfor further research on this promising application of LLMs."
                },
                "authors": [
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Matúš Pikuliak"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Michal Gregor"
                    },
                    {
                        "name": "Marián Šimko"
                    }
                ],
                "author_detail": {
                    "name": "Marián Šimko"
                },
                "author": "Marián Šimko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02733v1",
                "updated": "2025-03-04T15:54:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    54,
                    57,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:54:57Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    54,
                    57,
                    1,
                    63,
                    0
                ],
                "title": "UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural\n  Video Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural\n  Video Compression"
                },
                "summary": "Implicit Neural Representations (INRs) have demonstrated significant\npotential in video compression by representing videos as neural networks.\nHowever, as the number of frames increases, the memory consumption for training\nand inference increases substantially, posing challenges in\nresource-constrained scenarios. Inspired by the success of traditional video\ncompression frameworks, which process video frame by frame and can efficiently\ncompress long videos, we adopt this modeling strategy for INRs to decrease\nmemory consumption, while aiming to unify the frameworks from the perspective\nof timeline-based autoregressive modeling. In this work, we present a novel\nunderstanding of INR models from an autoregressive (AR) perspective and\nintroduce a Unified AutoRegressive Framework for memory-efficient Neural Video\nCompression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural\nvideo compression under a unified autoregressive paradigm. It partitions videos\ninto several clips and processes each clip using a different INR model\ninstance, leveraging the advantages of both compression frameworks while\nallowing seamless adaptation to either in form. To further reduce temporal\nredundancy between clips, we design two modules to optimize the initialization,\ntraining, and compression of these model parameters. UAR-NVC supports\nadjustable latencies by varying the clip length. Extensive experimental results\ndemonstrate that UAR-NVC, with its flexible video clip setting, can adapt to\nresource-constrained environments and significantly improve performance\ncompared to different baseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Neural Representations (INRs) have demonstrated significant\npotential in video compression by representing videos as neural networks.\nHowever, as the number of frames increases, the memory consumption for training\nand inference increases substantially, posing challenges in\nresource-constrained scenarios. Inspired by the success of traditional video\ncompression frameworks, which process video frame by frame and can efficiently\ncompress long videos, we adopt this modeling strategy for INRs to decrease\nmemory consumption, while aiming to unify the frameworks from the perspective\nof timeline-based autoregressive modeling. In this work, we present a novel\nunderstanding of INR models from an autoregressive (AR) perspective and\nintroduce a Unified AutoRegressive Framework for memory-efficient Neural Video\nCompression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural\nvideo compression under a unified autoregressive paradigm. It partitions videos\ninto several clips and processes each clip using a different INR model\ninstance, leveraging the advantages of both compression frameworks while\nallowing seamless adaptation to either in form. To further reduce temporal\nredundancy between clips, we design two modules to optimize the initialization,\ntraining, and compression of these model parameters. UAR-NVC supports\nadjustable latencies by varying the clip length. Extensive experimental results\ndemonstrate that UAR-NVC, with its flexible video clip setting, can adapt to\nresource-constrained environments and significantly improve performance\ncompared to different baseline models."
                },
                "authors": [
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Xinfeng Zhang"
                    },
                    {
                        "name": "Gai Zhang"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Lv Tang"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02725v1",
                "updated": "2025-03-04T15:44:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    44,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:44:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    44,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "A Joint Visual Compression and Perception Framework for Neuralmorphic\n  Spiking Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Joint Visual Compression and Perception Framework for Neuralmorphic\n  Spiking Camera"
                },
                "summary": "The advent of neuralmorphic spike cameras has garnered significant attention\nfor their ability to capture continuous motion with unparalleled temporal\nresolution.However, this imaging attribute necessitates considerable resources\nfor binary spike data storage and transmission.In light of compression and\nspike-driven intelligent applications, we present the notion of Spike Coding\nfor Intelligence (SCI), wherein spike sequences are compressed and optimized\nfor both bit-rate and task performance.Drawing inspiration from the mammalian\nvision system, we propose a dual-pathway architecture for separate processing\nof spatial semantics and motion information, which is then merged to produce\nfeatures for compression.A refinement scheme is also introduced to ensure\nconsistency between decoded features and motion vectors.We further propose a\ntemporal regression approach that integrates various motion dynamics,\ncapitalizing on the advancements in warping and deformation\nsimultaneously.Comprehensive experiments demonstrate our scheme achieves\nstate-of-the-art (SOTA) performance for spike compression and analysis.We\nachieve an average 17.25% BD-rate reduction compared to SOTA codecs and a 4.3%\naccuracy improvement over SpiReco for spike-based classification, with 88.26%\ncomplexity reduction and 42.41% inference time saving on the encoding side.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of neuralmorphic spike cameras has garnered significant attention\nfor their ability to capture continuous motion with unparalleled temporal\nresolution.However, this imaging attribute necessitates considerable resources\nfor binary spike data storage and transmission.In light of compression and\nspike-driven intelligent applications, we present the notion of Spike Coding\nfor Intelligence (SCI), wherein spike sequences are compressed and optimized\nfor both bit-rate and task performance.Drawing inspiration from the mammalian\nvision system, we propose a dual-pathway architecture for separate processing\nof spatial semantics and motion information, which is then merged to produce\nfeatures for compression.A refinement scheme is also introduced to ensure\nconsistency between decoded features and motion vectors.We further propose a\ntemporal regression approach that integrates various motion dynamics,\ncapitalizing on the advancements in warping and deformation\nsimultaneously.Comprehensive experiments demonstrate our scheme achieves\nstate-of-the-art (SOTA) performance for spike compression and analysis.We\nachieve an average 17.25% BD-rate reduction compared to SOTA codecs and a 4.3%\naccuracy improvement over SpiReco for spike-based classification, with 88.26%\ncomplexity reduction and 42.41% inference time saving on the encoding side."
                },
                "authors": [
                    {
                        "name": "Kexiang Feng"
                    },
                    {
                        "name": "Chuanmin Jia"
                    },
                    {
                        "name": "Siwei Ma"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01625v2",
                "updated": "2025-03-04T15:33:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    33,
                    16,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-03T15:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    15,
                    0,
                    36,
                    0,
                    62,
                    0
                ],
                "title": "Annotating and Inferring Compositional Structures in Numeral Systems\n  Across Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotating and Inferring Compositional Structures in Numeral Systems\n  Across Languages"
                },
                "summary": "Numeral systems across the world's languages vary in fascinating ways, both\nregarding their synchronic structure and the diachronic processes that\ndetermined how they evolved in their current shape. For a proper comparison of\nnumeral systems across different languages, however, it is important to code\nthem in a standardized form that allows for the comparison of basic properties.\nHere, we present a simple but effective coding scheme for numeral annotation,\nalong with a workflow that helps to code numeral systems in a computer-assisted\nmanner, providing sample data for numerals from 1 to 40 in 25 typologically\ndiverse languages. We perform a thorough analysis of the sample, focusing on\nthe systematic comparison between the underlying and the surface morphological\nstructure. We further experiment with automated models for morpheme\nsegmentation, where we find allomorphy as the major reason for segmentation\nerrors. Finally, we show that subword tokenization algorithms are not viable\nfor discovering morphemes in low-resource scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numeral systems across the world's languages vary in fascinating ways, both\nregarding their synchronic structure and the diachronic processes that\ndetermined how they evolved in their current shape. For a proper comparison of\nnumeral systems across different languages, however, it is important to code\nthem in a standardized form that allows for the comparison of basic properties.\nHere, we present a simple but effective coding scheme for numeral annotation,\nalong with a workflow that helps to code numeral systems in a computer-assisted\nmanner, providing sample data for numerals from 1 to 40 in 25 typologically\ndiverse languages. We perform a thorough analysis of the sample, focusing on\nthe systematic comparison between the underlying and the surface morphological\nstructure. We further experiment with automated models for morpheme\nsegmentation, where we find allomorphy as the major reason for segmentation\nerrors. Finally, we show that subword tokenization algorithms are not viable\nfor discovering morphemes in low-resource scenarios."
                },
                "authors": [
                    {
                        "name": "Arne Rubehn"
                    },
                    {
                        "name": "Christoph Rzymski"
                    },
                    {
                        "name": "Luca Ciucci"
                    },
                    {
                        "name": "Kellen Parker van Dam"
                    },
                    {
                        "name": "Alžběta Kučerová"
                    },
                    {
                        "name": "Katja Bocklage"
                    },
                    {
                        "name": "David Snee"
                    },
                    {
                        "name": "Abishek Stephen"
                    },
                    {
                        "name": "Johann-Mattis List"
                    }
                ],
                "author_detail": {
                    "name": "Johann-Mattis List"
                },
                "author": "Johann-Mattis List",
                "arxiv_comment": "Submitted to the 7th Workshop on Research in Computational Linguistic\n  Typology and Multilingual NLP (SIGTYP)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02718v1",
                "updated": "2025-03-04T15:32:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    32,
                    59,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:32:59Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    32,
                    59,
                    1,
                    63,
                    0
                ],
                "title": "Evaluating Knowledge Generation and Self-Refinement Strategies for\n  LLM-based Column Type Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Knowledge Generation and Self-Refinement Strategies for\n  LLM-based Column Type Annotation"
                },
                "summary": "Understanding the semantics of columns in relational tables is an important\npre-processing step for indexing data lakes in order to provide rich data\nsearch. An approach to establishing such understanding is column type\nannotation (CTA) where the goal is to annotate table columns with terms from a\ngiven vocabulary. This paper experimentally compares different knowledge\ngeneration and self-refinement strategies for LLM-based column type annotation.\nThe strategies include using LLMs to generate term definitions, error-based\nrefinement of term definitions, self-correction, and fine-tuning using examples\nand term definitions. We evaluate these strategies along two dimensions:\neffectiveness measured as F1 performance and efficiency measured in terms of\ntoken usage and cost. Our experiments show that the best performing strategy\ndepends on the model/dataset combination. We find that using training data to\ngenerate label definitions outperforms using the same data as demonstrations\nfor in-context learning for two out of three datasets using OpenAI models. The\nexperiments further show that using the LLMs to refine label definitions brings\nan average increase of 3.9% F1 in 10 out of 12 setups compared to the\nperformance of the non-refined definitions. Combining fine-tuned models with\nself-refined term definitions results in the overall highest performance,\noutperforming zero-shot prompting fine-tuned models by at least 3% in F1 score.\nThe costs analysis shows that while reaching similar F1 score, self-refinement\nvia prompting is more cost efficient for use cases requiring smaller amounts of\ntables to be annotated while fine-tuning is more efficient for large amounts of\ntables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the semantics of columns in relational tables is an important\npre-processing step for indexing data lakes in order to provide rich data\nsearch. An approach to establishing such understanding is column type\nannotation (CTA) where the goal is to annotate table columns with terms from a\ngiven vocabulary. This paper experimentally compares different knowledge\ngeneration and self-refinement strategies for LLM-based column type annotation.\nThe strategies include using LLMs to generate term definitions, error-based\nrefinement of term definitions, self-correction, and fine-tuning using examples\nand term definitions. We evaluate these strategies along two dimensions:\neffectiveness measured as F1 performance and efficiency measured in terms of\ntoken usage and cost. Our experiments show that the best performing strategy\ndepends on the model/dataset combination. We find that using training data to\ngenerate label definitions outperforms using the same data as demonstrations\nfor in-context learning for two out of three datasets using OpenAI models. The\nexperiments further show that using the LLMs to refine label definitions brings\nan average increase of 3.9% F1 in 10 out of 12 setups compared to the\nperformance of the non-refined definitions. Combining fine-tuned models with\nself-refined term definitions results in the overall highest performance,\noutperforming zero-shot prompting fine-tuned models by at least 3% in F1 score.\nThe costs analysis shows that while reaching similar F1 score, self-refinement\nvia prompting is more cost efficient for use cases requiring smaller amounts of\ntables to be annotated while fine-tuning is more efficient for large amounts of\ntables."
                },
                "authors": [
                    {
                        "name": "Keti Korini"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20704v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20704v3",
                "updated": "2025-03-04T15:30:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    30,
                    35,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-28T04:25:42Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    4,
                    25,
                    42,
                    4,
                    59,
                    0
                ],
                "title": "Fuzzy Speculative Decoding for a Tunable Accuracy-Runtime Tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzzy Speculative Decoding for a Tunable Accuracy-Runtime Tradeoff"
                },
                "summary": "Speculative Decoding (SD) enforces strict distributional equivalence to the\ntarget model, limiting potential speed ups as distributions of near-equivalence\nachieve comparable outcomes in many cases. Furthermore, enforcing\ndistributional equivalence means that users are unable to trade deviations from\nthe target model distribution for further inference speed gains. To address\nthese limitations, we introduce Fuzzy Speculative Decoding (FSD) - a decoding\nalgorithm that generalizes SD by accepting candidate tokens purely based on the\ndivergences between the target and draft model distributions. By allowing for\ncontrolled divergence from the target model, FSD enables users to flexibly\ntrade generation quality for inference speed. Across several benchmarks, our\nmethod is able to achieve significant runtime improvements of over 5 tokens per\nsecond faster than SD at only an approximate 2% absolute reduction in benchmark\naccuracy. In many cases, FSD is even able to match SD benchmark accuracy at\nover 2 tokens per second faster, demonstrating that distributional equivalence\nis not necessary to maintain target model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Decoding (SD) enforces strict distributional equivalence to the\ntarget model, limiting potential speed ups as distributions of near-equivalence\nachieve comparable outcomes in many cases. Furthermore, enforcing\ndistributional equivalence means that users are unable to trade deviations from\nthe target model distribution for further inference speed gains. To address\nthese limitations, we introduce Fuzzy Speculative Decoding (FSD) - a decoding\nalgorithm that generalizes SD by accepting candidate tokens purely based on the\ndivergences between the target and draft model distributions. By allowing for\ncontrolled divergence from the target model, FSD enables users to flexibly\ntrade generation quality for inference speed. Across several benchmarks, our\nmethod is able to achieve significant runtime improvements of over 5 tokens per\nsecond faster than SD at only an approximate 2% absolute reduction in benchmark\naccuracy. In many cases, FSD is even able to match SD benchmark accuracy at\nover 2 tokens per second faster, demonstrating that distributional equivalence\nis not necessary to maintain target model performance."
                },
                "authors": [
                    {
                        "name": "Maximilian Holsman"
                    },
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20704v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20704v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18658v2",
                "updated": "2025-03-04T15:26:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    26,
                    19,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-25T21:37:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    21,
                    37,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "Assistance or Disruption? Exploring and Evaluating the Design and\n  Trade-offs of Proactive AI Programming Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistance or Disruption? Exploring and Evaluating the Design and\n  Trade-offs of Proactive AI Programming Support"
                },
                "summary": "AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow."
                },
                "authors": [
                    {
                        "name": "Kevin Pu"
                    },
                    {
                        "name": "Daniel Lazaro"
                    },
                    {
                        "name": "Ian Arawjo"
                    },
                    {
                        "name": "Haijun Xia"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Tovi Grossman"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "arxiv_doi": "10.1145/3706598.3713357",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713357",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.18658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02702v1",
                "updated": "2025-03-04T15:18:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    18,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:18:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    18,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "RedChronos: A Large Language Model-Based Log Analysis System for Insider\n  Threat Detection in Enterprises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RedChronos: A Large Language Model-Based Log Analysis System for Insider\n  Threat Detection in Enterprises"
                },
                "summary": "Internal threat detection aims to address security threats within\norganizations or enterprises by identifying potential or already occurring\nmalicious threats within vast amounts of logs. Although organizations or\nenterprises have dedicated personnel responsible for reviewing these logs, it\nis impossible to manually examine all logs entirely. In response to the vast\nnumber of logs, we propose a system called RedChronos, which is a Large\nLanguage Model-Based Log Analysis System. This system incorporates innovative\nimprovements over previous research by employing Query-Aware Weighted Voting\nand a Semantic Expansion-based Genetic Algorithm with LLM-driven Mutations. On\nthe public datasets CERT 4.2 and 5.2, RedChronos outperforms or matches\nexisting approaches in terms of accuracy, precision, and detection rate.\nMoreover, RedChronos reduces the need for manual intervention in security log\nreviews by 90\\% in the Xiaohongshu SOC. Therefore, our RedChronos system\ndemonstrates exceptional performance in handling Internal Threat Detection\n(IDT) tasks, providing innovative solutions for these challenges. We believe\nthat future research can continue to enhance the system's performance in IDT\ntasks while also reducing the response time to internal risk events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal threat detection aims to address security threats within\norganizations or enterprises by identifying potential or already occurring\nmalicious threats within vast amounts of logs. Although organizations or\nenterprises have dedicated personnel responsible for reviewing these logs, it\nis impossible to manually examine all logs entirely. In response to the vast\nnumber of logs, we propose a system called RedChronos, which is a Large\nLanguage Model-Based Log Analysis System. This system incorporates innovative\nimprovements over previous research by employing Query-Aware Weighted Voting\nand a Semantic Expansion-based Genetic Algorithm with LLM-driven Mutations. On\nthe public datasets CERT 4.2 and 5.2, RedChronos outperforms or matches\nexisting approaches in terms of accuracy, precision, and detection rate.\nMoreover, RedChronos reduces the need for manual intervention in security log\nreviews by 90\\% in the Xiaohongshu SOC. Therefore, our RedChronos system\ndemonstrates exceptional performance in handling Internal Threat Detection\n(IDT) tasks, providing innovative solutions for these challenges. We believe\nthat future research can continue to enhance the system's performance in IDT\ntasks while also reducing the response time to internal risk events."
                },
                "authors": [
                    {
                        "name": "Chenyu Li"
                    },
                    {
                        "name": "Zhengjia Zhu"
                    },
                    {
                        "name": "Jiyan He"
                    },
                    {
                        "name": "Xiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiu Zhang"
                },
                "author": "Xiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02701v1",
                "updated": "2025-03-04T15:17:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    17,
                    57,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:17:57Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    17,
                    57,
                    1,
                    63,
                    0
                ],
                "title": "MindBridge: Scalable and Cross-Model Knowledge Editing via\n  Memory-Augmented Modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindBridge: Scalable and Cross-Model Knowledge Editing via\n  Memory-Augmented Modality"
                },
                "summary": "Knowledge editing is a technique for efficiently and accurately updating the\nknowledge of large language models (LLMs) to alleviate obsolescence and correct\nerrors. However, most existing methods overfit to specific models, causing\nedited knowledge to be discarded during each LLM update and requiring frequent\nre-editing, which is particularly burdensome in today's rapidly evolving\nopen-source community. To address this issue, we propose the problem of\ncross-model knowledge editing and introduce MindBridge, a scalable solution\ninspired by the low coupling between modality processing and LLMs in\nmulti-modal models. MindBridge introduces the novel concept of memory modality,\nwhich encodes edited knowledge as an independent modality. It first performs\nLLM-agnostic pre-training of the memory modality and then integrates it with\nvarious LLMs. Extensive experiments on multiple LLMs and popular knowledge\nediting datasets demonstrate that MindBridge achieves superior performance even\nin editing tens of thousands of knowledge entries and can flexibly adapt to\ndifferent LLMs. Our code is available at\nhttps://github.com/CrashBugger/MindBridge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing is a technique for efficiently and accurately updating the\nknowledge of large language models (LLMs) to alleviate obsolescence and correct\nerrors. However, most existing methods overfit to specific models, causing\nedited knowledge to be discarded during each LLM update and requiring frequent\nre-editing, which is particularly burdensome in today's rapidly evolving\nopen-source community. To address this issue, we propose the problem of\ncross-model knowledge editing and introduce MindBridge, a scalable solution\ninspired by the low coupling between modality processing and LLMs in\nmulti-modal models. MindBridge introduces the novel concept of memory modality,\nwhich encodes edited knowledge as an independent modality. It first performs\nLLM-agnostic pre-training of the memory modality and then integrates it with\nvarious LLMs. Extensive experiments on multiple LLMs and popular knowledge\nediting datasets demonstrate that MindBridge achieves superior performance even\nin editing tens of thousands of knowledge entries and can flexibly adapt to\ndifferent LLMs. Our code is available at\nhttps://github.com/CrashBugger/MindBridge."
                },
                "authors": [
                    {
                        "name": "Shuaike Li"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02698v1",
                "updated": "2025-03-04T15:14:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    14,
                    41,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:14:41Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    14,
                    41,
                    1,
                    63,
                    0
                ],
                "title": "FlowPlan: Zero-Shot Task Planning with LLM Flow Engineering for Robotic\n  Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowPlan: Zero-Shot Task Planning with LLM Flow Engineering for Robotic\n  Instruction Following"
                },
                "summary": "Robotic instruction following tasks require seamless integration of visual\nperception, task planning, target localization, and motion execution. However,\nexisting task planning methods for instruction following are either data-driven\nor underperform in zero-shot scenarios due to difficulties in grounding lengthy\ninstructions into actionable plans under operational constraints. To address\nthis, we propose FlowPlan, a structured multi-stage LLM workflow that elevates\nzero-shot pipeline and bridges the performance gap between zero-shot and\ndata-driven in-context learning methods. By decomposing the planning process\ninto modular stages--task information retrieval, language-level reasoning,\nsymbolic-level planning, and logical evaluation--FlowPlan generates logically\ncoherent action sequences while adhering to operational constraints and further\nextracts contextual guidance for precise instance-level target localization.\nBenchmarked on the ALFRED and validated in real-world applications, our method\nachieves competitive performance relative to data-driven in-context learning\nmethods and demonstrates adaptability across diverse environments. This work\nadvances zero-shot task planning in robotic systems without reliance on labeled\ndata. Project website: https://instruction-following-project.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic instruction following tasks require seamless integration of visual\nperception, task planning, target localization, and motion execution. However,\nexisting task planning methods for instruction following are either data-driven\nor underperform in zero-shot scenarios due to difficulties in grounding lengthy\ninstructions into actionable plans under operational constraints. To address\nthis, we propose FlowPlan, a structured multi-stage LLM workflow that elevates\nzero-shot pipeline and bridges the performance gap between zero-shot and\ndata-driven in-context learning methods. By decomposing the planning process\ninto modular stages--task information retrieval, language-level reasoning,\nsymbolic-level planning, and logical evaluation--FlowPlan generates logically\ncoherent action sequences while adhering to operational constraints and further\nextracts contextual guidance for precise instance-level target localization.\nBenchmarked on the ALFRED and validated in real-world applications, our method\nachieves competitive performance relative to data-driven in-context learning\nmethods and demonstrates adaptability across diverse environments. This work\nadvances zero-shot task planning in robotic systems without reliance on labeled\ndata. Project website: https://instruction-following-project.github.io/."
                },
                "authors": [
                    {
                        "name": "Zijun Lin"
                    },
                    {
                        "name": "Chao Tang"
                    },
                    {
                        "name": "Hanjing Ye"
                    },
                    {
                        "name": "Hong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hong Zhang"
                },
                "author": "Hong Zhang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12110v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12110v3",
                "updated": "2025-03-04T15:09:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    9,
                    10,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-17T18:36:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    36,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "A-MEM: Agentic Memory for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-MEM: Agentic Memory for LLM Agents"
                },
                "summary": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem."
                },
                "authors": [
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12110v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12110v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20903v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20903v4",
                "updated": "2025-03-04T15:05:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    5,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2024-12-30T12:29:02Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    12,
                    29,
                    2,
                    0,
                    365,
                    0
                ],
                "title": "WalkVLM:Aid Visually Impaired People Walking by Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WalkVLM:Aid Visually Impaired People Walking by Vision Language Model"
                },
                "summary": "Approximately 200 million individuals around the world suffer from varying\ndegrees of visual impairment, making it crucial to leverage AI technology to\noffer walking assistance for these people. With the recent progress of\nvision-language models (VLMs), applying VLMs to offer walking guidance has\nbecome popular. However, the existing methods of walking guidance are mainly\nbased on self-curated question-answering datasets that are not publicly\naccessible, without a standardized benchmark for training or evaluation.\nMoreover, walking assistance often requires real-time streaming video analysis\nand the generation of concise yet informative reminders, making VLMs struggle\ndue to excessive responses and low efficiency in inferences. In this paper, we\nintroduce the first large-scale dataset dedicated to walking assistance,\ncomprising 12,000 video-annotation pairs, to provide a unified benchmark for\ntraining and evaluating systems to help visually-impaired individuals walk.\nFurthermore, a WalkVLM model is proposed, which employs chain of thought for\nhierarchical planning to generate concise but informative reminders and\nutilizes temporal-aware adaptive prediction to reduce the temporal redundancy\nof reminders. Finally, we have established a solid benchmark for blind walking\ntask and verified the advantages of WalkVLM in stream video processing for this\ntask compared to other VLMs. Our dataset and code are available at\nhttps://walkvlm2024.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximately 200 million individuals around the world suffer from varying\ndegrees of visual impairment, making it crucial to leverage AI technology to\noffer walking assistance for these people. With the recent progress of\nvision-language models (VLMs), applying VLMs to offer walking guidance has\nbecome popular. However, the existing methods of walking guidance are mainly\nbased on self-curated question-answering datasets that are not publicly\naccessible, without a standardized benchmark for training or evaluation.\nMoreover, walking assistance often requires real-time streaming video analysis\nand the generation of concise yet informative reminders, making VLMs struggle\ndue to excessive responses and low efficiency in inferences. In this paper, we\nintroduce the first large-scale dataset dedicated to walking assistance,\ncomprising 12,000 video-annotation pairs, to provide a unified benchmark for\ntraining and evaluating systems to help visually-impaired individuals walk.\nFurthermore, a WalkVLM model is proposed, which employs chain of thought for\nhierarchical planning to generate concise but informative reminders and\nutilizes temporal-aware adaptive prediction to reduce the temporal redundancy\nof reminders. Finally, we have established a solid benchmark for blind walking\ntask and verified the advantages of WalkVLM in stream video processing for this\ntask compared to other VLMs. Our dataset and code are available at\nhttps://walkvlm2024.github.io."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Yuan"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ying Deng"
                    },
                    {
                        "name": "Jiapei Zhang"
                    },
                    {
                        "name": "Yeshuang Zhu"
                    },
                    {
                        "name": "Zexi Jia"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jinchao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jinchao Zhang"
                },
                "author": "Jinchao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20903v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20903v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02692v1",
                "updated": "2025-03-04T15:04:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    4,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:04:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    4,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "FinArena: A Human-Agent Collaboration Framework for Financial Market\n  Analysis and Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinArena: A Human-Agent Collaboration Framework for Financial Market\n  Analysis and Forecasting"
                },
                "summary": "To improve stock trend predictions and support personalized investment\ndecisions, this paper proposes FinArena, a novel Human-Agent collaboration\nframework. Inspired by the mixture of experts (MoE) approach, FinArena combines\nmultimodal financial data analysis with user interaction. The human module\nfeatures an interactive interface that captures individual risk preferences,\nallowing personalized investment strategies. The machine module utilizes a\nLarge Language Model-based (LLM-based) multi-agent system to integrate diverse\ndata sources, such as stock prices, news articles, and financial statements. To\naddress hallucinations in LLMs, FinArena employs the adaptive\nRetrieval-Augmented Generative (RAG) method for processing unstructured news\ndata. Finally, a universal expert agent makes investment decisions based on the\nfeatures extracted from multimodal data and investors' individual risk\npreferences. Extensive experiments show that FinArena surpasses both\ntraditional and state-of-the-art benchmarks in stock trend prediction and\nyields promising results in trading simulations across various risk profiles.\nThese findings highlight FinArena's potential to enhance investment outcomes by\naligning strategic insights with personalized risk considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve stock trend predictions and support personalized investment\ndecisions, this paper proposes FinArena, a novel Human-Agent collaboration\nframework. Inspired by the mixture of experts (MoE) approach, FinArena combines\nmultimodal financial data analysis with user interaction. The human module\nfeatures an interactive interface that captures individual risk preferences,\nallowing personalized investment strategies. The machine module utilizes a\nLarge Language Model-based (LLM-based) multi-agent system to integrate diverse\ndata sources, such as stock prices, news articles, and financial statements. To\naddress hallucinations in LLMs, FinArena employs the adaptive\nRetrieval-Augmented Generative (RAG) method for processing unstructured news\ndata. Finally, a universal expert agent makes investment decisions based on the\nfeatures extracted from multimodal data and investors' individual risk\npreferences. Extensive experiments show that FinArena surpasses both\ntraditional and state-of-the-art benchmarks in stock trend prediction and\nyields promising results in trading simulations across various risk profiles.\nThese findings highlight FinArena's potential to enhance investment outcomes by\naligning strategic insights with personalized risk considerations."
                },
                "authors": [
                    {
                        "name": "Congluo Xu"
                    },
                    {
                        "name": "Zhaobin Liu"
                    },
                    {
                        "name": "Ziyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ziyang Li"
                },
                "author": "Ziyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03291v2",
                "updated": "2025-03-04T15:03:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    3,
                    6,
                    1,
                    63,
                    0
                ],
                "published": "2025-01-06T08:20:04Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    8,
                    20,
                    4,
                    0,
                    6,
                    0
                ],
                "title": "ADePT: Adaptive Decomposed Prompt Tuning for Parameter-Efficient\n  Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADePT: Adaptive Decomposed Prompt Tuning for Parameter-Efficient\n  Fine-tuning"
                },
                "summary": "Prompt Tuning (PT) enables the adaptation of Pre-trained Large Language\nModels (PLMs) to downstream tasks by optimizing a small amount of soft virtual\ntokens, which are prepended to the input token embeddings. Recently, Decomposed\nPrompt Tuning (DePT) has demonstrated superior adaptation capabilities by\ndecomposing the soft prompt into a shorter soft prompt and a pair of low-rank\nmatrices. The product of the pair of low-rank matrices is added to the input\ntoken embeddings to offset them. Additionally, DePT achieves faster inference\ncompared to PT due to the shorter soft prompt. However, in this paper, we find\nthat the position-based token embedding offsets of DePT restrict its ability to\ngeneralize across diverse model inputs, and that the shared embedding offsets\nacross many token embeddings result in sub-optimization. To tackle these\nissues, we introduce Adaptive Decomposed Prompt Tuning (ADePT), which is\ncomposed of a short soft prompt and a shallow token-shared feed-forward neural\nnetwork. ADePT utilizes the token-shared feed-forward neural network to learn\nthe embedding offsets for each token, enabling adaptive embedding offsets that\nvary according to the model input and better optimization of token embedding\noffsets. This enables ADePT to achieve superior adaptation performance without\nrequiring more inference time or additional trainable parameters compared to\nvanilla PT and its variants. In comprehensive experiments across 23 natural\nlanguage processing tasks and 4 typical PLMs of different scales, ADePT\nconsistently surpasses the other leading parameter-efficient fine-tuning\nmethods, and even outperforms the full fine-tuning in certain scenarios. We\nalso provide a theoretical analysis towards ADePT. Code is available at\nhttps://github.com/HungerPWAY/ADePT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Tuning (PT) enables the adaptation of Pre-trained Large Language\nModels (PLMs) to downstream tasks by optimizing a small amount of soft virtual\ntokens, which are prepended to the input token embeddings. Recently, Decomposed\nPrompt Tuning (DePT) has demonstrated superior adaptation capabilities by\ndecomposing the soft prompt into a shorter soft prompt and a pair of low-rank\nmatrices. The product of the pair of low-rank matrices is added to the input\ntoken embeddings to offset them. Additionally, DePT achieves faster inference\ncompared to PT due to the shorter soft prompt. However, in this paper, we find\nthat the position-based token embedding offsets of DePT restrict its ability to\ngeneralize across diverse model inputs, and that the shared embedding offsets\nacross many token embeddings result in sub-optimization. To tackle these\nissues, we introduce Adaptive Decomposed Prompt Tuning (ADePT), which is\ncomposed of a short soft prompt and a shallow token-shared feed-forward neural\nnetwork. ADePT utilizes the token-shared feed-forward neural network to learn\nthe embedding offsets for each token, enabling adaptive embedding offsets that\nvary according to the model input and better optimization of token embedding\noffsets. This enables ADePT to achieve superior adaptation performance without\nrequiring more inference time or additional trainable parameters compared to\nvanilla PT and its variants. In comprehensive experiments across 23 natural\nlanguage processing tasks and 4 typical PLMs of different scales, ADePT\nconsistently surpasses the other leading parameter-efficient fine-tuning\nmethods, and even outperforms the full fine-tuning in certain scenarios. We\nalso provide a theoretical analysis towards ADePT. Code is available at\nhttps://github.com/HungerPWAY/ADePT."
                },
                "authors": [
                    {
                        "name": "Pengwei Tang"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.11858v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.11858v3",
                "updated": "2025-03-04T15:03:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    3,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2023-03-21T13:59:15Z",
                "published_parsed": [
                    2023,
                    3,
                    21,
                    13,
                    59,
                    15,
                    1,
                    80,
                    0
                ],
                "title": "Modeling Relational Patterns for Logical Query Answering over Knowledge\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Relational Patterns for Logical Query Answering over Knowledge\n  Graphs"
                },
                "summary": "Answering first-order logical (FOL) queries over knowledge graphs (KG)\nremains a challenging task mainly due to KG incompleteness. Query embedding\napproaches this problem by computing the low-dimensional vector representations\nof entities, relations, and logical queries. KGs exhibit relational patterns\nsuch as symmetry and composition and modeling the patterns can further enhance\nthe performance of query embedding models. However, the role of such patterns\nin answering FOL queries by query embedding models has not been yet studied in\nthe literature. In this paper, we fill in this research gap and empower FOL\nqueries reasoning with pattern inference by introducing an inductive bias that\nallows for learning relation patterns. To this end, we develop a novel query\nembedding method, RoConE, that defines query regions as geometric cones and\nalgebraic query operators by rotations in complex space. RoConE combines the\nadvantages of Cone as a well-specified geometric representation for query\nembedding, and also the rotation operator as a powerful algebraic operation for\npattern inference. Our experimental results on several benchmark datasets\nconfirm the advantage of relational patterns for enhancing logical query\nanswering task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering first-order logical (FOL) queries over knowledge graphs (KG)\nremains a challenging task mainly due to KG incompleteness. Query embedding\napproaches this problem by computing the low-dimensional vector representations\nof entities, relations, and logical queries. KGs exhibit relational patterns\nsuch as symmetry and composition and modeling the patterns can further enhance\nthe performance of query embedding models. However, the role of such patterns\nin answering FOL queries by query embedding models has not been yet studied in\nthe literature. In this paper, we fill in this research gap and empower FOL\nqueries reasoning with pattern inference by introducing an inductive bias that\nallows for learning relation patterns. To this end, we develop a novel query\nembedding method, RoConE, that defines query regions as geometric cones and\nalgebraic query operators by rotations in complex space. RoConE combines the\nadvantages of Cone as a well-specified geometric representation for query\nembedding, and also the rotation operator as a powerful algebraic operation for\npattern inference. Our experimental results on several benchmark datasets\nconfirm the advantage of relational patterns for enhancing logical query\nanswering task."
                },
                "authors": [
                    {
                        "name": "Yunjie He"
                    },
                    {
                        "name": "Mojtaba Nayyeri"
                    },
                    {
                        "name": "Bo Xiong"
                    },
                    {
                        "name": "Yuqicheng Zhu"
                    },
                    {
                        "name": "Evgeny Kharlamov"
                    },
                    {
                        "name": "Steffen Staab"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Staab"
                },
                "author": "Steffen Staab",
                "arxiv_comment": "The results reported in this paper are included in our accepted paper\n  arXiv:2407.09212 at ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.11858v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.11858v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02682v1",
                "updated": "2025-03-04T14:54:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    54,
                    45,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T14:54:45Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    54,
                    45,
                    1,
                    63,
                    0
                ],
                "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPO: Boosting LLM Agents with Meta Plan Optimization"
                },
                "summary": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios."
                },
                "authors": [
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Bingchan Zhao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02670v1",
                "updated": "2025-03-04T14:41:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    41,
                    5,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T14:41:05Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    41,
                    5,
                    1,
                    63,
                    0
                ],
                "title": "Multidimensional Consistency Improves Reasoning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multidimensional Consistency Improves Reasoning in Language Models"
                },
                "summary": "While Large language models (LLMs) have proved able to address some complex\nreasoning tasks, we also know that they are highly sensitive to input\nvariation, which can lead to different solution paths and final answers. Answer\nconsistency across input variations can thus be taken as a sign of stronger\nconfidence. Leveraging this insight, we introduce a framework, {\\em\nMultidimensional Reasoning Consistency} where, focusing on math problems,\nmodels are systematically pushed to diversify solution paths towards a final\nanswer, thereby testing them for answer consistency across multiple input\nvariations. We induce variations in (i) order of shots in prompt, (ii) problem\nphrasing, and (iii) languages used. Extensive experiments on a large range of\nopen-source state-of-the-art LLMs of various sizes show that reasoning\nconsistency differs by variation dimension, and that by aggregating consistency\nacross dimensions, our framework consistently enhances mathematical reasoning\nperformance on both monolingual dataset GSM8K and multilingual dataset MGSM,\nespecially for smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large language models (LLMs) have proved able to address some complex\nreasoning tasks, we also know that they are highly sensitive to input\nvariation, which can lead to different solution paths and final answers. Answer\nconsistency across input variations can thus be taken as a sign of stronger\nconfidence. Leveraging this insight, we introduce a framework, {\\em\nMultidimensional Reasoning Consistency} where, focusing on math problems,\nmodels are systematically pushed to diversify solution paths towards a final\nanswer, thereby testing them for answer consistency across multiple input\nvariations. We induce variations in (i) order of shots in prompt, (ii) problem\nphrasing, and (iii) languages used. Extensive experiments on a large range of\nopen-source state-of-the-art LLMs of various sizes show that reasoning\nconsistency differs by variation dimension, and that by aggregating consistency\nacross dimensions, our framework consistently enhances mathematical reasoning\nperformance on both monolingual dataset GSM8K and multilingual dataset MGSM,\nespecially for smaller models."
                },
                "authors": [
                    {
                        "name": "Huiyuan Lai"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Malvina Nissim"
                    }
                ],
                "author_detail": {
                    "name": "Malvina Nissim"
                },
                "author": "Malvina Nissim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06057v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06057v3",
                "updated": "2025-03-04T14:33:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    33,
                    50,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-08T15:59:44Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    15,
                    59,
                    44,
                    0,
                    190,
                    0
                ],
                "title": "Variational Best-of-N Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Best-of-N Alignment"
                },
                "summary": "Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on controlled\ngeneration and summarization tasks show that BoN is the most effective\nalignment method, and our variational approximation to BoN achieves the closest\nperformance to BoN and surpasses models fine-tuned using the standard\nKL-constrained RL objective. In the controlled generation task, vBoN appears\nmore frequently on the Pareto frontier of reward and KL divergence compared to\nother alignment methods. In the summarization task, vBoN achieves high reward\nvalues across various sampling temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on controlled\ngeneration and summarization tasks show that BoN is the most effective\nalignment method, and our variational approximation to BoN achieves the closest\nperformance to BoN and surpasses models fine-tuned using the standard\nKL-constrained RL objective. In the controlled generation task, vBoN appears\nmore frequently on the Pareto frontier of reward and KL divergence compared to\nother alignment methods. In the summarization task, vBoN achieves high reward\nvalues across various sampling temperatures."
                },
                "authors": [
                    {
                        "name": "Afra Amini"
                    },
                    {
                        "name": "Tim Vieira"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Ryan Cotterell"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Cotterell"
                },
                "author": "Ryan Cotterell",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06057v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06057v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00735v2",
                "updated": "2025-03-04T14:30:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    30,
                    32,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-02T05:16:43Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    5,
                    16,
                    43,
                    6,
                    61,
                    0
                ],
                "title": "LADDER: Self-Improving LLMs Through Recursive Problem Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Self-Improving LLMs Through Recursive Problem Decomposition"
                },
                "summary": "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example\nRecursion), a framework which enables Large Language Models to autonomously\nimprove their problem-solving capabilities through self-guided learning by\nrecursively generating and solving progressively simpler variants of complex\nproblems. Unlike prior approaches that require curated datasets or human\nfeedback, LADDER leverages a model's own capabilities to generate easier\nquestion variants. We demonstrate LADDER's effectiveness in the subject of\nmathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on\nundergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to\nachieve 73% on the MIT Integration Bee qualifying examination. We also\nintroduce TTRL (Test-Time Reinforcement Learning), where we perform\nreinforcement learning on variants of test problems at inference time. TTRL\nenables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of\n90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's\nperformance. These results show how self-directed strategic learning can\nachieve significant capability improvements without relying on architectural\nscaling or human supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example\nRecursion), a framework which enables Large Language Models to autonomously\nimprove their problem-solving capabilities through self-guided learning by\nrecursively generating and solving progressively simpler variants of complex\nproblems. Unlike prior approaches that require curated datasets or human\nfeedback, LADDER leverages a model's own capabilities to generate easier\nquestion variants. We demonstrate LADDER's effectiveness in the subject of\nmathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on\nundergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to\nachieve 73% on the MIT Integration Bee qualifying examination. We also\nintroduce TTRL (Test-Time Reinforcement Learning), where we perform\nreinforcement learning on variants of test problems at inference time. TTRL\nenables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of\n90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's\nperformance. These results show how self-directed strategic learning can\nachieve significant capability improvements without relying on architectural\nscaling or human supervision."
                },
                "authors": [
                    {
                        "name": "Toby Simonds"
                    },
                    {
                        "name": "Akira Yoshiyama"
                    }
                ],
                "author_detail": {
                    "name": "Akira Yoshiyama"
                },
                "author": "Akira Yoshiyama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02659v1",
                "updated": "2025-03-04T14:21:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    21,
                    8,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T14:21:08Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    21,
                    8,
                    1,
                    63,
                    0
                ],
                "title": "LoRA-Null: Low-Rank Adaptation via Null Space for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-Null: Low-Rank Adaptation via Null Space for Large Language Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) is the leading parameter-efficient fine-tuning\nmethod for Large Language Models (LLMs). However, the fine-tuned LLMs encounter\nthe issue of catastrophic forgetting of the pre-trained world knowledge. To\naddress this issue, inspired by theoretical insights of null space, we propose\nLoRA-Null, i.e., Low-Rank Adaptation via null space, which builds adapters\ninitialized from the null space of the pre-trained knowledge activation.\nConcretely, we randomly collect a few data samples and capture their\nactivations after passing through the LLM layer. We perform Singular Value\nDecomposition on the input activations to obtain their null space. We use the\nprojection of the pre-trained weights onto the null space as the initialization\nfor adapters. Experimental results demonstrate that this initialization\napproach can effectively preserve the original pre-trained world knowledge of\nthe LLMs during fine-tuning. Additionally, if we freeze the values of the\ndown-projection matrices during fine-tuning, it achieves even better\npreservation of the pre-trained world knowledge. LoRA-Null effectively\npreserves pre-trained world knowledge while maintaining strong fine-tuning\nperformance, as validated by extensive experiments on LLaMA series (LLaMA2,\nLLaMA3, LLaMA3.1, and LLaMA3.2) across Code, Math, and Instruction Following\ntasks. We also provide a theoretical guarantee for the capacity of LoRA-Null to\nretain pre-trained knowledge. Code is in\nhttps://github.com/HungerPWAY/LoRA-Null.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) is the leading parameter-efficient fine-tuning\nmethod for Large Language Models (LLMs). However, the fine-tuned LLMs encounter\nthe issue of catastrophic forgetting of the pre-trained world knowledge. To\naddress this issue, inspired by theoretical insights of null space, we propose\nLoRA-Null, i.e., Low-Rank Adaptation via null space, which builds adapters\ninitialized from the null space of the pre-trained knowledge activation.\nConcretely, we randomly collect a few data samples and capture their\nactivations after passing through the LLM layer. We perform Singular Value\nDecomposition on the input activations to obtain their null space. We use the\nprojection of the pre-trained weights onto the null space as the initialization\nfor adapters. Experimental results demonstrate that this initialization\napproach can effectively preserve the original pre-trained world knowledge of\nthe LLMs during fine-tuning. Additionally, if we freeze the values of the\ndown-projection matrices during fine-tuning, it achieves even better\npreservation of the pre-trained world knowledge. LoRA-Null effectively\npreserves pre-trained world knowledge while maintaining strong fine-tuning\nperformance, as validated by extensive experiments on LLaMA series (LLaMA2,\nLLaMA3, LLaMA3.1, and LLaMA3.2) across Code, Math, and Instruction Following\ntasks. We also provide a theoretical guarantee for the capacity of LoRA-Null to\nretain pre-trained knowledge. Code is in\nhttps://github.com/HungerPWAY/LoRA-Null."
                },
                "authors": [
                    {
                        "name": "Pengwei Tang"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Dongjie Zhang"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Debing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Debing Zhang"
                },
                "author": "Debing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02657v1",
                "updated": "2025-03-04T14:17:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    17,
                    9,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T14:17:09Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    17,
                    9,
                    1,
                    63,
                    0
                ],
                "title": "A Deep, High-Angular Resolution 3D Dust Map of the Southern Galactic\n  Plane",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Deep, High-Angular Resolution 3D Dust Map of the Southern Galactic\n  Plane"
                },
                "summary": "We present a deep, high-angular resolution 3D dust map of the southern\nGalactic plane over $239^\\circ < \\ell < 6^\\circ$ and $|b| < 10^\\circ$ built on\nphotometry from the DECaPS2 survey, in combination with photometry from VVV,\n2MASS, and unWISE and parallaxes from Gaia DR3 where available. To construct\nthe map, we first infer the distance, extinction, and stellar types of over 700\nmillion stars using the brutus stellar inference framework with a set of\ntheoretical MIST stellar models. Our resultant 3D dust map has an angular\nresolution of $1'$, roughly an order of magnitude finer than existing 3D dust\nmaps and comparable to the angular resolution of the Herschel 2D dust emission\nmaps. We detect complexes at the range of distances associated with the\nSagittarius-Carina and Scutum-Centaurus arms in the fourth quadrant, as well as\nmore distant structures out to a maximum reliable distance of $d \\approx$ 10\nkpc from the Sun. The map is sensitive up to a maximum extinction of roughly\n$A_V \\approx 12$ mag. We publicly release both the stellar catalog and the 3D\ndust map, the latter of which can easily be queried via the Python package\ndustmaps. When combined with the existing Bayestar19 3D dust map of the\nnorthern sky, the DECaPS 3D dust map fills in the missing piece of the Galactic\nplane, enabling extinction corrections over the entire disk $|b| < 10^\\circ$.\nOur map serves as a pathfinder for the future of 3D dust mapping in the era of\nLSST and Roman, targeting regimes accessible with deep optical and\nnear-infrared photometry but often inaccessible with Gaia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a deep, high-angular resolution 3D dust map of the southern\nGalactic plane over $239^\\circ < \\ell < 6^\\circ$ and $|b| < 10^\\circ$ built on\nphotometry from the DECaPS2 survey, in combination with photometry from VVV,\n2MASS, and unWISE and parallaxes from Gaia DR3 where available. To construct\nthe map, we first infer the distance, extinction, and stellar types of over 700\nmillion stars using the brutus stellar inference framework with a set of\ntheoretical MIST stellar models. Our resultant 3D dust map has an angular\nresolution of $1'$, roughly an order of magnitude finer than existing 3D dust\nmaps and comparable to the angular resolution of the Herschel 2D dust emission\nmaps. We detect complexes at the range of distances associated with the\nSagittarius-Carina and Scutum-Centaurus arms in the fourth quadrant, as well as\nmore distant structures out to a maximum reliable distance of $d \\approx$ 10\nkpc from the Sun. The map is sensitive up to a maximum extinction of roughly\n$A_V \\approx 12$ mag. We publicly release both the stellar catalog and the 3D\ndust map, the latter of which can easily be queried via the Python package\ndustmaps. When combined with the existing Bayestar19 3D dust map of the\nnorthern sky, the DECaPS 3D dust map fills in the missing piece of the Galactic\nplane, enabling extinction corrections over the entire disk $|b| < 10^\\circ$.\nOur map serves as a pathfinder for the future of 3D dust mapping in the era of\nLSST and Roman, targeting regimes accessible with deep optical and\nnear-infrared photometry but often inaccessible with Gaia."
                },
                "authors": [
                    {
                        "name": "Catherine Zucker"
                    },
                    {
                        "name": "Andrew K. Saydjari"
                    },
                    {
                        "name": "Joshua S. Speagle"
                    },
                    {
                        "name": "Edward F. Schlafly"
                    },
                    {
                        "name": "Gregory M. Green"
                    },
                    {
                        "name": "Robert Benjamin"
                    },
                    {
                        "name": "Joshua Peek"
                    },
                    {
                        "name": "Gordian Edenhofer"
                    },
                    {
                        "name": "Alyssa Goodman"
                    },
                    {
                        "name": "Michael A. Kuhn"
                    },
                    {
                        "name": "Douglas P. Finkbeiner"
                    }
                ],
                "author_detail": {
                    "name": "Douglas P. Finkbeiner"
                },
                "author": "Douglas P. Finkbeiner",
                "arxiv_comment": "Submitted to AAS Journals. 31 pages, 16 figures. Equal contribution\n  of first three authors (Zucker, Saydjari, & Speagle)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02652v1",
                "updated": "2025-03-04T14:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T14:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Cellular Automaton With CNN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cellular Automaton With CNN"
                },
                "summary": "Cellular automata (CA) models are widely used to simulate complex systems\nwith emergent behaviors, but identifying hidden parameters that govern their\ndynamics remains a significant challenge. This study explores the use of\nConvolutional Neural Networks (CNN) to identify jump parameters in a\ntwo-dimensional CA model. We propose a custom CNN architecture trained on\nCA-generated data to classify jump parameters, which dictates the neighborhood\nsize and movement rules of cells within the CA. Experiments were conducted\nacross varying domain sizes (25 x 25 to 150 x 150) and CA iterations (0 to 50),\ndemonstrating that the accuracy improves with larger domain sizes, as they\nprovide more spatial information for parameter estimation. Interestingly, while\ninitial CA iterations enhance the performance, increasing the number of\niterations beyond a certain threshold does not significantly improve accuracy,\nsuggesting that only specific temporal information is relevant for parameter\nidentification. The proposed CNN achieves competitive accuracy (89.31) compared\nto established architectures like LeNet-5 and AlexNet, while offering\nsignificantly faster inference times, making it suitable for real-time\napplications. This study highlights the potential of CNNs as a powerful tool\nfor fast and accurate parameter estimation in CA models, paving the way for\ntheir use in more complex systems and higher-dimensional domains. Future work\nwill explore the identification of multiple hidden parameters and extend the\napproach to three-dimensional CA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cellular automata (CA) models are widely used to simulate complex systems\nwith emergent behaviors, but identifying hidden parameters that govern their\ndynamics remains a significant challenge. This study explores the use of\nConvolutional Neural Networks (CNN) to identify jump parameters in a\ntwo-dimensional CA model. We propose a custom CNN architecture trained on\nCA-generated data to classify jump parameters, which dictates the neighborhood\nsize and movement rules of cells within the CA. Experiments were conducted\nacross varying domain sizes (25 x 25 to 150 x 150) and CA iterations (0 to 50),\ndemonstrating that the accuracy improves with larger domain sizes, as they\nprovide more spatial information for parameter estimation. Interestingly, while\ninitial CA iterations enhance the performance, increasing the number of\niterations beyond a certain threshold does not significantly improve accuracy,\nsuggesting that only specific temporal information is relevant for parameter\nidentification. The proposed CNN achieves competitive accuracy (89.31) compared\nto established architectures like LeNet-5 and AlexNet, while offering\nsignificantly faster inference times, making it suitable for real-time\napplications. This study highlights the potential of CNNs as a powerful tool\nfor fast and accurate parameter estimation in CA models, paving the way for\ntheir use in more complex systems and higher-dimensional domains. Future work\nwill explore the identification of multiple hidden parameters and extend the\napproach to three-dimensional CA models."
                },
                "authors": [
                    {
                        "name": "Valery Ashu"
                    },
                    {
                        "name": "Zhisong Liu"
                    },
                    {
                        "name": "Heikki Haario"
                    },
                    {
                        "name": "Andreas Rupp"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Rupp"
                },
                "author": "Andreas Rupp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02650v1",
                "updated": "2025-03-04T14:14:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    14,
                    28,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T14:14:28Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    14,
                    28,
                    1,
                    63,
                    0
                ],
                "title": "The Effectiveness of Large Language Models in Transforming Unstructured\n  Text to Standardized Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effectiveness of Large Language Models in Transforming Unstructured\n  Text to Standardized Formats"
                },
                "summary": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information."
                },
                "authors": [
                    {
                        "name": "William Brach"
                    },
                    {
                        "name": "Kristián Košťál"
                    },
                    {
                        "name": "Michal Ries"
                    }
                ],
                "author_detail": {
                    "name": "Michal Ries"
                },
                "author": "Michal Ries",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15077v3",
                "updated": "2025-03-04T13:58:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    58,
                    39,
                    1,
                    63,
                    0
                ],
                "published": "2024-01-26T18:59:01Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    18,
                    59,
                    1,
                    4,
                    26,
                    0
                ],
                "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty"
                },
                "summary": "Autoregressive decoding makes the inference of Large Language Models (LLMs)\ntime-consuming. In this paper, we reconsider speculative sampling and derive\ntwo key observations. Firstly, autoregression at the feature\n(second-to-top-layer) level is more straightforward than at the token level.\nSecondly, the inherent uncertainty in feature (second-to-top-layer) level\nautoregression constrains its performance. Based on these insights, we\nintroduce EAGLE (Extrapolation Algorithm for Greater Language-model\nEfficiency), a simple yet highly efficient speculative sampling framework. By\nincorporating a token sequence advanced by one time step, EAGLE effectively\nresolves the uncertainty, enabling precise second-to-top-layer feature\nprediction with minimal overhead. We conducted comprehensive evaluations of\nEAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE\nmodel Mixtral 8x7B Instruct, and tasks in dialogue, code generation,\nmathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE\nachieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while\nmaintaining the distribution of the generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding makes the inference of Large Language Models (LLMs)\ntime-consuming. In this paper, we reconsider speculative sampling and derive\ntwo key observations. Firstly, autoregression at the feature\n(second-to-top-layer) level is more straightforward than at the token level.\nSecondly, the inherent uncertainty in feature (second-to-top-layer) level\nautoregression constrains its performance. Based on these insights, we\nintroduce EAGLE (Extrapolation Algorithm for Greater Language-model\nEfficiency), a simple yet highly efficient speculative sampling framework. By\nincorporating a token sequence advanced by one time step, EAGLE effectively\nresolves the uncertainty, enabling precise second-to-top-layer feature\nprediction with minimal overhead. We conducted comprehensive evaluations of\nEAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE\nmodel Mixtral 8x7B Instruct, and tasks in dialogue, code generation,\nmathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE\nachieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while\nmaintaining the distribution of the generated text."
                },
                "authors": [
                    {
                        "name": "Yuhui Li"
                    },
                    {
                        "name": "Fangyun Wei"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Hongyang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Zhang"
                },
                "author": "Hongyang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02631v1",
                "updated": "2025-03-04T13:56:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    56,
                    18,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:56:18Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    56,
                    18,
                    1,
                    63,
                    0
                ],
                "title": "Reflection on Data Storytelling Tools in the Generative AI Era from the\n  Human-AI Collaboration Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflection on Data Storytelling Tools in the Generative AI Era from the\n  Human-AI Collaboration Perspective"
                },
                "summary": "Human-AI collaborative tools attract attentions from the data storytelling\ncommunity to lower the barrier of expertise and streamline the workflow. The\nrecent advance in large-scale generative AI techniques, e.g., large language\nmodels (LLMs) and text-to-image models, has the potential to enhance data\nstorytelling with their power in visual and narration generation. After two\nyears since these techniques were publicly available, it is important to\nreflect our progress of applying them and have an outlook for future\nopportunities. To achieve the goal, we compare the collaboration patterns of\nthe latest tools with those of earlier ones using a dedicated framework for\nunderstanding human-AI collaboration in data storytelling. Through comparison,\nwe identify persistent collaboration patterns, e.g., human-creator +\nAI-assistant, and emerging ones, e.g., AI-creator + human-reviewer. The\nbenefits of these AI techniques and other implications to human-AI\ncollaboration are also revealed. We further propose future directions to\nhopefully ignite innovations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-AI collaborative tools attract attentions from the data storytelling\ncommunity to lower the barrier of expertise and streamline the workflow. The\nrecent advance in large-scale generative AI techniques, e.g., large language\nmodels (LLMs) and text-to-image models, has the potential to enhance data\nstorytelling with their power in visual and narration generation. After two\nyears since these techniques were publicly available, it is important to\nreflect our progress of applying them and have an outlook for future\nopportunities. To achieve the goal, we compare the collaboration patterns of\nthe latest tools with those of earlier ones using a dedicated framework for\nunderstanding human-AI collaboration in data storytelling. Through comparison,\nwe identify persistent collaboration patterns, e.g., human-creator +\nAI-assistant, and emerging ones, e.g., AI-creator + human-reviewer. The\nbenefits of these AI techniques and other implications to human-AI\ncollaboration are also revealed. We further propose future directions to\nhopefully ignite innovations."
                },
                "authors": [
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Huamin Qu"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Qu"
                },
                "author": "Huamin Qu",
                "arxiv_comment": "This paper is a sequel to the CHI 24 paper \"Where Are We So Far?\n  Understanding Data Storytelling Tools from the Perspective of Human-AI\n  Collaboration (https://doi.org/10.1145/3613904.3642726), aiming to refresh\n  our understanding with the latest advancements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02630v1",
                "updated": "2025-03-04T13:55:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    55,
                    22,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:55:22Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    55,
                    22,
                    1,
                    63,
                    0
                ],
                "title": "Weighted Euclidean Distance Matrices over Mixed Continuous and\n  Categorical Inputs for Gaussian Process Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weighted Euclidean Distance Matrices over Mixed Continuous and\n  Categorical Inputs for Gaussian Process Models"
                },
                "summary": "Gaussian Process (GP) models are widely utilized as surrogate models in\nscientific and engineering fields. However, standard GP models are limited to\ncontinuous variables due to the difficulties in establishing correlation\nstructures for categorical variables. To overcome this limitati on, we\nintroduce WEighted Euclidean distance matrices Gaussian Process (WEGP). WEGP\nconstructs the kernel function for each categorical input by estimating the\nEuclidean distance matrix (EDM) among all categorical choices of this input.\nThe EDM is represented as a linear combination of several predefined base EDMs,\neach scaled by a positive weight. The weights, along with other kernel\nhyperparameters, are inferred using a fully Bayesian framework. We analyze the\npredictive performance of WEGP theoretically. Numerical experiments validate\nthe accuracy of our GP model, and by WEGP, into Bayesian Optimization (BO), we\nachieve superior performance on both synthetic and real-world optimization\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Process (GP) models are widely utilized as surrogate models in\nscientific and engineering fields. However, standard GP models are limited to\ncontinuous variables due to the difficulties in establishing correlation\nstructures for categorical variables. To overcome this limitati on, we\nintroduce WEighted Euclidean distance matrices Gaussian Process (WEGP). WEGP\nconstructs the kernel function for each categorical input by estimating the\nEuclidean distance matrix (EDM) among all categorical choices of this input.\nThe EDM is represented as a linear combination of several predefined base EDMs,\neach scaled by a positive weight. The weights, along with other kernel\nhyperparameters, are inferred using a fully Bayesian framework. We analyze the\npredictive performance of WEGP theoretically. Numerical experiments validate\nthe accuracy of our GP model, and by WEGP, into Bayesian Optimization (BO), we\nachieve superior performance on both synthetic and real-world optimization\nproblems."
                },
                "authors": [
                    {
                        "name": "Mingyu Pu"
                    },
                    {
                        "name": "Songhao Wang"
                    },
                    {
                        "name": "Haowei Wang"
                    },
                    {
                        "name": "Szu Hui Ng"
                    }
                ],
                "author_detail": {
                    "name": "Szu Hui Ng"
                },
                "author": "Szu Hui Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02628v1",
                "updated": "2025-03-04T13:53:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    53,
                    43,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:53:43Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    53,
                    43,
                    1,
                    63,
                    0
                ],
                "title": "Towards Event Extraction with Massive Types: LLM-based Collaborative\n  Annotation and Partitioning Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Event Extraction with Massive Types: LLM-based Collaborative\n  Annotation and Partitioning Extraction"
                },
                "summary": "Developing a general-purpose extraction system that can extract events with\nmassive types is a long-standing target in Event Extraction (EE). In doing so,\nthe challenge comes from two aspects: 1) The absence of an efficient and\neffective annotation method. 2) The absence of a powerful extraction method can\nhandle massive types. For the first challenge, we propose a collaborative\nannotation method based on Large Language Models (LLMs). Through collaboration\namong multiple LLMs, it first refines annotations of trigger words from distant\nsupervision and then carries out argument annotation. Next, a voting phase\nconsolidates the annotation preferences across different LLMs. Finally, we\ncreate the EEMT dataset, the largest EE dataset to date, featuring over 200,000\nsamples, 3,465 event types, and 6,297 role types. For the second challenge, we\npropose an LLM-based Partitioning EE method called LLM-PEE. To overcome the\nlimited context length of LLMs, LLM-PEE first recalls candidate event types and\nthen splits them into multiple partitions for LLMs to extract events. The\nresults in the supervised setting show that LLM-PEE outperforms the\nstate-of-the-art methods by 5.4 in event detection and 6.1 in argument\nextraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement\ncompared to mainstream LLMs, demonstrating its strong generalization\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing a general-purpose extraction system that can extract events with\nmassive types is a long-standing target in Event Extraction (EE). In doing so,\nthe challenge comes from two aspects: 1) The absence of an efficient and\neffective annotation method. 2) The absence of a powerful extraction method can\nhandle massive types. For the first challenge, we propose a collaborative\nannotation method based on Large Language Models (LLMs). Through collaboration\namong multiple LLMs, it first refines annotations of trigger words from distant\nsupervision and then carries out argument annotation. Next, a voting phase\nconsolidates the annotation preferences across different LLMs. Finally, we\ncreate the EEMT dataset, the largest EE dataset to date, featuring over 200,000\nsamples, 3,465 event types, and 6,297 role types. For the second challenge, we\npropose an LLM-based Partitioning EE method called LLM-PEE. To overcome the\nlimited context length of LLMs, LLM-PEE first recalls candidate event types and\nthen splits them into multiple partitions for LLMs to extract events. The\nresults in the supervised setting show that LLM-PEE outperforms the\nstate-of-the-art methods by 5.4 in event detection and 6.1 in argument\nextraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement\ncompared to mainstream LLMs, demonstrating its strong generalization\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Wenxuan Liu"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Long Bai"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Daozhu Xu"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13044v2",
                "updated": "2025-03-04T13:51:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    51,
                    34,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-18T16:56:15Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    56,
                    15,
                    1,
                    49,
                    0
                ],
                "title": "Do we still need Human Annotators? Prompting Large Language Models for\n  Aspect Sentiment Quad Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do we still need Human Annotators? Prompting Large Language Models for\n  Aspect Sentiment Quad Prediction"
                },
                "summary": "Aspect sentiment quadruple prediction (ASQP) facilitates a detailed\nunderstanding of opinions expressed in a text by identifying the opinion term,\naspect term, aspect category and sentiment polarity for each opinion. However,\nannotating a full set of training examples to fine-tune models for ASQP is a\nresource-intensive process. In this study, we explore the capabilities of large\nlanguage models (LLMs) for zero- and few-shot learning on the ASQP task across\nfive diverse datasets. We report F1 scores slightly below those obtained with\nstate-of-the-art fine-tuned models but exceeding previously reported zero- and\nfew-shot performance. In the 40-shot setting on the Rest16 restaurant domain\ndataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the\nbest-performing fine-tuned method MVP. Additionally, we report the performance\nof LLMs in target aspect sentiment detection (TASD), where the F1 scores were\nalso close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot\nsetting, compared to 72.76 with MVP. While human annotators remain essential\nfor achieving optimal performance, LLMs can reduce the need for extensive\nmanual annotation in ASQP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect sentiment quadruple prediction (ASQP) facilitates a detailed\nunderstanding of opinions expressed in a text by identifying the opinion term,\naspect term, aspect category and sentiment polarity for each opinion. However,\nannotating a full set of training examples to fine-tune models for ASQP is a\nresource-intensive process. In this study, we explore the capabilities of large\nlanguage models (LLMs) for zero- and few-shot learning on the ASQP task across\nfive diverse datasets. We report F1 scores slightly below those obtained with\nstate-of-the-art fine-tuned models but exceeding previously reported zero- and\nfew-shot performance. In the 40-shot setting on the Rest16 restaurant domain\ndataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the\nbest-performing fine-tuned method MVP. Additionally, we report the performance\nof LLMs in target aspect sentiment detection (TASD), where the F1 scores were\nalso close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot\nsetting, compared to 72.76 with MVP. While human annotators remain essential\nfor achieving optimal performance, LLMs can reduce the need for extensive\nmanual annotation in ASQP tasks."
                },
                "authors": [
                    {
                        "name": "Nils Constantin Hellwig"
                    },
                    {
                        "name": "Jakob Fehle"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    },
                    {
                        "name": "Christian Wolff"
                    }
                ],
                "author_detail": {
                    "name": "Christian Wolff"
                },
                "author": "Christian Wolff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04070v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04070v6",
                "updated": "2025-03-04T13:51:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    51,
                    14,
                    1,
                    63,
                    0
                ],
                "published": "2024-10-05T08:00:55Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    0,
                    55,
                    5,
                    279,
                    0
                ],
                "title": "PAD: Personalized Alignment of LLMs at Decoding-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAD: Personalized Alignment of LLMs at Decoding-Time"
                },
                "summary": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment."
                },
                "authors": [
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Xiaotian Zhang"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04070v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04070v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02623v1",
                "updated": "2025-03-04T13:48:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    48,
                    50,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:48:50Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    48,
                    50,
                    1,
                    63,
                    0
                ],
                "title": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence\n  Calibration of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence\n  Calibration of Large Language Models"
                },
                "summary": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs."
                },
                "authors": [
                    {
                        "name": "Paul Stangel"
                    },
                    {
                        "name": "David Bani-Harouni"
                    },
                    {
                        "name": "Chantal Pellegrini"
                    },
                    {
                        "name": "Ege Özsoy"
                    },
                    {
                        "name": "Kamilia Zaripova"
                    },
                    {
                        "name": "Matthias Keicher"
                    },
                    {
                        "name": "Nassir Navab"
                    }
                ],
                "author_detail": {
                    "name": "Nassir Navab"
                },
                "author": "Nassir Navab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00486v2",
                "updated": "2025-03-04T13:44:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    44,
                    8,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-01T13:24:09Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    13,
                    24,
                    9,
                    5,
                    60,
                    0
                ],
                "title": "Conformal Lyapunov Optimization: Optimal Resource Allocation under\n  Deterministic Reliability Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Lyapunov Optimization: Optimal Resource Allocation under\n  Deterministic Reliability Constraints"
                },
                "summary": "This paper introduces conformal Lyapunov optimization (CLO), a novel resource\nallocation framework for networked systems that optimizes average long-term\nobjectives, while satisfying deterministic long-term reliability constraints.\nUnlike traditional Lyapunov optimization (LO), which addresses resource\nallocation tasks under average long-term constraints, CLO provides formal\nworst-case deterministic reliability guarantees. This is achieved by\nintegrating the standard LO optimization framework with online conformal risk\ncontrol (O-CRC), an adaptive update mechanism controlling long-term risks. The\neffectiveness of CLO is verified via experiments for hierarchal edge inference\ntargeting image segmentation tasks in a networked computing architecture.\nSpecifically, simulation results confirm that CLO can control reliability\nconstraints, measured via the false negative rate of all the segmentation\ndecisions made in the network, while at the same time minimizing the weighted\nsum of energy consumption and imprecision, with the latter accounting for the\nrate of false positives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces conformal Lyapunov optimization (CLO), a novel resource\nallocation framework for networked systems that optimizes average long-term\nobjectives, while satisfying deterministic long-term reliability constraints.\nUnlike traditional Lyapunov optimization (LO), which addresses resource\nallocation tasks under average long-term constraints, CLO provides formal\nworst-case deterministic reliability guarantees. This is achieved by\nintegrating the standard LO optimization framework with online conformal risk\ncontrol (O-CRC), an adaptive update mechanism controlling long-term risks. The\neffectiveness of CLO is verified via experiments for hierarchal edge inference\ntargeting image segmentation tasks in a networked computing architecture.\nSpecifically, simulation results confirm that CLO can control reliability\nconstraints, measured via the false negative rate of all the segmentation\ndecisions made in the network, while at the same time minimizing the weighted\nsum of energy consumption and imprecision, with the latter accounting for the\nrate of false positives."
                },
                "authors": [
                    {
                        "name": "Francesco Binucci"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    },
                    {
                        "name": "Paolo Banelli"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Banelli"
                },
                "author": "Paolo Banelli",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13625v2",
                "updated": "2025-03-04T13:27:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    27,
                    34,
                    1,
                    63,
                    0
                ],
                "published": "2024-11-20T13:59:28Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    13,
                    59,
                    28,
                    2,
                    325,
                    0
                ],
                "title": "Partition function approach to non-Gaussian likelihoods: information\n  theory and state variables for Bayesian inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partition function approach to non-Gaussian likelihoods: information\n  theory and state variables for Bayesian inference"
                },
                "summary": "The significance of statistical physics concepts such as entropy extends far\nbeyond classical thermodynamics. We interpret the similarity between partitions\nin statistical mechanics and partitions in Bayesian inference as an\narticulation of a result by Jaynes (1957), who clarified that thermodynamics is\nin essence a theory of information. In this, every sampling process has a\nmechanical analogue. Consequently, the divide between ensembles of samplers in\nparameter space and sampling from a mechanical system in thermodynamic\nequilibrium would be artificial. Based on this realisation, we construct a\ncontinuous modelling of a Bayes update akin to a transition between\nthermodynamic ensembles. This leads to an information theoretic interpretation\nof Jazinsky's equality, relating the expenditure of work to the influence of\ndata via the likelihood. We propose one way to transfer the vocabulary and the\nformalism of thermodynamics (energy, work, heat) and statistical mechanics\n(partition functions) to statistical inference, starting from Bayes' law.\nDifferent kinds of inference processes are discussed and relative entropies are\nshown to follow from suitably constructed partitions as an analytical\nformulation of sampling processes. Lastly, we propose an effective dimension as\na measure of system complexity. A numerical example from cosmology is put\nforward to illustrate these results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significance of statistical physics concepts such as entropy extends far\nbeyond classical thermodynamics. We interpret the similarity between partitions\nin statistical mechanics and partitions in Bayesian inference as an\narticulation of a result by Jaynes (1957), who clarified that thermodynamics is\nin essence a theory of information. In this, every sampling process has a\nmechanical analogue. Consequently, the divide between ensembles of samplers in\nparameter space and sampling from a mechanical system in thermodynamic\nequilibrium would be artificial. Based on this realisation, we construct a\ncontinuous modelling of a Bayes update akin to a transition between\nthermodynamic ensembles. This leads to an information theoretic interpretation\nof Jazinsky's equality, relating the expenditure of work to the influence of\ndata via the likelihood. We propose one way to transfer the vocabulary and the\nformalism of thermodynamics (energy, work, heat) and statistical mechanics\n(partition functions) to statistical inference, starting from Bayes' law.\nDifferent kinds of inference processes are discussed and relative entropies are\nshown to follow from suitably constructed partitions as an analytical\nformulation of sampling processes. Lastly, we propose an effective dimension as\na measure of system complexity. A numerical example from cosmology is put\nforward to illustrate these results."
                },
                "authors": [
                    {
                        "name": "Rebecca Maria Kuntz"
                    },
                    {
                        "name": "Heinrich von Campe"
                    },
                    {
                        "name": "Tobias Röspel"
                    },
                    {
                        "name": "Maximilian Philipp Herzog"
                    },
                    {
                        "name": "Björn Malte Schäfer"
                    }
                ],
                "author_detail": {
                    "name": "Björn Malte Schäfer"
                },
                "author": "Björn Malte Schäfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02603v1",
                "updated": "2025-03-04T13:21:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    21,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:21:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    21,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "OkraLong: A Flexible Retrieval-Augmented Framework for Long-Text Query\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OkraLong: A Flexible Retrieval-Augmented Framework for Long-Text Query\n  Processing"
                },
                "summary": "Large Language Models (LLMs) encounter challenges in efficiently processing\nlong-text queries, as seen in applications like enterprise document analysis\nand financial report comprehension. While conventional solutions employ\nlong-context processing or Retrieval-Augmented Generation (RAG), they suffer\nfrom prohibitive input expenses or incomplete information. Recent advancements\nadopt context compression and dynamic retrieval loops, but still sacrifice\ncritical details or incur iterative costs.To address these limitations, we\npropose OkraLong, a novel framework that flexibly optimizes the entire\nprocessing workflow. Unlike prior static or coarse-grained adaptive strategies,\nOkraLong adopts fine-grained orchestration through three synergistic\ncomponents: analyzer, organizer and executor. The analyzer characterizes the\ntask states, which guide the organizer in dynamically scheduling the workflow.\nThe executor carries out the execution and generates the final answer.\nExperimental results demonstrate that OkraLong not only enhances answer\naccuracy but also achieves cost-effectiveness across a variety of datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encounter challenges in efficiently processing\nlong-text queries, as seen in applications like enterprise document analysis\nand financial report comprehension. While conventional solutions employ\nlong-context processing or Retrieval-Augmented Generation (RAG), they suffer\nfrom prohibitive input expenses or incomplete information. Recent advancements\nadopt context compression and dynamic retrieval loops, but still sacrifice\ncritical details or incur iterative costs.To address these limitations, we\npropose OkraLong, a novel framework that flexibly optimizes the entire\nprocessing workflow. Unlike prior static or coarse-grained adaptive strategies,\nOkraLong adopts fine-grained orchestration through three synergistic\ncomponents: analyzer, organizer and executor. The analyzer characterizes the\ntask states, which guide the organizer in dynamically scheduling the workflow.\nThe executor carries out the execution and generates the final answer.\nExperimental results demonstrate that OkraLong not only enhances answer\naccuracy but also achieves cost-effectiveness across a variety of datasets."
                },
                "authors": [
                    {
                        "name": "Yulong Hui"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Huanchen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huanchen Zhang"
                },
                "author": "Huanchen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02597v1",
                "updated": "2025-03-04T13:18:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    18,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:18:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    18,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual\n  Attention for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual\n  Attention for Multimodal LLMs"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of earlier\nmodalities (e.g., images) to incorporate information from later modalities\n(e.g., text). To address this problem, we propose AKI, a novel MLLM that\nunlocks causal attention into modality-mutual attention (MMA) to enable image\ntokens to attend to text tokens. This simple yet effective design allows AKI to\nachieve superior performance in 12 multimodal understanding benchmarks (+7.2%\non average) without introducing additional parameters and increasing training\ntime. Our MMA design is intended to be generic, allowing for application across\nvarious modalities, and scalable to accommodate diverse multimodal scenarios.\nThe code is publicly available at https://github.com/sony/aki, and we will\nrelease our AKI-4B model to encourage further advancements in MLLMs across\nvarious directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of earlier\nmodalities (e.g., images) to incorporate information from later modalities\n(e.g., text). To address this problem, we propose AKI, a novel MLLM that\nunlocks causal attention into modality-mutual attention (MMA) to enable image\ntokens to attend to text tokens. This simple yet effective design allows AKI to\nachieve superior performance in 12 multimodal understanding benchmarks (+7.2%\non average) without introducing additional parameters and increasing training\ntime. Our MMA design is intended to be generic, allowing for application across\nvarious modalities, and scalable to accommodate diverse multimodal scenarios.\nThe code is publicly available at https://github.com/sony/aki, and we will\nrelease our AKI-4B model to encourage further advancements in MLLMs across\nvarious directions."
                },
                "authors": [
                    {
                        "name": "Wei-Yao Wang"
                    },
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Helen Suzuki"
                    },
                    {
                        "name": "Yoshiyuki Kobayashi"
                    }
                ],
                "author_detail": {
                    "name": "Yoshiyuki Kobayashi"
                },
                "author": "Yoshiyuki Kobayashi",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02582v1",
                "updated": "2025-03-04T13:04:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    4,
                    48,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:04:48Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    4,
                    48,
                    1,
                    63,
                    0
                ],
                "title": "Playing games with Large language models: Randomness and strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Playing games with Large language models: Randomness and strategy"
                },
                "summary": "Playing games has a long history of describing intricate interactions in\nsimplified forms. In this paper we explore if large language models (LLMs) can\nplay games, investigating their capabilities for randomisation and strategic\nadaptation through both simultaneous and sequential game interactions. We focus\non GPT-4o-Mini-2024-08-17 and test two games between LLMs: Rock Paper Scissors\n(RPS) and games of strategy (Prisoners Dilemma PD). LLMs are often described as\nstochastic parrots, and while they may indeed be parrots, our results suggest\nthat they are not very stochastic in the sense that their outputs - when\nprompted to be random - are often very biased. Our research reveals that LLMs\nappear to develop loss aversion strategies in repeated games, with RPS\nconverging to stalemate conditions while PD shows systematic shifts between\ncooperative and competitive outcomes based on prompt design. We detail\nprogrammatic tools for independent agent interactions and the Agentic AI\nchallenges faced in implementation. We show that LLMs can indeed play games,\njust not very well. These results have implications for the use of LLMs in\nmulti-agent LLM systems and showcase limitations in current approaches to model\noutput for strategic decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Playing games has a long history of describing intricate interactions in\nsimplified forms. In this paper we explore if large language models (LLMs) can\nplay games, investigating their capabilities for randomisation and strategic\nadaptation through both simultaneous and sequential game interactions. We focus\non GPT-4o-Mini-2024-08-17 and test two games between LLMs: Rock Paper Scissors\n(RPS) and games of strategy (Prisoners Dilemma PD). LLMs are often described as\nstochastic parrots, and while they may indeed be parrots, our results suggest\nthat they are not very stochastic in the sense that their outputs - when\nprompted to be random - are often very biased. Our research reveals that LLMs\nappear to develop loss aversion strategies in repeated games, with RPS\nconverging to stalemate conditions while PD shows systematic shifts between\ncooperative and competitive outcomes based on prompt design. We detail\nprogrammatic tools for independent agent interactions and the Agentic AI\nchallenges faced in implementation. We show that LLMs can indeed play games,\njust not very well. These results have implications for the use of LLMs in\nmulti-agent LLM systems and showcase limitations in current approaches to model\noutput for strategic decision-making."
                },
                "authors": [
                    {
                        "name": "Alicia Vidler"
                    },
                    {
                        "name": "Toby Walsh"
                    }
                ],
                "author_detail": {
                    "name": "Toby Walsh"
                },
                "author": "Toby Walsh",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00946v2",
                "updated": "2025-03-04T13:01:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    32,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-02T15:54:45Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    15,
                    54,
                    45,
                    6,
                    61,
                    0
                ],
                "title": "A Review of LLM-Assisted Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of LLM-Assisted Ideation"
                },
                "summary": "We present a comprehensive, in-depth review of ideation assisted by large\nlanguage models (LLMs), highlighting emerging trends and identifying\nunaddressed research gaps. In total, we examined 61 studies investigating the\napplication of LLMs in both group and individual ideation processes. From these\nstudies, we derived the Hourglass Ideation Framework for LLM-assisted ideation,\ncomprising three phases and seven key ideation stages, which served as the\nbasis for our systematic survey. Our analysis reveals that LLMs are most\nfrequently used for idea generation and refinement, but their use in scope\nspecification, foundational material structuring and multi-idea evaluation and\nselection remains limited. We provide our findings in extensive tabular and\nonline formats. These catalogues detail research on LLM-assisted, purely\nLLM-based, and human-only activities across the seven ideation stages for each\nof the 61 studies. These also detail creative domains, publication outlets,\ninteraction designs, user study designs, and assessment methods. Our analysis\nof system interaction design reveals a predominant focus on supporting\nindividual ideation activities and text-based interaction, with a growing trend\nof incorporating multimedia elements. However, in group ideation, tools and\ninteraction modalities targeting both synchronous and asynchronous\ncollaboration are much scarcer. We synthesize the primary findings of our\nreview and outline promising directions for future research in LLM-assisted\nideation. We hope this review will help researchers quickly gain an overview of\nthis rapidly expanding area, efficiently locate relevant work, and identify\nunderexplored areas for further investigation. In addition, we believe the\nframework we present here will form the basis for the development of future\nproblem and solution space taxonomies, and methodologies for LLM-assisted\nideation development and use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive, in-depth review of ideation assisted by large\nlanguage models (LLMs), highlighting emerging trends and identifying\nunaddressed research gaps. In total, we examined 61 studies investigating the\napplication of LLMs in both group and individual ideation processes. From these\nstudies, we derived the Hourglass Ideation Framework for LLM-assisted ideation,\ncomprising three phases and seven key ideation stages, which served as the\nbasis for our systematic survey. Our analysis reveals that LLMs are most\nfrequently used for idea generation and refinement, but their use in scope\nspecification, foundational material structuring and multi-idea evaluation and\nselection remains limited. We provide our findings in extensive tabular and\nonline formats. These catalogues detail research on LLM-assisted, purely\nLLM-based, and human-only activities across the seven ideation stages for each\nof the 61 studies. These also detail creative domains, publication outlets,\ninteraction designs, user study designs, and assessment methods. Our analysis\nof system interaction design reveals a predominant focus on supporting\nindividual ideation activities and text-based interaction, with a growing trend\nof incorporating multimedia elements. However, in group ideation, tools and\ninteraction modalities targeting both synchronous and asynchronous\ncollaboration are much scarcer. We synthesize the primary findings of our\nreview and outline promising directions for future research in LLM-assisted\nideation. We hope this review will help researchers quickly gain an overview of\nthis rapidly expanding area, efficiently locate relevant work, and identify\nunderexplored areas for further investigation. In addition, we believe the\nframework we present here will form the basis for the development of future\nproblem and solution space taxonomies, and methodologies for LLM-assisted\nideation development and use."
                },
                "authors": [
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Stefano Padilla"
                    },
                    {
                        "name": "Pierre Le Bras"
                    },
                    {
                        "name": "Junyu Dong"
                    },
                    {
                        "name": "Mike Chantler"
                    }
                ],
                "author_detail": {
                    "name": "Mike Chantler"
                },
                "author": "Mike Chantler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01622v2",
                "updated": "2025-03-04T13:00:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    0,
                    55,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-03T14:55:41Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    55,
                    41,
                    0,
                    62,
                    0
                ],
                "title": "DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards\n  Meaningful LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards\n  Meaningful LLM Evaluation"
                },
                "summary": "Recent work found that LLMs are sensitive to a wide range of arbitrary prompt\ndimensions, including the type of delimiters, answer enumerators, instruction\nwording, and more. This throws into question popular single-prompt evaluation\npractices. We present DOVE (Dataset Of Variation Evaluation) a large-scale\ndataset containing prompt perturbations of various evaluation benchmarks. In\ncontrast to previous work, we examine LLM sensitivity from an holistic\nperspective, and assess the joint effects of perturbations along various\ndimensions, resulting in thousands of perturbations per instance. We evaluate\nseveral model families against DOVE, leading to several findings, including\nefficient methods for choosing well-performing prompts, observing that few-shot\nexamples reduce sensitivity, and identifying instances which are inherently\nhard across all perturbations. DOVE consists of more than 250M prompt\nperturbations and model outputs, which we make publicly available to spur a\ncommunity-wide effort toward meaningful, robust, and efficient evaluation.\n  Browse the data, contribute, and more: https://slab-nlp.github.io/DOVE/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work found that LLMs are sensitive to a wide range of arbitrary prompt\ndimensions, including the type of delimiters, answer enumerators, instruction\nwording, and more. This throws into question popular single-prompt evaluation\npractices. We present DOVE (Dataset Of Variation Evaluation) a large-scale\ndataset containing prompt perturbations of various evaluation benchmarks. In\ncontrast to previous work, we examine LLM sensitivity from an holistic\nperspective, and assess the joint effects of perturbations along various\ndimensions, resulting in thousands of perturbations per instance. We evaluate\nseveral model families against DOVE, leading to several findings, including\nefficient methods for choosing well-performing prompts, observing that few-shot\nexamples reduce sensitivity, and identifying instances which are inherently\nhard across all perturbations. DOVE consists of more than 250M prompt\nperturbations and model outputs, which we make publicly available to spur a\ncommunity-wide effort toward meaningful, robust, and efficient evaluation.\n  Browse the data, contribute, and more: https://slab-nlp.github.io/DOVE/"
                },
                "authors": [
                    {
                        "name": "Eliya Habba"
                    },
                    {
                        "name": "Ofir Arviv"
                    },
                    {
                        "name": "Itay Itzhak"
                    },
                    {
                        "name": "Yotam Perlitz"
                    },
                    {
                        "name": "Elron Bandel"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Michal Shmueli-Scheuer"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Stanovsky"
                },
                "author": "Gabriel Stanovsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02574v1",
                "updated": "2025-03-04T12:55:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    55,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T12:55:07Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    55,
                    7,
                    1,
                    63,
                    0
                ],
                "title": "LLM-Safety Evaluations Lack Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Safety Evaluations Lack Robustness"
                },
                "summary": "In this paper, we argue that current safety alignment research efforts for\nlarge language models are hindered by many intertwined sources of noise, such\nas small datasets, methodological inconsistencies, and unreliable evaluation\nsetups. This can, at times, make it impossible to evaluate and compare attacks\nand defenses fairly, thereby slowing progress. We systematically analyze the\nLLM safety evaluation pipeline, covering dataset curation, optimization\nstrategies for automated red-teaming, response generation, and response\nevaluation using LLM judges. At each stage, we identify key issues and\nhighlight their practical impact. We also propose a set of guidelines for\nreducing noise and bias in evaluations of future attack and defense papers.\nLastly, we offer an opposing perspective, highlighting practical reasons for\nexisting limitations. We believe that addressing the outlined problems in\nfuture research will improve the field's ability to generate easily comparable\nresults and make measurable progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we argue that current safety alignment research efforts for\nlarge language models are hindered by many intertwined sources of noise, such\nas small datasets, methodological inconsistencies, and unreliable evaluation\nsetups. This can, at times, make it impossible to evaluate and compare attacks\nand defenses fairly, thereby slowing progress. We systematically analyze the\nLLM safety evaluation pipeline, covering dataset curation, optimization\nstrategies for automated red-teaming, response generation, and response\nevaluation using LLM judges. At each stage, we identify key issues and\nhighlight their practical impact. We also propose a set of guidelines for\nreducing noise and bias in evaluations of future attack and defense papers.\nLastly, we offer an opposing perspective, highlighting practical reasons for\nexisting limitations. We believe that addressing the outlined problems in\nfuture research will improve the field's ability to generate easily comparable\nresults and make measurable progress."
                },
                "authors": [
                    {
                        "name": "Tim Beyer"
                    },
                    {
                        "name": "Sophie Xhonneux"
                    },
                    {
                        "name": "Simon Geisler"
                    },
                    {
                        "name": "Gauthier Gidel"
                    },
                    {
                        "name": "Leo Schwinn"
                    },
                    {
                        "name": "Stephan Günnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Günnemann"
                },
                "author": "Stephan Günnemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06145v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06145v3",
                "updated": "2025-03-04T12:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    36,
                    51,
                    1,
                    63,
                    0
                ],
                "published": "2024-11-09T11:13:14Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    11,
                    13,
                    14,
                    5,
                    314,
                    0
                ],
                "title": "Escalating LLM-based Code Translation Benchmarking into the Class-level\n  Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Escalating LLM-based Code Translation Benchmarking into the Class-level\n  Era"
                },
                "summary": "In recent years, Large Language Models (LLMs) have dramatically advanced the\nperformance of automated code translation, making their computational accuracy\nscore reach up to over 80% on many previous benchmarks. However, most code\nsamples in these benchmarks are short, standalone, statement/method-level, and\nalgorithmic, which is not aligned with practical coding tasks. Therefore, it is\nstill unknown the actual capability of LLMs in translating code samples written\nfor daily development. To achieve this, we construct a class-level code\ntranslation benchmark, ClassEval-T, and make the first attempt to extensively\nassess recent LLMs' performance on class-level code translation. ClassEval-T is\nextended from ClassEval, a well-known class-level Python code generation\nbenchmark consisting of multiple practical coding topics, such as database\noperation and game design, and diverse contextual dependencies (e.g., fields,\nmethods, and libraries). It cost us 360 person-hours to accomplish the manual\nmigration to Java and C++ with complete code samples and associated test\nsuites. Subsequently, we design three translation strategies (i.e., holistic,\nmin-dependency, and standalone) for class-level code translations and evaluate\neight recent LLMs of commercial, general, and code kinds in diverse families\nand sizes on ClassEval-T. Experimental results demonstrate a remarkable\nperformance drop compared with the most widely studied method-level code\ntranslation benchmark, and obvious discrepancies among LLMs appear, showing the\neffectiveness of ClassEval-T in measuring recent LLMs. Afterwards, we further\ndiscuss the usage scenarios for diverse translation strategies and LLMs'\nability to dependency awareness when translating class samples. Finally, 1,243\nfailure cases made by the best-performing LLM under test are analyzed and\ncategorized in this paper for practical guidance and future enlightenment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have dramatically advanced the\nperformance of automated code translation, making their computational accuracy\nscore reach up to over 80% on many previous benchmarks. However, most code\nsamples in these benchmarks are short, standalone, statement/method-level, and\nalgorithmic, which is not aligned with practical coding tasks. Therefore, it is\nstill unknown the actual capability of LLMs in translating code samples written\nfor daily development. To achieve this, we construct a class-level code\ntranslation benchmark, ClassEval-T, and make the first attempt to extensively\nassess recent LLMs' performance on class-level code translation. ClassEval-T is\nextended from ClassEval, a well-known class-level Python code generation\nbenchmark consisting of multiple practical coding topics, such as database\noperation and game design, and diverse contextual dependencies (e.g., fields,\nmethods, and libraries). It cost us 360 person-hours to accomplish the manual\nmigration to Java and C++ with complete code samples and associated test\nsuites. Subsequently, we design three translation strategies (i.e., holistic,\nmin-dependency, and standalone) for class-level code translations and evaluate\neight recent LLMs of commercial, general, and code kinds in diverse families\nand sizes on ClassEval-T. Experimental results demonstrate a remarkable\nperformance drop compared with the most widely studied method-level code\ntranslation benchmark, and obvious discrepancies among LLMs appear, showing the\neffectiveness of ClassEval-T in measuring recent LLMs. Afterwards, we further\ndiscuss the usage scenarios for diverse translation strategies and LLMs'\nability to dependency awareness when translating class samples. Finally, 1,243\nfailure cases made by the best-performing LLM under test are analyzed and\ncategorized in this paper for practical guidance and future enlightenment."
                },
                "authors": [
                    {
                        "name": "Pengyu Xue"
                    },
                    {
                        "name": "Linhao Wu"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Chengyi Wang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Ruikai Jin"
                    },
                    {
                        "name": "Yifei Pei"
                    },
                    {
                        "name": "Zhaoyan Shen"
                    },
                    {
                        "name": "Xiran Lyu"
                    },
                    {
                        "name": "Jacky Wai Keung"
                    }
                ],
                "author_detail": {
                    "name": "Jacky Wai Keung"
                },
                "author": "Jacky Wai Keung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06145v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06145v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00196v2",
                "updated": "2025-03-04T12:36:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    36,
                    10,
                    1,
                    63,
                    0
                ],
                "published": "2025-01-31T22:26:33Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    22,
                    26,
                    33,
                    4,
                    31,
                    0
                ],
                "title": "DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access\n  Dermatology Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access\n  Dermatology Datasets"
                },
                "summary": "A major barrier to developing vision large language models (LLMs) in\ndermatology is the lack of large image--text pairs dataset. We introduce\nDermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated\nfrom 45,205 images (13,568 clinical and 35,561 dermatoscopic) for\ndermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using\nGemini 2.0, we used clinically related prompts and self-instruct method to\ngenerate diverse and rich synthetic texts. Metadata of the datasets were\nincorporated into the input prompts by targeting to reduce potential\nhallucinations. The resulting dataset builds upon open access dermatological\nimage repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have\npermissive CC-BY-4.0 licenses. We also fine-tuned a preliminary\nLlama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We\nanticipate this dataset to support and accelerate AI research in dermatology.\nData and code underlying this work are accessible at\nhttps://github.com/abdurrahimyilmaz/DermaSynth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major barrier to developing vision large language models (LLMs) in\ndermatology is the lack of large image--text pairs dataset. We introduce\nDermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated\nfrom 45,205 images (13,568 clinical and 35,561 dermatoscopic) for\ndermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using\nGemini 2.0, we used clinically related prompts and self-instruct method to\ngenerate diverse and rich synthetic texts. Metadata of the datasets were\nincorporated into the input prompts by targeting to reduce potential\nhallucinations. The resulting dataset builds upon open access dermatological\nimage repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have\npermissive CC-BY-4.0 licenses. We also fine-tuned a preliminary\nLlama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We\nanticipate this dataset to support and accelerate AI research in dermatology.\nData and code underlying this work are accessible at\nhttps://github.com/abdurrahimyilmaz/DermaSynth."
                },
                "authors": [
                    {
                        "name": "Abdurrahim Yilmaz"
                    },
                    {
                        "name": "Furkan Yuceyalcin"
                    },
                    {
                        "name": "Ece Gokyayla"
                    },
                    {
                        "name": "Donghee Choi"
                    },
                    {
                        "name": "Ozan Erdem"
                    },
                    {
                        "name": "Ali Anil Demircali"
                    },
                    {
                        "name": "Rahmetullah Varol"
                    },
                    {
                        "name": "Ufuk Gorkem Kirabali"
                    },
                    {
                        "name": "Gulsum Gencoglan"
                    },
                    {
                        "name": "Joram M. Posma"
                    },
                    {
                        "name": "Burak Temelkuran"
                    }
                ],
                "author_detail": {
                    "name": "Burak Temelkuran"
                },
                "author": "Burak Temelkuran",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02552v1",
                "updated": "2025-03-04T12:25:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    25,
                    1,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T12:25:01Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    25,
                    1,
                    1,
                    63,
                    0
                ],
                "title": "World Models for Anomaly Detection during Model-Based Reinforcement\n  Learning Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "World Models for Anomaly Detection during Model-Based Reinforcement\n  Learning Inference"
                },
                "summary": "Learning-based controllers are often purposefully kept out of real-world\napplications due to concerns about their safety and reliability. We explore how\nstate-of-the-art world models in Model-Based Reinforcement Learning can be\nutilized beyond the training phase to ensure a deployed policy only operates\nwithin regions of the state-space it is sufficiently familiar with. This is\nachieved by continuously monitoring discrepancies between a world model's\npredictions and observed system behavior during inference. It allows for\ntriggering appropriate measures, such as an emergency stop, once an error\nthreshold is surpassed. This does not require any task-specific knowledge and\nis thus universally applicable. Simulated experiments on established robot\ncontrol tasks show the effectiveness of this method, recognizing changes in\nlocal robot geometry and global gravitational magnitude. Real-world experiments\nusing an agile quadcopter further demonstrate the benefits of this approach by\ndetecting unexpected forces acting on the vehicle. These results indicate how\neven in new and adverse conditions, safe and reliable operation of otherwise\nunpredictable learning-based controllers can be achieved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based controllers are often purposefully kept out of real-world\napplications due to concerns about their safety and reliability. We explore how\nstate-of-the-art world models in Model-Based Reinforcement Learning can be\nutilized beyond the training phase to ensure a deployed policy only operates\nwithin regions of the state-space it is sufficiently familiar with. This is\nachieved by continuously monitoring discrepancies between a world model's\npredictions and observed system behavior during inference. It allows for\ntriggering appropriate measures, such as an emergency stop, once an error\nthreshold is surpassed. This does not require any task-specific knowledge and\nis thus universally applicable. Simulated experiments on established robot\ncontrol tasks show the effectiveness of this method, recognizing changes in\nlocal robot geometry and global gravitational magnitude. Real-world experiments\nusing an agile quadcopter further demonstrate the benefits of this approach by\ndetecting unexpected forces acting on the vehicle. These results indicate how\neven in new and adverse conditions, safe and reliable operation of otherwise\nunpredictable learning-based controllers can be achieved."
                },
                "authors": [
                    {
                        "name": "Fabian Domberg"
                    },
                    {
                        "name": "Georg Schildbach"
                    }
                ],
                "author_detail": {
                    "name": "Georg Schildbach"
                },
                "author": "Georg Schildbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02550v1",
                "updated": "2025-03-04T12:21:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    21,
                    28,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T12:21:28Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    21,
                    28,
                    1,
                    63,
                    0
                ],
                "title": "SpecInF: Exploiting Idle GPU Resources in Distributed DL Training via\n  Speculative Inference Filling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecInF: Exploiting Idle GPU Resources in Distributed DL Training via\n  Speculative Inference Filling"
                },
                "summary": "Deep Learning (DL), especially with Large Language Models (LLMs), brings\nbenefits to various areas. However, DL training systems usually yield prominent\nidling GPU resources due to many factors, such as resource allocation and\ncollective communication. To improve GPU utilization, we present SpecInF, which\nadopts a \\textbf{Spec}ulative \\textbf{In}ference \\textbf{F}illing method to\nexploit idle GPU resources. It collocates each primary training instance with\nadditional inference instances on the same GPU, detects the training bubbles\nand adaptively fills with online or offline inference workloads. Our results\nshow that SpecInF can effectively enhance GPU utilization under mainstream\nparallel training modes, delivering additional up to 14$\\times$ offline\ninference throughputs than TGS and 67\\% reduction in online inference p95\nlatency than MPS, while guaranteeing collocated training throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning (DL), especially with Large Language Models (LLMs), brings\nbenefits to various areas. However, DL training systems usually yield prominent\nidling GPU resources due to many factors, such as resource allocation and\ncollective communication. To improve GPU utilization, we present SpecInF, which\nadopts a \\textbf{Spec}ulative \\textbf{In}ference \\textbf{F}illing method to\nexploit idle GPU resources. It collocates each primary training instance with\nadditional inference instances on the same GPU, detects the training bubbles\nand adaptively fills with online or offline inference workloads. Our results\nshow that SpecInF can effectively enhance GPU utilization under mainstream\nparallel training modes, delivering additional up to 14$\\times$ offline\ninference throughputs than TGS and 67\\% reduction in online inference p95\nlatency than MPS, while guaranteeing collocated training throughput."
                },
                "authors": [
                    {
                        "name": "Cunchi Lv"
                    },
                    {
                        "name": "Xiao Shi"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Wenting Tan"
                    },
                    {
                        "name": "Xiaofang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofang Zhao"
                },
                "author": "Xiaofang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02542v1",
                "updated": "2025-03-04T12:12:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    12,
                    37,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T12:12:37Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    12,
                    37,
                    1,
                    63,
                    0
                ],
                "title": "Efficient Long Sequential Low-rank Adaptive Attention for Click-through\n  rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long Sequential Low-rank Adaptive Attention for Click-through\n  rate Prediction"
                },
                "summary": "In the context of burgeoning user historical behavior data, Accurate\nclick-through rate(CTR) prediction requires effective modeling of lengthy user\nbehavior sequences. As the volume of such data keeps swelling, the focus of\nresearch has shifted towards developing effective long-term behavior modeling\nmethods to capture latent user interests. Nevertheless, the complexity\nintroduced by large scale data brings about computational hurdles. There is a\npressing need to strike a balance between achieving high model performance and\nmeeting the strict response time requirements of online services. While\nexisting retrieval-based methods (e.g., similarity filtering or attention\napproximation) achieve practical runtime efficiency, they inherently compromise\ninformation fidelity through aggressive sequence truncation or attention\nsparsification. This paper presents a novel attention mechanism. It overcomes\nthe shortcomings of existing methods while ensuring computational efficiency.\nThis mechanism learn compressed representation of sequence with length $L$ via\nlow-rank projection matrices (rank $r \\ll L$), reducing attention complexity\nfrom $O(L)$ to $O(r)$. It also integrates a uniquely designed loss function to\npreserve nonlinearity of attention. In the inference stage, the mechanism\nadopts matrix absorption and prestorage strategies. These strategies enable it\nto effectively satisfy online constraints. Comprehensive offline and online\nexperiments demonstrate that the proposed method outperforms current\nstate-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of burgeoning user historical behavior data, Accurate\nclick-through rate(CTR) prediction requires effective modeling of lengthy user\nbehavior sequences. As the volume of such data keeps swelling, the focus of\nresearch has shifted towards developing effective long-term behavior modeling\nmethods to capture latent user interests. Nevertheless, the complexity\nintroduced by large scale data brings about computational hurdles. There is a\npressing need to strike a balance between achieving high model performance and\nmeeting the strict response time requirements of online services. While\nexisting retrieval-based methods (e.g., similarity filtering or attention\napproximation) achieve practical runtime efficiency, they inherently compromise\ninformation fidelity through aggressive sequence truncation or attention\nsparsification. This paper presents a novel attention mechanism. It overcomes\nthe shortcomings of existing methods while ensuring computational efficiency.\nThis mechanism learn compressed representation of sequence with length $L$ via\nlow-rank projection matrices (rank $r \\ll L$), reducing attention complexity\nfrom $O(L)$ to $O(r)$. It also integrates a uniquely designed loss function to\npreserve nonlinearity of attention. In the inference stage, the mechanism\nadopts matrix absorption and prestorage strategies. These strategies enable it\nto effectively satisfy online constraints. Comprehensive offline and online\nexperiments demonstrate that the proposed method outperforms current\nstate-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Xin Song"
                    },
                    {
                        "name": "Xiaochen Li"
                    },
                    {
                        "name": "Jinxin Hu"
                    },
                    {
                        "name": "Hong Wen"
                    },
                    {
                        "name": "Zulong Chen"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Xiaoyi Zeng"
                    },
                    {
                        "name": "Zhang Jing"
                    }
                ],
                "author_detail": {
                    "name": "Zhang Jing"
                },
                "author": "Zhang Jing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00401v2",
                "updated": "2025-03-04T12:04:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    4,
                    26,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-01T08:29:59Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    8,
                    29,
                    59,
                    5,
                    60,
                    0
                ],
                "title": "Smoothing Grounding and Reasoning for MLLM-Powered GUI Agents with\n  Query-Oriented Pivot Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smoothing Grounding and Reasoning for MLLM-Powered GUI Agents with\n  Query-Oriented Pivot Tasks"
                },
                "summary": "Perception-enhanced pre-training, particularly through grounding techniques,\nis widely adopted to enhance the performance of graphical user interface (GUI)\nagents. However, in resource-constrained scenarios, the format discrepancy\nbetween coordinate-oriented grounding and action-oriented reasoning limits the\neffectiveness of grounding for reasoning tasks. To address this challenge, we\npropose a query-oriented pivot approach called query inference, which serves as\na bridge between GUI grounding and reasoning. By inferring potential user\nqueries from a screenshot and its associated element coordinates, query\ninference improves the understanding of coordinates while aligning more closely\nwith reasoning tasks. Experimental results show that query inference\noutperforms previous grounding techniques under the same training data scale.\nNotably, query inference achieves comparable or even better performance to\nlarge-scale grounding-enhanced OS-Atlas with less than 0.1% of training data.\nFurthermore, we explore the impact of reasoning formats and demonstrate that\nintegrating additional semantic information into the input further boosts\nreasoning performance. The code is publicly available at\nhttps://github.com/ZrW00/GUIPivot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-enhanced pre-training, particularly through grounding techniques,\nis widely adopted to enhance the performance of graphical user interface (GUI)\nagents. However, in resource-constrained scenarios, the format discrepancy\nbetween coordinate-oriented grounding and action-oriented reasoning limits the\neffectiveness of grounding for reasoning tasks. To address this challenge, we\npropose a query-oriented pivot approach called query inference, which serves as\na bridge between GUI grounding and reasoning. By inferring potential user\nqueries from a screenshot and its associated element coordinates, query\ninference improves the understanding of coordinates while aligning more closely\nwith reasoning tasks. Experimental results show that query inference\noutperforms previous grounding techniques under the same training data scale.\nNotably, query inference achieves comparable or even better performance to\nlarge-scale grounding-enhanced OS-Atlas with less than 0.1% of training data.\nFurthermore, we explore the impact of reasoning formats and demonstrate that\nintegrating additional semantic information into the input further boosts\nreasoning performance. The code is publicly available at\nhttps://github.com/ZrW00/GUIPivot."
                },
                "authors": [
                    {
                        "name": "Zongru Wu"
                    },
                    {
                        "name": "Pengzhou Cheng"
                    },
                    {
                        "name": "Zheng Wu"
                    },
                    {
                        "name": "Tianjie Ju"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Gongshen Liu"
                },
                "author": "Gongshen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02532v1",
                "updated": "2025-03-04T11:56:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    56,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:56:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    56,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Use Me Wisely: AI-Driven Assessment for LLM Prompting Skills Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Use Me Wisely: AI-Driven Assessment for LLM Prompting Skills Development"
                },
                "summary": "The use of large language model (LLM)-powered chatbots, such as ChatGPT, has\nbecome popular across various domains, supporting a range of tasks and\nprocesses. However, due to the intrinsic complexity of LLMs, effective\nprompting is more challenging than it may seem. This highlights the need for\ninnovative educational and support strategies that are both widely accessible\nand seamlessly integrated into task workflows. Yet, LLM prompting is highly\ntask- and domain-dependent, limiting the effectiveness of generic approaches.\nIn this study, we explore whether LLM-based methods can facilitate learning\nassessments by using ad-hoc guidelines and a minimal number of annotated prompt\nsamples. Our framework transforms these guidelines into features that can be\nidentified within learners' prompts. Using these feature descriptions and\nannotated examples, we create few-shot learning detectors. We then evaluate\ndifferent configurations of these detectors, testing three state-of-the-art\nLLMs and ensembles. We run experiments with cross-validation on a sample of\noriginal prompts, as well as tests on prompts collected from task-naive\nlearners. Our results show how LLMs perform on feature detection. Notably, GPT-\n4 demonstrates strong performance on most features, while closely related\nmodels, such as GPT-3 and GPT-3.5 Turbo (Instruct), show inconsistent behaviors\nin feature classification. These differences highlight the need for further\nresearch into how design choices impact feature selection and prompt detection.\nOur findings contribute to the fields of generative AI literacy and\ncomputer-supported learning assessment, offering valuable insights for both\nresearchers and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language model (LLM)-powered chatbots, such as ChatGPT, has\nbecome popular across various domains, supporting a range of tasks and\nprocesses. However, due to the intrinsic complexity of LLMs, effective\nprompting is more challenging than it may seem. This highlights the need for\ninnovative educational and support strategies that are both widely accessible\nand seamlessly integrated into task workflows. Yet, LLM prompting is highly\ntask- and domain-dependent, limiting the effectiveness of generic approaches.\nIn this study, we explore whether LLM-based methods can facilitate learning\nassessments by using ad-hoc guidelines and a minimal number of annotated prompt\nsamples. Our framework transforms these guidelines into features that can be\nidentified within learners' prompts. Using these feature descriptions and\nannotated examples, we create few-shot learning detectors. We then evaluate\ndifferent configurations of these detectors, testing three state-of-the-art\nLLMs and ensembles. We run experiments with cross-validation on a sample of\noriginal prompts, as well as tests on prompts collected from task-naive\nlearners. Our results show how LLMs perform on feature detection. Notably, GPT-\n4 demonstrates strong performance on most features, while closely related\nmodels, such as GPT-3 and GPT-3.5 Turbo (Instruct), show inconsistent behaviors\nin feature classification. These differences highlight the need for further\nresearch into how design choices impact feature selection and prompt detection.\nOur findings contribute to the fields of generative AI literacy and\ncomputer-supported learning assessment, offering valuable insights for both\nresearchers and practitioners."
                },
                "authors": [
                    {
                        "name": "Dimitri Ognibene"
                    },
                    {
                        "name": "Gregor Donabauer"
                    },
                    {
                        "name": "Emily Theophilou"
                    },
                    {
                        "name": "Cansu Koyuturk"
                    },
                    {
                        "name": "Mona Yavari"
                    },
                    {
                        "name": "Sathya Bursic"
                    },
                    {
                        "name": "Alessia Telari"
                    },
                    {
                        "name": "Alessia Testa"
                    },
                    {
                        "name": "Raffaele Boiano"
                    },
                    {
                        "name": "Davide Taibi"
                    },
                    {
                        "name": "Davinia Hernandez-Leo"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    },
                    {
                        "name": "Martin Ruskov"
                    }
                ],
                "author_detail": {
                    "name": "Martin Ruskov"
                },
                "author": "Martin Ruskov",
                "arxiv_comment": "Preprint accepted for Publication in Educational Technology & Society\n  (ET&S)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06860v2",
                "updated": "2025-03-04T11:47:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    47,
                    27,
                    1,
                    63,
                    0
                ],
                "published": "2024-12-09T02:36:38Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    2,
                    36,
                    38,
                    0,
                    344,
                    0
                ],
                "title": "Balancing Efficiency and Effectiveness: An LLM-Infused Approach for\n  Optimized CTR Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Efficiency and Effectiveness: An LLM-Infused Approach for\n  Optimized CTR Prediction"
                },
                "summary": "Click-Through Rate (CTR) prediction is essential in online advertising, where\nsemantic information plays a pivotal role in shaping user decisions and\nenhancing CTR effectiveness. Capturing and modeling deep semantic information,\nsuch as a user's preference for \"H\\\"aagen-Dazs' HEAVEN strawberry light ice\ncream\" due to its health-conscious and premium attributes, is challenging.\nTraditional semantic modeling often overlooks these intricate details at the\nuser and item levels. To bridge this gap, we introduce a novel approach that\nmodels deep semantic information end-to-end, leveraging the comprehensive world\nknowledge capabilities of Large Language Models (LLMs). Our proposed\nLLM-infused CTR prediction framework(Multi-level Deep Semantic Information\nInfused CTR model via Distillation, MSD) is designed to uncover deep semantic\ninsights by utilizing LLMs to extract and distill critical information into a\nsmaller, more efficient model, enabling seamless end-to-end training and\ninference. Importantly, our framework is carefully designed to balance\nefficiency and effectiveness, ensuring that the model not only achieves high\nperformance but also operates with optimal resource utilization. Online A/B\ntests conducted on the Meituan sponsored-search system demonstrate that our\nmethod significantly outperforms baseline models in terms of Cost Per Mile\n(CPM) and CTR, validating its effectiveness, scalability, and balanced approach\nin real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Click-Through Rate (CTR) prediction is essential in online advertising, where\nsemantic information plays a pivotal role in shaping user decisions and\nenhancing CTR effectiveness. Capturing and modeling deep semantic information,\nsuch as a user's preference for \"H\\\"aagen-Dazs' HEAVEN strawberry light ice\ncream\" due to its health-conscious and premium attributes, is challenging.\nTraditional semantic modeling often overlooks these intricate details at the\nuser and item levels. To bridge this gap, we introduce a novel approach that\nmodels deep semantic information end-to-end, leveraging the comprehensive world\nknowledge capabilities of Large Language Models (LLMs). Our proposed\nLLM-infused CTR prediction framework(Multi-level Deep Semantic Information\nInfused CTR model via Distillation, MSD) is designed to uncover deep semantic\ninsights by utilizing LLMs to extract and distill critical information into a\nsmaller, more efficient model, enabling seamless end-to-end training and\ninference. Importantly, our framework is carefully designed to balance\nefficiency and effectiveness, ensuring that the model not only achieves high\nperformance but also operates with optimal resource utilization. Online A/B\ntests conducted on the Meituan sponsored-search system demonstrate that our\nmethod significantly outperforms baseline models in terms of Cost Per Mile\n(CPM) and CTR, validating its effectiveness, scalability, and balanced approach\nin real-world applications."
                },
                "authors": [
                    {
                        "name": "Guoxiao Zhang"
                    },
                    {
                        "name": "Yi Wei"
                    },
                    {
                        "name": "Yadong Zhang"
                    },
                    {
                        "name": "Huajian Feng"
                    },
                    {
                        "name": "Qiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Liu"
                },
                "author": "Qiang Liu",
                "arxiv_comment": "5 pages, 4 figures,4 tables",
                "arxiv_journal_ref": "WWW2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16533v2",
                "updated": "2025-03-04T11:33:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    33,
                    50,
                    1,
                    63,
                    0
                ],
                "published": "2024-05-26T11:40:58Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    11,
                    40,
                    58,
                    6,
                    147,
                    0
                ],
                "title": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool\n  Agents"
                },
                "summary": "Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to extend their utility, enabling them to solve practical\ntasks. Previous methods manually parse tool documentation and create in-context\ndemonstrations, transforming tools into structured formats for LLMs to use in\ntheir step-by-step reasoning. However, this manual process requires domain\nexpertise and struggles to scale to large toolsets. Additionally, these methods\nrely heavily on ad-hoc inference techniques or special tokens to integrate\nfree-form LLM generation with tool-calling actions, limiting the LLM's\nflexibility in handling diverse tool specifications and integrating multiple\ntools.\n  In this work, we propose AutoTools, a framework that enables LLMs to automate\nthe tool-use workflow. Specifically, the LLM automatically transforms tool\ndocumentation into callable functions, verifying syntax and runtime\ncorrectness. Then, the LLM integrates these functions into executable programs\nto solve practical tasks, flexibly grounding tool-use actions into its\nreasoning processes. Extensive experiments on existing and newly collected,\nmore challenging benchmarks illustrate the superiority of our framework.\nInspired by these promising results, we further investigate how to improve the\nexpertise of LLMs, especially open-source LLMs with fewer parameters, within\nAutoTools. Thus, we propose the AutoTools-learning approach, training the LLMs\nwith three learning tasks on 34k instances of high-quality synthetic data,\nincluding documentation understanding, relevance learning, and function\nprogramming. Fine-grained results validate the effectiveness of our overall\ntraining approach and each individual task. Our methods are an important step\ntowards the use of LLMs for solving real-world tasks with external tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to extend their utility, enabling them to solve practical\ntasks. Previous methods manually parse tool documentation and create in-context\ndemonstrations, transforming tools into structured formats for LLMs to use in\ntheir step-by-step reasoning. However, this manual process requires domain\nexpertise and struggles to scale to large toolsets. Additionally, these methods\nrely heavily on ad-hoc inference techniques or special tokens to integrate\nfree-form LLM generation with tool-calling actions, limiting the LLM's\nflexibility in handling diverse tool specifications and integrating multiple\ntools.\n  In this work, we propose AutoTools, a framework that enables LLMs to automate\nthe tool-use workflow. Specifically, the LLM automatically transforms tool\ndocumentation into callable functions, verifying syntax and runtime\ncorrectness. Then, the LLM integrates these functions into executable programs\nto solve practical tasks, flexibly grounding tool-use actions into its\nreasoning processes. Extensive experiments on existing and newly collected,\nmore challenging benchmarks illustrate the superiority of our framework.\nInspired by these promising results, we further investigate how to improve the\nexpertise of LLMs, especially open-source LLMs with fewer parameters, within\nAutoTools. Thus, we propose the AutoTools-learning approach, training the LLMs\nwith three learning tasks on 34k instances of high-quality synthetic data,\nincluding documentation understanding, relevance learning, and function\nprogramming. Fine-grained results validate the effectiveness of our overall\ntraining approach and each individual task. Our methods are an important step\ntowards the use of LLMs for solving real-world tasks with external tools."
                },
                "authors": [
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Lingyong Yan"
                    },
                    {
                        "name": "Yue Feng"
                    },
                    {
                        "name": "Xiuyi Chen"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Zhaochun Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhaochun Ren"
                },
                "author": "Zhaochun Ren",
                "arxiv_comment": "Accepted by WWW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02519v1",
                "updated": "2025-03-04T11:31:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    31,
                    5,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:31:05Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    31,
                    5,
                    1,
                    63,
                    0
                ],
                "title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model\n  Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generator-Assistant Stepwise Rollback Framework for Large Language Model\n  Agent"
                },
                "summary": "Large language model (LLM) agents typically adopt a step-by-step reasoning\nframework, in which they interleave the processes of thinking and acting to\naccomplish the given task. However, this paradigm faces a deep-rooted one-pass\nissue whereby each generated intermediate thought is plugged into the\ntrajectory regardless of its correctness, which can cause irreversible error\npropagation. To address the issue, this paper proposes a novel framework called\nGenerator-Assistant Stepwise Rollback (GA-Rollback) to induce better\ndecision-making for LLM agents. Particularly, GA-Rollback utilizes a generator\nto interact with the environment and an assistant to examine each action\nproduced by the generator, where the assistant triggers a rollback operation\nupon detection of incorrect actions. Moreover, we introduce two additional\nstrategies tailored for the rollback scenario to further improve its\neffectiveness. Extensive experiments show that GA-Rollback achieves significant\nimprovements over several strong baselines on three widely used benchmarks. Our\nanalysis further reveals that GA-Rollback can function as a robust\nplug-and-play module, integrating seamlessly with other methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents typically adopt a step-by-step reasoning\nframework, in which they interleave the processes of thinking and acting to\naccomplish the given task. However, this paradigm faces a deep-rooted one-pass\nissue whereby each generated intermediate thought is plugged into the\ntrajectory regardless of its correctness, which can cause irreversible error\npropagation. To address the issue, this paper proposes a novel framework called\nGenerator-Assistant Stepwise Rollback (GA-Rollback) to induce better\ndecision-making for LLM agents. Particularly, GA-Rollback utilizes a generator\nto interact with the environment and an assistant to examine each action\nproduced by the generator, where the assistant triggers a rollback operation\nupon detection of incorrect actions. Moreover, we introduce two additional\nstrategies tailored for the rollback scenario to further improve its\neffectiveness. Extensive experiments show that GA-Rollback achieves significant\nimprovements over several strong baselines on three widely used benchmarks. Our\nanalysis further reveals that GA-Rollback can function as a robust\nplug-and-play module, integrating seamlessly with other methods."
                },
                "authors": [
                    {
                        "name": "Xingzuo Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Yunfei Long"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01747v2",
                "updated": "2025-03-04T11:30:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    30,
                    30,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-03T17:15:17Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    17,
                    15,
                    17,
                    0,
                    62,
                    0
                ],
                "title": "Position: Don't use the CLT in LLM evals with fewer than a few hundred\n  datapoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Don't use the CLT in LLM evals with fewer than a few hundred\n  datapoints"
                },
                "summary": "Rigorous statistical evaluations of large language models (LLMs), including\nvalid error bars and significance testing, are essential for meaningful and\nreliable performance assessment. Currently, when such statistical measures are\nreported, they typically rely on the Central Limit Theorem (CLT). In this\nposition paper, we argue that while CLT-based methods for uncertainty\nquantification are appropriate when benchmarks consist of thousands of\nexamples, they fail to provide adequate uncertainty estimates for LLM\nevaluations that rely on smaller, highly specialized benchmarks. In these\nsmall-data settings, we demonstrate that CLT-based methods perform very poorly,\nusually dramatically underestimating uncertainty (i.e. producing error bars\nthat are too small). We give recommendations for alternative frequentist and\nBayesian methods that are both easy to implement and more appropriate in these\nincreasingly common scenarios. We provide a simple Python library for these\nBayesian methods at https://github.com/sambowyer/bayes_evals .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous statistical evaluations of large language models (LLMs), including\nvalid error bars and significance testing, are essential for meaningful and\nreliable performance assessment. Currently, when such statistical measures are\nreported, they typically rely on the Central Limit Theorem (CLT). In this\nposition paper, we argue that while CLT-based methods for uncertainty\nquantification are appropriate when benchmarks consist of thousands of\nexamples, they fail to provide adequate uncertainty estimates for LLM\nevaluations that rely on smaller, highly specialized benchmarks. In these\nsmall-data settings, we demonstrate that CLT-based methods perform very poorly,\nusually dramatically underestimating uncertainty (i.e. producing error bars\nthat are too small). We give recommendations for alternative frequentist and\nBayesian methods that are both easy to implement and more appropriate in these\nincreasingly common scenarios. We provide a simple Python library for these\nBayesian methods at https://github.com/sambowyer/bayes_evals ."
                },
                "authors": [
                    {
                        "name": "Sam Bowyer"
                    },
                    {
                        "name": "Laurence Aitchison"
                    },
                    {
                        "name": "Desi R. Ivanova"
                    }
                ],
                "author_detail": {
                    "name": "Desi R. Ivanova"
                },
                "author": "Desi R. Ivanova",
                "arxiv_comment": "36 pages, 37 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02511v1",
                "updated": "2025-03-04T11:20:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    20,
                    10,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:20:10Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    20,
                    10,
                    1,
                    63,
                    0
                ],
                "title": "TeTRA-VPR: A Ternary Transformer Approach for Compact Visual Place\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeTRA-VPR: A Ternary Transformer Approach for Compact Visual Place\n  Recognition"
                },
                "summary": "Visual Place Recognition (VPR) localizes a query image by matching it against\na database of geo-tagged reference images, making it essential for navigation\nand mapping in robotics. Although Vision Transformer (ViT) solutions deliver\nhigh accuracy, their large models often exceed the memory and compute budgets\nof resource-constrained platforms such as drones and mobile robots. To address\nthis issue, we propose TeTRA, a ternary transformer approach that progressively\nquantizes the ViT backbone to 2-bit precision and binarizes its final embedding\nlayer, offering substantial reductions in model size and latency. A carefully\ndesigned progressive distillation strategy preserves the representational power\nof a full-precision teacher, allowing TeTRA to retain or even surpass the\naccuracy of uncompressed convolutional counterparts, despite using fewer\nresources. Experiments on standard VPR benchmarks demonstrate that TeTRA\nreduces memory consumption by up to 69% compared to efficient baselines, while\nlowering inference latency by 35%, with either no loss or a slight improvement\nin recall@1. These gains enable high-accuracy VPR on power-constrained,\nmemory-limited robotic platforms, making TeTRA an appealing solution for\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Place Recognition (VPR) localizes a query image by matching it against\na database of geo-tagged reference images, making it essential for navigation\nand mapping in robotics. Although Vision Transformer (ViT) solutions deliver\nhigh accuracy, their large models often exceed the memory and compute budgets\nof resource-constrained platforms such as drones and mobile robots. To address\nthis issue, we propose TeTRA, a ternary transformer approach that progressively\nquantizes the ViT backbone to 2-bit precision and binarizes its final embedding\nlayer, offering substantial reductions in model size and latency. A carefully\ndesigned progressive distillation strategy preserves the representational power\nof a full-precision teacher, allowing TeTRA to retain or even surpass the\naccuracy of uncompressed convolutional counterparts, despite using fewer\nresources. Experiments on standard VPR benchmarks demonstrate that TeTRA\nreduces memory consumption by up to 69% compared to efficient baselines, while\nlowering inference latency by 35%, with either no loss or a slight improvement\nin recall@1. These gains enable high-accuracy VPR on power-constrained,\nmemory-limited robotic platforms, making TeTRA an appealing solution for\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Oliver Grainge"
                    },
                    {
                        "name": "Michael Milford"
                    },
                    {
                        "name": "Indu Bodala"
                    },
                    {
                        "name": "Sarvapali D. Ramchurn"
                    },
                    {
                        "name": "Shoaib Ehsan"
                    }
                ],
                "author_detail": {
                    "name": "Shoaib Ehsan"
                },
                "author": "Shoaib Ehsan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02505v1",
                "updated": "2025-03-04T11:16:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    16,
                    46,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:16:46Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    16,
                    46,
                    1,
                    63,
                    0
                ],
                "title": "ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment"
                },
                "summary": "We aim to develop a goal specification method that is semantically clear,\nspatially sensitive, and intuitive for human users to guide agent interactions\nin embodied environments. Specifically, we propose a novel cross-view goal\nalignment framework that allows users to specify target objects using\nsegmentation masks from their own camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x. We show ROCKET-2 can\ndirectly interpret goals from human camera views for the first time, paving the\nway for better human-agent interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We aim to develop a goal specification method that is semantically clear,\nspatially sensitive, and intuitive for human users to guide agent interactions\nin embodied environments. Specifically, we propose a novel cross-view goal\nalignment framework that allows users to specify target objects using\nsegmentation masks from their own camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x. We show ROCKET-2 can\ndirectly interpret goals from human camera views for the first time, paving the\nway for better human-agent interaction."
                },
                "authors": [
                    {
                        "name": "Shaofei Cai"
                    },
                    {
                        "name": "Zhancun Mu"
                    },
                    {
                        "name": "Anji Liu"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02502v1",
                "updated": "2025-03-04T11:10:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    10,
                    13,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:10:13Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    10,
                    13,
                    1,
                    63,
                    0
                ],
                "title": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs"
                },
                "summary": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training."
                },
                "authors": [
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Yangyifan Xu"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Submitted to ACL ARR 2024 December",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02497v1",
                "updated": "2025-03-04T11:04:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    4,
                    35,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:04:35Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    4,
                    35,
                    1,
                    63,
                    0
                ],
                "title": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel\n  PennyLane-Centric Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel\n  PennyLane-Centric Dataset"
                },
                "summary": "Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning. Their\npotential in aiding quantum software development remains underexplored,\nparticularly for the PennyLane framework-a leading platform for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific code samples of\nquantum circuits and their contextual descriptions, specifically curated to\ntrain/fine-tune LLM-based quantum code assistance. Our key contributions are\nthreefold: (1) the automatic creation and open-source release of a\ncomprehensive PennyLane dataset leveraging quantum computing textbooks,\nofficial documentation, and open-source repositories; (2) the development of a\nsystematic methodology for data refinement, annotation, and formatting to\noptimize LLM training efficiency; and (3) a thorough evaluation, based on a\nRetrieval-Augmented Generation (RAG) framework, demonstrating the effectiveness\nof our dataset in streamlining PennyLane code generation and improving quantum\ndevelopment workflows. Compared to existing efforts that predominantly focus on\nQiskit, our dataset significantly broadens the spectrum of quantum frameworks\ncovered in AI-driven code assistance. By bridging this gap and providing\nreproducible dataset-creation methodologies, we aim to advance the field of\nAI-assisted quantum programming, making quantum computing more accessible to\nboth newcomers and experienced developers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning. Their\npotential in aiding quantum software development remains underexplored,\nparticularly for the PennyLane framework-a leading platform for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific code samples of\nquantum circuits and their contextual descriptions, specifically curated to\ntrain/fine-tune LLM-based quantum code assistance. Our key contributions are\nthreefold: (1) the automatic creation and open-source release of a\ncomprehensive PennyLane dataset leveraging quantum computing textbooks,\nofficial documentation, and open-source repositories; (2) the development of a\nsystematic methodology for data refinement, annotation, and formatting to\noptimize LLM training efficiency; and (3) a thorough evaluation, based on a\nRetrieval-Augmented Generation (RAG) framework, demonstrating the effectiveness\nof our dataset in streamlining PennyLane code generation and improving quantum\ndevelopment workflows. Compared to existing efforts that predominantly focus on\nQiskit, our dataset significantly broadens the spectrum of quantum frameworks\ncovered in AI-driven code assistance. By bridging this gap and providing\nreproducible dataset-creation methodologies, we aim to advance the field of\nAI-assisted quantum programming, making quantum computing more accessible to\nboth newcomers and experienced developers."
                },
                "authors": [
                    {
                        "name": "Haider Asif"
                    },
                    {
                        "name": "Abdul Basit"
                    },
                    {
                        "name": "Nouhaila Innan"
                    },
                    {
                        "name": "Muhammad Kashif"
                    },
                    {
                        "name": "Alberto Marchisio"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_comment": "10 pages, 8 figures, 6 tables, submitted for review under IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02490v1",
                "updated": "2025-03-04T10:56:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    56,
                    8,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T10:56:08Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    56,
                    8,
                    1,
                    63,
                    0
                ],
                "title": "Deep Robust Reversible Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Robust Reversible Watermarking"
                },
                "summary": "Robust Reversible Watermarking (RRW) enables perfect recovery of cover images\nand watermarks in lossless channels while ensuring robust watermark extraction\nin lossy channels. Existing RRW methods, mostly non-deep learning-based, face\ncomplex designs, high computational costs, and poor robustness, limiting their\npractical use. This paper proposes Deep Robust Reversible Watermarking (DRRW),\na deep learning-based RRW scheme. DRRW uses an Integer Invertible Watermark\nNetwork (iIWN) to map integer data distributions invertibly, addressing\nconventional RRW limitations. Unlike traditional RRW, which needs\ndistortion-specific designs, DRRW employs an encoder-noise layer-decoder\nframework for adaptive robustness via end-to-end training. In inference, cover\nimage and watermark map to an overflowed stego image and latent variables,\ncompressed by arithmetic coding into a bitstream embedded via reversible data\nhiding for lossless recovery. We introduce an overflow penalty loss to reduce\npixel overflow, shortening the auxiliary bitstream while enhancing robustness\nand stego image quality. An adaptive weight adjustment strategy avoids manual\nwatermark loss weighting, improving training stability and performance.\nExperiments show DRRW outperforms state-of-the-art RRW methods, boosting\nrobustness and cutting embedding, extraction, and recovery complexities by\n55.14\\(\\times\\), 5.95\\(\\times\\), and 3.57\\(\\times\\), respectively. The\nauxiliary bitstream shrinks by 43.86\\(\\times\\), with reversible embedding\nsucceeding on 16,762 PASCAL VOC 2012 images, advancing practical RRW. DRRW\nexceeds irreversible robust watermarking in robustness and quality while\nmaintaining reversibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Reversible Watermarking (RRW) enables perfect recovery of cover images\nand watermarks in lossless channels while ensuring robust watermark extraction\nin lossy channels. Existing RRW methods, mostly non-deep learning-based, face\ncomplex designs, high computational costs, and poor robustness, limiting their\npractical use. This paper proposes Deep Robust Reversible Watermarking (DRRW),\na deep learning-based RRW scheme. DRRW uses an Integer Invertible Watermark\nNetwork (iIWN) to map integer data distributions invertibly, addressing\nconventional RRW limitations. Unlike traditional RRW, which needs\ndistortion-specific designs, DRRW employs an encoder-noise layer-decoder\nframework for adaptive robustness via end-to-end training. In inference, cover\nimage and watermark map to an overflowed stego image and latent variables,\ncompressed by arithmetic coding into a bitstream embedded via reversible data\nhiding for lossless recovery. We introduce an overflow penalty loss to reduce\npixel overflow, shortening the auxiliary bitstream while enhancing robustness\nand stego image quality. An adaptive weight adjustment strategy avoids manual\nwatermark loss weighting, improving training stability and performance.\nExperiments show DRRW outperforms state-of-the-art RRW methods, boosting\nrobustness and cutting embedding, extraction, and recovery complexities by\n55.14\\(\\times\\), 5.95\\(\\times\\), and 3.57\\(\\times\\), respectively. The\nauxiliary bitstream shrinks by 43.86\\(\\times\\), with reversible embedding\nsucceeding on 16,762 PASCAL VOC 2012 images, advancing practical RRW. DRRW\nexceeds irreversible robust watermarking in robustness and quality while\nmaintaining reversibility."
                },
                "authors": [
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Chongyang Shi"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Yuanman Li"
                    },
                    {
                        "name": "Xiping Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiping Hu"
                },
                "author": "Xiping Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20577v2",
                "updated": "2025-03-04T10:53:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    53,
                    45,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-27T22:37:09Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    22,
                    37,
                    9,
                    3,
                    58,
                    0
                ],
                "title": "InstaFace: Identity-Preserving Facial Editing with Single Image\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstaFace: Identity-Preserving Facial Editing with Single Image\n  Inference"
                },
                "summary": "Facial appearance editing is crucial for digital avatars, AR/VR, and\npersonalized content creation, driving realistic user experiences. However,\npreserving identity with generative models is challenging, especially in\nscenarios with limited data availability. Traditional methods often require\nmultiple images and still struggle with unnatural face shifts, inconsistent\nhair alignment, or excessive smoothing effects. To overcome these challenges,\nwe introduce a novel diffusion-based framework, InstaFace, to generate\nrealistic images while preserving identity using only a single image. Central\nto InstaFace, we introduce an efficient guidance network that harnesses 3D\nperspectives by integrating multiple 3DMM-based conditionals without\nintroducing additional trainable parameters. Moreover, to ensure maximum\nidentity retention as well as preservation of background, hair, and other\ncontextual features like accessories, we introduce a novel module that utilizes\nfeature embeddings from a facial recognition model and a pre-trained\nvision-language model. Quantitative evaluations demonstrate that our method\noutperforms several state-of-the-art approaches in terms of identity\npreservation, photorealism, and effective control of pose, expression, and\nlighting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial appearance editing is crucial for digital avatars, AR/VR, and\npersonalized content creation, driving realistic user experiences. However,\npreserving identity with generative models is challenging, especially in\nscenarios with limited data availability. Traditional methods often require\nmultiple images and still struggle with unnatural face shifts, inconsistent\nhair alignment, or excessive smoothing effects. To overcome these challenges,\nwe introduce a novel diffusion-based framework, InstaFace, to generate\nrealistic images while preserving identity using only a single image. Central\nto InstaFace, we introduce an efficient guidance network that harnesses 3D\nperspectives by integrating multiple 3DMM-based conditionals without\nintroducing additional trainable parameters. Moreover, to ensure maximum\nidentity retention as well as preservation of background, hair, and other\ncontextual features like accessories, we introduce a novel module that utilizes\nfeature embeddings from a facial recognition model and a pre-trained\nvision-language model. Quantitative evaluations demonstrate that our method\noutperforms several state-of-the-art approaches in terms of identity\npreservation, photorealism, and effective control of pose, expression, and\nlighting."
                },
                "authors": [
                    {
                        "name": "MD Wahiduzzaman Khan"
                    },
                    {
                        "name": "Mingshan Jia"
                    },
                    {
                        "name": "Xiaolin Zhang"
                    },
                    {
                        "name": "En Yu"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Kaska Musial-Gabrys"
                    }
                ],
                "author_detail": {
                    "name": "Kaska Musial-Gabrys"
                },
                "author": "Kaska Musial-Gabrys",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02476v1",
                "updated": "2025-03-04T10:39:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    39,
                    42,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T10:39:42Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    39,
                    42,
                    1,
                    63,
                    0
                ],
                "title": "BioD2C: A Dual-level Semantic Consistency Constraint Framework for\n  Biomedical VQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioD2C: A Dual-level Semantic Consistency Constraint Framework for\n  Biomedical VQA"
                },
                "summary": "Biomedical visual question answering (VQA) has been widely studied and has\ndemonstrated significant application value and potential in fields such as\nassistive medical diagnosis. Despite their success, current biomedical VQA\nmodels perform multimodal information interaction only at the model level\nwithin large language models (LLMs), leading to suboptimal multimodal semantic\nalignment when dealing with complex tasks. To address this issue, we propose\nBioD2C: a novel Dual-level Semantic Consistency Constraint Framework for\nBiomedical VQA, which achieves dual-level semantic interaction alignment at\nboth the model and feature levels, enabling the model to adaptively learn\nvisual features based on the question. Specifically, we firstly integrate\ntextual features into visual features via an image-text fusion mechanism as\nfeature-level semantic interaction, obtaining visual features conditioned on\nthe given text; and then introduce a text-queue-based cross-modal soft semantic\nloss function to further align the image semantics with the question semantics.\nSpecifically, in this work, we establish a new dataset, BioVGQ, to address\ninherent biases in prior datasets by filtering manually-altered images and\naligning question-answer pairs with multimodal context, and train our model on\nthis dataset. Extensive experimental results demonstrate that BioD2C achieves\nstate-of-the-art (SOTA) performance across multiple downstream datasets,\nshowcasing its robustness, generalizability, and potential to advance\nbiomedical VQA research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical visual question answering (VQA) has been widely studied and has\ndemonstrated significant application value and potential in fields such as\nassistive medical diagnosis. Despite their success, current biomedical VQA\nmodels perform multimodal information interaction only at the model level\nwithin large language models (LLMs), leading to suboptimal multimodal semantic\nalignment when dealing with complex tasks. To address this issue, we propose\nBioD2C: a novel Dual-level Semantic Consistency Constraint Framework for\nBiomedical VQA, which achieves dual-level semantic interaction alignment at\nboth the model and feature levels, enabling the model to adaptively learn\nvisual features based on the question. Specifically, we firstly integrate\ntextual features into visual features via an image-text fusion mechanism as\nfeature-level semantic interaction, obtaining visual features conditioned on\nthe given text; and then introduce a text-queue-based cross-modal soft semantic\nloss function to further align the image semantics with the question semantics.\nSpecifically, in this work, we establish a new dataset, BioVGQ, to address\ninherent biases in prior datasets by filtering manually-altered images and\naligning question-answer pairs with multimodal context, and train our model on\nthis dataset. Extensive experimental results demonstrate that BioD2C achieves\nstate-of-the-art (SOTA) performance across multiple downstream datasets,\nshowcasing its robustness, generalizability, and potential to advance\nbiomedical VQA research."
                },
                "authors": [
                    {
                        "name": "Zhengyang Ji"
                    },
                    {
                        "name": "Shang Gao"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Yifan Jia"
                    },
                    {
                        "name": "Yutao Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Yue"
                },
                "author": "Yutao Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02465v1",
                "updated": "2025-03-04T10:21:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    21,
                    58,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T10:21:58Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    21,
                    58,
                    1,
                    63,
                    0
                ],
                "title": "UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search\n  and Rescue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search\n  and Rescue"
                },
                "summary": "Emergency search and rescue (SAR) operations often require rapid and precise\ntarget identification in complex environments where traditional manual drone\ncontrol is inefficient. In order to address these scenarios, a rapid SAR\nsystem, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this\nresearch. This system consists of two aspects: 1) A multimodal system which\nharnesses the power of Visual Language Model (VLM) and the natural language\nprocessing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A\nnon-linearmodel predictive control (NMPC) with built-in obstacle avoidance for\nrapid response by a drone to fly according to the output of the multimodal\nsystem. This work aims at improving response times in emergency SAR operations\nby providing a more intuitive and natural approach to the operator to plan the\nSAR mission while allowing the drone to carry out that mission in a rapid and\nsafe manner. When tested, our approach was faster on an average by 33.75% when\ncompared with an off-the-shelf autopilot and 54.6% when compared with a human\npilot. Video of UAV-VLRR: https://youtu.be/KJqQGKKt1xY",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency search and rescue (SAR) operations often require rapid and precise\ntarget identification in complex environments where traditional manual drone\ncontrol is inefficient. In order to address these scenarios, a rapid SAR\nsystem, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this\nresearch. This system consists of two aspects: 1) A multimodal system which\nharnesses the power of Visual Language Model (VLM) and the natural language\nprocessing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A\nnon-linearmodel predictive control (NMPC) with built-in obstacle avoidance for\nrapid response by a drone to fly according to the output of the multimodal\nsystem. This work aims at improving response times in emergency SAR operations\nby providing a more intuitive and natural approach to the operator to plan the\nSAR mission while allowing the drone to carry out that mission in a rapid and\nsafe manner. When tested, our approach was faster on an average by 33.75% when\ncompared with an off-the-shelf autopilot and 54.6% when compared with a human\npilot. Video of UAV-VLRR: https://youtu.be/KJqQGKKt1xY"
                },
                "authors": [
                    {
                        "name": "Yasheerah Yaqoot"
                    },
                    {
                        "name": "Muhammad Ahsan Mustafa"
                    },
                    {
                        "name": "Oleg Sautenkov"
                    },
                    {
                        "name": "Dzmitry Tsetserukou"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Tsetserukou"
                },
                "author": "Dzmitry Tsetserukou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02463v1",
                "updated": "2025-03-04T10:17:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    17,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T10:17:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    17,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate\n  Mutually via Selective Rationale Optimisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate\n  Mutually via Selective Rationale Optimisation"
                },
                "summary": "Very large language models (LLMs) such as GPT-4 have shown the ability to\nhandle complex tasks by generating and self-refining step-by-step rationales.\nSmaller language models (SLMs), typically with < 13B parameters, have been\nimproved by using the data generated from very-large LMs through knowledge\ndistillation. However, various practical constraints such as API costs,\ncopyright, legal and ethical policies restrict using large (often opaque)\nmodels to train smaller models for commercial use. Limited success has been\nachieved at improving the ability of an SLM to explore the space of possible\nrationales and evaluate them by itself through self-deliberation. To address\nthis, we propose COALITION, a trainable framework that facilitates interaction\nbetween two variants of the same SLM and trains them to generate and refine\nrationales optimized for the end-task. The variants exhibit different behaviors\nto produce a set of diverse candidate rationales during the generation and\nrefinement steps. The model is then trained via Selective Rationale\nOptimization (SRO) to prefer generating rationale candidates that maximize the\nlikelihood of producing the ground-truth answer. During inference, COALITION\nemploys a controller to select the suitable variant for generating and refining\nthe rationales. On five different datasets covering mathematical problems,\ncommonsense reasoning, and natural language inference, COALITION outperforms\nseveral baselines by up to 5%. Our ablation studies reveal that\ncross-communication between the two variants performs better than using the\nsingle model to self-refine the rationales. We also demonstrate the\napplicability of COALITION for LMs of varying scales (4B to 14B parameters) and\nmodel families (Mistral, Llama, Qwen, Phi). We release the code for this work\nat https://github.com/Sohanpatnaik106/coalition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Very large language models (LLMs) such as GPT-4 have shown the ability to\nhandle complex tasks by generating and self-refining step-by-step rationales.\nSmaller language models (SLMs), typically with < 13B parameters, have been\nimproved by using the data generated from very-large LMs through knowledge\ndistillation. However, various practical constraints such as API costs,\ncopyright, legal and ethical policies restrict using large (often opaque)\nmodels to train smaller models for commercial use. Limited success has been\nachieved at improving the ability of an SLM to explore the space of possible\nrationales and evaluate them by itself through self-deliberation. To address\nthis, we propose COALITION, a trainable framework that facilitates interaction\nbetween two variants of the same SLM and trains them to generate and refine\nrationales optimized for the end-task. The variants exhibit different behaviors\nto produce a set of diverse candidate rationales during the generation and\nrefinement steps. The model is then trained via Selective Rationale\nOptimization (SRO) to prefer generating rationale candidates that maximize the\nlikelihood of producing the ground-truth answer. During inference, COALITION\nemploys a controller to select the suitable variant for generating and refining\nthe rationales. On five different datasets covering mathematical problems,\ncommonsense reasoning, and natural language inference, COALITION outperforms\nseveral baselines by up to 5%. Our ablation studies reveal that\ncross-communication between the two variants performs better than using the\nsingle model to self-refine the rationales. We also demonstrate the\napplicability of COALITION for LMs of varying scales (4B to 14B parameters) and\nmodel families (Mistral, Llama, Qwen, Phi). We release the code for this work\nat https://github.com/Sohanpatnaik106/coalition."
                },
                "authors": [
                    {
                        "name": "Sohan Patnaik"
                    },
                    {
                        "name": "Milan Aggarwal"
                    },
                    {
                        "name": "Sumit Bhatia"
                    },
                    {
                        "name": "Balaji Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Balaji Krishnamurthy"
                },
                "author": "Balaji Krishnamurthy",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02457v1",
                "updated": "2025-03-04T10:06:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    6,
                    41,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T10:06:41Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    6,
                    41,
                    1,
                    63,
                    0
                ],
                "title": "Don't Get Too Excited -- Eliciting Emotions in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Get Too Excited -- Eliciting Emotions in LLMs"
                },
                "summary": "This paper investigates the challenges of affect control in large language\nmodels (LLMs), focusing on their ability to express appropriate emotional\nstates during extended dialogues. We evaluated state-of-the-art open-weight\nLLMs to assess their affective expressive range in terms of arousal and\nvalence. Our study employs a novel methodology combining LLM-based sentiment\nanalysis with multiturn dialogue simulations between LLMs. We quantify the\nmodels' capacity to express a wide spectrum of emotions and how they fluctuate\nduring interactions. Our findings reveal significant variations among LLMs in\ntheir ability to maintain consistent affect, with some models demonstrating\nmore stable emotional trajectories than others. Furthermore, we identify key\nchallenges in affect control, including difficulties in producing and\nmaintaining extreme emotional states and limitations in adapting affect to\nchanging conversational contexts. These findings have important implications\nfor the development of more emotionally intelligent AI systems and highlight\nthe need for improved affect modelling in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the challenges of affect control in large language\nmodels (LLMs), focusing on their ability to express appropriate emotional\nstates during extended dialogues. We evaluated state-of-the-art open-weight\nLLMs to assess their affective expressive range in terms of arousal and\nvalence. Our study employs a novel methodology combining LLM-based sentiment\nanalysis with multiturn dialogue simulations between LLMs. We quantify the\nmodels' capacity to express a wide spectrum of emotions and how they fluctuate\nduring interactions. Our findings reveal significant variations among LLMs in\ntheir ability to maintain consistent affect, with some models demonstrating\nmore stable emotional trajectories than others. Furthermore, we identify key\nchallenges in affect control, including difficulties in producing and\nmaintaining extreme emotional states and limitations in adapting affect to\nchanging conversational contexts. These findings have important implications\nfor the development of more emotionally intelligent AI systems and highlight\nthe need for improved affect modelling in LLMs."
                },
                "authors": [
                    {
                        "name": "Gino Franco Fazzi"
                    },
                    {
                        "name": "Julie Skoven Hinge"
                    },
                    {
                        "name": "Stefan Heinrich"
                    },
                    {
                        "name": "Paolo Burelli"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Burelli"
                },
                "author": "Paolo Burelli",
                "arxiv_doi": "10.1145/3706599.3720191",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706599.3720191",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02456v1",
                "updated": "2025-03-04T10:05:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    5,
                    58,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T10:05:58Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    5,
                    58,
                    1,
                    63,
                    0
                ],
                "title": "Inferring Galactic Parameters from Chemical Abundances with\n  Simulation-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Galactic Parameters from Chemical Abundances with\n  Simulation-Based Inference"
                },
                "summary": "Galactic chemical abundances provide crucial insights into fundamental\ngalactic parameters, such as the high-mass slope of the initial mass function\n(IMF) and the normalization of Type Ia supernova (SN Ia) rates. Constraining\nthese parameters is essential for advancing our understanding of stellar\nfeedback, metal enrichment, and galaxy formation processes. However,\ntraditional Bayesian inference techniques, such as Hamiltonian Monte Carlo\n(HMC), are computationally prohibitive when applied to large datasets of modern\nstellar surveys. We leverage simulation-based-inference (SBI) as a scalable,\nrobust, and efficient method for constraining galactic parameters from stellar\nchemical abundances and demonstrate its the advantages over HMC in terms of\nspeed, scalability, and robustness against model misspecifications. We combine\na Galactic Chemical Evolution (GCE) model, CHEMPY, with a neural network\nemulator and a Neural Posterior Estimator (NPE) to train our SBI pipeline. Mock\ndatasets are generated using CHEMPY, including scenarios with mismatched\nnucleosynthetic yields, with additional tests conducted on data from a\nsimulated Milky Way-like galaxy. SBI results are benchmarked against HMC-based\ninference, focusing on computational performance, accuracy, and resilience to\nsystematic discrepancies. SBI achieves a $\\sim75,600\\times$ speed-up compared\nto HMC, reducing inference runtime from $\\gtrsim42$ hours to mere seconds for\nthousands of stars. Inference on $1,000$ stars yields precise estimates for the\nIMF slope ($\\alpha_{\\rm IMF} = -2.298 \\pm 0.002$) and SN Ia normalization\n($\\log_{10}(N_{\\rm Ia}) = -2.885 \\pm 0.003$), deviating less than 0.05% from\nthe ground truth. SBI also demonstrates similar robustness to model\nmisspecification than HMC, recovering accurate parameters even with alternate\nyield tables or data from a cosmological simulation. (shortened...)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galactic chemical abundances provide crucial insights into fundamental\ngalactic parameters, such as the high-mass slope of the initial mass function\n(IMF) and the normalization of Type Ia supernova (SN Ia) rates. Constraining\nthese parameters is essential for advancing our understanding of stellar\nfeedback, metal enrichment, and galaxy formation processes. However,\ntraditional Bayesian inference techniques, such as Hamiltonian Monte Carlo\n(HMC), are computationally prohibitive when applied to large datasets of modern\nstellar surveys. We leverage simulation-based-inference (SBI) as a scalable,\nrobust, and efficient method for constraining galactic parameters from stellar\nchemical abundances and demonstrate its the advantages over HMC in terms of\nspeed, scalability, and robustness against model misspecifications. We combine\na Galactic Chemical Evolution (GCE) model, CHEMPY, with a neural network\nemulator and a Neural Posterior Estimator (NPE) to train our SBI pipeline. Mock\ndatasets are generated using CHEMPY, including scenarios with mismatched\nnucleosynthetic yields, with additional tests conducted on data from a\nsimulated Milky Way-like galaxy. SBI results are benchmarked against HMC-based\ninference, focusing on computational performance, accuracy, and resilience to\nsystematic discrepancies. SBI achieves a $\\sim75,600\\times$ speed-up compared\nto HMC, reducing inference runtime from $\\gtrsim42$ hours to mere seconds for\nthousands of stars. Inference on $1,000$ stars yields precise estimates for the\nIMF slope ($\\alpha_{\\rm IMF} = -2.298 \\pm 0.002$) and SN Ia normalization\n($\\log_{10}(N_{\\rm Ia}) = -2.885 \\pm 0.003$), deviating less than 0.05% from\nthe ground truth. SBI also demonstrates similar robustness to model\nmisspecification than HMC, recovering accurate parameters even with alternate\nyield tables or data from a cosmological simulation. (shortened...)"
                },
                "authors": [
                    {
                        "name": "Tobias Buck"
                    },
                    {
                        "name": "Berkay Günes"
                    },
                    {
                        "name": "Giuseppe Viterbo"
                    },
                    {
                        "name": "William H. Oliver"
                    },
                    {
                        "name": "Sven Buder"
                    }
                ],
                "author_detail": {
                    "name": "Sven Buder"
                },
                "author": "Sven Buder",
                "arxiv_comment": "submitted to A&A, comments welcome, all source code to reproduce this\n  work can be found on GitHub under url: https://github.com/TobiBu/sbi-chempy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.06603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06603v2",
                "updated": "2025-03-04T18:59:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    59,
                    26,
                    1,
                    63,
                    0
                ],
                "published": "2024-12-09T15:53:00Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    53,
                    0,
                    0,
                    344,
                    0
                ],
                "title": "Examining the Use and Impact of an AI Code Assistant on Developer\n  Productivity and Experience in the Enterprise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Use and Impact of an AI Code Assistant on Developer\n  Productivity and Experience in the Enterprise"
                },
                "summary": "AI assistants are being created to help software engineers conduct a variety\nof coding-related tasks, such as writing, documenting, and testing code. We\ndescribe the use of the watsonx Code Assistant (WCA), an LLM-powered coding\nassistant deployed internally within IBM. Through surveys of two user cohorts\n(N=669) and unmoderated usability testing (N=15), we examined developers'\nexperiences with WCA and its impact on their productivity. We learned about\ntheir motivations for using (or not using) WCA, we examined their expectations\nof its speed and quality, and we identified new considerations regarding\nownership of and responsibility for generated code. Our case study\ncharacterizes the impact of an LLM-powered assistant on developers' perceptions\nof productivity and it shows that although such tools do often provide net\nproductivity increases, these benefits may not always be experienced by all\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI assistants are being created to help software engineers conduct a variety\nof coding-related tasks, such as writing, documenting, and testing code. We\ndescribe the use of the watsonx Code Assistant (WCA), an LLM-powered coding\nassistant deployed internally within IBM. Through surveys of two user cohorts\n(N=669) and unmoderated usability testing (N=15), we examined developers'\nexperiences with WCA and its impact on their productivity. We learned about\ntheir motivations for using (or not using) WCA, we examined their expectations\nof its speed and quality, and we identified new considerations regarding\nownership of and responsibility for generated code. Our case study\ncharacterizes the impact of an LLM-powered assistant on developers' perceptions\nof productivity and it shows that although such tools do often provide net\nproductivity increases, these benefits may not always be experienced by all\nusers."
                },
                "authors": [
                    {
                        "name": "Justin D. Weisz"
                    },
                    {
                        "name": "Shraddha Kumar"
                    },
                    {
                        "name": "Michael Muller"
                    },
                    {
                        "name": "Karen-Ellen Browne"
                    },
                    {
                        "name": "Arielle Goldberg"
                    },
                    {
                        "name": "Ellice Heintze"
                    },
                    {
                        "name": "Shagun Bajpai"
                    }
                ],
                "author_detail": {
                    "name": "Shagun Bajpai"
                },
                "author": "Shagun Bajpai",
                "arxiv_comment": "21 pages, 3 figures. CHI EA '25, April 26-May 01, 2025, Yokohama,\n  Japan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02879v1",
                "updated": "2025-03-04T18:58:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    58,
                    13,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:58:13Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    58,
                    13,
                    1,
                    63,
                    0
                ],
                "title": "Wikipedia in the Era of LLMs: Evolution and Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wikipedia in the Era of LLMs: Evolution and Risks"
                },
                "summary": "In this paper, we present a thorough analysis of the impact of Large Language\nModels (LLMs) on Wikipedia, examining the evolution of Wikipedia through\nexisting data and using simulations to explore potential risks. We begin by\nanalyzing page views and article content to study Wikipedia's recent changes\nand assess the impact of LLMs. Subsequently, we evaluate how LLMs affect\nvarious Natural Language Processing (NLP) tasks related to Wikipedia, including\nmachine translation and retrieval-augmented generation (RAG). Our findings and\nsimulation results reveal that Wikipedia articles have been influenced by LLMs,\nwith an impact of approximately 1%-2% in certain categories. If the machine\ntranslation benchmark based on Wikipedia is influenced by LLMs, the scores of\nthe models may become inflated, and the comparative results among models might\nshift as well. Moreover, the effectiveness of RAG might decrease if the\nknowledge base becomes polluted by LLM-generated content. While LLMs have not\nyet fully changed Wikipedia's language and knowledge structures, we believe\nthat our empirical findings signal the need for careful consideration of\npotential future risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a thorough analysis of the impact of Large Language\nModels (LLMs) on Wikipedia, examining the evolution of Wikipedia through\nexisting data and using simulations to explore potential risks. We begin by\nanalyzing page views and article content to study Wikipedia's recent changes\nand assess the impact of LLMs. Subsequently, we evaluate how LLMs affect\nvarious Natural Language Processing (NLP) tasks related to Wikipedia, including\nmachine translation and retrieval-augmented generation (RAG). Our findings and\nsimulation results reveal that Wikipedia articles have been influenced by LLMs,\nwith an impact of approximately 1%-2% in certain categories. If the machine\ntranslation benchmark based on Wikipedia is influenced by LLMs, the scores of\nthe models may become inflated, and the comparative results among models might\nshift as well. Moreover, the effectiveness of RAG might decrease if the\nknowledge base becomes polluted by LLM-generated content. While LLMs have not\nyet fully changed Wikipedia's language and knowledge structures, we believe\nthat our empirical findings signal the need for careful consideration of\npotential future risks."
                },
                "authors": [
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Yuliang Xu"
                    },
                    {
                        "name": "Mingmeng Geng"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Dongping Chen"
                    }
                ],
                "author_detail": {
                    "name": "Dongping Chen"
                },
                "author": "Dongping Chen",
                "arxiv_comment": "We release all the experimental dataset and source code at:\n  https://github.com/HSM316/LLM_Wikipedia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02878v1",
                "updated": "2025-03-04T18:58:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    58,
                    11,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:58:11Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    58,
                    11,
                    1,
                    63,
                    0
                ],
                "title": "Language Models can Self-Improve at State-Value Estimation for Better\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models can Self-Improve at State-Value Estimation for Better\n  Search"
                },
                "summary": "Collecting ground truth task completion rewards or human demonstrations for\nmulti-step reasoning tasks is often cost-prohibitive and time-consuming,\nespecially in interactive domains like web tasks. To address this bottleneck,\nwe present self-taught lookahead, a self-supervised method that leverages\nstate-transition dynamics to train a value model capable of effectively guiding\nlanguage model-controlled search. We find that moderately sized (8 billion\nparameters) open-weight value models improved with self-taught lookahead can\nmatch the performance of using a frontier LLM such as gpt-4o as the value\nmodel. Furthermore, we find that self-taught lookahead improves performance by\n20% while reducing costs 37x compared to previous LLM-based tree search,\nwithout relying on ground truth rewards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collecting ground truth task completion rewards or human demonstrations for\nmulti-step reasoning tasks is often cost-prohibitive and time-consuming,\nespecially in interactive domains like web tasks. To address this bottleneck,\nwe present self-taught lookahead, a self-supervised method that leverages\nstate-transition dynamics to train a value model capable of effectively guiding\nlanguage model-controlled search. We find that moderately sized (8 billion\nparameters) open-weight value models improved with self-taught lookahead can\nmatch the performance of using a frontier LLM such as gpt-4o as the value\nmodel. Furthermore, we find that self-taught lookahead improves performance by\n20% while reducing costs 37x compared to previous LLM-based tree search,\nwithout relying on ground truth rewards."
                },
                "authors": [
                    {
                        "name": "Ethan Mendes"
                    },
                    {
                        "name": "Alan Ritter"
                    }
                ],
                "author_detail": {
                    "name": "Alan Ritter"
                },
                "author": "Alan Ritter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02875v1",
                "updated": "2025-03-04T18:56:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    56,
                    3,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:56:03Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    56,
                    3,
                    1,
                    63,
                    0
                ],
                "title": "The First Few Tokens Are All You Need: An Efficient and Effective\n  Unsupervised Prefix Fine-Tuning Method for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The First Few Tokens Are All You Need: An Efficient and Effective\n  Unsupervised Prefix Fine-Tuning Method for Reasoning Models"
                },
                "summary": "Improving the reasoning capabilities of large language models (LLMs)\ntypically requires supervised fine-tuning with labeled data or computationally\nexpensive sampling. We introduce Unsupervised Prefix Fine-Tuning (UPFT), which\nleverages the observation of Prefix Self-Consistency -- the shared initial\nreasoning steps across diverse solution trajectories -- to enhance LLM\nreasoning efficiency. By training exclusively on the initial prefix substrings\n(as few as 8 tokens), UPFT removes the need for labeled data or exhaustive\nsampling. Experiments on reasoning benchmarks show that UPFT matches the\nperformance of supervised methods such as Rejection Sampling Fine-Tuning, while\nreducing training time by 75% and sampling cost by 99%. Further analysis\nreveals that errors tend to appear in later stages of the reasoning process and\nthat prefix-based training preserves the model's structural knowledge. This\nwork demonstrates how minimal unsupervised fine-tuning can unlock substantial\nreasoning gains in LLMs, offering a scalable and resource-efficient alternative\nto conventional approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the reasoning capabilities of large language models (LLMs)\ntypically requires supervised fine-tuning with labeled data or computationally\nexpensive sampling. We introduce Unsupervised Prefix Fine-Tuning (UPFT), which\nleverages the observation of Prefix Self-Consistency -- the shared initial\nreasoning steps across diverse solution trajectories -- to enhance LLM\nreasoning efficiency. By training exclusively on the initial prefix substrings\n(as few as 8 tokens), UPFT removes the need for labeled data or exhaustive\nsampling. Experiments on reasoning benchmarks show that UPFT matches the\nperformance of supervised methods such as Rejection Sampling Fine-Tuning, while\nreducing training time by 75% and sampling cost by 99%. Further analysis\nreveals that errors tend to appear in later stages of the reasoning process and\nthat prefix-based training preserves the model's structural knowledge. This\nwork demonstrates how minimal unsupervised fine-tuning can unlock substantial\nreasoning gains in LLMs, offering a scalable and resource-efficient alternative\nto conventional approaches."
                },
                "authors": [
                    {
                        "name": "Ke Ji"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Xiaoyuan Liu"
                    },
                    {
                        "name": "Zhijie Wang"
                    },
                    {
                        "name": "Junying Chen"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_doi": "10.13140/RG.2.2.33772.07043",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.33772.07043",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14509v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14509v5",
                "updated": "2025-03-04T18:55:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    55,
                    27,
                    1,
                    63,
                    0
                ],
                "published": "2024-09-22T16:13:00Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    13,
                    0,
                    6,
                    266,
                    0
                ],
                "title": "Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving\n  Human-AI Alignment in the Writing Process through Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving\n  Human-AI Alignment in the Writing Process through Edits"
                },
                "summary": "LLM-based applications are helping people write, and LLM-generated text is\nmaking its way into social media, journalism, and our classrooms. However, the\ndifferences between LLM-generated and human written text remain unclear. To\nexplore this, we hired professional writers to edit paragraphs in several\ncreative domains. We first found these writers agree on undesirable\nidiosyncrasies in LLM generated text, formalizing it into a seven-category\ntaxonomy (e.g. clich\\'es, unnecessary exposition). Second, we curated the LAMP\ncorpus: 1,057 LLM-generated paragraphs edited by professional writers according\nto our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our\nstudy (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms\nof writing quality, revealing common limitations across model families. Third,\nbuilding on existing work in automatic editing we evaluated methods to improve\nLLM-generated text. A large-scale preference annotation confirms that although\nexperts largely prefer text edited by other experts, automatic editing methods\nshow promise in improving alignment between LLM-generated and human-written\ntext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based applications are helping people write, and LLM-generated text is\nmaking its way into social media, journalism, and our classrooms. However, the\ndifferences between LLM-generated and human written text remain unclear. To\nexplore this, we hired professional writers to edit paragraphs in several\ncreative domains. We first found these writers agree on undesirable\nidiosyncrasies in LLM generated text, formalizing it into a seven-category\ntaxonomy (e.g. clich\\'es, unnecessary exposition). Second, we curated the LAMP\ncorpus: 1,057 LLM-generated paragraphs edited by professional writers according\nto our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our\nstudy (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms\nof writing quality, revealing common limitations across model families. Third,\nbuilding on existing work in automatic editing we evaluated methods to improve\nLLM-generated text. A large-scale preference annotation confirms that although\nexperts largely prefer text edited by other experts, automatic editing methods\nshow promise in improving alignment between LLM-generated and human-written\ntext."
                },
                "authors": [
                    {
                        "name": "Tuhin Chakrabarty"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "arxiv_comment": "ACM CHI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14509v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14509v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02865v1",
                "updated": "2025-03-04T18:43:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    43,
                    57,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:43:57Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    43,
                    57,
                    1,
                    63,
                    0
                ],
                "title": "FairSense-AI: Responsible AI Meets Sustainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairSense-AI: Responsible AI Meets Sustainability"
                },
                "summary": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Mukund Sayeeganesh Chettiar"
                    },
                    {
                        "name": "Matin Yousefabadi"
                    },
                    {
                        "name": "Tahniat Khan"
                    },
                    {
                        "name": "Marcelo Lotif"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo Lotif"
                },
                "author": "Marcelo Lotif",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02863v1",
                "updated": "2025-03-04T18:40:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    40,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:40:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    40,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Calibrating LLM Confidence with Semantic Steering: A Multi-Prompt\n  Aggregation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating LLM Confidence with Semantic Steering: A Multi-Prompt\n  Aggregation Framework"
                },
                "summary": "Large Language Models (LLMs) often exhibit misaligned confidence scores,\nusually overestimating the reliability of their predictions. While verbalized\nconfidence in Large Language Models (LLMs) has gained attention, prior work\nremains divided on whether confidence scores can be systematically steered\nthrough prompting. Recent studies even argue that such prompt-induced\nconfidence shifts are negligible, suggesting LLMs' confidence calibration is\nrigid to linguistic interventions. Contrary to these claims, we first\nrigorously confirm the existence of directional confidence shifts by probing\nthree models (including GPT3.5, LLAMA3-70b, GPT4) across 7 benchmarks,\ndemonstrating that explicit instructions can inflate or deflate confidence\nscores in a regulated manner. Based on this observation, we propose a novel\nframework containing three components: confidence steering, steered confidence\naggregation and steered answers selection, named SteeringConf. Our method,\nSteeringConf, leverages a confidence manipulation mechanism to steer the\nconfidence scores of LLMs in several desired directions, followed by a\nsummarization module that aggregates the steered confidence scores to produce a\nfinal prediction. We evaluate our method on 7 benchmarks and it consistently\noutperforms the baselines in terms of calibration metrics in task of confidence\ncalibration and failure detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often exhibit misaligned confidence scores,\nusually overestimating the reliability of their predictions. While verbalized\nconfidence in Large Language Models (LLMs) has gained attention, prior work\nremains divided on whether confidence scores can be systematically steered\nthrough prompting. Recent studies even argue that such prompt-induced\nconfidence shifts are negligible, suggesting LLMs' confidence calibration is\nrigid to linguistic interventions. Contrary to these claims, we first\nrigorously confirm the existence of directional confidence shifts by probing\nthree models (including GPT3.5, LLAMA3-70b, GPT4) across 7 benchmarks,\ndemonstrating that explicit instructions can inflate or deflate confidence\nscores in a regulated manner. Based on this observation, we propose a novel\nframework containing three components: confidence steering, steered confidence\naggregation and steered answers selection, named SteeringConf. Our method,\nSteeringConf, leverages a confidence manipulation mechanism to steer the\nconfidence scores of LLMs in several desired directions, followed by a\nsummarization module that aggregates the steered confidence scores to produce a\nfinal prediction. We evaluate our method on 7 benchmarks and it consistently\noutperforms the baselines in terms of calibration metrics in task of confidence\ncalibration and failure detection."
                },
                "authors": [
                    {
                        "name": "Ziang Zhou"
                    },
                    {
                        "name": "Tianyuan Jin"
                    },
                    {
                        "name": "Jieming Shi"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02851v1",
                "updated": "2025-03-04T18:27:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    27,
                    0,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:27:00Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    27,
                    0,
                    1,
                    63,
                    0
                ],
                "title": "Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs'\n  Decoding Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs'\n  Decoding Layers"
                },
                "summary": "Large language models (LLMs) are known to hallucinate, a phenomenon often\nlinked to creativity. While previous research has primarily explored this\nconnection through theoretical or qualitative lenses, our work takes a\nquantitative approach to systematically examine the relationship between\nhallucination and creativity in LLMs. Given the complex nature of creativity,\nwe propose a narrow definition tailored to LLMs and introduce an evaluation\nframework, HCL, which quantifies Hallucination and Creativity across different\nLayers of LLMs during decoding. Our empirical analysis reveals a tradeoff\nbetween hallucination and creativity that is consistent across layer depth,\nmodel type, and model size. Notably, across different model architectures, we\nidentify a specific layer at each model size that optimally balances this\ntradeoff. Additionally, the optimal layer tends to appear in the early layers\nof larger models, and the confidence of the model is also significantly higher\nat this layer. These findings provide a quantitative perspective that offers\nnew insights into the interplay between LLM creativity and hallucination. The\ncode and data for our experiments are available at\nhttps://github.com/ZicongHe2002/HCL-Spark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to hallucinate, a phenomenon often\nlinked to creativity. While previous research has primarily explored this\nconnection through theoretical or qualitative lenses, our work takes a\nquantitative approach to systematically examine the relationship between\nhallucination and creativity in LLMs. Given the complex nature of creativity,\nwe propose a narrow definition tailored to LLMs and introduce an evaluation\nframework, HCL, which quantifies Hallucination and Creativity across different\nLayers of LLMs during decoding. Our empirical analysis reveals a tradeoff\nbetween hallucination and creativity that is consistent across layer depth,\nmodel type, and model size. Notably, across different model architectures, we\nidentify a specific layer at each model size that optimally balances this\ntradeoff. Additionally, the optimal layer tends to appear in the early layers\nof larger models, and the confidence of the model is also significantly higher\nat this layer. These findings provide a quantitative perspective that offers\nnew insights into the interplay between LLM creativity and hallucination. The\ncode and data for our experiments are available at\nhttps://github.com/ZicongHe2002/HCL-Spark."
                },
                "authors": [
                    {
                        "name": "Zicong He"
                    },
                    {
                        "name": "Boxuan Zhang"
                    },
                    {
                        "name": "Lu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Cheng"
                },
                "author": "Lu Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02846v1",
                "updated": "2025-03-04T18:20:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    20,
                    24,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T18:20:24Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    20,
                    24,
                    1,
                    63,
                    0
                ],
                "title": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs"
                },
                "summary": "Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or\nnonsensical information) when serving as AI assistants in various domains.\nSince hallucinations always come with truthful content in the LLM responses,\nprevious factuality alignment methods that conduct response-level preference\nlearning inevitably introduced noises during training. Therefore, this paper\nproposes a fine-grained factuality alignment method based on Direct Preference\nOptimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as\nmask signals, Mask-DPO only learns from factually correct sentences in the\npreferred samples and prevents the penalty on factual contents in the not\npreferred samples, which resolves the ambiguity in the preference learning.\nExtensive experimental results demonstrate that Mask-DPO can significantly\nimprove the factuality of LLMs responses to questions from both in-domain and\nout-of-domain datasets, although these questions and their corresponding topics\nare unseen during training. Only trained on the ANAH train set, the score of\nLlama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,\neven surpassing the score of Llama3.1-70B-Instruct (53.44%), while its\nFactScore on the out-of-domain Biography dataset is also improved from 30.29%\nto 39.39%. We further study the generalization property of Mask-DPO using\ndifferent training sample scaling strategies and find that scaling the number\nof topics in the dataset is more effective than the number of questions. We\nprovide a hypothesis of what factual alignment is doing with LLMs, on the\nimplication of this phenomenon, and conduct proof-of-concept experiments to\nverify it. We hope the method and the findings pave the way for future research\non scaling factuality alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or\nnonsensical information) when serving as AI assistants in various domains.\nSince hallucinations always come with truthful content in the LLM responses,\nprevious factuality alignment methods that conduct response-level preference\nlearning inevitably introduced noises during training. Therefore, this paper\nproposes a fine-grained factuality alignment method based on Direct Preference\nOptimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as\nmask signals, Mask-DPO only learns from factually correct sentences in the\npreferred samples and prevents the penalty on factual contents in the not\npreferred samples, which resolves the ambiguity in the preference learning.\nExtensive experimental results demonstrate that Mask-DPO can significantly\nimprove the factuality of LLMs responses to questions from both in-domain and\nout-of-domain datasets, although these questions and their corresponding topics\nare unseen during training. Only trained on the ANAH train set, the score of\nLlama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,\neven surpassing the score of Llama3.1-70B-Instruct (53.44%), while its\nFactScore on the out-of-domain Biography dataset is also improved from 30.29%\nto 39.39%. We further study the generalization property of Mask-DPO using\ndifferent training sample scaling strategies and find that scaling the number\nof topics in the dataset is more effective than the number of questions. We\nprovide a hypothesis of what factual alignment is doing with LLMs, on the\nimplication of this phenomenon, and conduct proof-of-concept experiments to\nverify it. We hope the method and the findings pave the way for future research\non scaling factuality alignment."
                },
                "authors": [
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Chengqi Lyu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Accepted by ICLR 2025. Code is available at\n  https://github.com/open-compass/ANAH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16172v2",
                "updated": "2025-03-04T18:15:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    15,
                    36,
                    1,
                    63,
                    0
                ],
                "published": "2024-12-07T00:15:24Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    0,
                    15,
                    24,
                    5,
                    342,
                    0
                ],
                "title": "LABIIUM: AI-Enhanced Zero-configuration Measurement Automation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LABIIUM: AI-Enhanced Zero-configuration Measurement Automation System"
                },
                "summary": "The complexity of laboratory environments requires solutions that simplify\ninstrument interaction and enhance measurement automation. Traditional tools\noften require configuration, software, and programming skills, creating\nbarriers to productivity. Previous approaches, including dedicated software\nsuites and custom scripts, frequently fall short in providing user-friendly\nsolutions that align with programming practices. We present LABIIUM, an\nAI-enhanced, zero-configuration measurement automation system designed to\nstreamline experimental workflows and improve user productivity. LABIIUM\nintegrates an AI assistant powered by Large Language Models (LLMs) to generate\ncode. LABIIUM's Lab-Automation-Measurement Bridges (LAMBs) enable seamless\ninstrument connectivity using standard tools such as VSCode and Python,\neliminating setup overhead. To demonstrate its capabilities, we conducted\nexperiments involving the measurement of the parametric transfer curve of a\nsimple two-transistor inverting amplifier with a current source load. The AI\nassistant was evaluated using different prompt scenarios and compared with\nmultiple models, including Claude Sonnet 3.5, Gemini Pro 1.5, and GPT-4o. An\nexpert solution implementing the Gradient-Weighted Adaptive Stochastic Sampling\n(GWASS) method was used as a baseline. The solutions generated by the AI\nassistant were compared with the expert solution and a uniform linear sweep\nbaseline with 10,000 points. The graph results show that the LLMs were able to\nsuccessfully complete the most basic uniform sweep, but LLMs were unable to\ndevelop adaptive sweeping algorithms to compete with GWASS. The evaluation\nunderscores LABIIUM's ability to enhance laboratory productivity and support\ndigital transformation in research and industry, and emphasizes the future work\nrequired to improve LLM performance in Electronic Measurement Science Tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The complexity of laboratory environments requires solutions that simplify\ninstrument interaction and enhance measurement automation. Traditional tools\noften require configuration, software, and programming skills, creating\nbarriers to productivity. Previous approaches, including dedicated software\nsuites and custom scripts, frequently fall short in providing user-friendly\nsolutions that align with programming practices. We present LABIIUM, an\nAI-enhanced, zero-configuration measurement automation system designed to\nstreamline experimental workflows and improve user productivity. LABIIUM\nintegrates an AI assistant powered by Large Language Models (LLMs) to generate\ncode. LABIIUM's Lab-Automation-Measurement Bridges (LAMBs) enable seamless\ninstrument connectivity using standard tools such as VSCode and Python,\neliminating setup overhead. To demonstrate its capabilities, we conducted\nexperiments involving the measurement of the parametric transfer curve of a\nsimple two-transistor inverting amplifier with a current source load. The AI\nassistant was evaluated using different prompt scenarios and compared with\nmultiple models, including Claude Sonnet 3.5, Gemini Pro 1.5, and GPT-4o. An\nexpert solution implementing the Gradient-Weighted Adaptive Stochastic Sampling\n(GWASS) method was used as a baseline. The solutions generated by the AI\nassistant were compared with the expert solution and a uniform linear sweep\nbaseline with 10,000 points. The graph results show that the LLMs were able to\nsuccessfully complete the most basic uniform sweep, but LLMs were unable to\ndevelop adaptive sweeping algorithms to compete with GWASS. The evaluation\nunderscores LABIIUM's ability to enhance laboratory productivity and support\ndigital transformation in research and industry, and emphasizes the future work\nrequired to improve LLM performance in Electronic Measurement Science Tasks."
                },
                "authors": [
                    {
                        "name": "Emmanuel A. Olowe"
                    },
                    {
                        "name": "Danial Chitnis"
                    }
                ],
                "author_detail": {
                    "name": "Danial Chitnis"
                },
                "author": "Danial Chitnis",
                "arxiv_comment": "accepted for IEEE I2MTC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01335v2",
                "updated": "2025-03-04T18:15:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    15,
                    16,
                    1,
                    63,
                    0
                ],
                "published": "2024-10-02T08:53:07Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    8,
                    53,
                    7,
                    2,
                    276,
                    0
                ],
                "title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language\n  Models"
                },
                "summary": "Model merging, such as model souping, is the practice of combining different\nmodels with the same architecture together without further training. In this\nwork, we present a model merging methodology that addresses the difficulty of\nfine-tuning Large Language Models (LLMs) for target tasks in non-English\nlanguages, where task-specific data is often unavailable. We focus on\nmathematical reasoning and without in-language math data, facilitate\ncross-lingual transfer by composing language and math capabilities. Starting\nfrom the same pretrained model, we fine-tune separate \"experts\" on math\ninstruction data in English and on generic instruction data in the target\nlanguage. We then replace the top and bottom transformer layers of the math\nexpert directly with layers from the language expert, which consequently\nenhances math performance in the target language. The resulting merged models\noutperform the individual experts and other merging methods on the math\nbenchmark, MGSM, by 10% across four major languages where math instruction data\nis scarce. In addition, this layer swapping is simple, inexpensive, and\nintuitive, as it is based on an interpretative analysis of the most important\nparameter changes during the fine-tuning of each expert. The ability to\nsuccessfully re-compose LLMs for cross-lingual transfer in this manner opens up\nfuture possibilities to combine model expertise, create modular solutions, and\ntransfer reasoning capabilities across languages all post hoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging, such as model souping, is the practice of combining different\nmodels with the same architecture together without further training. In this\nwork, we present a model merging methodology that addresses the difficulty of\nfine-tuning Large Language Models (LLMs) for target tasks in non-English\nlanguages, where task-specific data is often unavailable. We focus on\nmathematical reasoning and without in-language math data, facilitate\ncross-lingual transfer by composing language and math capabilities. Starting\nfrom the same pretrained model, we fine-tune separate \"experts\" on math\ninstruction data in English and on generic instruction data in the target\nlanguage. We then replace the top and bottom transformer layers of the math\nexpert directly with layers from the language expert, which consequently\nenhances math performance in the target language. The resulting merged models\noutperform the individual experts and other merging methods on the math\nbenchmark, MGSM, by 10% across four major languages where math instruction data\nis scarce. In addition, this layer swapping is simple, inexpensive, and\nintuitive, as it is based on an interpretative analysis of the most important\nparameter changes during the fine-tuning of each expert. The ability to\nsuccessfully re-compose LLMs for cross-lingual transfer in this manner opens up\nfuture possibilities to combine model expertise, create modular solutions, and\ntransfer reasoning capabilities across languages all post hoc."
                },
                "authors": [
                    {
                        "name": "Lucas Bandarkar"
                    },
                    {
                        "name": "Benjamin Muller"
                    },
                    {
                        "name": "Pritish Yuvraj"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Nayan Singhal"
                    },
                    {
                        "name": "Hongjiang Lv"
                    },
                    {
                        "name": "Bing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Liu"
                },
                "author": "Bing Liu",
                "arxiv_comment": "ICLR 2025, Spotlight Paper, In The Thirteenth International\n  Conference on Learning Representations, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05516v3",
                "updated": "2025-03-04T18:07:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    18,
                    7,
                    34,
                    1,
                    63,
                    0
                ],
                "published": "2024-06-08T16:35:31Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    16,
                    35,
                    31,
                    5,
                    160,
                    0
                ],
                "title": "Verbalized Probabilistic Graphical Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbalized Probabilistic Graphical Modeling"
                },
                "summary": "Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality."
                },
                "authors": [
                    {
                        "name": "Hengguan Huang"
                    },
                    {
                        "name": "Xing Shen"
                    },
                    {
                        "name": "Songtao Wang"
                    },
                    {
                        "name": "Lingfa Meng"
                    },
                    {
                        "name": "Dianbo Liu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Samir Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Samir Bhatt"
                },
                "author": "Samir Bhatt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02832v1",
                "updated": "2025-03-04T17:57:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    57,
                    9,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:57:09Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    57,
                    9,
                    1,
                    63,
                    0
                ],
                "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy\n  Distillation"
                },
                "summary": "In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization."
                },
                "authors": [
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Bojie Hu"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinan Xu"
                },
                "author": "Jinan Xu",
                "arxiv_comment": "15 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Éric de la Clergerie"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21239v2",
                "updated": "2025-03-04T17:31:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    31,
                    25,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-28T17:09:08Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    17,
                    9,
                    8,
                    4,
                    59,
                    0
                ],
                "title": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Volume: Quantifying and Detecting both External and Internal\n  Uncertainty in LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring white-box access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\ndiverse tasks by encoding vast amounts of factual knowledge. However, they are\nstill prone to hallucinations, generating incorrect or misleading information,\noften accompanied by high uncertainty. Existing methods for hallucination\ndetection primarily focus on quantifying internal uncertainty, which arises\nfrom missing or conflicting knowledge within the model. However, hallucinations\ncan also stem from external uncertainty, where ambiguous user queries lead to\nmultiple possible interpretations. In this work, we introduce Semantic Volume,\na novel mathematical measure for quantifying both external and internal\nuncertainty in LLMs. Our approach perturbs queries and responses, embeds them\nin a semantic space, and computes the determinant of the Gram matrix of the\nembedding vectors, capturing their dispersion as a measure of uncertainty. Our\nframework provides a generalizable and unsupervised uncertainty detection\nmethod without requiring white-box access to LLMs. We conduct extensive\nexperiments on both external and internal uncertainty detection, demonstrating\nthat our Semantic Volume method consistently outperforms existing baselines in\nboth tasks. Additionally, we provide theoretical insights linking our measure\nto differential entropy, unifying and extending previous sampling-based\nuncertainty measures such as the semantic entropy. Semantic Volume is shown to\nbe a robust and interpretable approach to improving the reliability of LLMs by\nsystematically detecting uncertainty in both user queries and model responses."
                },
                "authors": [
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Ziji Zhang"
                    },
                    {
                        "name": "Yingying Zhuang"
                    },
                    {
                        "name": "Swair Shah"
                    },
                    {
                        "name": "Anurag Beniwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Beniwal"
                },
                "author": "Anurag Beniwal",
                "arxiv_comment": "This paper needs approval from Amazon for open resource release",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16600v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16600v3",
                "updated": "2025-03-04T17:23:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    23,
                    23,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-23T15:00:53Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    15,
                    0,
                    53,
                    6,
                    54,
                    0
                ],
                "title": "Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the Pragmatic Dilemma for Moral Reasoning Acquisition in\n  Language Models"
                },
                "summary": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs."
                },
                "authors": [
                    {
                        "name": "Guangliang Liu"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Xitong Zhang"
                    },
                    {
                        "name": "Kristen Marie Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Marie Johnson"
                },
                "author": "Kristen Marie Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16600v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16600v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02800v1",
                "updated": "2025-03-04T17:20:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    20,
                    43,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:20:43Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    20,
                    43,
                    1,
                    63,
                    0
                ],
                "title": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration"
                },
                "summary": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7 to 89.1 on the real-world dataset. By allowing for the enriching of input\nseries data with semantics, RAAD-LLM incorporates multimodal capabilities that\nfacilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7 to 89.1 on the real-world dataset. By allowing for the enriching of input\nseries data with semantics, RAAD-LLM incorporates multimodal capabilities that\nfacilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries."
                },
                "authors": [
                    {
                        "name": "Alicia Russell-Gilbert"
                    },
                    {
                        "name": "Sudip Mittal"
                    },
                    {
                        "name": "Shahram Rahimi"
                    },
                    {
                        "name": "Maria Seale"
                    },
                    {
                        "name": "Joseph Jabour"
                    },
                    {
                        "name": "Thomas Arnold"
                    },
                    {
                        "name": "Joshua Church"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Church"
                },
                "author": "Joshua Church",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2411.00914",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "1.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13079v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13079v2",
                "updated": "2025-03-04T17:07:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    7,
                    15,
                    1,
                    63,
                    0
                ],
                "published": "2024-11-20T07:07:42Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    7,
                    7,
                    42,
                    2,
                    325,
                    0
                ],
                "title": "Neural Internal Model Control: Learning a Robust Control Policy via\n  Predictive Error Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Internal Model Control: Learning a Robust Control Policy via\n  Predictive Error Feedback"
                },
                "summary": "Accurate motion control in the face of disturbances within complex\nenvironments remains a major challenge in robotics. Classical model-based\napproaches often struggle with nonlinearities and unstructured disturbances,\nwhile RL-based methods can be fragile when encountering unseen scenarios. In\nthis paper, we propose a novel framework, Neural Internal Model Control, which\nintegrates model-based control with RL-based control to enhance robustness. Our\nframework streamlines the predictive model by applying Newton-Euler equations\nfor rigid-body dynamics, eliminating the need to capture complex\nhigh-dimensional nonlinearities. This internal model combines model-free RL\nalgorithms with predictive error feedback. Such a design enables a closed-loop\ncontrol structure to enhance the robustness and generalizability of the control\nsystem. We demonstrate the effectiveness of our framework on both quadrotors\nand quadrupedal robots, achieving superior performance compared to\nstate-of-the-art methods. Furthermore, real-world deployment on a quadrotor\nwith rope-suspended payloads highlights the framework's robustness in\nsim-to-real transfer. Our code is released at\nhttps://github.com/thu-uav/NeuralIMC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate motion control in the face of disturbances within complex\nenvironments remains a major challenge in robotics. Classical model-based\napproaches often struggle with nonlinearities and unstructured disturbances,\nwhile RL-based methods can be fragile when encountering unseen scenarios. In\nthis paper, we propose a novel framework, Neural Internal Model Control, which\nintegrates model-based control with RL-based control to enhance robustness. Our\nframework streamlines the predictive model by applying Newton-Euler equations\nfor rigid-body dynamics, eliminating the need to capture complex\nhigh-dimensional nonlinearities. This internal model combines model-free RL\nalgorithms with predictive error feedback. Such a design enables a closed-loop\ncontrol structure to enhance the robustness and generalizability of the control\nsystem. We demonstrate the effectiveness of our framework on both quadrotors\nand quadrupedal robots, achieving superior performance compared to\nstate-of-the-art methods. Furthermore, real-world deployment on a quadrotor\nwith rope-suspended payloads highlights the framework's robustness in\nsim-to-real transfer. Our code is released at\nhttps://github.com/thu-uav/NeuralIMC."
                },
                "authors": [
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "arxiv_comment": "Submitted to RAL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13079v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13079v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01711v2",
                "updated": "2025-03-04T17:02:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    2,
                    27,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-03T16:24:36Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    16,
                    24,
                    36,
                    0,
                    62,
                    0
                ],
                "title": "MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation\n  Alignment"
                },
                "summary": "Personalized product search aims to retrieve and rank items that match users'\npreferences and search intent. Despite their effectiveness, existing approaches\ntypically assume that users' query fully captures their real motivation.\nHowever, our analysis of a real-world e-commerce platform reveals that users\noften engage in relevant consultations before searching, indicating they refine\nintents through consultations based on motivation and need. The implied\nmotivation in consultations is a key enhancing factor for personalized search.\nThis unexplored area comes with new challenges including aligning contextual\nmotivations with concise queries, bridging the category-text gap, and filtering\nnoise within sequence history. To address these, we propose a Motivation-Aware\nPersonalized Search (MAPS) method. It embeds queries and consultations into a\nunified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE)\nto prioritize critical semantics, and introduces dual alignment: (1)\ncontrastive learning aligns consultations, reviews, and product features; (2)\nbidirectional attention integrates motivation-aware embeddings with user\npreferences. Extensive experiments on real and synthetic data show MAPS\noutperforms existing methods in both retrieval and ranking tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized product search aims to retrieve and rank items that match users'\npreferences and search intent. Despite their effectiveness, existing approaches\ntypically assume that users' query fully captures their real motivation.\nHowever, our analysis of a real-world e-commerce platform reveals that users\noften engage in relevant consultations before searching, indicating they refine\nintents through consultations based on motivation and need. The implied\nmotivation in consultations is a key enhancing factor for personalized search.\nThis unexplored area comes with new challenges including aligning contextual\nmotivations with concise queries, bridging the category-text gap, and filtering\nnoise within sequence history. To address these, we propose a Motivation-Aware\nPersonalized Search (MAPS) method. It embeds queries and consultations into a\nunified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE)\nto prioritize critical semantics, and introduces dual alignment: (1)\ncontrastive learning aligns consultations, reviews, and product features; (2)\nbidirectional attention integrates motivation-aware embeddings with user\npreferences. Extensive experiments on real and synthetic data show MAPS\noutperforms existing methods in both retrieval and ranking tasks."
                },
                "authors": [
                    {
                        "name": "Weicong Qin"
                    },
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Chenglei Shen"
                    },
                    {
                        "name": "Ming He"
                    },
                    {
                        "name": "Jianping Fan"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02783v1",
                "updated": "2025-03-04T16:56:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    56,
                    34,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:56:34Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    56,
                    34,
                    1,
                    63,
                    0
                ],
                "title": "IterPref: Focal Preference Learning for Code Generation via Iterative\n  Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterPref: Focal Preference Learning for Code Generation via Iterative\n  Debugging"
                },
                "summary": "Preference learning enhances Code LLMs beyond supervised fine-tuning by\nleveraging relative quality comparisons. Existing methods construct preference\npairs from\n  candidates based on test case success, treating the higher pass rate sample\nas positive and the lower as negative. However, this approach does not pinpoint\nspecific errors in the code, which prevents the model from learning more\ninformative error correction patterns, as aligning failing code as a whole\nlacks the granularity needed to capture meaningful error-resolution\nrelationships. To address these issues, we propose IterPref, a new preference\nalignment framework that mimics human iterative debugging to refine Code LLMs.\nIterPref explicitly locates error regions and aligns the corresponding tokens\nvia a tailored DPO algorithm. To generate informative pairs, we introduce the\nCodeFlow dataset, where samples are iteratively refined until passing tests,\nwith modifications capturing error corrections. Extensive experiments show that\na diverse suite of Code LLMs equipped with IterPref achieves significant\nperformance gains in code generation and improves on challenging tasks like\nBigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our\ncode and data will be made publicaly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference learning enhances Code LLMs beyond supervised fine-tuning by\nleveraging relative quality comparisons. Existing methods construct preference\npairs from\n  candidates based on test case success, treating the higher pass rate sample\nas positive and the lower as negative. However, this approach does not pinpoint\nspecific errors in the code, which prevents the model from learning more\ninformative error correction patterns, as aligning failing code as a whole\nlacks the granularity needed to capture meaningful error-resolution\nrelationships. To address these issues, we propose IterPref, a new preference\nalignment framework that mimics human iterative debugging to refine Code LLMs.\nIterPref explicitly locates error regions and aligns the corresponding tokens\nvia a tailored DPO algorithm. To generate informative pairs, we introduce the\nCodeFlow dataset, where samples are iteratively refined until passing tests,\nwith modifications capturing error corrections. Extensive experiments show that\na diverse suite of Code LLMs equipped with IterPref achieves significant\nperformance gains in code generation and improves on challenging tasks like\nBigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our\ncode and data will be made publicaly available."
                },
                "authors": [
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Haoling Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Jianwen Luo"
                    },
                    {
                        "name": "Yangyu Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Scarlett Li"
                    }
                ],
                "author_detail": {
                    "name": "Scarlett Li"
                },
                "author": "Scarlett Li",
                "arxiv_comment": "The code and data will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02776v1",
                "updated": "2025-03-04T16:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    49,
                    37,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    49,
                    37,
                    1,
                    63,
                    0
                ],
                "title": "Implicit Bias in LLMs: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Bias in LLMs: A Survey"
                },
                "summary": "Due to the implement of guardrails by developers, Large language models\n(LLMs) have demonstrated exceptional performance in explicit bias tests.\nHowever, bias in LLMs may occur not only explicitly, but also implicitly, much\nlike humans who consciously strive for impartiality yet still harbor implicit\nbias. The unconscious and automatic nature of implicit bias makes it\nparticularly challenging to study. This paper provides a comprehensive review\nof the existing literature on implicit bias in LLMs. We begin by introducing\nkey concepts, theories and methods related to implicit bias in psychology,\nextending them from humans to LLMs. Drawing on the Implicit Association Test\n(IAT) and other psychological frameworks, we categorize detection methods into\nthree primary approaches: word association, task-oriented text generation and\ndecision-making. We divide our taxonomy of evaluation metrics for implicit bias\ninto two categories: single-value-based metrics and comparison-value-based\nmetrics. We classify datasets into two types: sentences with masked tokens and\ncomplete sentences, incorporating datasets from various domains to reflect the\nbroad application of LLMs. Although research on mitigating implicit bias in\nLLMs is still limited, we summarize existing efforts and offer insights on\nfuture challenges. We aim for this work to serve as a clear guide for\nresearchers and inspire innovative ideas to advance exploration in this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the implement of guardrails by developers, Large language models\n(LLMs) have demonstrated exceptional performance in explicit bias tests.\nHowever, bias in LLMs may occur not only explicitly, but also implicitly, much\nlike humans who consciously strive for impartiality yet still harbor implicit\nbias. The unconscious and automatic nature of implicit bias makes it\nparticularly challenging to study. This paper provides a comprehensive review\nof the existing literature on implicit bias in LLMs. We begin by introducing\nkey concepts, theories and methods related to implicit bias in psychology,\nextending them from humans to LLMs. Drawing on the Implicit Association Test\n(IAT) and other psychological frameworks, we categorize detection methods into\nthree primary approaches: word association, task-oriented text generation and\ndecision-making. We divide our taxonomy of evaluation metrics for implicit bias\ninto two categories: single-value-based metrics and comparison-value-based\nmetrics. We classify datasets into two types: sentences with masked tokens and\ncomplete sentences, incorporating datasets from various domains to reflect the\nbroad application of LLMs. Although research on mitigating implicit bias in\nLLMs is still limited, we summarize existing efforts and offer insights on\nfuture challenges. We aim for this work to serve as a clear guide for\nresearchers and inspire innovative ideas to advance exploration in this task."
                },
                "authors": [
                    {
                        "name": "Xinru Lin"
                    },
                    {
                        "name": "Luyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Luyang Li"
                },
                "author": "Luyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02774v1",
                "updated": "2025-03-04T16:44:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    44,
                    53,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:44:53Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    44,
                    53,
                    1,
                    63,
                    0
                ],
                "title": "Digital Model-Driven Genetic Algorithm for Optimizing Layout and Task\n  Allocation in Human-Robot Collaborative Assemblies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Model-Driven Genetic Algorithm for Optimizing Layout and Task\n  Allocation in Human-Robot Collaborative Assemblies"
                },
                "summary": "This paper addresses the optimization of human-robot collaborative work-cells\nbefore their physical deployment. Most of the times, such environments are\ndesigned based on the experience of the system integrators, often leading to\nsub-optimal solutions. Accurate simulators of the robotic cell, accounting for\nthe presence of the human as well, are available today and can be used in the\npre-deployment. We propose an iterative optimization scheme where a digital\nmodel of the work-cell is updated based on a genetic algorithm. The methodology\nfocuses on the layout optimization and task allocation, encoding both the\nproblems simultaneously in the design variables handled by the genetic\nalgorithm, while the task scheduling problem depends on the result of the\nupper-level one. The final solution balances conflicting objectives in the\nfitness function and is validated to show the impact of the objectives with\nrespect to a baseline, which represents possible initial choices selected based\non the human judgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the optimization of human-robot collaborative work-cells\nbefore their physical deployment. Most of the times, such environments are\ndesigned based on the experience of the system integrators, often leading to\nsub-optimal solutions. Accurate simulators of the robotic cell, accounting for\nthe presence of the human as well, are available today and can be used in the\npre-deployment. We propose an iterative optimization scheme where a digital\nmodel of the work-cell is updated based on a genetic algorithm. The methodology\nfocuses on the layout optimization and task allocation, encoding both the\nproblems simultaneously in the design variables handled by the genetic\nalgorithm, while the task scheduling problem depends on the result of the\nupper-level one. The final solution balances conflicting objectives in the\nfitness function and is validated to show the impact of the objectives with\nrespect to a baseline, which represents possible initial choices selected based\non the human judgment."
                },
                "authors": [
                    {
                        "name": "Christian Cella"
                    },
                    {
                        "name": "Matteo Bruce Robin"
                    },
                    {
                        "name": "Marco Faroni"
                    },
                    {
                        "name": "Andrea Maria Zanchettin"
                    },
                    {
                        "name": "Paolo Rocco"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Rocco"
                },
                "author": "Paolo Rocco",
                "arxiv_comment": "Accepted at IEEE ICRA 2025 (Atlanta, USA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00320v2",
                "updated": "2025-03-04T16:36:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    36,
                    54,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-01T03:15:13Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    15,
                    13,
                    5,
                    60,
                    0
                ],
                "title": "Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of\n  Bilateral Financial Exchanges, A bond market study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of\n  Bilateral Financial Exchanges, A bond market study"
                },
                "summary": "Bilateral markets, such as those for government bonds, involve decentralized\nand opaque transactions between market makers (MMs) and clients, posing\nsignificant challenges for traditional modeling approaches. To address these\ncomplexities, we introduce TRIBE an agent-based model augmented with a large\nlanguage model (LLM) to simulate human-like decision-making in trading\nenvironments. TRIBE leverages publicly available data and stylized facts to\ncapture realistic trading dynamics, integrating human biases like risk aversion\nand ambiguity sensitivity into the decision-making processes of agents. Our\nresearch yields three key contributions: first, we demonstrate that integrating\nLLMs into agent-based models to enhance client agency is feasible and enriches\nthe simulation of agent behaviors in complex markets; second, we find that even\nslight trade aversion encoded within the LLM leads to a complete cessation of\ntrading activity, highlighting the sensitivity of market dynamics to agents'\nrisk profiles; third, we show that incorporating human-like variability shifts\npower dynamics towards clients and can disproportionately affect the entire\nsystem, often resulting in systemic agent collapse across simulations. These\nfindings underscore the emergent properties that arise when introducing\nstochastic, human-like decision processes, revealing new system behaviors that\nenhance the realism and complexity of artificial societies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bilateral markets, such as those for government bonds, involve decentralized\nand opaque transactions between market makers (MMs) and clients, posing\nsignificant challenges for traditional modeling approaches. To address these\ncomplexities, we introduce TRIBE an agent-based model augmented with a large\nlanguage model (LLM) to simulate human-like decision-making in trading\nenvironments. TRIBE leverages publicly available data and stylized facts to\ncapture realistic trading dynamics, integrating human biases like risk aversion\nand ambiguity sensitivity into the decision-making processes of agents. Our\nresearch yields three key contributions: first, we demonstrate that integrating\nLLMs into agent-based models to enhance client agency is feasible and enriches\nthe simulation of agent behaviors in complex markets; second, we find that even\nslight trade aversion encoded within the LLM leads to a complete cessation of\ntrading activity, highlighting the sensitivity of market dynamics to agents'\nrisk profiles; third, we show that incorporating human-like variability shifts\npower dynamics towards clients and can disproportionately affect the entire\nsystem, often resulting in systemic agent collapse across simulations. These\nfindings underscore the emergent properties that arise when introducing\nstochastic, human-like decision processes, revealing new system behaviors that\nenhance the realism and complexity of artificial societies."
                },
                "authors": [
                    {
                        "name": "Alicia Vidler"
                    },
                    {
                        "name": "Toby Walsh"
                    }
                ],
                "author_detail": {
                    "name": "Toby Walsh"
                },
                "author": "Toby Walsh",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17403v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17403v2",
                "updated": "2025-03-04T16:36:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    36,
                    52,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-24T18:30:36Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    30,
                    36,
                    0,
                    55,
                    0
                ],
                "title": "Large Language Models are Powerful EHR Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Powerful EHR Encoders"
                },
                "summary": "Electronic Health Records (EHRs) offer rich potential for clinical\nprediction, yet their inherent complexity and heterogeneity pose significant\nchallenges for traditional machine learning approaches. Domain-specific EHR\nfoundation models trained on large collections of unlabeled EHR data have\ndemonstrated promising improvements in predictive accuracy and generalization;\nhowever, their training is constrained by limited access to diverse,\nhigh-quality datasets and inconsistencies in coding standards and healthcare\npractices. In this study, we explore the possibility of using general-purpose\nLarge Language Models (LLMs) based embedding methods as EHR encoders. By\nserializing patient records into structured Markdown text, transforming codes\ninto human-readable descriptors, we leverage the extensive generalization\ncapabilities of LLMs pretrained on vast public corpora, thereby bypassing the\nneed for proprietary medical datasets. We systematically evaluate two\nstate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and\nLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from\nthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundation\nmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our results\ndemonstrate that LLM-based embeddings frequently match or exceed the\nperformance of specialized models, even in few-shot settings, and that their\neffectiveness scales with the size of the underlying LLM and the available\ncontext window. Overall, our findings demonstrate that repurposing LLMs for EHR\nencoding offers a scalable and effective approach for clinical prediction,\ncapable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Health Records (EHRs) offer rich potential for clinical\nprediction, yet their inherent complexity and heterogeneity pose significant\nchallenges for traditional machine learning approaches. Domain-specific EHR\nfoundation models trained on large collections of unlabeled EHR data have\ndemonstrated promising improvements in predictive accuracy and generalization;\nhowever, their training is constrained by limited access to diverse,\nhigh-quality datasets and inconsistencies in coding standards and healthcare\npractices. In this study, we explore the possibility of using general-purpose\nLarge Language Models (LLMs) based embedding methods as EHR encoders. By\nserializing patient records into structured Markdown text, transforming codes\ninto human-readable descriptors, we leverage the extensive generalization\ncapabilities of LLMs pretrained on vast public corpora, thereby bypassing the\nneed for proprietary medical datasets. We systematically evaluate two\nstate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and\nLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from\nthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundation\nmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our results\ndemonstrate that LLM-based embeddings frequently match or exceed the\nperformance of specialized models, even in few-shot settings, and that their\neffectiveness scales with the size of the underlying LLM and the available\ncontext window. Overall, our findings demonstrate that repurposing LLMs for EHR\nencoding offers a scalable and effective approach for clinical prediction,\ncapable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications."
                },
                "authors": [
                    {
                        "name": "Stefan Hegselmann"
                    },
                    {
                        "name": "Georg von Arnim"
                    },
                    {
                        "name": "Tillmann Rheude"
                    },
                    {
                        "name": "Noel Kronenberg"
                    },
                    {
                        "name": "David Sontag"
                    },
                    {
                        "name": "Gerhard Hindricks"
                    },
                    {
                        "name": "Roland Eils"
                    },
                    {
                        "name": "Benjamin Wild"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Wild"
                },
                "author": "Benjamin Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17403v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17403v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02760v1",
                "updated": "2025-03-04T16:22:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    22,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:22:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    22,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "From Metaphor to Mechanism: How LLMs Decode Traditional Chinese Medicine\n  Symbolic Language for Modern Clinical Relevance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Metaphor to Mechanism: How LLMs Decode Traditional Chinese Medicine\n  Symbolic Language for Modern Clinical Relevance"
                },
                "summary": "Metaphorical expressions are abundant in Traditional Chinese Medicine (TCM),\nconveying complex disease mechanisms and holistic health concepts through\nculturally rich and often abstract terminology. Bridging these metaphors to\nanatomically driven Western medical (WM) concepts poses significant challenges\nfor both automated language processing and real-world clinical practice. To\naddress this gap, we propose a novel multi-agent and chain-of-thought (CoT)\nframework designed to interpret TCM metaphors accurately and map them to WM\npathophysiology. Specifically, our approach combines domain-specialized agents\n(TCM Expert, WM Expert) with a Coordinator Agent, leveraging stepwise\nchain-of-thought prompts to ensure transparent reasoning and conflict\nresolution. We detail a methodology for building a metaphor-rich TCM dataset,\ndiscuss strategies for effectively integrating multi-agent collaboration and\nCoT reasoning, and articulate the theoretical underpinnings that guide metaphor\ninterpretation across distinct medical paradigms. We present a comprehensive\nsystem design and highlight both the potential benefits and limitations of our\napproach, while leaving placeholders for future experimental validation. Our\nwork aims to support clinical decision-making, cross-system educational\ninitiatives, and integrated healthcare research, ultimately offering a robust\nscaffold for reconciling TCM's symbolic language with the mechanistic focus of\nWestern medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphorical expressions are abundant in Traditional Chinese Medicine (TCM),\nconveying complex disease mechanisms and holistic health concepts through\nculturally rich and often abstract terminology. Bridging these metaphors to\nanatomically driven Western medical (WM) concepts poses significant challenges\nfor both automated language processing and real-world clinical practice. To\naddress this gap, we propose a novel multi-agent and chain-of-thought (CoT)\nframework designed to interpret TCM metaphors accurately and map them to WM\npathophysiology. Specifically, our approach combines domain-specialized agents\n(TCM Expert, WM Expert) with a Coordinator Agent, leveraging stepwise\nchain-of-thought prompts to ensure transparent reasoning and conflict\nresolution. We detail a methodology for building a metaphor-rich TCM dataset,\ndiscuss strategies for effectively integrating multi-agent collaboration and\nCoT reasoning, and articulate the theoretical underpinnings that guide metaphor\ninterpretation across distinct medical paradigms. We present a comprehensive\nsystem design and highlight both the potential benefits and limitations of our\napproach, while leaving placeholders for future experimental validation. Our\nwork aims to support clinical decision-making, cross-system educational\ninitiatives, and integrated healthcare research, ultimately offering a robust\nscaffold for reconciling TCM's symbolic language with the mechanistic focus of\nWestern medicine."
                },
                "authors": [
                    {
                        "name": "Jiacheng Tang"
                    },
                    {
                        "name": "Nankai Wu"
                    },
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Chengxiao Dai"
                    },
                    {
                        "name": "Mengyao Zhao"
                    },
                    {
                        "name": "Xinjie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xinjie Zhao"
                },
                "author": "Xinjie Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06435v2",
                "updated": "2025-03-04T16:22:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    22,
                    34,
                    1,
                    63,
                    0
                ],
                "published": "2024-12-09T12:21:20Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    21,
                    20,
                    0,
                    344,
                    0
                ],
                "title": "Simulating Human-like Daily Activities with Desire-driven Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Human-like Daily Activities with Desire-driven Autonomy"
                },
                "summary": "Desires motivate humans to interact autonomously with the complex world. In\ncontrast, current AI agents require explicit task specifications, such as\ninstructions or reward functions, which constrain their autonomy and behavioral\ndiversity. In this paper, we introduce a Desire-driven Autonomous Agent (D2A)\nthat can enable a large language model (LLM) to autonomously propose and select\ntasks, motivated by satisfying its multi-dimensional desires. Specifically, the\nmotivational framework of D2A is mainly constructed by a dynamic Value System,\ninspired by the Theory of Needs. It incorporates an understanding of human-like\ndesires, such as the need for social interaction, personal fulfillment, and\nself-care. At each step, the agent evaluates the value of its current state,\nproposes a set of candidate activities, and selects the one that best aligns\nwith its intrinsic motivations. We conduct experiments on Concordia, a\ntext-based simulator, to demonstrate that our agent generates coherent,\ncontextually relevant daily activities while exhibiting variability and\nadaptability similar to human behavior. A comparative analysis with other\nLLM-based agents demonstrates that our approach significantly enhances the\nrationality of the simulated activities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Desires motivate humans to interact autonomously with the complex world. In\ncontrast, current AI agents require explicit task specifications, such as\ninstructions or reward functions, which constrain their autonomy and behavioral\ndiversity. In this paper, we introduce a Desire-driven Autonomous Agent (D2A)\nthat can enable a large language model (LLM) to autonomously propose and select\ntasks, motivated by satisfying its multi-dimensional desires. Specifically, the\nmotivational framework of D2A is mainly constructed by a dynamic Value System,\ninspired by the Theory of Needs. It incorporates an understanding of human-like\ndesires, such as the need for social interaction, personal fulfillment, and\nself-care. At each step, the agent evaluates the value of its current state,\nproposes a set of candidate activities, and selects the one that best aligns\nwith its intrinsic motivations. We conduct experiments on Concordia, a\ntext-based simulator, to demonstrate that our agent generates coherent,\ncontextually relevant daily activities while exhibiting variability and\nadaptability similar to human behavior. A comparative analysis with other\nLLM-based agents demonstrates that our approach significantly enhances the\nrationality of the simulated activities."
                },
                "authors": [
                    {
                        "name": "Yiding Wang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Fangwei Zhong"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05821v2",
                "updated": "2025-03-04T16:21:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    26,
                    1,
                    63,
                    0
                ],
                "published": "2024-10-08T08:51:44Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    8,
                    51,
                    44,
                    1,
                    282,
                    0
                ],
                "title": "Towards Zero-Shot, Controllable Dialog Planning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Zero-Shot, Controllable Dialog Planning with LLMs"
                },
                "summary": "Recently, Large Language Models (LLMs) have emerged as an alternative to\ntraining task-specific dialog agents, due to their broad reasoning capabilities\nand performance in zero-shot learning scenarios. However, many LLM-based dialog\nsystems fall short in planning towards an overarching dialog goal and therefore\ncannot steer the conversation appropriately. Furthermore, these models struggle\nwith hallucination, making them unsuitable for information access in sensitive\ndomains, such as legal or medical domains, where correctness of information\ngiven to users is critical. The recently introduced task Conversational Tree\nSearch (CTS) proposes the use of dialog graphs to avoid hallucination in\nsensitive domains, however, state-of-the-art agents are Reinforcement Learning\n(RL) based and require long training times, despite excelling at dialog\nstrategy. This paper introduces a novel zero-shot method for controllable CTS\nagents, where LLMs guide the dialog planning through domain graphs by searching\nand pruning relevant graph nodes based on user interaction preferences. We show\nthat these agents significantly outperform state-of-the-art CTS agents\n($p<0.0001$; Barnard Exact test) in simulation. This generalizes to all\navailable CTS domains. Finally, we perform user evaluation to test the agent's\nperformance in the wild, showing that our policy significantly ($p<0.05$;\nBarnard Exact) improves task-success compared to the state-of-the-art RL-based\nCTS agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have emerged as an alternative to\ntraining task-specific dialog agents, due to their broad reasoning capabilities\nand performance in zero-shot learning scenarios. However, many LLM-based dialog\nsystems fall short in planning towards an overarching dialog goal and therefore\ncannot steer the conversation appropriately. Furthermore, these models struggle\nwith hallucination, making them unsuitable for information access in sensitive\ndomains, such as legal or medical domains, where correctness of information\ngiven to users is critical. The recently introduced task Conversational Tree\nSearch (CTS) proposes the use of dialog graphs to avoid hallucination in\nsensitive domains, however, state-of-the-art agents are Reinforcement Learning\n(RL) based and require long training times, despite excelling at dialog\nstrategy. This paper introduces a novel zero-shot method for controllable CTS\nagents, where LLMs guide the dialog planning through domain graphs by searching\nand pruning relevant graph nodes based on user interaction preferences. We show\nthat these agents significantly outperform state-of-the-art CTS agents\n($p<0.0001$; Barnard Exact test) in simulation. This generalizes to all\navailable CTS domains. Finally, we perform user evaluation to test the agent's\nperformance in the wild, showing that our policy significantly ($p<0.05$;\nBarnard Exact) improves task-success compared to the state-of-the-art RL-based\nCTS agent."
                },
                "authors": [
                    {
                        "name": "Dirk Väth"
                    },
                    {
                        "name": "Ngoc Thang Vu"
                    }
                ],
                "author_detail": {
                    "name": "Ngoc Thang Vu"
                },
                "author": "Ngoc Thang Vu",
                "arxiv_comment": "This paper has been accepted for publication at the AAAI 2022\n  Workshop on Planning in the Era of LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18632v2",
                "updated": "2025-03-04T16:20:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    20,
                    59,
                    1,
                    63,
                    0
                ],
                "published": "2025-01-27T22:07:52Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    7,
                    52,
                    0,
                    27,
                    0
                ],
                "title": "Towards Safe AI Clinicians: A Comprehensive Study on Large Language\n  Model Jailbreaking in Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Safe AI Clinicians: A Comprehensive Study on Large Language\n  Model Jailbreaking in Healthcare"
                },
                "summary": "Large language models (LLMs) are increasingly utilized in healthcare\napplications. However, their deployment in clinical practice raises significant\nsafety concerns, including the potential spread of harmful information. This\nstudy systematically assesses the vulnerabilities of seven LLMs to three\nadvanced black-box jailbreaking techniques within medical contexts. To quantify\nthe effectiveness of these techniques, we propose an automated and\ndomain-adapted agentic evaluation pipeline. Experiment results indicate that\nleading commercial and open-source LLMs are highly vulnerable to medical\njailbreaking attacks. To bolster model safety and reliability, we further\ninvestigate the effectiveness of Continual Fine-Tuning (CFT) in defending\nagainst medical adversarial attacks. Our findings underscore the necessity for\nevolving attack methods evaluation, domain-specific safety alignment, and LLM\nsafety-utility balancing. This research offers actionable insights for\nadvancing the safety and reliability of AI clinicians, contributing to ethical\nand effective AI deployment in healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized in healthcare\napplications. However, their deployment in clinical practice raises significant\nsafety concerns, including the potential spread of harmful information. This\nstudy systematically assesses the vulnerabilities of seven LLMs to three\nadvanced black-box jailbreaking techniques within medical contexts. To quantify\nthe effectiveness of these techniques, we propose an automated and\ndomain-adapted agentic evaluation pipeline. Experiment results indicate that\nleading commercial and open-source LLMs are highly vulnerable to medical\njailbreaking attacks. To bolster model safety and reliability, we further\ninvestigate the effectiveness of Continual Fine-Tuning (CFT) in defending\nagainst medical adversarial attacks. Our findings underscore the necessity for\nevolving attack methods evaluation, domain-specific safety alignment, and LLM\nsafety-utility balancing. This research offers actionable insights for\nadvancing the safety and reliability of AI clinicians, contributing to ethical\nand effective AI deployment in healthcare."
                },
                "authors": [
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Qian Lou"
                    },
                    {
                        "name": "Yanshan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanshan Wang"
                },
                "author": "Yanshan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02756v1",
                "updated": "2025-03-04T16:20:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    20,
                    52,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:20:52Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    20,
                    52,
                    1,
                    63,
                    0
                ],
                "title": "BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched\n  Prompting and Prompt Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched\n  Prompting and Prompt Compression"
                },
                "summary": "Recent advancements in Large Language Model (LLM)-based Natural Language\nGeneration evaluation have largely focused on single-example prompting,\nresulting in significant token overhead and computational inefficiencies. In\nthis work, we introduce BatchGEMBA-MQM, a framework that integrates batched\nprompting with the GEMBA-MQM metric for machine translation evaluation. Our\napproach aggregates multiple translation examples into a single prompt,\nreducing token usage by 2-4 times (depending on the batch size) relative to\nsingle-example prompting. Furthermore, we propose a batching-aware prompt\ncompression model that achieves an additional token reduction of 13-15% on\naverage while also showing ability to help mitigate batching-induced quality\ndegradation. Evaluations across several LLMs (GPT-4o, GPT-4o-mini, Mistral\nSmall, Phi4, and CommandR7B) and varying batch sizes reveal that while batching\ngenerally negatively affects quality (but sometimes not substantially), prompt\ncompression does not degrade further, and in some cases, recovers quality loss.\nFor instance, GPT-4o retains over 90% of its baseline performance at a batch\nsize of 4 when compression is applied, compared to a 44.6% drop without\ncompression. We plan to release our code and trained models at\nhttps://github.com/NL2G/batchgemba to support future research in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Model (LLM)-based Natural Language\nGeneration evaluation have largely focused on single-example prompting,\nresulting in significant token overhead and computational inefficiencies. In\nthis work, we introduce BatchGEMBA-MQM, a framework that integrates batched\nprompting with the GEMBA-MQM metric for machine translation evaluation. Our\napproach aggregates multiple translation examples into a single prompt,\nreducing token usage by 2-4 times (depending on the batch size) relative to\nsingle-example prompting. Furthermore, we propose a batching-aware prompt\ncompression model that achieves an additional token reduction of 13-15% on\naverage while also showing ability to help mitigate batching-induced quality\ndegradation. Evaluations across several LLMs (GPT-4o, GPT-4o-mini, Mistral\nSmall, Phi4, and CommandR7B) and varying batch sizes reveal that while batching\ngenerally negatively affects quality (but sometimes not substantially), prompt\ncompression does not degrade further, and in some cases, recovers quality loss.\nFor instance, GPT-4o retains over 90% of its baseline performance at a batch\nsize of 4 when compression is applied, compared to a 44.6% drop without\ncompression. We plan to release our code and trained models at\nhttps://github.com/NL2G/batchgemba to support future research in this domain."
                },
                "authors": [
                    {
                        "name": "Daniil Larionov"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15935v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15935v3",
                "updated": "2025-03-04T16:09:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    9,
                    43,
                    1,
                    63,
                    0
                ],
                "published": "2024-06-22T20:47:03Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    20,
                    47,
                    3,
                    5,
                    174,
                    0
                ],
                "title": "X5G: An Open, Programmable, Multi-vendor, End-to-end, Private 5G O-RAN\n  Testbed with NVIDIA ARC and OpenAirInterface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X5G: An Open, Programmable, Multi-vendor, End-to-end, Private 5G O-RAN\n  Testbed with NVIDIA ARC and OpenAirInterface"
                },
                "summary": "As Fifth generation (5G) cellular systems transition to softwarized,\nprogrammable, and intelligent networks, it becomes fundamental to enable public\nand private 5G deployments that are (i) primarily based on software components\nwhile (ii) maintaining or exceeding the performance of traditional monolithic\nsystems and (iii) enabling programmability through bespoke configurations and\noptimized deployments. This requires hardware acceleration to scale the\nPhysical (PHY) layer performance, programmable elements in the Radio Access\nNetwork (RAN) and intelligent controllers at the edge, careful planning of the\nRadio Frequency (RF) environment, as well as end-to-end integration and\ntesting. In this paper, we describe how we developed the programmable X5G\ntestbed, addressing these challenges through the deployment of the first 8-node\nnetwork based on the integration of NVIDIA Aerial RAN CoLab Over-the-Air\n(ARC-OTA), OpenAirInterface (OAI), and a near-real-time RAN Intelligent\nController (RIC). The Aerial Software Development Kit (SDK) provides the PHY\nlayer, accelerated on Graphics Processing Unit (GPU), with the higher layers\nfrom the OAI open-source project interfaced with the PHY through the Small Cell\nForum (SCF) Functional Application Platform Interface (FAPI). An E2 agent\nprovides connectivity to the O-RAN Software Community (OSC) near-real-time RIC.\nWe discuss software integration, network infrastructure, and a digital twin\nframework for RF planning. We then profile the performance with up to 4\nCommercial Off-the-Shelf (COTS) smartphones for each base station with iPerf\nand video streaming applications, as well as up to 25 emulated User Equipments\n(UEs), measuring a cell rate higher than 1.65 Gbps in downlink and 143 Mbps in\nuplink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Fifth generation (5G) cellular systems transition to softwarized,\nprogrammable, and intelligent networks, it becomes fundamental to enable public\nand private 5G deployments that are (i) primarily based on software components\nwhile (ii) maintaining or exceeding the performance of traditional monolithic\nsystems and (iii) enabling programmability through bespoke configurations and\noptimized deployments. This requires hardware acceleration to scale the\nPhysical (PHY) layer performance, programmable elements in the Radio Access\nNetwork (RAN) and intelligent controllers at the edge, careful planning of the\nRadio Frequency (RF) environment, as well as end-to-end integration and\ntesting. In this paper, we describe how we developed the programmable X5G\ntestbed, addressing these challenges through the deployment of the first 8-node\nnetwork based on the integration of NVIDIA Aerial RAN CoLab Over-the-Air\n(ARC-OTA), OpenAirInterface (OAI), and a near-real-time RAN Intelligent\nController (RIC). The Aerial Software Development Kit (SDK) provides the PHY\nlayer, accelerated on Graphics Processing Unit (GPU), with the higher layers\nfrom the OAI open-source project interfaced with the PHY through the Small Cell\nForum (SCF) Functional Application Platform Interface (FAPI). An E2 agent\nprovides connectivity to the O-RAN Software Community (OSC) near-real-time RIC.\nWe discuss software integration, network infrastructure, and a digital twin\nframework for RF planning. We then profile the performance with up to 4\nCommercial Off-the-Shelf (COTS) smartphones for each base station with iPerf\nand video streaming applications, as well as up to 25 emulated User Equipments\n(UEs), measuring a cell rate higher than 1.65 Gbps in downlink and 143 Mbps in\nuplink."
                },
                "authors": [
                    {
                        "name": "Davide Villa"
                    },
                    {
                        "name": "Imran Khan"
                    },
                    {
                        "name": "Florian Kaltenberger"
                    },
                    {
                        "name": "Nicholas Hedberg"
                    },
                    {
                        "name": "Rúben Soares da Silva"
                    },
                    {
                        "name": "Stefano Maxenti"
                    },
                    {
                        "name": "Leonardo Bonati"
                    },
                    {
                        "name": "Anupa Kelkar"
                    },
                    {
                        "name": "Chris Dick"
                    },
                    {
                        "name": "Eduardo Baena"
                    },
                    {
                        "name": "Josep M. Jornet"
                    },
                    {
                        "name": "Tommaso Melodia"
                    },
                    {
                        "name": "Michele Polese"
                    },
                    {
                        "name": "Dimitrios Koutsonikolas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Koutsonikolas"
                },
                "author": "Dimitrios Koutsonikolas",
                "arxiv_comment": "18 pages, 19 figures, 3 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15935v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15935v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02737v1",
                "updated": "2025-03-04T15:56:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    56,
                    43,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:56:43Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    56,
                    43,
                    1,
                    63,
                    0
                ],
                "title": "Large Language Models for Multilingual Previously Fact-Checked Claim\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Multilingual Previously Fact-Checked Claim\n  Detection"
                },
                "summary": "In our era of widespread false information, human fact-checkers often face\nthe challenge of duplicating efforts when verifying claims that may have\nalready been addressed in other countries or languages. As false information\ntranscends linguistic boundaries, the ability to automatically detect\npreviously fact-checked claims across languages has become an increasingly\nimportant task. This paper presents the first comprehensive evaluation of large\nlanguage models (LLMs) for multilingual previously fact-checked claim\ndetection. We assess seven LLMs across 20 languages in both monolingual and\ncross-lingual settings. Our results show that while LLMs perform well for\nhigh-resource languages, they struggle with low-resource languages. Moreover,\ntranslating original texts into English proved to be beneficial for\nlow-resource languages. These findings highlight the potential of LLMs for\nmultilingual previously fact-checked claim detection and provide a foundation\nfor further research on this promising application of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our era of widespread false information, human fact-checkers often face\nthe challenge of duplicating efforts when verifying claims that may have\nalready been addressed in other countries or languages. As false information\ntranscends linguistic boundaries, the ability to automatically detect\npreviously fact-checked claims across languages has become an increasingly\nimportant task. This paper presents the first comprehensive evaluation of large\nlanguage models (LLMs) for multilingual previously fact-checked claim\ndetection. We assess seven LLMs across 20 languages in both monolingual and\ncross-lingual settings. Our results show that while LLMs perform well for\nhigh-resource languages, they struggle with low-resource languages. Moreover,\ntranslating original texts into English proved to be beneficial for\nlow-resource languages. These findings highlight the potential of LLMs for\nmultilingual previously fact-checked claim detection and provide a foundation\nfor further research on this promising application of LLMs."
                },
                "authors": [
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Matúš Pikuliak"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Michal Gregor"
                    },
                    {
                        "name": "Marián Šimko"
                    }
                ],
                "author_detail": {
                    "name": "Marián Šimko"
                },
                "author": "Marián Šimko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02718v1",
                "updated": "2025-03-04T15:32:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    32,
                    59,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:32:59Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    32,
                    59,
                    1,
                    63,
                    0
                ],
                "title": "Evaluating Knowledge Generation and Self-Refinement Strategies for\n  LLM-based Column Type Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Knowledge Generation and Self-Refinement Strategies for\n  LLM-based Column Type Annotation"
                },
                "summary": "Understanding the semantics of columns in relational tables is an important\npre-processing step for indexing data lakes in order to provide rich data\nsearch. An approach to establishing such understanding is column type\nannotation (CTA) where the goal is to annotate table columns with terms from a\ngiven vocabulary. This paper experimentally compares different knowledge\ngeneration and self-refinement strategies for LLM-based column type annotation.\nThe strategies include using LLMs to generate term definitions, error-based\nrefinement of term definitions, self-correction, and fine-tuning using examples\nand term definitions. We evaluate these strategies along two dimensions:\neffectiveness measured as F1 performance and efficiency measured in terms of\ntoken usage and cost. Our experiments show that the best performing strategy\ndepends on the model/dataset combination. We find that using training data to\ngenerate label definitions outperforms using the same data as demonstrations\nfor in-context learning for two out of three datasets using OpenAI models. The\nexperiments further show that using the LLMs to refine label definitions brings\nan average increase of 3.9% F1 in 10 out of 12 setups compared to the\nperformance of the non-refined definitions. Combining fine-tuned models with\nself-refined term definitions results in the overall highest performance,\noutperforming zero-shot prompting fine-tuned models by at least 3% in F1 score.\nThe costs analysis shows that while reaching similar F1 score, self-refinement\nvia prompting is more cost efficient for use cases requiring smaller amounts of\ntables to be annotated while fine-tuning is more efficient for large amounts of\ntables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the semantics of columns in relational tables is an important\npre-processing step for indexing data lakes in order to provide rich data\nsearch. An approach to establishing such understanding is column type\nannotation (CTA) where the goal is to annotate table columns with terms from a\ngiven vocabulary. This paper experimentally compares different knowledge\ngeneration and self-refinement strategies for LLM-based column type annotation.\nThe strategies include using LLMs to generate term definitions, error-based\nrefinement of term definitions, self-correction, and fine-tuning using examples\nand term definitions. We evaluate these strategies along two dimensions:\neffectiveness measured as F1 performance and efficiency measured in terms of\ntoken usage and cost. Our experiments show that the best performing strategy\ndepends on the model/dataset combination. We find that using training data to\ngenerate label definitions outperforms using the same data as demonstrations\nfor in-context learning for two out of three datasets using OpenAI models. The\nexperiments further show that using the LLMs to refine label definitions brings\nan average increase of 3.9% F1 in 10 out of 12 setups compared to the\nperformance of the non-refined definitions. Combining fine-tuned models with\nself-refined term definitions results in the overall highest performance,\noutperforming zero-shot prompting fine-tuned models by at least 3% in F1 score.\nThe costs analysis shows that while reaching similar F1 score, self-refinement\nvia prompting is more cost efficient for use cases requiring smaller amounts of\ntables to be annotated while fine-tuning is more efficient for large amounts of\ntables."
                },
                "authors": [
                    {
                        "name": "Keti Korini"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18658v2",
                "updated": "2025-03-04T15:26:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    26,
                    19,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-25T21:37:25Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    21,
                    37,
                    25,
                    1,
                    56,
                    0
                ],
                "title": "Assistance or Disruption? Exploring and Evaluating the Design and\n  Trade-offs of Proactive AI Programming Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistance or Disruption? Exploring and Evaluating the Design and\n  Trade-offs of Proactive AI Programming Support"
                },
                "summary": "AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow."
                },
                "authors": [
                    {
                        "name": "Kevin Pu"
                    },
                    {
                        "name": "Daniel Lazaro"
                    },
                    {
                        "name": "Ian Arawjo"
                    },
                    {
                        "name": "Haijun Xia"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Tovi Grossman"
                    },
                    {
                        "name": "Yan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yan Chen"
                },
                "author": "Yan Chen",
                "arxiv_doi": "10.1145/3706598.3713357",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713357",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.18658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02702v1",
                "updated": "2025-03-04T15:18:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    18,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:18:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    18,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "RedChronos: A Large Language Model-Based Log Analysis System for Insider\n  Threat Detection in Enterprises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RedChronos: A Large Language Model-Based Log Analysis System for Insider\n  Threat Detection in Enterprises"
                },
                "summary": "Internal threat detection aims to address security threats within\norganizations or enterprises by identifying potential or already occurring\nmalicious threats within vast amounts of logs. Although organizations or\nenterprises have dedicated personnel responsible for reviewing these logs, it\nis impossible to manually examine all logs entirely. In response to the vast\nnumber of logs, we propose a system called RedChronos, which is a Large\nLanguage Model-Based Log Analysis System. This system incorporates innovative\nimprovements over previous research by employing Query-Aware Weighted Voting\nand a Semantic Expansion-based Genetic Algorithm with LLM-driven Mutations. On\nthe public datasets CERT 4.2 and 5.2, RedChronos outperforms or matches\nexisting approaches in terms of accuracy, precision, and detection rate.\nMoreover, RedChronos reduces the need for manual intervention in security log\nreviews by 90\\% in the Xiaohongshu SOC. Therefore, our RedChronos system\ndemonstrates exceptional performance in handling Internal Threat Detection\n(IDT) tasks, providing innovative solutions for these challenges. We believe\nthat future research can continue to enhance the system's performance in IDT\ntasks while also reducing the response time to internal risk events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal threat detection aims to address security threats within\norganizations or enterprises by identifying potential or already occurring\nmalicious threats within vast amounts of logs. Although organizations or\nenterprises have dedicated personnel responsible for reviewing these logs, it\nis impossible to manually examine all logs entirely. In response to the vast\nnumber of logs, we propose a system called RedChronos, which is a Large\nLanguage Model-Based Log Analysis System. This system incorporates innovative\nimprovements over previous research by employing Query-Aware Weighted Voting\nand a Semantic Expansion-based Genetic Algorithm with LLM-driven Mutations. On\nthe public datasets CERT 4.2 and 5.2, RedChronos outperforms or matches\nexisting approaches in terms of accuracy, precision, and detection rate.\nMoreover, RedChronos reduces the need for manual intervention in security log\nreviews by 90\\% in the Xiaohongshu SOC. Therefore, our RedChronos system\ndemonstrates exceptional performance in handling Internal Threat Detection\n(IDT) tasks, providing innovative solutions for these challenges. We believe\nthat future research can continue to enhance the system's performance in IDT\ntasks while also reducing the response time to internal risk events."
                },
                "authors": [
                    {
                        "name": "Chenyu Li"
                    },
                    {
                        "name": "Zhengjia Zhu"
                    },
                    {
                        "name": "Jiyan He"
                    },
                    {
                        "name": "Xiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiu Zhang"
                },
                "author": "Xiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02701v1",
                "updated": "2025-03-04T15:17:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    17,
                    57,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:17:57Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    17,
                    57,
                    1,
                    63,
                    0
                ],
                "title": "MindBridge: Scalable and Cross-Model Knowledge Editing via\n  Memory-Augmented Modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindBridge: Scalable and Cross-Model Knowledge Editing via\n  Memory-Augmented Modality"
                },
                "summary": "Knowledge editing is a technique for efficiently and accurately updating the\nknowledge of large language models (LLMs) to alleviate obsolescence and correct\nerrors. However, most existing methods overfit to specific models, causing\nedited knowledge to be discarded during each LLM update and requiring frequent\nre-editing, which is particularly burdensome in today's rapidly evolving\nopen-source community. To address this issue, we propose the problem of\ncross-model knowledge editing and introduce MindBridge, a scalable solution\ninspired by the low coupling between modality processing and LLMs in\nmulti-modal models. MindBridge introduces the novel concept of memory modality,\nwhich encodes edited knowledge as an independent modality. It first performs\nLLM-agnostic pre-training of the memory modality and then integrates it with\nvarious LLMs. Extensive experiments on multiple LLMs and popular knowledge\nediting datasets demonstrate that MindBridge achieves superior performance even\nin editing tens of thousands of knowledge entries and can flexibly adapt to\ndifferent LLMs. Our code is available at\nhttps://github.com/CrashBugger/MindBridge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing is a technique for efficiently and accurately updating the\nknowledge of large language models (LLMs) to alleviate obsolescence and correct\nerrors. However, most existing methods overfit to specific models, causing\nedited knowledge to be discarded during each LLM update and requiring frequent\nre-editing, which is particularly burdensome in today's rapidly evolving\nopen-source community. To address this issue, we propose the problem of\ncross-model knowledge editing and introduce MindBridge, a scalable solution\ninspired by the low coupling between modality processing and LLMs in\nmulti-modal models. MindBridge introduces the novel concept of memory modality,\nwhich encodes edited knowledge as an independent modality. It first performs\nLLM-agnostic pre-training of the memory modality and then integrates it with\nvarious LLMs. Extensive experiments on multiple LLMs and popular knowledge\nediting datasets demonstrate that MindBridge achieves superior performance even\nin editing tens of thousands of knowledge entries and can flexibly adapt to\ndifferent LLMs. Our code is available at\nhttps://github.com/CrashBugger/MindBridge."
                },
                "authors": [
                    {
                        "name": "Shuaike Li"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02698v1",
                "updated": "2025-03-04T15:14:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    14,
                    41,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:14:41Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    14,
                    41,
                    1,
                    63,
                    0
                ],
                "title": "FlowPlan: Zero-Shot Task Planning with LLM Flow Engineering for Robotic\n  Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowPlan: Zero-Shot Task Planning with LLM Flow Engineering for Robotic\n  Instruction Following"
                },
                "summary": "Robotic instruction following tasks require seamless integration of visual\nperception, task planning, target localization, and motion execution. However,\nexisting task planning methods for instruction following are either data-driven\nor underperform in zero-shot scenarios due to difficulties in grounding lengthy\ninstructions into actionable plans under operational constraints. To address\nthis, we propose FlowPlan, a structured multi-stage LLM workflow that elevates\nzero-shot pipeline and bridges the performance gap between zero-shot and\ndata-driven in-context learning methods. By decomposing the planning process\ninto modular stages--task information retrieval, language-level reasoning,\nsymbolic-level planning, and logical evaluation--FlowPlan generates logically\ncoherent action sequences while adhering to operational constraints and further\nextracts contextual guidance for precise instance-level target localization.\nBenchmarked on the ALFRED and validated in real-world applications, our method\nachieves competitive performance relative to data-driven in-context learning\nmethods and demonstrates adaptability across diverse environments. This work\nadvances zero-shot task planning in robotic systems without reliance on labeled\ndata. Project website: https://instruction-following-project.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic instruction following tasks require seamless integration of visual\nperception, task planning, target localization, and motion execution. However,\nexisting task planning methods for instruction following are either data-driven\nor underperform in zero-shot scenarios due to difficulties in grounding lengthy\ninstructions into actionable plans under operational constraints. To address\nthis, we propose FlowPlan, a structured multi-stage LLM workflow that elevates\nzero-shot pipeline and bridges the performance gap between zero-shot and\ndata-driven in-context learning methods. By decomposing the planning process\ninto modular stages--task information retrieval, language-level reasoning,\nsymbolic-level planning, and logical evaluation--FlowPlan generates logically\ncoherent action sequences while adhering to operational constraints and further\nextracts contextual guidance for precise instance-level target localization.\nBenchmarked on the ALFRED and validated in real-world applications, our method\nachieves competitive performance relative to data-driven in-context learning\nmethods and demonstrates adaptability across diverse environments. This work\nadvances zero-shot task planning in robotic systems without reliance on labeled\ndata. Project website: https://instruction-following-project.github.io/."
                },
                "authors": [
                    {
                        "name": "Zijun Lin"
                    },
                    {
                        "name": "Chao Tang"
                    },
                    {
                        "name": "Hanjing Ye"
                    },
                    {
                        "name": "Hong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hong Zhang"
                },
                "author": "Hong Zhang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12110v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12110v3",
                "updated": "2025-03-04T15:09:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    9,
                    10,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-17T18:36:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    36,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "A-MEM: Agentic Memory for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-MEM: Agentic Memory for LLM Agents"
                },
                "summary": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem."
                },
                "authors": [
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12110v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12110v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02692v1",
                "updated": "2025-03-04T15:04:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    4,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:04:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    4,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "FinArena: A Human-Agent Collaboration Framework for Financial Market\n  Analysis and Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinArena: A Human-Agent Collaboration Framework for Financial Market\n  Analysis and Forecasting"
                },
                "summary": "To improve stock trend predictions and support personalized investment\ndecisions, this paper proposes FinArena, a novel Human-Agent collaboration\nframework. Inspired by the mixture of experts (MoE) approach, FinArena combines\nmultimodal financial data analysis with user interaction. The human module\nfeatures an interactive interface that captures individual risk preferences,\nallowing personalized investment strategies. The machine module utilizes a\nLarge Language Model-based (LLM-based) multi-agent system to integrate diverse\ndata sources, such as stock prices, news articles, and financial statements. To\naddress hallucinations in LLMs, FinArena employs the adaptive\nRetrieval-Augmented Generative (RAG) method for processing unstructured news\ndata. Finally, a universal expert agent makes investment decisions based on the\nfeatures extracted from multimodal data and investors' individual risk\npreferences. Extensive experiments show that FinArena surpasses both\ntraditional and state-of-the-art benchmarks in stock trend prediction and\nyields promising results in trading simulations across various risk profiles.\nThese findings highlight FinArena's potential to enhance investment outcomes by\naligning strategic insights with personalized risk considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve stock trend predictions and support personalized investment\ndecisions, this paper proposes FinArena, a novel Human-Agent collaboration\nframework. Inspired by the mixture of experts (MoE) approach, FinArena combines\nmultimodal financial data analysis with user interaction. The human module\nfeatures an interactive interface that captures individual risk preferences,\nallowing personalized investment strategies. The machine module utilizes a\nLarge Language Model-based (LLM-based) multi-agent system to integrate diverse\ndata sources, such as stock prices, news articles, and financial statements. To\naddress hallucinations in LLMs, FinArena employs the adaptive\nRetrieval-Augmented Generative (RAG) method for processing unstructured news\ndata. Finally, a universal expert agent makes investment decisions based on the\nfeatures extracted from multimodal data and investors' individual risk\npreferences. Extensive experiments show that FinArena surpasses both\ntraditional and state-of-the-art benchmarks in stock trend prediction and\nyields promising results in trading simulations across various risk profiles.\nThese findings highlight FinArena's potential to enhance investment outcomes by\naligning strategic insights with personalized risk considerations."
                },
                "authors": [
                    {
                        "name": "Congluo Xu"
                    },
                    {
                        "name": "Zhaobin Liu"
                    },
                    {
                        "name": "Ziyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ziyang Li"
                },
                "author": "Ziyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02691v1",
                "updated": "2025-03-04T15:03:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    3,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T15:03:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    15,
                    3,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Memory Efficient Continual Learning for Edge-Based Visual Anomaly\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Efficient Continual Learning for Edge-Based Visual Anomaly\n  Detection"
                },
                "summary": "Visual Anomaly Detection (VAD) is a critical task in computer vision with\nnumerous real-world applications. However, deploying these models on edge\ndevices presents significant challenges, such as constrained computational and\nmemory resources. Additionally, dynamic data distributions in real-world\nsettings necessitate continuous model adaptation, further complicating\ndeployment under limited resources. To address these challenges, we present a\nnovel investigation into the problem of Continual Learning for Visual Anomaly\nDetection (CLAD) on edge devices. We evaluate the STFPM approach, given its low\nmemory footprint on edge devices, which demonstrates good performance when\ncombined with the Replay approach. Furthermore, we propose to study the\nbehavior of a recently proposed approach, PaSTe, specifically designed for the\nedge but not yet explored in the Continual Learning context. Our results show\nthat PaSTe is not only a lighter version of STPFM, but it also achieves\nsuperior anomaly detection performance, improving the f1 pixel performance by\n10% with the Replay technique. In particular, the structure of PaSTe allows us\nto test it using a series of Compressed Replay techniques, reducing memory\noverhead by a maximum of 91.5% compared to the traditional Replay for STFPM.\nOur study proves the feasibility of deploying VAD models that adapt and learn\nincrementally on CLAD scenarios on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Anomaly Detection (VAD) is a critical task in computer vision with\nnumerous real-world applications. However, deploying these models on edge\ndevices presents significant challenges, such as constrained computational and\nmemory resources. Additionally, dynamic data distributions in real-world\nsettings necessitate continuous model adaptation, further complicating\ndeployment under limited resources. To address these challenges, we present a\nnovel investigation into the problem of Continual Learning for Visual Anomaly\nDetection (CLAD) on edge devices. We evaluate the STFPM approach, given its low\nmemory footprint on edge devices, which demonstrates good performance when\ncombined with the Replay approach. Furthermore, we propose to study the\nbehavior of a recently proposed approach, PaSTe, specifically designed for the\nedge but not yet explored in the Continual Learning context. Our results show\nthat PaSTe is not only a lighter version of STPFM, but it also achieves\nsuperior anomaly detection performance, improving the f1 pixel performance by\n10% with the Replay technique. In particular, the structure of PaSTe allows us\nto test it using a series of Compressed Replay techniques, reducing memory\noverhead by a maximum of 91.5% compared to the traditional Replay for STFPM.\nOur study proves the feasibility of deploying VAD models that adapt and learn\nincrementally on CLAD scenarios on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Manuel Barusco"
                    },
                    {
                        "name": "Lorenzo D'Antoni"
                    },
                    {
                        "name": "Davide Dalle Pezze"
                    },
                    {
                        "name": "Francesco Borsatti"
                    },
                    {
                        "name": "Gian Antonio Susto"
                    }
                ],
                "author_detail": {
                    "name": "Gian Antonio Susto"
                },
                "author": "Gian Antonio Susto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02682v1",
                "updated": "2025-03-04T14:54:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    54,
                    45,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T14:54:45Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    54,
                    45,
                    1,
                    63,
                    0
                ],
                "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPO: Boosting LLM Agents with Meta Plan Optimization"
                },
                "summary": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios."
                },
                "authors": [
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Bingchan Zhao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02670v1",
                "updated": "2025-03-04T14:41:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    41,
                    5,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T14:41:05Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    41,
                    5,
                    1,
                    63,
                    0
                ],
                "title": "Multidimensional Consistency Improves Reasoning in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multidimensional Consistency Improves Reasoning in Language Models"
                },
                "summary": "While Large language models (LLMs) have proved able to address some complex\nreasoning tasks, we also know that they are highly sensitive to input\nvariation, which can lead to different solution paths and final answers. Answer\nconsistency across input variations can thus be taken as a sign of stronger\nconfidence. Leveraging this insight, we introduce a framework, {\\em\nMultidimensional Reasoning Consistency} where, focusing on math problems,\nmodels are systematically pushed to diversify solution paths towards a final\nanswer, thereby testing them for answer consistency across multiple input\nvariations. We induce variations in (i) order of shots in prompt, (ii) problem\nphrasing, and (iii) languages used. Extensive experiments on a large range of\nopen-source state-of-the-art LLMs of various sizes show that reasoning\nconsistency differs by variation dimension, and that by aggregating consistency\nacross dimensions, our framework consistently enhances mathematical reasoning\nperformance on both monolingual dataset GSM8K and multilingual dataset MGSM,\nespecially for smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large language models (LLMs) have proved able to address some complex\nreasoning tasks, we also know that they are highly sensitive to input\nvariation, which can lead to different solution paths and final answers. Answer\nconsistency across input variations can thus be taken as a sign of stronger\nconfidence. Leveraging this insight, we introduce a framework, {\\em\nMultidimensional Reasoning Consistency} where, focusing on math problems,\nmodels are systematically pushed to diversify solution paths towards a final\nanswer, thereby testing them for answer consistency across multiple input\nvariations. We induce variations in (i) order of shots in prompt, (ii) problem\nphrasing, and (iii) languages used. Extensive experiments on a large range of\nopen-source state-of-the-art LLMs of various sizes show that reasoning\nconsistency differs by variation dimension, and that by aggregating consistency\nacross dimensions, our framework consistently enhances mathematical reasoning\nperformance on both monolingual dataset GSM8K and multilingual dataset MGSM,\nespecially for smaller models."
                },
                "authors": [
                    {
                        "name": "Huiyuan Lai"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Malvina Nissim"
                    }
                ],
                "author_detail": {
                    "name": "Malvina Nissim"
                },
                "author": "Malvina Nissim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00735v2",
                "updated": "2025-03-04T14:30:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    30,
                    32,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-02T05:16:43Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    5,
                    16,
                    43,
                    6,
                    61,
                    0
                ],
                "title": "LADDER: Self-Improving LLMs Through Recursive Problem Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADDER: Self-Improving LLMs Through Recursive Problem Decomposition"
                },
                "summary": "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example\nRecursion), a framework which enables Large Language Models to autonomously\nimprove their problem-solving capabilities through self-guided learning by\nrecursively generating and solving progressively simpler variants of complex\nproblems. Unlike prior approaches that require curated datasets or human\nfeedback, LADDER leverages a model's own capabilities to generate easier\nquestion variants. We demonstrate LADDER's effectiveness in the subject of\nmathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on\nundergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to\nachieve 73% on the MIT Integration Bee qualifying examination. We also\nintroduce TTRL (Test-Time Reinforcement Learning), where we perform\nreinforcement learning on variants of test problems at inference time. TTRL\nenables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of\n90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's\nperformance. These results show how self-directed strategic learning can\nachieve significant capability improvements without relying on architectural\nscaling or human supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example\nRecursion), a framework which enables Large Language Models to autonomously\nimprove their problem-solving capabilities through self-guided learning by\nrecursively generating and solving progressively simpler variants of complex\nproblems. Unlike prior approaches that require curated datasets or human\nfeedback, LADDER leverages a model's own capabilities to generate easier\nquestion variants. We demonstrate LADDER's effectiveness in the subject of\nmathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on\nundergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to\nachieve 73% on the MIT Integration Bee qualifying examination. We also\nintroduce TTRL (Test-Time Reinforcement Learning), where we perform\nreinforcement learning on variants of test problems at inference time. TTRL\nenables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of\n90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's\nperformance. These results show how self-directed strategic learning can\nachieve significant capability improvements without relying on architectural\nscaling or human supervision."
                },
                "authors": [
                    {
                        "name": "Toby Simonds"
                    },
                    {
                        "name": "Akira Yoshiyama"
                    }
                ],
                "author_detail": {
                    "name": "Akira Yoshiyama"
                },
                "author": "Akira Yoshiyama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02662v1",
                "updated": "2025-03-04T14:25:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    25,
                    51,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T14:25:51Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    25,
                    51,
                    1,
                    63,
                    0
                ],
                "title": "10K is Enough: An Ultra-Lightweight Binarized Network for Infrared\n  Small-Target Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "10K is Enough: An Ultra-Lightweight Binarized Network for Infrared\n  Small-Target Detection"
                },
                "summary": "The widespread deployment of InfRared Small-Target Detection(IRSTD)\nalgorithms on edge devices necessitates the exploration of model compression\ntechniques. Binary neural networks (BNNs) are distinguished by their\nexceptional efficiency in model compression. However, the small size of\ninfrared targets introduces stringent precision requirements for the IRSTD\ntask, while the inherent precision loss during binarization presents a\nsignificant challenge. To address this, we propose the Binarized Infrared\nSmall-Target Detection Network (BiisNet), which preserves the core operations\nof binarized convolutions while integrating full-precision features into the\nnetwork's information flow. Specifically, we propose the Dot-Binary\nConvolution, which retains fine-grained semantic information in feature maps\nwhile still leveraging the binarized convolution operations. In addition, we\nintroduce a smooth and adaptive Dynamic Softsign function, which provides more\ncomprehensive and progressively finer gradient during back-propagation,\nenhancing model stability and promoting an optimal weight\ndistribution.Experimental results demonstrate that BiisNet not only\nsignificantly outperforms other binary architectures but also demonstrates\nstrong competitiveness among state-of-the-art full-precision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of InfRared Small-Target Detection(IRSTD)\nalgorithms on edge devices necessitates the exploration of model compression\ntechniques. Binary neural networks (BNNs) are distinguished by their\nexceptional efficiency in model compression. However, the small size of\ninfrared targets introduces stringent precision requirements for the IRSTD\ntask, while the inherent precision loss during binarization presents a\nsignificant challenge. To address this, we propose the Binarized Infrared\nSmall-Target Detection Network (BiisNet), which preserves the core operations\nof binarized convolutions while integrating full-precision features into the\nnetwork's information flow. Specifically, we propose the Dot-Binary\nConvolution, which retains fine-grained semantic information in feature maps\nwhile still leveraging the binarized convolution operations. In addition, we\nintroduce a smooth and adaptive Dynamic Softsign function, which provides more\ncomprehensive and progressively finer gradient during back-propagation,\nenhancing model stability and promoting an optimal weight\ndistribution.Experimental results demonstrate that BiisNet not only\nsignificantly outperforms other binary architectures but also demonstrates\nstrong competitiveness among state-of-the-art full-precision models."
                },
                "authors": [
                    {
                        "name": "Biqiao Xin"
                    },
                    {
                        "name": "Qianchen Mao"
                    },
                    {
                        "name": "Bingshu Wang"
                    },
                    {
                        "name": "Jiangbin Zheng"
                    },
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "C. L. Philip Chen"
                    }
                ],
                "author_detail": {
                    "name": "C. L. Philip Chen"
                },
                "author": "C. L. Philip Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02659v1",
                "updated": "2025-03-04T14:21:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    21,
                    8,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T14:21:08Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    21,
                    8,
                    1,
                    63,
                    0
                ],
                "title": "LoRA-Null: Low-Rank Adaptation via Null Space for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-Null: Low-Rank Adaptation via Null Space for Large Language Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) is the leading parameter-efficient fine-tuning\nmethod for Large Language Models (LLMs). However, the fine-tuned LLMs encounter\nthe issue of catastrophic forgetting of the pre-trained world knowledge. To\naddress this issue, inspired by theoretical insights of null space, we propose\nLoRA-Null, i.e., Low-Rank Adaptation via null space, which builds adapters\ninitialized from the null space of the pre-trained knowledge activation.\nConcretely, we randomly collect a few data samples and capture their\nactivations after passing through the LLM layer. We perform Singular Value\nDecomposition on the input activations to obtain their null space. We use the\nprojection of the pre-trained weights onto the null space as the initialization\nfor adapters. Experimental results demonstrate that this initialization\napproach can effectively preserve the original pre-trained world knowledge of\nthe LLMs during fine-tuning. Additionally, if we freeze the values of the\ndown-projection matrices during fine-tuning, it achieves even better\npreservation of the pre-trained world knowledge. LoRA-Null effectively\npreserves pre-trained world knowledge while maintaining strong fine-tuning\nperformance, as validated by extensive experiments on LLaMA series (LLaMA2,\nLLaMA3, LLaMA3.1, and LLaMA3.2) across Code, Math, and Instruction Following\ntasks. We also provide a theoretical guarantee for the capacity of LoRA-Null to\nretain pre-trained knowledge. Code is in\nhttps://github.com/HungerPWAY/LoRA-Null.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) is the leading parameter-efficient fine-tuning\nmethod for Large Language Models (LLMs). However, the fine-tuned LLMs encounter\nthe issue of catastrophic forgetting of the pre-trained world knowledge. To\naddress this issue, inspired by theoretical insights of null space, we propose\nLoRA-Null, i.e., Low-Rank Adaptation via null space, which builds adapters\ninitialized from the null space of the pre-trained knowledge activation.\nConcretely, we randomly collect a few data samples and capture their\nactivations after passing through the LLM layer. We perform Singular Value\nDecomposition on the input activations to obtain their null space. We use the\nprojection of the pre-trained weights onto the null space as the initialization\nfor adapters. Experimental results demonstrate that this initialization\napproach can effectively preserve the original pre-trained world knowledge of\nthe LLMs during fine-tuning. Additionally, if we freeze the values of the\ndown-projection matrices during fine-tuning, it achieves even better\npreservation of the pre-trained world knowledge. LoRA-Null effectively\npreserves pre-trained world knowledge while maintaining strong fine-tuning\nperformance, as validated by extensive experiments on LLaMA series (LLaMA2,\nLLaMA3, LLaMA3.1, and LLaMA3.2) across Code, Math, and Instruction Following\ntasks. We also provide a theoretical guarantee for the capacity of LoRA-Null to\nretain pre-trained knowledge. Code is in\nhttps://github.com/HungerPWAY/LoRA-Null."
                },
                "authors": [
                    {
                        "name": "Pengwei Tang"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Dongjie Zhang"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Debing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Debing Zhang"
                },
                "author": "Debing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20125v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20125v2",
                "updated": "2025-03-04T14:15:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    15,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-27T14:16:22Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    16,
                    22,
                    3,
                    58,
                    0
                ],
                "title": "Discovering Antagonists in Networks of Systems: Robot Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Antagonists in Networks of Systems: Robot Deployment"
                },
                "summary": "A contextual anomaly detection method is proposed and applied to the physical\nmotions of a robot swarm executing a coverage task. Using simulations of a\nswarm's normal behavior, a normalizing flow is trained to predict the\nlikelihood of a robot motion within the current context of its environment.\nDuring application, the predicted likelihood of the observed motions is used by\na detection criterion that categorizes a robot agent as normal or antagonistic.\nThe proposed method is evaluated on five different strategies of antagonistic\nbehavior. Importantly, only readily available simulated data of normal robot\nbehavior is used for training such that the nature of the anomalies need not be\nknown beforehand. The best detection criterion correctly categorizes at least\n80% of each antagonistic type while maintaining a false positive rate of less\nthan 5% for normal robot agents. Additionally, the method is validated in\nhardware experiments, yielding results similar to the simulated scenarios.\nCompared to the state-of-the-art approach, both the predictive performance of\nthe normalizing flow and the robustness of the detection criterion are\nincreased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A contextual anomaly detection method is proposed and applied to the physical\nmotions of a robot swarm executing a coverage task. Using simulations of a\nswarm's normal behavior, a normalizing flow is trained to predict the\nlikelihood of a robot motion within the current context of its environment.\nDuring application, the predicted likelihood of the observed motions is used by\na detection criterion that categorizes a robot agent as normal or antagonistic.\nThe proposed method is evaluated on five different strategies of antagonistic\nbehavior. Importantly, only readily available simulated data of normal robot\nbehavior is used for training such that the nature of the anomalies need not be\nknown beforehand. The best detection criterion correctly categorizes at least\n80% of each antagonistic type while maintaining a false positive rate of less\nthan 5% for normal robot agents. Additionally, the method is validated in\nhardware experiments, yielding results similar to the simulated scenarios.\nCompared to the state-of-the-art approach, both the predictive performance of\nthe normalizing flow and the robustness of the detection criterion are\nincreased."
                },
                "authors": [
                    {
                        "name": "Ingeborg Wenger"
                    },
                    {
                        "name": "Peter Eberhard"
                    },
                    {
                        "name": "Henrik Ebel"
                    }
                ],
                "author_detail": {
                    "name": "Henrik Ebel"
                },
                "author": "Henrik Ebel",
                "arxiv_comment": "reduced file size",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20125v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20125v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.9; I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02650v1",
                "updated": "2025-03-04T14:14:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    14,
                    28,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T14:14:28Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    14,
                    28,
                    1,
                    63,
                    0
                ],
                "title": "The Effectiveness of Large Language Models in Transforming Unstructured\n  Text to Standardized Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effectiveness of Large Language Models in Transforming Unstructured\n  Text to Standardized Formats"
                },
                "summary": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information."
                },
                "authors": [
                    {
                        "name": "William Brach"
                    },
                    {
                        "name": "Kristián Košťál"
                    },
                    {
                        "name": "Michal Ries"
                    }
                ],
                "author_detail": {
                    "name": "Michal Ries"
                },
                "author": "Michal Ries",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15077v3",
                "updated": "2025-03-04T13:58:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    58,
                    39,
                    1,
                    63,
                    0
                ],
                "published": "2024-01-26T18:59:01Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    18,
                    59,
                    1,
                    4,
                    26,
                    0
                ],
                "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty"
                },
                "summary": "Autoregressive decoding makes the inference of Large Language Models (LLMs)\ntime-consuming. In this paper, we reconsider speculative sampling and derive\ntwo key observations. Firstly, autoregression at the feature\n(second-to-top-layer) level is more straightforward than at the token level.\nSecondly, the inherent uncertainty in feature (second-to-top-layer) level\nautoregression constrains its performance. Based on these insights, we\nintroduce EAGLE (Extrapolation Algorithm for Greater Language-model\nEfficiency), a simple yet highly efficient speculative sampling framework. By\nincorporating a token sequence advanced by one time step, EAGLE effectively\nresolves the uncertainty, enabling precise second-to-top-layer feature\nprediction with minimal overhead. We conducted comprehensive evaluations of\nEAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE\nmodel Mixtral 8x7B Instruct, and tasks in dialogue, code generation,\nmathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE\nachieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while\nmaintaining the distribution of the generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding makes the inference of Large Language Models (LLMs)\ntime-consuming. In this paper, we reconsider speculative sampling and derive\ntwo key observations. Firstly, autoregression at the feature\n(second-to-top-layer) level is more straightforward than at the token level.\nSecondly, the inherent uncertainty in feature (second-to-top-layer) level\nautoregression constrains its performance. Based on these insights, we\nintroduce EAGLE (Extrapolation Algorithm for Greater Language-model\nEfficiency), a simple yet highly efficient speculative sampling framework. By\nincorporating a token sequence advanced by one time step, EAGLE effectively\nresolves the uncertainty, enabling precise second-to-top-layer feature\nprediction with minimal overhead. We conducted comprehensive evaluations of\nEAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE\nmodel Mixtral 8x7B Instruct, and tasks in dialogue, code generation,\nmathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE\nachieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while\nmaintaining the distribution of the generated text."
                },
                "authors": [
                    {
                        "name": "Yuhui Li"
                    },
                    {
                        "name": "Fangyun Wei"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Hongyang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Zhang"
                },
                "author": "Hongyang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02631v1",
                "updated": "2025-03-04T13:56:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    56,
                    18,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:56:18Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    56,
                    18,
                    1,
                    63,
                    0
                ],
                "title": "Reflection on Data Storytelling Tools in the Generative AI Era from the\n  Human-AI Collaboration Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflection on Data Storytelling Tools in the Generative AI Era from the\n  Human-AI Collaboration Perspective"
                },
                "summary": "Human-AI collaborative tools attract attentions from the data storytelling\ncommunity to lower the barrier of expertise and streamline the workflow. The\nrecent advance in large-scale generative AI techniques, e.g., large language\nmodels (LLMs) and text-to-image models, has the potential to enhance data\nstorytelling with their power in visual and narration generation. After two\nyears since these techniques were publicly available, it is important to\nreflect our progress of applying them and have an outlook for future\nopportunities. To achieve the goal, we compare the collaboration patterns of\nthe latest tools with those of earlier ones using a dedicated framework for\nunderstanding human-AI collaboration in data storytelling. Through comparison,\nwe identify persistent collaboration patterns, e.g., human-creator +\nAI-assistant, and emerging ones, e.g., AI-creator + human-reviewer. The\nbenefits of these AI techniques and other implications to human-AI\ncollaboration are also revealed. We further propose future directions to\nhopefully ignite innovations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-AI collaborative tools attract attentions from the data storytelling\ncommunity to lower the barrier of expertise and streamline the workflow. The\nrecent advance in large-scale generative AI techniques, e.g., large language\nmodels (LLMs) and text-to-image models, has the potential to enhance data\nstorytelling with their power in visual and narration generation. After two\nyears since these techniques were publicly available, it is important to\nreflect our progress of applying them and have an outlook for future\nopportunities. To achieve the goal, we compare the collaboration patterns of\nthe latest tools with those of earlier ones using a dedicated framework for\nunderstanding human-AI collaboration in data storytelling. Through comparison,\nwe identify persistent collaboration patterns, e.g., human-creator +\nAI-assistant, and emerging ones, e.g., AI-creator + human-reviewer. The\nbenefits of these AI techniques and other implications to human-AI\ncollaboration are also revealed. We further propose future directions to\nhopefully ignite innovations."
                },
                "authors": [
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Huamin Qu"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Qu"
                },
                "author": "Huamin Qu",
                "arxiv_comment": "This paper is a sequel to the CHI 24 paper \"Where Are We So Far?\n  Understanding Data Storytelling Tools from the Perspective of Human-AI\n  Collaboration (https://doi.org/10.1145/3613904.3642726), aiming to refresh\n  our understanding with the latest advancements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02628v1",
                "updated": "2025-03-04T13:53:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    53,
                    43,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:53:43Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    53,
                    43,
                    1,
                    63,
                    0
                ],
                "title": "Towards Event Extraction with Massive Types: LLM-based Collaborative\n  Annotation and Partitioning Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Event Extraction with Massive Types: LLM-based Collaborative\n  Annotation and Partitioning Extraction"
                },
                "summary": "Developing a general-purpose extraction system that can extract events with\nmassive types is a long-standing target in Event Extraction (EE). In doing so,\nthe challenge comes from two aspects: 1) The absence of an efficient and\neffective annotation method. 2) The absence of a powerful extraction method can\nhandle massive types. For the first challenge, we propose a collaborative\nannotation method based on Large Language Models (LLMs). Through collaboration\namong multiple LLMs, it first refines annotations of trigger words from distant\nsupervision and then carries out argument annotation. Next, a voting phase\nconsolidates the annotation preferences across different LLMs. Finally, we\ncreate the EEMT dataset, the largest EE dataset to date, featuring over 200,000\nsamples, 3,465 event types, and 6,297 role types. For the second challenge, we\npropose an LLM-based Partitioning EE method called LLM-PEE. To overcome the\nlimited context length of LLMs, LLM-PEE first recalls candidate event types and\nthen splits them into multiple partitions for LLMs to extract events. The\nresults in the supervised setting show that LLM-PEE outperforms the\nstate-of-the-art methods by 5.4 in event detection and 6.1 in argument\nextraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement\ncompared to mainstream LLMs, demonstrating its strong generalization\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing a general-purpose extraction system that can extract events with\nmassive types is a long-standing target in Event Extraction (EE). In doing so,\nthe challenge comes from two aspects: 1) The absence of an efficient and\neffective annotation method. 2) The absence of a powerful extraction method can\nhandle massive types. For the first challenge, we propose a collaborative\nannotation method based on Large Language Models (LLMs). Through collaboration\namong multiple LLMs, it first refines annotations of trigger words from distant\nsupervision and then carries out argument annotation. Next, a voting phase\nconsolidates the annotation preferences across different LLMs. Finally, we\ncreate the EEMT dataset, the largest EE dataset to date, featuring over 200,000\nsamples, 3,465 event types, and 6,297 role types. For the second challenge, we\npropose an LLM-based Partitioning EE method called LLM-PEE. To overcome the\nlimited context length of LLMs, LLM-PEE first recalls candidate event types and\nthen splits them into multiple partitions for LLMs to extract events. The\nresults in the supervised setting show that LLM-PEE outperforms the\nstate-of-the-art methods by 5.4 in event detection and 6.1 in argument\nextraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement\ncompared to mainstream LLMs, demonstrating its strong generalization\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Wenxuan Liu"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Long Bai"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Daozhu Xu"
                    },
                    {
                        "name": "Xiaolong Jin"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13044v2",
                "updated": "2025-03-04T13:51:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    51,
                    34,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-18T16:56:15Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    56,
                    15,
                    1,
                    49,
                    0
                ],
                "title": "Do we still need Human Annotators? Prompting Large Language Models for\n  Aspect Sentiment Quad Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do we still need Human Annotators? Prompting Large Language Models for\n  Aspect Sentiment Quad Prediction"
                },
                "summary": "Aspect sentiment quadruple prediction (ASQP) facilitates a detailed\nunderstanding of opinions expressed in a text by identifying the opinion term,\naspect term, aspect category and sentiment polarity for each opinion. However,\nannotating a full set of training examples to fine-tune models for ASQP is a\nresource-intensive process. In this study, we explore the capabilities of large\nlanguage models (LLMs) for zero- and few-shot learning on the ASQP task across\nfive diverse datasets. We report F1 scores slightly below those obtained with\nstate-of-the-art fine-tuned models but exceeding previously reported zero- and\nfew-shot performance. In the 40-shot setting on the Rest16 restaurant domain\ndataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the\nbest-performing fine-tuned method MVP. Additionally, we report the performance\nof LLMs in target aspect sentiment detection (TASD), where the F1 scores were\nalso close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot\nsetting, compared to 72.76 with MVP. While human annotators remain essential\nfor achieving optimal performance, LLMs can reduce the need for extensive\nmanual annotation in ASQP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect sentiment quadruple prediction (ASQP) facilitates a detailed\nunderstanding of opinions expressed in a text by identifying the opinion term,\naspect term, aspect category and sentiment polarity for each opinion. However,\nannotating a full set of training examples to fine-tune models for ASQP is a\nresource-intensive process. In this study, we explore the capabilities of large\nlanguage models (LLMs) for zero- and few-shot learning on the ASQP task across\nfive diverse datasets. We report F1 scores slightly below those obtained with\nstate-of-the-art fine-tuned models but exceeding previously reported zero- and\nfew-shot performance. In the 40-shot setting on the Rest16 restaurant domain\ndataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the\nbest-performing fine-tuned method MVP. Additionally, we report the performance\nof LLMs in target aspect sentiment detection (TASD), where the F1 scores were\nalso close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot\nsetting, compared to 72.76 with MVP. While human annotators remain essential\nfor achieving optimal performance, LLMs can reduce the need for extensive\nmanual annotation in ASQP tasks."
                },
                "authors": [
                    {
                        "name": "Nils Constantin Hellwig"
                    },
                    {
                        "name": "Jakob Fehle"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    },
                    {
                        "name": "Christian Wolff"
                    }
                ],
                "author_detail": {
                    "name": "Christian Wolff"
                },
                "author": "Christian Wolff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04070v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04070v6",
                "updated": "2025-03-04T13:51:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    51,
                    14,
                    1,
                    63,
                    0
                ],
                "published": "2024-10-05T08:00:55Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    0,
                    55,
                    5,
                    279,
                    0
                ],
                "title": "PAD: Personalized Alignment of LLMs at Decoding-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAD: Personalized Alignment of LLMs at Decoding-Time"
                },
                "summary": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment."
                },
                "authors": [
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Xiaotian Zhang"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04070v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04070v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02623v1",
                "updated": "2025-03-04T13:48:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    48,
                    50,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:48:50Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    48,
                    50,
                    1,
                    63,
                    0
                ],
                "title": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence\n  Calibration of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence\n  Calibration of Large Language Models"
                },
                "summary": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs."
                },
                "authors": [
                    {
                        "name": "Paul Stangel"
                    },
                    {
                        "name": "David Bani-Harouni"
                    },
                    {
                        "name": "Chantal Pellegrini"
                    },
                    {
                        "name": "Ege Özsoy"
                    },
                    {
                        "name": "Kamilia Zaripova"
                    },
                    {
                        "name": "Matthias Keicher"
                    },
                    {
                        "name": "Nassir Navab"
                    }
                ],
                "author_detail": {
                    "name": "Nassir Navab"
                },
                "author": "Nassir Navab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02603v1",
                "updated": "2025-03-04T13:21:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    21,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:21:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    21,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "OkraLong: A Flexible Retrieval-Augmented Framework for Long-Text Query\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OkraLong: A Flexible Retrieval-Augmented Framework for Long-Text Query\n  Processing"
                },
                "summary": "Large Language Models (LLMs) encounter challenges in efficiently processing\nlong-text queries, as seen in applications like enterprise document analysis\nand financial report comprehension. While conventional solutions employ\nlong-context processing or Retrieval-Augmented Generation (RAG), they suffer\nfrom prohibitive input expenses or incomplete information. Recent advancements\nadopt context compression and dynamic retrieval loops, but still sacrifice\ncritical details or incur iterative costs.To address these limitations, we\npropose OkraLong, a novel framework that flexibly optimizes the entire\nprocessing workflow. Unlike prior static or coarse-grained adaptive strategies,\nOkraLong adopts fine-grained orchestration through three synergistic\ncomponents: analyzer, organizer and executor. The analyzer characterizes the\ntask states, which guide the organizer in dynamically scheduling the workflow.\nThe executor carries out the execution and generates the final answer.\nExperimental results demonstrate that OkraLong not only enhances answer\naccuracy but also achieves cost-effectiveness across a variety of datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encounter challenges in efficiently processing\nlong-text queries, as seen in applications like enterprise document analysis\nand financial report comprehension. While conventional solutions employ\nlong-context processing or Retrieval-Augmented Generation (RAG), they suffer\nfrom prohibitive input expenses or incomplete information. Recent advancements\nadopt context compression and dynamic retrieval loops, but still sacrifice\ncritical details or incur iterative costs.To address these limitations, we\npropose OkraLong, a novel framework that flexibly optimizes the entire\nprocessing workflow. Unlike prior static or coarse-grained adaptive strategies,\nOkraLong adopts fine-grained orchestration through three synergistic\ncomponents: analyzer, organizer and executor. The analyzer characterizes the\ntask states, which guide the organizer in dynamically scheduling the workflow.\nThe executor carries out the execution and generates the final answer.\nExperimental results demonstrate that OkraLong not only enhances answer\naccuracy but also achieves cost-effectiveness across a variety of datasets."
                },
                "authors": [
                    {
                        "name": "Yulong Hui"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Huanchen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huanchen Zhang"
                },
                "author": "Huanchen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02600v1",
                "updated": "2025-03-04T13:20:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    20,
                    42,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:20:42Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    20,
                    42,
                    1,
                    63,
                    0
                ],
                "title": "Resource-Efficient Affordance Grounding with Complementary Depth and\n  Semantic Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Affordance Grounding with Complementary Depth and\n  Semantic Prompts"
                },
                "summary": "Affordance refers to the functional properties that an agent perceives and\nutilizes from its environment, and is key perceptual information required for\nrobots to perform actions. This information is rich and multimodal in nature.\nExisting multimodal affordance methods face limitations in extracting useful\ninformation, mainly due to simple structural designs, basic fusion methods, and\nlarge model parameters, making it difficult to meet the performance\nrequirements for practical deployment. To address these issues, this paper\nproposes the BiT-Align image-depth-text affordance mapping framework. The\nframework includes a Bypass Prompt Module (BPM) and a Text Feature Guidance\n(TFG) attention selection mechanism. BPM integrates the auxiliary modality\ndepth image directly as a prompt to the primary modality RGB image, embedding\nit into the primary modality encoder without introducing additional encoders.\nThis reduces the model's parameter count and effectively improves functional\nregion localization accuracy. The TFG mechanism guides the selection and\nenhancement of attention heads in the image encoder using textual features,\nimproving the understanding of affordance characteristics. Experimental results\ndemonstrate that the proposed method achieves significant performance\nimprovements on public AGD20K and HICO-IIF datasets. On the AGD20K dataset,\ncompared with the current state-of-the-art method, we achieve a 6.0%\nimprovement in the KLD metric, while reducing model parameters by 88.8%,\ndemonstrating practical application values. The source code will be made\npublicly available at https://github.com/DAWDSE/BiT-Align.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordance refers to the functional properties that an agent perceives and\nutilizes from its environment, and is key perceptual information required for\nrobots to perform actions. This information is rich and multimodal in nature.\nExisting multimodal affordance methods face limitations in extracting useful\ninformation, mainly due to simple structural designs, basic fusion methods, and\nlarge model parameters, making it difficult to meet the performance\nrequirements for practical deployment. To address these issues, this paper\nproposes the BiT-Align image-depth-text affordance mapping framework. The\nframework includes a Bypass Prompt Module (BPM) and a Text Feature Guidance\n(TFG) attention selection mechanism. BPM integrates the auxiliary modality\ndepth image directly as a prompt to the primary modality RGB image, embedding\nit into the primary modality encoder without introducing additional encoders.\nThis reduces the model's parameter count and effectively improves functional\nregion localization accuracy. The TFG mechanism guides the selection and\nenhancement of attention heads in the image encoder using textual features,\nimproving the understanding of affordance characteristics. Experimental results\ndemonstrate that the proposed method achieves significant performance\nimprovements on public AGD20K and HICO-IIF datasets. On the AGD20K dataset,\ncompared with the current state-of-the-art method, we achieve a 6.0%\nimprovement in the KLD metric, while reducing model parameters by 88.8%,\ndemonstrating practical application values. The source code will be made\npublicly available at https://github.com/DAWDSE/BiT-Align."
                },
                "authors": [
                    {
                        "name": "Yizhou Huang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Guoliang Zhu"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Yukun Zuo"
                    },
                    {
                        "name": "Wenrui Chen"
                    },
                    {
                        "name": "Zhiyong Li"
                    },
                    {
                        "name": "Kailun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kailun Yang"
                },
                "author": "Kailun Yang",
                "arxiv_comment": "The source code will be made publicly available at\n  https://github.com/DAWDSE/BiT-Align",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02597v1",
                "updated": "2025-03-04T13:18:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    18,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:18:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    18,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual\n  Attention for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual\n  Attention for Multimodal LLMs"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of earlier\nmodalities (e.g., images) to incorporate information from later modalities\n(e.g., text). To address this problem, we propose AKI, a novel MLLM that\nunlocks causal attention into modality-mutual attention (MMA) to enable image\ntokens to attend to text tokens. This simple yet effective design allows AKI to\nachieve superior performance in 12 multimodal understanding benchmarks (+7.2%\non average) without introducing additional parameters and increasing training\ntime. Our MMA design is intended to be generic, allowing for application across\nvarious modalities, and scalable to accommodate diverse multimodal scenarios.\nThe code is publicly available at https://github.com/sony/aki, and we will\nrelease our AKI-4B model to encourage further advancements in MLLMs across\nvarious directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of earlier\nmodalities (e.g., images) to incorporate information from later modalities\n(e.g., text). To address this problem, we propose AKI, a novel MLLM that\nunlocks causal attention into modality-mutual attention (MMA) to enable image\ntokens to attend to text tokens. This simple yet effective design allows AKI to\nachieve superior performance in 12 multimodal understanding benchmarks (+7.2%\non average) without introducing additional parameters and increasing training\ntime. Our MMA design is intended to be generic, allowing for application across\nvarious modalities, and scalable to accommodate diverse multimodal scenarios.\nThe code is publicly available at https://github.com/sony/aki, and we will\nrelease our AKI-4B model to encourage further advancements in MLLMs across\nvarious directions."
                },
                "authors": [
                    {
                        "name": "Wei-Yao Wang"
                    },
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Helen Suzuki"
                    },
                    {
                        "name": "Yoshiyuki Kobayashi"
                    }
                ],
                "author_detail": {
                    "name": "Yoshiyuki Kobayashi"
                },
                "author": "Yoshiyuki Kobayashi",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07209v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07209v2",
                "updated": "2025-03-04T13:14:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    14,
                    20,
                    1,
                    63,
                    0
                ],
                "published": "2025-01-13T11:04:05Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    4,
                    5,
                    0,
                    13,
                    0
                ],
                "title": "Privacy-Preserving Authentication: Theory vs. Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Authentication: Theory vs. Practice"
                },
                "summary": "With the increasing use of online services, the protection of the privacy of\nusers becomes more and more important. This is particularly critical as\nauthentication and authorization as realized on the Internet nowadays,\ntypically relies on centralized identity management solutions. Although those\nare very convenient from a user's perspective, they are quite intrusive from a\nprivacy perspective and are currently far from implementing the concept of data\nminimization. Fortunately, cryptography offers exciting primitives such as\nzero-knowledge proofs and advanced signature schemes to realize various forms\nof so-called anonymous credentials. Such primitives allow to realize online\nauthentication and authorization with a high level of built-in privacy\nprotection (what we call privacy-preserving authentication). Though these\nprimitives have already been researched for various decades and are well\nunderstood in the research community, unfortunately, they lack widespread\nadoption. In this paper, we look at the problems, what cryptography can do,\nsome deployment examples, and barriers to widespread adoption. Latter using the\nexample of the EU Digital Identity Wallet (EUDIW) and the recent discussion and\nfeedback from cryptography experts around this topic. We also briefly comment\non the transition to post-quantum cryptography.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing use of online services, the protection of the privacy of\nusers becomes more and more important. This is particularly critical as\nauthentication and authorization as realized on the Internet nowadays,\ntypically relies on centralized identity management solutions. Although those\nare very convenient from a user's perspective, they are quite intrusive from a\nprivacy perspective and are currently far from implementing the concept of data\nminimization. Fortunately, cryptography offers exciting primitives such as\nzero-knowledge proofs and advanced signature schemes to realize various forms\nof so-called anonymous credentials. Such primitives allow to realize online\nauthentication and authorization with a high level of built-in privacy\nprotection (what we call privacy-preserving authentication). Though these\nprimitives have already been researched for various decades and are well\nunderstood in the research community, unfortunately, they lack widespread\nadoption. In this paper, we look at the problems, what cryptography can do,\nsome deployment examples, and barriers to widespread adoption. Latter using the\nexample of the EU Digital Identity Wallet (EUDIW) and the recent discussion and\nfeedback from cryptography experts around this topic. We also briefly comment\non the transition to post-quantum cryptography."
                },
                "authors": [
                    {
                        "name": "Daniel Slamanig"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Slamanig"
                },
                "arxiv_affiliation": "Research Institute CODE, Universität der Bundeswehr München",
                "author": "Daniel Slamanig",
                "arxiv_comment": "This paper is based on a keynote with the same title given at the\n  19th IFIP Summer School on Privacy and Identity Management held between 10th\n  and 13th September 2024 in Madrid, Spain and appears in the Proceedings of\n  the Summer School",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07209v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07209v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02582v1",
                "updated": "2025-03-04T13:04:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    4,
                    48,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T13:04:48Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    4,
                    48,
                    1,
                    63,
                    0
                ],
                "title": "Playing games with Large language models: Randomness and strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Playing games with Large language models: Randomness and strategy"
                },
                "summary": "Playing games has a long history of describing intricate interactions in\nsimplified forms. In this paper we explore if large language models (LLMs) can\nplay games, investigating their capabilities for randomisation and strategic\nadaptation through both simultaneous and sequential game interactions. We focus\non GPT-4o-Mini-2024-08-17 and test two games between LLMs: Rock Paper Scissors\n(RPS) and games of strategy (Prisoners Dilemma PD). LLMs are often described as\nstochastic parrots, and while they may indeed be parrots, our results suggest\nthat they are not very stochastic in the sense that their outputs - when\nprompted to be random - are often very biased. Our research reveals that LLMs\nappear to develop loss aversion strategies in repeated games, with RPS\nconverging to stalemate conditions while PD shows systematic shifts between\ncooperative and competitive outcomes based on prompt design. We detail\nprogrammatic tools for independent agent interactions and the Agentic AI\nchallenges faced in implementation. We show that LLMs can indeed play games,\njust not very well. These results have implications for the use of LLMs in\nmulti-agent LLM systems and showcase limitations in current approaches to model\noutput for strategic decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Playing games has a long history of describing intricate interactions in\nsimplified forms. In this paper we explore if large language models (LLMs) can\nplay games, investigating their capabilities for randomisation and strategic\nadaptation through both simultaneous and sequential game interactions. We focus\non GPT-4o-Mini-2024-08-17 and test two games between LLMs: Rock Paper Scissors\n(RPS) and games of strategy (Prisoners Dilemma PD). LLMs are often described as\nstochastic parrots, and while they may indeed be parrots, our results suggest\nthat they are not very stochastic in the sense that their outputs - when\nprompted to be random - are often very biased. Our research reveals that LLMs\nappear to develop loss aversion strategies in repeated games, with RPS\nconverging to stalemate conditions while PD shows systematic shifts between\ncooperative and competitive outcomes based on prompt design. We detail\nprogrammatic tools for independent agent interactions and the Agentic AI\nchallenges faced in implementation. We show that LLMs can indeed play games,\njust not very well. These results have implications for the use of LLMs in\nmulti-agent LLM systems and showcase limitations in current approaches to model\noutput for strategic decision-making."
                },
                "authors": [
                    {
                        "name": "Alicia Vidler"
                    },
                    {
                        "name": "Toby Walsh"
                    }
                ],
                "author_detail": {
                    "name": "Toby Walsh"
                },
                "author": "Toby Walsh",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00946v2",
                "updated": "2025-03-04T13:01:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    32,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-02T15:54:45Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    15,
                    54,
                    45,
                    6,
                    61,
                    0
                ],
                "title": "A Review of LLM-Assisted Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of LLM-Assisted Ideation"
                },
                "summary": "We present a comprehensive, in-depth review of ideation assisted by large\nlanguage models (LLMs), highlighting emerging trends and identifying\nunaddressed research gaps. In total, we examined 61 studies investigating the\napplication of LLMs in both group and individual ideation processes. From these\nstudies, we derived the Hourglass Ideation Framework for LLM-assisted ideation,\ncomprising three phases and seven key ideation stages, which served as the\nbasis for our systematic survey. Our analysis reveals that LLMs are most\nfrequently used for idea generation and refinement, but their use in scope\nspecification, foundational material structuring and multi-idea evaluation and\nselection remains limited. We provide our findings in extensive tabular and\nonline formats. These catalogues detail research on LLM-assisted, purely\nLLM-based, and human-only activities across the seven ideation stages for each\nof the 61 studies. These also detail creative domains, publication outlets,\ninteraction designs, user study designs, and assessment methods. Our analysis\nof system interaction design reveals a predominant focus on supporting\nindividual ideation activities and text-based interaction, with a growing trend\nof incorporating multimedia elements. However, in group ideation, tools and\ninteraction modalities targeting both synchronous and asynchronous\ncollaboration are much scarcer. We synthesize the primary findings of our\nreview and outline promising directions for future research in LLM-assisted\nideation. We hope this review will help researchers quickly gain an overview of\nthis rapidly expanding area, efficiently locate relevant work, and identify\nunderexplored areas for further investigation. In addition, we believe the\nframework we present here will form the basis for the development of future\nproblem and solution space taxonomies, and methodologies for LLM-assisted\nideation development and use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive, in-depth review of ideation assisted by large\nlanguage models (LLMs), highlighting emerging trends and identifying\nunaddressed research gaps. In total, we examined 61 studies investigating the\napplication of LLMs in both group and individual ideation processes. From these\nstudies, we derived the Hourglass Ideation Framework for LLM-assisted ideation,\ncomprising three phases and seven key ideation stages, which served as the\nbasis for our systematic survey. Our analysis reveals that LLMs are most\nfrequently used for idea generation and refinement, but their use in scope\nspecification, foundational material structuring and multi-idea evaluation and\nselection remains limited. We provide our findings in extensive tabular and\nonline formats. These catalogues detail research on LLM-assisted, purely\nLLM-based, and human-only activities across the seven ideation stages for each\nof the 61 studies. These also detail creative domains, publication outlets,\ninteraction designs, user study designs, and assessment methods. Our analysis\nof system interaction design reveals a predominant focus on supporting\nindividual ideation activities and text-based interaction, with a growing trend\nof incorporating multimedia elements. However, in group ideation, tools and\ninteraction modalities targeting both synchronous and asynchronous\ncollaboration are much scarcer. We synthesize the primary findings of our\nreview and outline promising directions for future research in LLM-assisted\nideation. We hope this review will help researchers quickly gain an overview of\nthis rapidly expanding area, efficiently locate relevant work, and identify\nunderexplored areas for further investigation. In addition, we believe the\nframework we present here will form the basis for the development of future\nproblem and solution space taxonomies, and methodologies for LLM-assisted\nideation development and use."
                },
                "authors": [
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Stefano Padilla"
                    },
                    {
                        "name": "Pierre Le Bras"
                    },
                    {
                        "name": "Junyu Dong"
                    },
                    {
                        "name": "Mike Chantler"
                    }
                ],
                "author_detail": {
                    "name": "Mike Chantler"
                },
                "author": "Mike Chantler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01622v2",
                "updated": "2025-03-04T13:00:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    0,
                    55,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-03T14:55:41Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    55,
                    41,
                    0,
                    62,
                    0
                ],
                "title": "DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards\n  Meaningful LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards\n  Meaningful LLM Evaluation"
                },
                "summary": "Recent work found that LLMs are sensitive to a wide range of arbitrary prompt\ndimensions, including the type of delimiters, answer enumerators, instruction\nwording, and more. This throws into question popular single-prompt evaluation\npractices. We present DOVE (Dataset Of Variation Evaluation) a large-scale\ndataset containing prompt perturbations of various evaluation benchmarks. In\ncontrast to previous work, we examine LLM sensitivity from an holistic\nperspective, and assess the joint effects of perturbations along various\ndimensions, resulting in thousands of perturbations per instance. We evaluate\nseveral model families against DOVE, leading to several findings, including\nefficient methods for choosing well-performing prompts, observing that few-shot\nexamples reduce sensitivity, and identifying instances which are inherently\nhard across all perturbations. DOVE consists of more than 250M prompt\nperturbations and model outputs, which we make publicly available to spur a\ncommunity-wide effort toward meaningful, robust, and efficient evaluation.\n  Browse the data, contribute, and more: https://slab-nlp.github.io/DOVE/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work found that LLMs are sensitive to a wide range of arbitrary prompt\ndimensions, including the type of delimiters, answer enumerators, instruction\nwording, and more. This throws into question popular single-prompt evaluation\npractices. We present DOVE (Dataset Of Variation Evaluation) a large-scale\ndataset containing prompt perturbations of various evaluation benchmarks. In\ncontrast to previous work, we examine LLM sensitivity from an holistic\nperspective, and assess the joint effects of perturbations along various\ndimensions, resulting in thousands of perturbations per instance. We evaluate\nseveral model families against DOVE, leading to several findings, including\nefficient methods for choosing well-performing prompts, observing that few-shot\nexamples reduce sensitivity, and identifying instances which are inherently\nhard across all perturbations. DOVE consists of more than 250M prompt\nperturbations and model outputs, which we make publicly available to spur a\ncommunity-wide effort toward meaningful, robust, and efficient evaluation.\n  Browse the data, contribute, and more: https://slab-nlp.github.io/DOVE/"
                },
                "authors": [
                    {
                        "name": "Eliya Habba"
                    },
                    {
                        "name": "Ofir Arviv"
                    },
                    {
                        "name": "Itay Itzhak"
                    },
                    {
                        "name": "Yotam Perlitz"
                    },
                    {
                        "name": "Elron Bandel"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Michal Shmueli-Scheuer"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Stanovsky"
                },
                "author": "Gabriel Stanovsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02574v1",
                "updated": "2025-03-04T12:55:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    55,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T12:55:07Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    55,
                    7,
                    1,
                    63,
                    0
                ],
                "title": "LLM-Safety Evaluations Lack Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Safety Evaluations Lack Robustness"
                },
                "summary": "In this paper, we argue that current safety alignment research efforts for\nlarge language models are hindered by many intertwined sources of noise, such\nas small datasets, methodological inconsistencies, and unreliable evaluation\nsetups. This can, at times, make it impossible to evaluate and compare attacks\nand defenses fairly, thereby slowing progress. We systematically analyze the\nLLM safety evaluation pipeline, covering dataset curation, optimization\nstrategies for automated red-teaming, response generation, and response\nevaluation using LLM judges. At each stage, we identify key issues and\nhighlight their practical impact. We also propose a set of guidelines for\nreducing noise and bias in evaluations of future attack and defense papers.\nLastly, we offer an opposing perspective, highlighting practical reasons for\nexisting limitations. We believe that addressing the outlined problems in\nfuture research will improve the field's ability to generate easily comparable\nresults and make measurable progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we argue that current safety alignment research efforts for\nlarge language models are hindered by many intertwined sources of noise, such\nas small datasets, methodological inconsistencies, and unreliable evaluation\nsetups. This can, at times, make it impossible to evaluate and compare attacks\nand defenses fairly, thereby slowing progress. We systematically analyze the\nLLM safety evaluation pipeline, covering dataset curation, optimization\nstrategies for automated red-teaming, response generation, and response\nevaluation using LLM judges. At each stage, we identify key issues and\nhighlight their practical impact. We also propose a set of guidelines for\nreducing noise and bias in evaluations of future attack and defense papers.\nLastly, we offer an opposing perspective, highlighting practical reasons for\nexisting limitations. We believe that addressing the outlined problems in\nfuture research will improve the field's ability to generate easily comparable\nresults and make measurable progress."
                },
                "authors": [
                    {
                        "name": "Tim Beyer"
                    },
                    {
                        "name": "Sophie Xhonneux"
                    },
                    {
                        "name": "Simon Geisler"
                    },
                    {
                        "name": "Gauthier Gidel"
                    },
                    {
                        "name": "Leo Schwinn"
                    },
                    {
                        "name": "Stephan Günnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Günnemann"
                },
                "author": "Stephan Günnemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06145v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06145v3",
                "updated": "2025-03-04T12:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    36,
                    51,
                    1,
                    63,
                    0
                ],
                "published": "2024-11-09T11:13:14Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    11,
                    13,
                    14,
                    5,
                    314,
                    0
                ],
                "title": "Escalating LLM-based Code Translation Benchmarking into the Class-level\n  Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Escalating LLM-based Code Translation Benchmarking into the Class-level\n  Era"
                },
                "summary": "In recent years, Large Language Models (LLMs) have dramatically advanced the\nperformance of automated code translation, making their computational accuracy\nscore reach up to over 80% on many previous benchmarks. However, most code\nsamples in these benchmarks are short, standalone, statement/method-level, and\nalgorithmic, which is not aligned with practical coding tasks. Therefore, it is\nstill unknown the actual capability of LLMs in translating code samples written\nfor daily development. To achieve this, we construct a class-level code\ntranslation benchmark, ClassEval-T, and make the first attempt to extensively\nassess recent LLMs' performance on class-level code translation. ClassEval-T is\nextended from ClassEval, a well-known class-level Python code generation\nbenchmark consisting of multiple practical coding topics, such as database\noperation and game design, and diverse contextual dependencies (e.g., fields,\nmethods, and libraries). It cost us 360 person-hours to accomplish the manual\nmigration to Java and C++ with complete code samples and associated test\nsuites. Subsequently, we design three translation strategies (i.e., holistic,\nmin-dependency, and standalone) for class-level code translations and evaluate\neight recent LLMs of commercial, general, and code kinds in diverse families\nand sizes on ClassEval-T. Experimental results demonstrate a remarkable\nperformance drop compared with the most widely studied method-level code\ntranslation benchmark, and obvious discrepancies among LLMs appear, showing the\neffectiveness of ClassEval-T in measuring recent LLMs. Afterwards, we further\ndiscuss the usage scenarios for diverse translation strategies and LLMs'\nability to dependency awareness when translating class samples. Finally, 1,243\nfailure cases made by the best-performing LLM under test are analyzed and\ncategorized in this paper for practical guidance and future enlightenment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have dramatically advanced the\nperformance of automated code translation, making their computational accuracy\nscore reach up to over 80% on many previous benchmarks. However, most code\nsamples in these benchmarks are short, standalone, statement/method-level, and\nalgorithmic, which is not aligned with practical coding tasks. Therefore, it is\nstill unknown the actual capability of LLMs in translating code samples written\nfor daily development. To achieve this, we construct a class-level code\ntranslation benchmark, ClassEval-T, and make the first attempt to extensively\nassess recent LLMs' performance on class-level code translation. ClassEval-T is\nextended from ClassEval, a well-known class-level Python code generation\nbenchmark consisting of multiple practical coding topics, such as database\noperation and game design, and diverse contextual dependencies (e.g., fields,\nmethods, and libraries). It cost us 360 person-hours to accomplish the manual\nmigration to Java and C++ with complete code samples and associated test\nsuites. Subsequently, we design three translation strategies (i.e., holistic,\nmin-dependency, and standalone) for class-level code translations and evaluate\neight recent LLMs of commercial, general, and code kinds in diverse families\nand sizes on ClassEval-T. Experimental results demonstrate a remarkable\nperformance drop compared with the most widely studied method-level code\ntranslation benchmark, and obvious discrepancies among LLMs appear, showing the\neffectiveness of ClassEval-T in measuring recent LLMs. Afterwards, we further\ndiscuss the usage scenarios for diverse translation strategies and LLMs'\nability to dependency awareness when translating class samples. Finally, 1,243\nfailure cases made by the best-performing LLM under test are analyzed and\ncategorized in this paper for practical guidance and future enlightenment."
                },
                "authors": [
                    {
                        "name": "Pengyu Xue"
                    },
                    {
                        "name": "Linhao Wu"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Chengyi Wang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Ruikai Jin"
                    },
                    {
                        "name": "Yifei Pei"
                    },
                    {
                        "name": "Zhaoyan Shen"
                    },
                    {
                        "name": "Xiran Lyu"
                    },
                    {
                        "name": "Jacky Wai Keung"
                    }
                ],
                "author_detail": {
                    "name": "Jacky Wai Keung"
                },
                "author": "Jacky Wai Keung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06145v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06145v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00196v2",
                "updated": "2025-03-04T12:36:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    36,
                    10,
                    1,
                    63,
                    0
                ],
                "published": "2025-01-31T22:26:33Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    22,
                    26,
                    33,
                    4,
                    31,
                    0
                ],
                "title": "DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access\n  Dermatology Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access\n  Dermatology Datasets"
                },
                "summary": "A major barrier to developing vision large language models (LLMs) in\ndermatology is the lack of large image--text pairs dataset. We introduce\nDermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated\nfrom 45,205 images (13,568 clinical and 35,561 dermatoscopic) for\ndermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using\nGemini 2.0, we used clinically related prompts and self-instruct method to\ngenerate diverse and rich synthetic texts. Metadata of the datasets were\nincorporated into the input prompts by targeting to reduce potential\nhallucinations. The resulting dataset builds upon open access dermatological\nimage repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have\npermissive CC-BY-4.0 licenses. We also fine-tuned a preliminary\nLlama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We\nanticipate this dataset to support and accelerate AI research in dermatology.\nData and code underlying this work are accessible at\nhttps://github.com/abdurrahimyilmaz/DermaSynth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major barrier to developing vision large language models (LLMs) in\ndermatology is the lack of large image--text pairs dataset. We introduce\nDermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated\nfrom 45,205 images (13,568 clinical and 35,561 dermatoscopic) for\ndermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using\nGemini 2.0, we used clinically related prompts and self-instruct method to\ngenerate diverse and rich synthetic texts. Metadata of the datasets were\nincorporated into the input prompts by targeting to reduce potential\nhallucinations. The resulting dataset builds upon open access dermatological\nimage repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have\npermissive CC-BY-4.0 licenses. We also fine-tuned a preliminary\nLlama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We\nanticipate this dataset to support and accelerate AI research in dermatology.\nData and code underlying this work are accessible at\nhttps://github.com/abdurrahimyilmaz/DermaSynth."
                },
                "authors": [
                    {
                        "name": "Abdurrahim Yilmaz"
                    },
                    {
                        "name": "Furkan Yuceyalcin"
                    },
                    {
                        "name": "Ece Gokyayla"
                    },
                    {
                        "name": "Donghee Choi"
                    },
                    {
                        "name": "Ozan Erdem"
                    },
                    {
                        "name": "Ali Anil Demircali"
                    },
                    {
                        "name": "Rahmetullah Varol"
                    },
                    {
                        "name": "Ufuk Gorkem Kirabali"
                    },
                    {
                        "name": "Gulsum Gencoglan"
                    },
                    {
                        "name": "Joram M. Posma"
                    },
                    {
                        "name": "Burak Temelkuran"
                    }
                ],
                "author_detail": {
                    "name": "Burak Temelkuran"
                },
                "author": "Burak Temelkuran",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02550v1",
                "updated": "2025-03-04T12:21:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    21,
                    28,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T12:21:28Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    21,
                    28,
                    1,
                    63,
                    0
                ],
                "title": "SpecInF: Exploiting Idle GPU Resources in Distributed DL Training via\n  Speculative Inference Filling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecInF: Exploiting Idle GPU Resources in Distributed DL Training via\n  Speculative Inference Filling"
                },
                "summary": "Deep Learning (DL), especially with Large Language Models (LLMs), brings\nbenefits to various areas. However, DL training systems usually yield prominent\nidling GPU resources due to many factors, such as resource allocation and\ncollective communication. To improve GPU utilization, we present SpecInF, which\nadopts a \\textbf{Spec}ulative \\textbf{In}ference \\textbf{F}illing method to\nexploit idle GPU resources. It collocates each primary training instance with\nadditional inference instances on the same GPU, detects the training bubbles\nand adaptively fills with online or offline inference workloads. Our results\nshow that SpecInF can effectively enhance GPU utilization under mainstream\nparallel training modes, delivering additional up to 14$\\times$ offline\ninference throughputs than TGS and 67\\% reduction in online inference p95\nlatency than MPS, while guaranteeing collocated training throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning (DL), especially with Large Language Models (LLMs), brings\nbenefits to various areas. However, DL training systems usually yield prominent\nidling GPU resources due to many factors, such as resource allocation and\ncollective communication. To improve GPU utilization, we present SpecInF, which\nadopts a \\textbf{Spec}ulative \\textbf{In}ference \\textbf{F}illing method to\nexploit idle GPU resources. It collocates each primary training instance with\nadditional inference instances on the same GPU, detects the training bubbles\nand adaptively fills with online or offline inference workloads. Our results\nshow that SpecInF can effectively enhance GPU utilization under mainstream\nparallel training modes, delivering additional up to 14$\\times$ offline\ninference throughputs than TGS and 67\\% reduction in online inference p95\nlatency than MPS, while guaranteeing collocated training throughput."
                },
                "authors": [
                    {
                        "name": "Cunchi Lv"
                    },
                    {
                        "name": "Xiao Shi"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Wenting Tan"
                    },
                    {
                        "name": "Xiaofang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofang Zhao"
                },
                "author": "Xiaofang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02549v1",
                "updated": "2025-03-04T12:20:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    20,
                    6,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T12:20:06Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    20,
                    6,
                    1,
                    63,
                    0
                ],
                "title": "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation"
                },
                "summary": "The nnU-Net framework has played a crucial role in medical image segmentation\nand has become the gold standard in multitudes of applications targeting\ndifferent diseases, organs, and modalities. However, so far it has been used\nprimarily in a centralized approach where the data collected from hospitals are\nstored in one center and used to train the nnU-Net. This centralized approach\nhas various limitations, such as leakage of sensitive patient information and\nviolation of patient privacy. Federated learning is one of the approaches to\ntrain a segmentation model in a decentralized manner that helps preserve\npatient privacy. In this paper, we propose FednnU-Net, a federated learning\nextension of nnU-Net. We introduce two novel federated learning methods to the\nnnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric\nFederated Averaging (AsymFedAvg) - and experimentally show their consistent\nperformance for breast, cardiac and fetal segmentation using 6 datasets\nrepresenting samples from 18 institutions. Additionally, to further promote\nresearch and deployment of decentralized training in privacy constrained\ninstitutions, we make our plug-n-play framework public. The source-code is\navailable at https://github.com/faildeny/FednnUNet .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nnU-Net framework has played a crucial role in medical image segmentation\nand has become the gold standard in multitudes of applications targeting\ndifferent diseases, organs, and modalities. However, so far it has been used\nprimarily in a centralized approach where the data collected from hospitals are\nstored in one center and used to train the nnU-Net. This centralized approach\nhas various limitations, such as leakage of sensitive patient information and\nviolation of patient privacy. Federated learning is one of the approaches to\ntrain a segmentation model in a decentralized manner that helps preserve\npatient privacy. In this paper, we propose FednnU-Net, a federated learning\nextension of nnU-Net. We introduce two novel federated learning methods to the\nnnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric\nFederated Averaging (AsymFedAvg) - and experimentally show their consistent\nperformance for breast, cardiac and fetal segmentation using 6 datasets\nrepresenting samples from 18 institutions. Additionally, to further promote\nresearch and deployment of decentralized training in privacy constrained\ninstitutions, we make our plug-n-play framework public. The source-code is\navailable at https://github.com/faildeny/FednnUNet ."
                },
                "authors": [
                    {
                        "name": "Grzegorz Skorupko"
                    },
                    {
                        "name": "Fotios Avgoustidis"
                    },
                    {
                        "name": "Carlos Martín-Isla"
                    },
                    {
                        "name": "Lidia Garrucho"
                    },
                    {
                        "name": "Dimitri A. Kessler"
                    },
                    {
                        "name": "Esmeralda Ruiz Pujadas"
                    },
                    {
                        "name": "Oliver Díaz"
                    },
                    {
                        "name": "Maciej Bobowicz"
                    },
                    {
                        "name": "Katarzyna Gwoździewicz"
                    },
                    {
                        "name": "Xavier Bargalló"
                    },
                    {
                        "name": "Paulius Jaruševičius"
                    },
                    {
                        "name": "Kaisar Kushibar"
                    },
                    {
                        "name": "Karim Lekadir"
                    }
                ],
                "author_detail": {
                    "name": "Karim Lekadir"
                },
                "author": "Karim Lekadir",
                "arxiv_comment": "In review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10117v2",
                "updated": "2025-03-04T12:17:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    17,
                    16,
                    1,
                    63,
                    0
                ],
                "published": "2024-03-15T09:06:50Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    9,
                    6,
                    50,
                    4,
                    75,
                    0
                ],
                "title": "Do Visual-Language Grid Maps Capture Latent Semantics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Visual-Language Grid Maps Capture Latent Semantics?"
                },
                "summary": "Visual-language models (VLMs) have recently been introduced in robotic\nmapping using the latent representations, i.e., embeddings, of the VLMs to\nrepresent semantics in the map. They allow moving from a limited set of\nhuman-created labels toward open-vocabulary scene understanding, which is very\nuseful for robots when operating in complex real-world environments and\ninteracting with humans. While there is anecdotal evidence that maps built this\nway support downstream tasks, such as navigation, rigorous analysis of the\nquality of the maps using these embeddings is missing. In this paper, we\npropose a way to analyze the quality of maps created using VLMs. We investigate\ntwo critical properties of map quality: queryability and distinctness. The\nevaluation of queryability addresses the ability to retrieve information from\nthe embeddings. We investigate intra-map distinctness to study the ability of\nthe embeddings to represent abstract semantic classes and inter-map\ndistinctness to evaluate the generalization properties of the representation.\nWe propose metrics to evaluate these properties and evaluate two\nstate-of-the-art mapping methods, VLMaps and OpenScene, using two encoders,\nLSeg and OpenSeg, using real-world data from the Matterport3D data set. Our\nfindings show that while 3D features improve queryability, they are not scale\ninvariant, whereas image-based embeddings generalize to multiple map\nresolutions. This allows the image-based methods to maintain smaller map sizes,\nwhich can be crucial for using these methods in real-world deployments.\nFurthermore, we show that the choice of the encoder has an effect on the\nresults. The results imply that properly thresholding open-vocabulary queries\nis an open problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language models (VLMs) have recently been introduced in robotic\nmapping using the latent representations, i.e., embeddings, of the VLMs to\nrepresent semantics in the map. They allow moving from a limited set of\nhuman-created labels toward open-vocabulary scene understanding, which is very\nuseful for robots when operating in complex real-world environments and\ninteracting with humans. While there is anecdotal evidence that maps built this\nway support downstream tasks, such as navigation, rigorous analysis of the\nquality of the maps using these embeddings is missing. In this paper, we\npropose a way to analyze the quality of maps created using VLMs. We investigate\ntwo critical properties of map quality: queryability and distinctness. The\nevaluation of queryability addresses the ability to retrieve information from\nthe embeddings. We investigate intra-map distinctness to study the ability of\nthe embeddings to represent abstract semantic classes and inter-map\ndistinctness to evaluate the generalization properties of the representation.\nWe propose metrics to evaluate these properties and evaluate two\nstate-of-the-art mapping methods, VLMaps and OpenScene, using two encoders,\nLSeg and OpenSeg, using real-world data from the Matterport3D data set. Our\nfindings show that while 3D features improve queryability, they are not scale\ninvariant, whereas image-based embeddings generalize to multiple map\nresolutions. This allows the image-based methods to maintain smaller map sizes,\nwhich can be crucial for using these methods in real-world deployments.\nFurthermore, we show that the choice of the encoder has an effect on the\nresults. The results imply that properly thresholding open-vocabulary queries\nis an open problem."
                },
                "authors": [
                    {
                        "name": "Matti Pekkanen"
                    },
                    {
                        "name": "Tsvetomila Mihaylova"
                    },
                    {
                        "name": "Francesco Verdoja"
                    },
                    {
                        "name": "Ville Kyrki"
                    }
                ],
                "author_detail": {
                    "name": "Ville Kyrki"
                },
                "author": "Ville Kyrki",
                "arxiv_comment": "Submitted to IEEE-IROS-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02532v1",
                "updated": "2025-03-04T11:56:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    56,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:56:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    56,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Use Me Wisely: AI-Driven Assessment for LLM Prompting Skills Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Use Me Wisely: AI-Driven Assessment for LLM Prompting Skills Development"
                },
                "summary": "The use of large language model (LLM)-powered chatbots, such as ChatGPT, has\nbecome popular across various domains, supporting a range of tasks and\nprocesses. However, due to the intrinsic complexity of LLMs, effective\nprompting is more challenging than it may seem. This highlights the need for\ninnovative educational and support strategies that are both widely accessible\nand seamlessly integrated into task workflows. Yet, LLM prompting is highly\ntask- and domain-dependent, limiting the effectiveness of generic approaches.\nIn this study, we explore whether LLM-based methods can facilitate learning\nassessments by using ad-hoc guidelines and a minimal number of annotated prompt\nsamples. Our framework transforms these guidelines into features that can be\nidentified within learners' prompts. Using these feature descriptions and\nannotated examples, we create few-shot learning detectors. We then evaluate\ndifferent configurations of these detectors, testing three state-of-the-art\nLLMs and ensembles. We run experiments with cross-validation on a sample of\noriginal prompts, as well as tests on prompts collected from task-naive\nlearners. Our results show how LLMs perform on feature detection. Notably, GPT-\n4 demonstrates strong performance on most features, while closely related\nmodels, such as GPT-3 and GPT-3.5 Turbo (Instruct), show inconsistent behaviors\nin feature classification. These differences highlight the need for further\nresearch into how design choices impact feature selection and prompt detection.\nOur findings contribute to the fields of generative AI literacy and\ncomputer-supported learning assessment, offering valuable insights for both\nresearchers and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language model (LLM)-powered chatbots, such as ChatGPT, has\nbecome popular across various domains, supporting a range of tasks and\nprocesses. However, due to the intrinsic complexity of LLMs, effective\nprompting is more challenging than it may seem. This highlights the need for\ninnovative educational and support strategies that are both widely accessible\nand seamlessly integrated into task workflows. Yet, LLM prompting is highly\ntask- and domain-dependent, limiting the effectiveness of generic approaches.\nIn this study, we explore whether LLM-based methods can facilitate learning\nassessments by using ad-hoc guidelines and a minimal number of annotated prompt\nsamples. Our framework transforms these guidelines into features that can be\nidentified within learners' prompts. Using these feature descriptions and\nannotated examples, we create few-shot learning detectors. We then evaluate\ndifferent configurations of these detectors, testing three state-of-the-art\nLLMs and ensembles. We run experiments with cross-validation on a sample of\noriginal prompts, as well as tests on prompts collected from task-naive\nlearners. Our results show how LLMs perform on feature detection. Notably, GPT-\n4 demonstrates strong performance on most features, while closely related\nmodels, such as GPT-3 and GPT-3.5 Turbo (Instruct), show inconsistent behaviors\nin feature classification. These differences highlight the need for further\nresearch into how design choices impact feature selection and prompt detection.\nOur findings contribute to the fields of generative AI literacy and\ncomputer-supported learning assessment, offering valuable insights for both\nresearchers and practitioners."
                },
                "authors": [
                    {
                        "name": "Dimitri Ognibene"
                    },
                    {
                        "name": "Gregor Donabauer"
                    },
                    {
                        "name": "Emily Theophilou"
                    },
                    {
                        "name": "Cansu Koyuturk"
                    },
                    {
                        "name": "Mona Yavari"
                    },
                    {
                        "name": "Sathya Bursic"
                    },
                    {
                        "name": "Alessia Telari"
                    },
                    {
                        "name": "Alessia Testa"
                    },
                    {
                        "name": "Raffaele Boiano"
                    },
                    {
                        "name": "Davide Taibi"
                    },
                    {
                        "name": "Davinia Hernandez-Leo"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    },
                    {
                        "name": "Martin Ruskov"
                    }
                ],
                "author_detail": {
                    "name": "Martin Ruskov"
                },
                "author": "Martin Ruskov",
                "arxiv_comment": "Preprint accepted for Publication in Educational Technology & Society\n  (ET&S)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06860v2",
                "updated": "2025-03-04T11:47:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    47,
                    27,
                    1,
                    63,
                    0
                ],
                "published": "2024-12-09T02:36:38Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    2,
                    36,
                    38,
                    0,
                    344,
                    0
                ],
                "title": "Balancing Efficiency and Effectiveness: An LLM-Infused Approach for\n  Optimized CTR Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Efficiency and Effectiveness: An LLM-Infused Approach for\n  Optimized CTR Prediction"
                },
                "summary": "Click-Through Rate (CTR) prediction is essential in online advertising, where\nsemantic information plays a pivotal role in shaping user decisions and\nenhancing CTR effectiveness. Capturing and modeling deep semantic information,\nsuch as a user's preference for \"H\\\"aagen-Dazs' HEAVEN strawberry light ice\ncream\" due to its health-conscious and premium attributes, is challenging.\nTraditional semantic modeling often overlooks these intricate details at the\nuser and item levels. To bridge this gap, we introduce a novel approach that\nmodels deep semantic information end-to-end, leveraging the comprehensive world\nknowledge capabilities of Large Language Models (LLMs). Our proposed\nLLM-infused CTR prediction framework(Multi-level Deep Semantic Information\nInfused CTR model via Distillation, MSD) is designed to uncover deep semantic\ninsights by utilizing LLMs to extract and distill critical information into a\nsmaller, more efficient model, enabling seamless end-to-end training and\ninference. Importantly, our framework is carefully designed to balance\nefficiency and effectiveness, ensuring that the model not only achieves high\nperformance but also operates with optimal resource utilization. Online A/B\ntests conducted on the Meituan sponsored-search system demonstrate that our\nmethod significantly outperforms baseline models in terms of Cost Per Mile\n(CPM) and CTR, validating its effectiveness, scalability, and balanced approach\nin real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Click-Through Rate (CTR) prediction is essential in online advertising, where\nsemantic information plays a pivotal role in shaping user decisions and\nenhancing CTR effectiveness. Capturing and modeling deep semantic information,\nsuch as a user's preference for \"H\\\"aagen-Dazs' HEAVEN strawberry light ice\ncream\" due to its health-conscious and premium attributes, is challenging.\nTraditional semantic modeling often overlooks these intricate details at the\nuser and item levels. To bridge this gap, we introduce a novel approach that\nmodels deep semantic information end-to-end, leveraging the comprehensive world\nknowledge capabilities of Large Language Models (LLMs). Our proposed\nLLM-infused CTR prediction framework(Multi-level Deep Semantic Information\nInfused CTR model via Distillation, MSD) is designed to uncover deep semantic\ninsights by utilizing LLMs to extract and distill critical information into a\nsmaller, more efficient model, enabling seamless end-to-end training and\ninference. Importantly, our framework is carefully designed to balance\nefficiency and effectiveness, ensuring that the model not only achieves high\nperformance but also operates with optimal resource utilization. Online A/B\ntests conducted on the Meituan sponsored-search system demonstrate that our\nmethod significantly outperforms baseline models in terms of Cost Per Mile\n(CPM) and CTR, validating its effectiveness, scalability, and balanced approach\nin real-world applications."
                },
                "authors": [
                    {
                        "name": "Guoxiao Zhang"
                    },
                    {
                        "name": "Yi Wei"
                    },
                    {
                        "name": "Yadong Zhang"
                    },
                    {
                        "name": "Huajian Feng"
                    },
                    {
                        "name": "Qiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Liu"
                },
                "author": "Qiang Liu",
                "arxiv_comment": "5 pages, 4 figures,4 tables",
                "arxiv_journal_ref": "WWW2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16533v2",
                "updated": "2025-03-04T11:33:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    33,
                    50,
                    1,
                    63,
                    0
                ],
                "published": "2024-05-26T11:40:58Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    11,
                    40,
                    58,
                    6,
                    147,
                    0
                ],
                "title": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool\n  Agents"
                },
                "summary": "Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to extend their utility, enabling them to solve practical\ntasks. Previous methods manually parse tool documentation and create in-context\ndemonstrations, transforming tools into structured formats for LLMs to use in\ntheir step-by-step reasoning. However, this manual process requires domain\nexpertise and struggles to scale to large toolsets. Additionally, these methods\nrely heavily on ad-hoc inference techniques or special tokens to integrate\nfree-form LLM generation with tool-calling actions, limiting the LLM's\nflexibility in handling diverse tool specifications and integrating multiple\ntools.\n  In this work, we propose AutoTools, a framework that enables LLMs to automate\nthe tool-use workflow. Specifically, the LLM automatically transforms tool\ndocumentation into callable functions, verifying syntax and runtime\ncorrectness. Then, the LLM integrates these functions into executable programs\nto solve practical tasks, flexibly grounding tool-use actions into its\nreasoning processes. Extensive experiments on existing and newly collected,\nmore challenging benchmarks illustrate the superiority of our framework.\nInspired by these promising results, we further investigate how to improve the\nexpertise of LLMs, especially open-source LLMs with fewer parameters, within\nAutoTools. Thus, we propose the AutoTools-learning approach, training the LLMs\nwith three learning tasks on 34k instances of high-quality synthetic data,\nincluding documentation understanding, relevance learning, and function\nprogramming. Fine-grained results validate the effectiveness of our overall\ntraining approach and each individual task. Our methods are an important step\ntowards the use of LLMs for solving real-world tasks with external tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to extend their utility, enabling them to solve practical\ntasks. Previous methods manually parse tool documentation and create in-context\ndemonstrations, transforming tools into structured formats for LLMs to use in\ntheir step-by-step reasoning. However, this manual process requires domain\nexpertise and struggles to scale to large toolsets. Additionally, these methods\nrely heavily on ad-hoc inference techniques or special tokens to integrate\nfree-form LLM generation with tool-calling actions, limiting the LLM's\nflexibility in handling diverse tool specifications and integrating multiple\ntools.\n  In this work, we propose AutoTools, a framework that enables LLMs to automate\nthe tool-use workflow. Specifically, the LLM automatically transforms tool\ndocumentation into callable functions, verifying syntax and runtime\ncorrectness. Then, the LLM integrates these functions into executable programs\nto solve practical tasks, flexibly grounding tool-use actions into its\nreasoning processes. Extensive experiments on existing and newly collected,\nmore challenging benchmarks illustrate the superiority of our framework.\nInspired by these promising results, we further investigate how to improve the\nexpertise of LLMs, especially open-source LLMs with fewer parameters, within\nAutoTools. Thus, we propose the AutoTools-learning approach, training the LLMs\nwith three learning tasks on 34k instances of high-quality synthetic data,\nincluding documentation understanding, relevance learning, and function\nprogramming. Fine-grained results validate the effectiveness of our overall\ntraining approach and each individual task. Our methods are an important step\ntowards the use of LLMs for solving real-world tasks with external tools."
                },
                "authors": [
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Lingyong Yan"
                    },
                    {
                        "name": "Yue Feng"
                    },
                    {
                        "name": "Xiuyi Chen"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Zhaochun Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhaochun Ren"
                },
                "author": "Zhaochun Ren",
                "arxiv_comment": "Accepted by WWW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02519v1",
                "updated": "2025-03-04T11:31:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    31,
                    5,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:31:05Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    31,
                    5,
                    1,
                    63,
                    0
                ],
                "title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model\n  Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generator-Assistant Stepwise Rollback Framework for Large Language Model\n  Agent"
                },
                "summary": "Large language model (LLM) agents typically adopt a step-by-step reasoning\nframework, in which they interleave the processes of thinking and acting to\naccomplish the given task. However, this paradigm faces a deep-rooted one-pass\nissue whereby each generated intermediate thought is plugged into the\ntrajectory regardless of its correctness, which can cause irreversible error\npropagation. To address the issue, this paper proposes a novel framework called\nGenerator-Assistant Stepwise Rollback (GA-Rollback) to induce better\ndecision-making for LLM agents. Particularly, GA-Rollback utilizes a generator\nto interact with the environment and an assistant to examine each action\nproduced by the generator, where the assistant triggers a rollback operation\nupon detection of incorrect actions. Moreover, we introduce two additional\nstrategies tailored for the rollback scenario to further improve its\neffectiveness. Extensive experiments show that GA-Rollback achieves significant\nimprovements over several strong baselines on three widely used benchmarks. Our\nanalysis further reveals that GA-Rollback can function as a robust\nplug-and-play module, integrating seamlessly with other methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents typically adopt a step-by-step reasoning\nframework, in which they interleave the processes of thinking and acting to\naccomplish the given task. However, this paradigm faces a deep-rooted one-pass\nissue whereby each generated intermediate thought is plugged into the\ntrajectory regardless of its correctness, which can cause irreversible error\npropagation. To address the issue, this paper proposes a novel framework called\nGenerator-Assistant Stepwise Rollback (GA-Rollback) to induce better\ndecision-making for LLM agents. Particularly, GA-Rollback utilizes a generator\nto interact with the environment and an assistant to examine each action\nproduced by the generator, where the assistant triggers a rollback operation\nupon detection of incorrect actions. Moreover, we introduce two additional\nstrategies tailored for the rollback scenario to further improve its\neffectiveness. Extensive experiments show that GA-Rollback achieves significant\nimprovements over several strong baselines on three widely used benchmarks. Our\nanalysis further reveals that GA-Rollback can function as a robust\nplug-and-play module, integrating seamlessly with other methods."
                },
                "authors": [
                    {
                        "name": "Xingzuo Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Yunfei Long"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01747v2",
                "updated": "2025-03-04T11:30:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    30,
                    30,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-03T17:15:17Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    17,
                    15,
                    17,
                    0,
                    62,
                    0
                ],
                "title": "Position: Don't use the CLT in LLM evals with fewer than a few hundred\n  datapoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Don't use the CLT in LLM evals with fewer than a few hundred\n  datapoints"
                },
                "summary": "Rigorous statistical evaluations of large language models (LLMs), including\nvalid error bars and significance testing, are essential for meaningful and\nreliable performance assessment. Currently, when such statistical measures are\nreported, they typically rely on the Central Limit Theorem (CLT). In this\nposition paper, we argue that while CLT-based methods for uncertainty\nquantification are appropriate when benchmarks consist of thousands of\nexamples, they fail to provide adequate uncertainty estimates for LLM\nevaluations that rely on smaller, highly specialized benchmarks. In these\nsmall-data settings, we demonstrate that CLT-based methods perform very poorly,\nusually dramatically underestimating uncertainty (i.e. producing error bars\nthat are too small). We give recommendations for alternative frequentist and\nBayesian methods that are both easy to implement and more appropriate in these\nincreasingly common scenarios. We provide a simple Python library for these\nBayesian methods at https://github.com/sambowyer/bayes_evals .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous statistical evaluations of large language models (LLMs), including\nvalid error bars and significance testing, are essential for meaningful and\nreliable performance assessment. Currently, when such statistical measures are\nreported, they typically rely on the Central Limit Theorem (CLT). In this\nposition paper, we argue that while CLT-based methods for uncertainty\nquantification are appropriate when benchmarks consist of thousands of\nexamples, they fail to provide adequate uncertainty estimates for LLM\nevaluations that rely on smaller, highly specialized benchmarks. In these\nsmall-data settings, we demonstrate that CLT-based methods perform very poorly,\nusually dramatically underestimating uncertainty (i.e. producing error bars\nthat are too small). We give recommendations for alternative frequentist and\nBayesian methods that are both easy to implement and more appropriate in these\nincreasingly common scenarios. We provide a simple Python library for these\nBayesian methods at https://github.com/sambowyer/bayes_evals ."
                },
                "authors": [
                    {
                        "name": "Sam Bowyer"
                    },
                    {
                        "name": "Laurence Aitchison"
                    },
                    {
                        "name": "Desi R. Ivanova"
                    }
                ],
                "author_detail": {
                    "name": "Desi R. Ivanova"
                },
                "author": "Desi R. Ivanova",
                "arxiv_comment": "36 pages, 37 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02511v1",
                "updated": "2025-03-04T11:20:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    20,
                    10,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:20:10Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    20,
                    10,
                    1,
                    63,
                    0
                ],
                "title": "TeTRA-VPR: A Ternary Transformer Approach for Compact Visual Place\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeTRA-VPR: A Ternary Transformer Approach for Compact Visual Place\n  Recognition"
                },
                "summary": "Visual Place Recognition (VPR) localizes a query image by matching it against\na database of geo-tagged reference images, making it essential for navigation\nand mapping in robotics. Although Vision Transformer (ViT) solutions deliver\nhigh accuracy, their large models often exceed the memory and compute budgets\nof resource-constrained platforms such as drones and mobile robots. To address\nthis issue, we propose TeTRA, a ternary transformer approach that progressively\nquantizes the ViT backbone to 2-bit precision and binarizes its final embedding\nlayer, offering substantial reductions in model size and latency. A carefully\ndesigned progressive distillation strategy preserves the representational power\nof a full-precision teacher, allowing TeTRA to retain or even surpass the\naccuracy of uncompressed convolutional counterparts, despite using fewer\nresources. Experiments on standard VPR benchmarks demonstrate that TeTRA\nreduces memory consumption by up to 69% compared to efficient baselines, while\nlowering inference latency by 35%, with either no loss or a slight improvement\nin recall@1. These gains enable high-accuracy VPR on power-constrained,\nmemory-limited robotic platforms, making TeTRA an appealing solution for\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Place Recognition (VPR) localizes a query image by matching it against\na database of geo-tagged reference images, making it essential for navigation\nand mapping in robotics. Although Vision Transformer (ViT) solutions deliver\nhigh accuracy, their large models often exceed the memory and compute budgets\nof resource-constrained platforms such as drones and mobile robots. To address\nthis issue, we propose TeTRA, a ternary transformer approach that progressively\nquantizes the ViT backbone to 2-bit precision and binarizes its final embedding\nlayer, offering substantial reductions in model size and latency. A carefully\ndesigned progressive distillation strategy preserves the representational power\nof a full-precision teacher, allowing TeTRA to retain or even surpass the\naccuracy of uncompressed convolutional counterparts, despite using fewer\nresources. Experiments on standard VPR benchmarks demonstrate that TeTRA\nreduces memory consumption by up to 69% compared to efficient baselines, while\nlowering inference latency by 35%, with either no loss or a slight improvement\nin recall@1. These gains enable high-accuracy VPR on power-constrained,\nmemory-limited robotic platforms, making TeTRA an appealing solution for\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Oliver Grainge"
                    },
                    {
                        "name": "Michael Milford"
                    },
                    {
                        "name": "Indu Bodala"
                    },
                    {
                        "name": "Sarvapali D. Ramchurn"
                    },
                    {
                        "name": "Shoaib Ehsan"
                    }
                ],
                "author_detail": {
                    "name": "Shoaib Ehsan"
                },
                "author": "Shoaib Ehsan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02502v1",
                "updated": "2025-03-04T11:10:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    10,
                    13,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:10:13Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    10,
                    13,
                    1,
                    63,
                    0
                ],
                "title": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LADM: Long-context Training Data Selection with Attention-based\n  Dependency Measurement for LLMs"
                },
                "summary": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training."
                },
                "authors": [
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Junhong Wu"
                    },
                    {
                        "name": "Yangyifan Xu"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Submitted to ACL ARR 2024 December",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02498v1",
                "updated": "2025-03-04T11:04:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    4,
                    36,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:04:36Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    4,
                    36,
                    1,
                    63,
                    0
                ],
                "title": "A Systematic Literature Review on Safety of the Intended Functionality\n  for Automated Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Literature Review on Safety of the Intended Functionality\n  for Automated Driving Systems"
                },
                "summary": "In the automobile industry, ensuring the safety of automated vehicles\nequipped with the Automated Driving System (ADS) is becoming a significant\nfocus due to the increasing development and deployment of automated driving.\nAutomated driving depends on sensing both the external and internal\nenvironments of a vehicle, utilizing perception sensors and algorithms, and\nElectrical/Electronic (E/E) systems for situational awareness and response. ISO\n21448 is the standard for Safety of the Intended Functionality (SOTIF) that\naims to ensure that the ADS operate safely within their intended functionality.\nSOTIF focuses on preventing or mitigating potential hazards that may arise from\nthe limitations or failures of the ADS, including hazards due to\ninsufficiencies of specification, or performance insufficiencies, as well as\nforeseeable misuse of the intended functionality. However, the challenge lies\nin ensuring the safety of vehicles despite the limited availability of\nextensive and systematic literature on SOTIF. To address this challenge, a\nSystematic Literature Review (SLR) on SOTIF for the ADS is performed following\nthe Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)\nguidelines. The objective is to methodically gather and analyze the existing\nliterature on SOTIF. The major contributions of this paper are: (i) presenting\na summary of the literature by synthesizing and organizing the collective\nfindings, methodologies, and insights into distinct thematic groups, and (ii)\nsummarizing and categorizing the acknowledged limitations based on data\nextracted from an SLR of 51 research papers published between 2018 and 2023.\nFurthermore, research gaps are determined, and future research directions are\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the automobile industry, ensuring the safety of automated vehicles\nequipped with the Automated Driving System (ADS) is becoming a significant\nfocus due to the increasing development and deployment of automated driving.\nAutomated driving depends on sensing both the external and internal\nenvironments of a vehicle, utilizing perception sensors and algorithms, and\nElectrical/Electronic (E/E) systems for situational awareness and response. ISO\n21448 is the standard for Safety of the Intended Functionality (SOTIF) that\naims to ensure that the ADS operate safely within their intended functionality.\nSOTIF focuses on preventing or mitigating potential hazards that may arise from\nthe limitations or failures of the ADS, including hazards due to\ninsufficiencies of specification, or performance insufficiencies, as well as\nforeseeable misuse of the intended functionality. However, the challenge lies\nin ensuring the safety of vehicles despite the limited availability of\nextensive and systematic literature on SOTIF. To address this challenge, a\nSystematic Literature Review (SLR) on SOTIF for the ADS is performed following\nthe Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)\nguidelines. The objective is to methodically gather and analyze the existing\nliterature on SOTIF. The major contributions of this paper are: (i) presenting\na summary of the literature by synthesizing and organizing the collective\nfindings, methodologies, and insights into distinct thematic groups, and (ii)\nsummarizing and categorizing the acknowledged limitations based on data\nextracted from an SLR of 51 research papers published between 2018 and 2023.\nFurthermore, research gaps are determined, and future research directions are\nproposed."
                },
                "authors": [
                    {
                        "name": "Milin Patel"
                    },
                    {
                        "name": "Rolf Jung"
                    },
                    {
                        "name": "Marzana Khatun"
                    }
                ],
                "author_detail": {
                    "name": "Marzana Khatun"
                },
                "author": "Marzana Khatun",
                "arxiv_comment": "Scheduled to be published in SAE journal as technical paper as a part\n  of non-technical event and will be available as open access in 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02497v1",
                "updated": "2025-03-04T11:04:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    4,
                    35,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:04:35Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    4,
                    35,
                    1,
                    63,
                    0
                ],
                "title": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel\n  PennyLane-Centric Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel\n  PennyLane-Centric Dataset"
                },
                "summary": "Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning. Their\npotential in aiding quantum software development remains underexplored,\nparticularly for the PennyLane framework-a leading platform for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific code samples of\nquantum circuits and their contextual descriptions, specifically curated to\ntrain/fine-tune LLM-based quantum code assistance. Our key contributions are\nthreefold: (1) the automatic creation and open-source release of a\ncomprehensive PennyLane dataset leveraging quantum computing textbooks,\nofficial documentation, and open-source repositories; (2) the development of a\nsystematic methodology for data refinement, annotation, and formatting to\noptimize LLM training efficiency; and (3) a thorough evaluation, based on a\nRetrieval-Augmented Generation (RAG) framework, demonstrating the effectiveness\nof our dataset in streamlining PennyLane code generation and improving quantum\ndevelopment workflows. Compared to existing efforts that predominantly focus on\nQiskit, our dataset significantly broadens the spectrum of quantum frameworks\ncovered in AI-driven code assistance. By bridging this gap and providing\nreproducible dataset-creation methodologies, we aim to advance the field of\nAI-assisted quantum programming, making quantum computing more accessible to\nboth newcomers and experienced developers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning. Their\npotential in aiding quantum software development remains underexplored,\nparticularly for the PennyLane framework-a leading platform for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific code samples of\nquantum circuits and their contextual descriptions, specifically curated to\ntrain/fine-tune LLM-based quantum code assistance. Our key contributions are\nthreefold: (1) the automatic creation and open-source release of a\ncomprehensive PennyLane dataset leveraging quantum computing textbooks,\nofficial documentation, and open-source repositories; (2) the development of a\nsystematic methodology for data refinement, annotation, and formatting to\noptimize LLM training efficiency; and (3) a thorough evaluation, based on a\nRetrieval-Augmented Generation (RAG) framework, demonstrating the effectiveness\nof our dataset in streamlining PennyLane code generation and improving quantum\ndevelopment workflows. Compared to existing efforts that predominantly focus on\nQiskit, our dataset significantly broadens the spectrum of quantum frameworks\ncovered in AI-driven code assistance. By bridging this gap and providing\nreproducible dataset-creation methodologies, we aim to advance the field of\nAI-assisted quantum programming, making quantum computing more accessible to\nboth newcomers and experienced developers."
                },
                "authors": [
                    {
                        "name": "Haider Asif"
                    },
                    {
                        "name": "Abdul Basit"
                    },
                    {
                        "name": "Nouhaila Innan"
                    },
                    {
                        "name": "Muhammad Kashif"
                    },
                    {
                        "name": "Alberto Marchisio"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_comment": "10 pages, 8 figures, 6 tables, submitted for review under IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02476v1",
                "updated": "2025-03-04T10:39:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    39,
                    42,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T10:39:42Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    39,
                    42,
                    1,
                    63,
                    0
                ],
                "title": "BioD2C: A Dual-level Semantic Consistency Constraint Framework for\n  Biomedical VQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioD2C: A Dual-level Semantic Consistency Constraint Framework for\n  Biomedical VQA"
                },
                "summary": "Biomedical visual question answering (VQA) has been widely studied and has\ndemonstrated significant application value and potential in fields such as\nassistive medical diagnosis. Despite their success, current biomedical VQA\nmodels perform multimodal information interaction only at the model level\nwithin large language models (LLMs), leading to suboptimal multimodal semantic\nalignment when dealing with complex tasks. To address this issue, we propose\nBioD2C: a novel Dual-level Semantic Consistency Constraint Framework for\nBiomedical VQA, which achieves dual-level semantic interaction alignment at\nboth the model and feature levels, enabling the model to adaptively learn\nvisual features based on the question. Specifically, we firstly integrate\ntextual features into visual features via an image-text fusion mechanism as\nfeature-level semantic interaction, obtaining visual features conditioned on\nthe given text; and then introduce a text-queue-based cross-modal soft semantic\nloss function to further align the image semantics with the question semantics.\nSpecifically, in this work, we establish a new dataset, BioVGQ, to address\ninherent biases in prior datasets by filtering manually-altered images and\naligning question-answer pairs with multimodal context, and train our model on\nthis dataset. Extensive experimental results demonstrate that BioD2C achieves\nstate-of-the-art (SOTA) performance across multiple downstream datasets,\nshowcasing its robustness, generalizability, and potential to advance\nbiomedical VQA research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical visual question answering (VQA) has been widely studied and has\ndemonstrated significant application value and potential in fields such as\nassistive medical diagnosis. Despite their success, current biomedical VQA\nmodels perform multimodal information interaction only at the model level\nwithin large language models (LLMs), leading to suboptimal multimodal semantic\nalignment when dealing with complex tasks. To address this issue, we propose\nBioD2C: a novel Dual-level Semantic Consistency Constraint Framework for\nBiomedical VQA, which achieves dual-level semantic interaction alignment at\nboth the model and feature levels, enabling the model to adaptively learn\nvisual features based on the question. Specifically, we firstly integrate\ntextual features into visual features via an image-text fusion mechanism as\nfeature-level semantic interaction, obtaining visual features conditioned on\nthe given text; and then introduce a text-queue-based cross-modal soft semantic\nloss function to further align the image semantics with the question semantics.\nSpecifically, in this work, we establish a new dataset, BioVGQ, to address\ninherent biases in prior datasets by filtering manually-altered images and\naligning question-answer pairs with multimodal context, and train our model on\nthis dataset. Extensive experimental results demonstrate that BioD2C achieves\nstate-of-the-art (SOTA) performance across multiple downstream datasets,\nshowcasing its robustness, generalizability, and potential to advance\nbiomedical VQA research."
                },
                "authors": [
                    {
                        "name": "Zhengyang Ji"
                    },
                    {
                        "name": "Shang Gao"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Yifan Jia"
                    },
                    {
                        "name": "Yutao Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Yue"
                },
                "author": "Yutao Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10661v2",
                "updated": "2025-03-04T10:34:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    34,
                    38,
                    1,
                    63,
                    0
                ],
                "published": "2024-10-14T16:09:03Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    9,
                    3,
                    0,
                    288,
                    0
                ],
                "title": "Energetic Analysis of Emerging Quantum Communication Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energetic Analysis of Emerging Quantum Communication Protocols"
                },
                "summary": "With the rapid development and early industrialization of quantum\ntechnologies, it is of great interest to analyze their overall energy\nconsumption before planning for their wide-scale deployments. The evaluation of\nthe total energy requirements of quantum networks is a challenging task:\ndifferent networks require very disparate techniques to create, distribute,\nmanipulate, detect, and process quantum signals. This paper aims to lay the\nfoundations of a framework to model the energy requirements of different\nquantum technologies and protocols applied to near-term quantum networks.\nDifferent figures of merit are discussed and a benchmark on the energy\nconsumption of bipartite and multipartite network protocols is presented. An\nopen-source software to estimate the energy consumption of photonic setups is\nalso provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development and early industrialization of quantum\ntechnologies, it is of great interest to analyze their overall energy\nconsumption before planning for their wide-scale deployments. The evaluation of\nthe total energy requirements of quantum networks is a challenging task:\ndifferent networks require very disparate techniques to create, distribute,\nmanipulate, detect, and process quantum signals. This paper aims to lay the\nfoundations of a framework to model the energy requirements of different\nquantum technologies and protocols applied to near-term quantum networks.\nDifferent figures of merit are discussed and a benchmark on the energy\nconsumption of bipartite and multipartite network protocols is presented. An\nopen-source software to estimate the energy consumption of photonic setups is\nalso provided."
                },
                "authors": [
                    {
                        "name": "Raja Yehia"
                    },
                    {
                        "name": "Yoann Piétri"
                    },
                    {
                        "name": "Carlos Pascual-García"
                    },
                    {
                        "name": "Pascal Lefebvre"
                    },
                    {
                        "name": "Federico Centrone"
                    }
                ],
                "author_detail": {
                    "name": "Federico Centrone"
                },
                "author": "Federico Centrone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02465v1",
                "updated": "2025-03-04T10:21:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    21,
                    58,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T10:21:58Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    21,
                    58,
                    1,
                    63,
                    0
                ],
                "title": "UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search\n  and Rescue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search\n  and Rescue"
                },
                "summary": "Emergency search and rescue (SAR) operations often require rapid and precise\ntarget identification in complex environments where traditional manual drone\ncontrol is inefficient. In order to address these scenarios, a rapid SAR\nsystem, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this\nresearch. This system consists of two aspects: 1) A multimodal system which\nharnesses the power of Visual Language Model (VLM) and the natural language\nprocessing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A\nnon-linearmodel predictive control (NMPC) with built-in obstacle avoidance for\nrapid response by a drone to fly according to the output of the multimodal\nsystem. This work aims at improving response times in emergency SAR operations\nby providing a more intuitive and natural approach to the operator to plan the\nSAR mission while allowing the drone to carry out that mission in a rapid and\nsafe manner. When tested, our approach was faster on an average by 33.75% when\ncompared with an off-the-shelf autopilot and 54.6% when compared with a human\npilot. Video of UAV-VLRR: https://youtu.be/KJqQGKKt1xY",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency search and rescue (SAR) operations often require rapid and precise\ntarget identification in complex environments where traditional manual drone\ncontrol is inefficient. In order to address these scenarios, a rapid SAR\nsystem, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this\nresearch. This system consists of two aspects: 1) A multimodal system which\nharnesses the power of Visual Language Model (VLM) and the natural language\nprocessing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A\nnon-linearmodel predictive control (NMPC) with built-in obstacle avoidance for\nrapid response by a drone to fly according to the output of the multimodal\nsystem. This work aims at improving response times in emergency SAR operations\nby providing a more intuitive and natural approach to the operator to plan the\nSAR mission while allowing the drone to carry out that mission in a rapid and\nsafe manner. When tested, our approach was faster on an average by 33.75% when\ncompared with an off-the-shelf autopilot and 54.6% when compared with a human\npilot. Video of UAV-VLRR: https://youtu.be/KJqQGKKt1xY"
                },
                "authors": [
                    {
                        "name": "Yasheerah Yaqoot"
                    },
                    {
                        "name": "Muhammad Ahsan Mustafa"
                    },
                    {
                        "name": "Oleg Sautenkov"
                    },
                    {
                        "name": "Dzmitry Tsetserukou"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Tsetserukou"
                },
                "author": "Dzmitry Tsetserukou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02463v1",
                "updated": "2025-03-04T10:17:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    17,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T10:17:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    17,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate\n  Mutually via Selective Rationale Optimisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate\n  Mutually via Selective Rationale Optimisation"
                },
                "summary": "Very large language models (LLMs) such as GPT-4 have shown the ability to\nhandle complex tasks by generating and self-refining step-by-step rationales.\nSmaller language models (SLMs), typically with < 13B parameters, have been\nimproved by using the data generated from very-large LMs through knowledge\ndistillation. However, various practical constraints such as API costs,\ncopyright, legal and ethical policies restrict using large (often opaque)\nmodels to train smaller models for commercial use. Limited success has been\nachieved at improving the ability of an SLM to explore the space of possible\nrationales and evaluate them by itself through self-deliberation. To address\nthis, we propose COALITION, a trainable framework that facilitates interaction\nbetween two variants of the same SLM and trains them to generate and refine\nrationales optimized for the end-task. The variants exhibit different behaviors\nto produce a set of diverse candidate rationales during the generation and\nrefinement steps. The model is then trained via Selective Rationale\nOptimization (SRO) to prefer generating rationale candidates that maximize the\nlikelihood of producing the ground-truth answer. During inference, COALITION\nemploys a controller to select the suitable variant for generating and refining\nthe rationales. On five different datasets covering mathematical problems,\ncommonsense reasoning, and natural language inference, COALITION outperforms\nseveral baselines by up to 5%. Our ablation studies reveal that\ncross-communication between the two variants performs better than using the\nsingle model to self-refine the rationales. We also demonstrate the\napplicability of COALITION for LMs of varying scales (4B to 14B parameters) and\nmodel families (Mistral, Llama, Qwen, Phi). We release the code for this work\nat https://github.com/Sohanpatnaik106/coalition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Very large language models (LLMs) such as GPT-4 have shown the ability to\nhandle complex tasks by generating and self-refining step-by-step rationales.\nSmaller language models (SLMs), typically with < 13B parameters, have been\nimproved by using the data generated from very-large LMs through knowledge\ndistillation. However, various practical constraints such as API costs,\ncopyright, legal and ethical policies restrict using large (often opaque)\nmodels to train smaller models for commercial use. Limited success has been\nachieved at improving the ability of an SLM to explore the space of possible\nrationales and evaluate them by itself through self-deliberation. To address\nthis, we propose COALITION, a trainable framework that facilitates interaction\nbetween two variants of the same SLM and trains them to generate and refine\nrationales optimized for the end-task. The variants exhibit different behaviors\nto produce a set of diverse candidate rationales during the generation and\nrefinement steps. The model is then trained via Selective Rationale\nOptimization (SRO) to prefer generating rationale candidates that maximize the\nlikelihood of producing the ground-truth answer. During inference, COALITION\nemploys a controller to select the suitable variant for generating and refining\nthe rationales. On five different datasets covering mathematical problems,\ncommonsense reasoning, and natural language inference, COALITION outperforms\nseveral baselines by up to 5%. Our ablation studies reveal that\ncross-communication between the two variants performs better than using the\nsingle model to self-refine the rationales. We also demonstrate the\napplicability of COALITION for LMs of varying scales (4B to 14B parameters) and\nmodel families (Mistral, Llama, Qwen, Phi). We release the code for this work\nat https://github.com/Sohanpatnaik106/coalition."
                },
                "authors": [
                    {
                        "name": "Sohan Patnaik"
                    },
                    {
                        "name": "Milan Aggarwal"
                    },
                    {
                        "name": "Sumit Bhatia"
                    },
                    {
                        "name": "Balaji Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Balaji Krishnamurthy"
                },
                "author": "Balaji Krishnamurthy",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02457v1",
                "updated": "2025-03-04T10:06:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    6,
                    41,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T10:06:41Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    10,
                    6,
                    41,
                    1,
                    63,
                    0
                ],
                "title": "Don't Get Too Excited -- Eliciting Emotions in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Get Too Excited -- Eliciting Emotions in LLMs"
                },
                "summary": "This paper investigates the challenges of affect control in large language\nmodels (LLMs), focusing on their ability to express appropriate emotional\nstates during extended dialogues. We evaluated state-of-the-art open-weight\nLLMs to assess their affective expressive range in terms of arousal and\nvalence. Our study employs a novel methodology combining LLM-based sentiment\nanalysis with multiturn dialogue simulations between LLMs. We quantify the\nmodels' capacity to express a wide spectrum of emotions and how they fluctuate\nduring interactions. Our findings reveal significant variations among LLMs in\ntheir ability to maintain consistent affect, with some models demonstrating\nmore stable emotional trajectories than others. Furthermore, we identify key\nchallenges in affect control, including difficulties in producing and\nmaintaining extreme emotional states and limitations in adapting affect to\nchanging conversational contexts. These findings have important implications\nfor the development of more emotionally intelligent AI systems and highlight\nthe need for improved affect modelling in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the challenges of affect control in large language\nmodels (LLMs), focusing on their ability to express appropriate emotional\nstates during extended dialogues. We evaluated state-of-the-art open-weight\nLLMs to assess their affective expressive range in terms of arousal and\nvalence. Our study employs a novel methodology combining LLM-based sentiment\nanalysis with multiturn dialogue simulations between LLMs. We quantify the\nmodels' capacity to express a wide spectrum of emotions and how they fluctuate\nduring interactions. Our findings reveal significant variations among LLMs in\ntheir ability to maintain consistent affect, with some models demonstrating\nmore stable emotional trajectories than others. Furthermore, we identify key\nchallenges in affect control, including difficulties in producing and\nmaintaining extreme emotional states and limitations in adapting affect to\nchanging conversational contexts. These findings have important implications\nfor the development of more emotionally intelligent AI systems and highlight\nthe need for improved affect modelling in LLMs."
                },
                "authors": [
                    {
                        "name": "Gino Franco Fazzi"
                    },
                    {
                        "name": "Julie Skoven Hinge"
                    },
                    {
                        "name": "Stefan Heinrich"
                    },
                    {
                        "name": "Paolo Burelli"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Burelli"
                },
                "author": "Paolo Burelli",
                "arxiv_doi": "10.1145/3706599.3720191",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706599.3720191",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2110.02784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2110.02784v2",
                "updated": "2025-03-04T09:54:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    54,
                    34,
                    1,
                    63,
                    0
                ],
                "published": "2021-10-06T14:05:26Z",
                "published_parsed": [
                    2021,
                    10,
                    6,
                    14,
                    5,
                    26,
                    2,
                    279,
                    0
                ],
                "title": "Scalable Multi-Agent Reinforcement Learning for Residential Load\n  Scheduling under Data Governance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Multi-Agent Reinforcement Learning for Residential Load\n  Scheduling under Data Governance"
                },
                "summary": "As a data-driven approach, multi-agent reinforcement learning (MARL) has made\nremarkable advances in solving cooperative residential load scheduling\nproblems. However, centralized training, the most common paradigm for MARL,\nlimits large-scale deployment in communication-constrained cloud-edge\nenvironments. As a remedy, distributed training shows unparalleled advantages\nin real-world applications but still faces challenge with system scalability,\ne.g., the high cost of communication overhead during coordinating individual\nagents, and needs to comply with data governance in terms of privacy. In this\nwork, we propose a novel MARL solution to address these two practical issues.\nOur proposed approach is based on actor-critic methods, where the global critic\nis a learned function of individual critics computed solely based on local\nobservations of households. This scheme preserves household privacy completely\nand significantly reduces communication cost. Simulation experiments\ndemonstrate that the proposed framework achieves comparable performance to the\nstate-of-the-art actor-critic framework without data governance and\ncommunication constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a data-driven approach, multi-agent reinforcement learning (MARL) has made\nremarkable advances in solving cooperative residential load scheduling\nproblems. However, centralized training, the most common paradigm for MARL,\nlimits large-scale deployment in communication-constrained cloud-edge\nenvironments. As a remedy, distributed training shows unparalleled advantages\nin real-world applications but still faces challenge with system scalability,\ne.g., the high cost of communication overhead during coordinating individual\nagents, and needs to comply with data governance in terms of privacy. In this\nwork, we propose a novel MARL solution to address these two practical issues.\nOur proposed approach is based on actor-critic methods, where the global critic\nis a learned function of individual critics computed solely based on local\nobservations of households. This scheme preserves household privacy completely\nand significantly reduces communication cost. Simulation experiments\ndemonstrate that the proposed framework achieves comparable performance to the\nstate-of-the-art actor-critic framework without data governance and\ncommunication constraints."
                },
                "authors": [
                    {
                        "name": "Zhaoming Qin"
                    },
                    {
                        "name": "Nanqing Dong"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Zhefan Wang"
                    },
                    {
                        "name": "Junwei Cao"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Cao"
                },
                "author": "Junwei Cao",
                "arxiv_doi": "10.1109/TICPS.2024.3501278",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TICPS.2024.3501278",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2110.02784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2110.02784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02450v1",
                "updated": "2025-03-04T09:53:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    53,
                    26,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T09:53:26Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    53,
                    26,
                    1,
                    63,
                    0
                ],
                "title": "Measuring What Makes You Unique: Difference-Aware User Modeling for\n  Enhancing LLM Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring What Makes You Unique: Difference-Aware User Modeling for\n  Enhancing LLM Personalization"
                },
                "summary": "Personalizing Large Language Models (LLMs) has become a critical step in\nfacilitating their widespread application to enhance individual life\nexperiences. In pursuit of personalization, distilling key preference\ninformation from an individual's historical data as instructional preference\ncontext to customize LLM generation has emerged as a promising direction.\nHowever, these methods face a fundamental limitation by overlooking the\ninter-user comparative analysis, which is essential for identifying the\ninter-user differences that truly shape preferences. To address this\nlimitation, we propose Difference-aware Personalization Learning (DPL), a novel\napproach that emphasizes extracting inter-user differences to enhance LLM\npersonalization. DPL strategically selects representative users for comparison\nand establishes a structured standard to extract meaningful, task-relevant\ndifferences for customizing LLM generation. Extensive experiments on real-world\ndatasets demonstrate that DPL significantly enhances LLM personalization. We\nrelease our code at https://github.com/SnowCharmQ/DPL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalizing Large Language Models (LLMs) has become a critical step in\nfacilitating their widespread application to enhance individual life\nexperiences. In pursuit of personalization, distilling key preference\ninformation from an individual's historical data as instructional preference\ncontext to customize LLM generation has emerged as a promising direction.\nHowever, these methods face a fundamental limitation by overlooking the\ninter-user comparative analysis, which is essential for identifying the\ninter-user differences that truly shape preferences. To address this\nlimitation, we propose Difference-aware Personalization Learning (DPL), a novel\napproach that emphasizes extracting inter-user differences to enhance LLM\npersonalization. DPL strategically selects representative users for comparison\nand establishes a structured standard to extract meaningful, task-relevant\ndifferences for customizing LLM generation. Extensive experiments on real-world\ndatasets demonstrate that DPL significantly enhances LLM personalization. We\nrelease our code at https://github.com/SnowCharmQ/DPL."
                },
                "authors": [
                    {
                        "name": "Yilun Qiu"
                    },
                    {
                        "name": "Xiaoyan Zhao"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yimeng Bai"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02445v1",
                "updated": "2025-03-04T09:40:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    40,
                    0,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T09:40:00Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    40,
                    0,
                    1,
                    63,
                    0
                ],
                "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modelling"
                },
                "summary": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yu-Hao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Ren-He Jiang"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Goran Nenadic"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "Preprint. Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02442v1",
                "updated": "2025-03-04T09:38:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    38,
                    57,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T09:38:57Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    38,
                    57,
                    1,
                    63,
                    0
                ],
                "title": "AILS-NTUA at SemEval-2025 Task 3: Leveraging Large Language Models and\n  Translation Strategies for Multilingual Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AILS-NTUA at SemEval-2025 Task 3: Leveraging Large Language Models and\n  Translation Strategies for Multilingual Hallucination Detection"
                },
                "summary": "Multilingual hallucination detection stands as an underexplored challenge,\nwhich the Mu-SHROOM shared task seeks to address. In this work, we propose an\nefficient, training-free LLM prompting strategy that enhances detection by\ntranslating multilingual text spans into English. Our approach achieves\ncompetitive rankings across multiple languages, securing two first positions in\nlow-resource languages. The consistency of our results highlights the\neffectiveness of our translation strategy for hallucination detection,\ndemonstrating its applicability regardless of the source language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual hallucination detection stands as an underexplored challenge,\nwhich the Mu-SHROOM shared task seeks to address. In this work, we propose an\nefficient, training-free LLM prompting strategy that enhances detection by\ntranslating multilingual text spans into English. Our approach achieves\ncompetitive rankings across multiple languages, securing two first positions in\nlow-resource languages. The consistency of our results highlights the\neffectiveness of our translation strategy for hallucination detection,\ndemonstrating its applicability regardless of the source language."
                },
                "authors": [
                    {
                        "name": "Dimitra Karkani"
                    },
                    {
                        "name": "Maria Lymperaiou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Nikolaos Spanos"
                    },
                    {
                        "name": "Athanasios Voulodimos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Stamou"
                },
                "author": "Giorgos Stamou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02436v1",
                "updated": "2025-03-04T09:22:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    22,
                    59,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T09:22:59Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    22,
                    59,
                    1,
                    63,
                    0
                ],
                "title": "Realizing Quantum Adversarial Defense on a Trapped-ion Quantum Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realizing Quantum Adversarial Defense on a Trapped-ion Quantum Processor"
                },
                "summary": "Classification is a fundamental task in machine learning, typically performed\nusing classical models. Quantum machine learning (QML), however, offers\ndistinct advantages, such as enhanced representational power through\nhigh-dimensional Hilbert spaces and energy-efficient reversible gate\noperations. Despite these theoretical benefits, the robustness of QML\nclassifiers against adversarial attacks and inherent quantum noise remains\nlargely under-explored. In this work, we implement a data re-uploading-based\nquantum classifier on an ion-trap quantum processor using a single qubit to\nassess its resilience under realistic conditions. We introduce a novel\nconvolutional quantum classifier architecture leveraging data re-uploading and\ndemonstrate its superior robustness on the MNIST dataset. Additionally, we\nquantify the effects of polarization noise in a realistic setting, where both\nbit and phase noises are present, further validating the classifier's\nrobustness. Our findings provide insights into the practical security and\nreliability of quantum classifiers, bridging the gap between theoretical\npotential and real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification is a fundamental task in machine learning, typically performed\nusing classical models. Quantum machine learning (QML), however, offers\ndistinct advantages, such as enhanced representational power through\nhigh-dimensional Hilbert spaces and energy-efficient reversible gate\noperations. Despite these theoretical benefits, the robustness of QML\nclassifiers against adversarial attacks and inherent quantum noise remains\nlargely under-explored. In this work, we implement a data re-uploading-based\nquantum classifier on an ion-trap quantum processor using a single qubit to\nassess its resilience under realistic conditions. We introduce a novel\nconvolutional quantum classifier architecture leveraging data re-uploading and\ndemonstrate its superior robustness on the MNIST dataset. Additionally, we\nquantify the effects of polarization noise in a realistic setting, where both\nbit and phase noises are present, further validating the classifier's\nrobustness. Our findings provide insights into the practical security and\nreliability of quantum classifiers, bridging the gap between theoretical\npotential and real-world deployment."
                },
                "authors": [
                    {
                        "name": "Alex Jin"
                    },
                    {
                        "name": "Tarun Dutta"
                    },
                    {
                        "name": "Anh Tu Ngo"
                    },
                    {
                        "name": "Anupam Chattopadhyay"
                    },
                    {
                        "name": "Manas Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Manas Mukherjee"
                },
                "author": "Manas Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02435v1",
                "updated": "2025-03-04T09:22:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    22,
                    50,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T09:22:50Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    22,
                    50,
                    1,
                    63,
                    0
                ],
                "title": "NLI4DB: A Systematic Review of Natural Language Interfaces for Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLI4DB: A Systematic Review of Natural Language Interfaces for Databases"
                },
                "summary": "As the demand for querying databases in all areas of life continues to grow,\nresearchers have devoted significant attention to the natural language\ninterface for databases (NLIDB). This paper presents a comprehensive survey of\nrecently proposed NLIDBs. We begin with a brief introduction to natural\nlanguage processing techniques, executable database languages and the\nintermediate representation between natural language and executable language,\nand then provide an overview of the translation process from natural language\nto executable database language. The translation process is divided into three\nstages: (i) natural language preprocessing, (ii) natural language\nunderstanding, and (iii) natural language translation. Traditional and\ndata-driven methods are utilized in the preprocessing stage. Traditional\napproaches rely on predefined rules and grammars, and involve techniques such\nas regular expressions, dependency parsing and named entity recognition.\nData-driven approaches depend on large-scale data and machine learning models,\nusing techniques including word embedding and pattern linking. Natural language\nunderstanding methods are classified into three categories: (i) rule-based,\n(ii) machine learning-based, and (iii) hybrid. We then describe a general\nconstruction process for executable languages over relational and\nspatio-temporal databases. Subsequently, common benchmarks and evaluation\nmetrics for transforming natural language into executable language are\npresented, and methods for generating new benchmarks are explored. Finally, we\nsummarize the classification, development, and enhancement of NLIDB systems,\nand discuss deep language understanding and database interaction techniques\nrelated to NLIDB, including (i) using LLM for Text2SQL tasks, (ii) generating\nnatural language interpretations from SQL, and (iii) transforming speech\nqueries into SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for querying databases in all areas of life continues to grow,\nresearchers have devoted significant attention to the natural language\ninterface for databases (NLIDB). This paper presents a comprehensive survey of\nrecently proposed NLIDBs. We begin with a brief introduction to natural\nlanguage processing techniques, executable database languages and the\nintermediate representation between natural language and executable language,\nand then provide an overview of the translation process from natural language\nto executable database language. The translation process is divided into three\nstages: (i) natural language preprocessing, (ii) natural language\nunderstanding, and (iii) natural language translation. Traditional and\ndata-driven methods are utilized in the preprocessing stage. Traditional\napproaches rely on predefined rules and grammars, and involve techniques such\nas regular expressions, dependency parsing and named entity recognition.\nData-driven approaches depend on large-scale data and machine learning models,\nusing techniques including word embedding and pattern linking. Natural language\nunderstanding methods are classified into three categories: (i) rule-based,\n(ii) machine learning-based, and (iii) hybrid. We then describe a general\nconstruction process for executable languages over relational and\nspatio-temporal databases. Subsequently, common benchmarks and evaluation\nmetrics for transforming natural language into executable language are\npresented, and methods for generating new benchmarks are explored. Finally, we\nsummarize the classification, development, and enhancement of NLIDB systems,\nand discuss deep language understanding and database interaction techniques\nrelated to NLIDB, including (i) using LLM for Text2SQL tasks, (ii) generating\nnatural language interpretations from SQL, and (iii) transforming speech\nqueries into SQL."
                },
                "authors": [
                    {
                        "name": "Mengyi Liu"
                    },
                    {
                        "name": "Jianqiu Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jianqiu Xu"
                },
                "author": "Jianqiu Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16498v2",
                "updated": "2025-03-04T09:13:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    13,
                    23,
                    1,
                    63,
                    0
                ],
                "published": "2024-08-29T12:56:06Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    56,
                    6,
                    3,
                    242,
                    0
                ],
                "title": "A Survey on Evaluating Large Language Models in Code Generation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Evaluating Large Language Models in Code Generation Tasks"
                },
                "summary": "This paper provides a comprehensive review of the current methods and metrics\nused to evaluate the performance of Large Language Models (LLMs) in code\ngeneration tasks. With the rapid growth in demand for automated software\ndevelopment, LLMs have demonstrated significant potential in the field of code\ngeneration. The paper begins by reviewing the historical development of LLMs\nand their applications in code generation. Next, it details various methods and\nmetrics for assessing the code generation capabilities of LLMs, including code\ncorrectness, efficiency, readability, and evaluation methods based on expert\nreview and user experience. The paper also evaluates the widely used benchmark\ndatasets, identifying their limitations and proposing directions for future\nimprovements. Specifically, the paper analyzes the performance of code\ngeneration models across different tasks by combining multiple evaluation\nmetrics, such as code compilation/interpretation success rates, unit test pass\nrates, and performance and efficiency metrics, to comprehensively assess the\npractical application of LLMs in code generation. Finally, the paper discusses\nthe challenges faced in evaluating LLMs in code generation, particularly how to\nensure the comprehensiveness and accuracy of evaluation methods and how to\nadapt to the evolving practices of software development. These analyses and\ndiscussions provide valuable insights for further optimizing and improving the\napplication of LLMs in code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive review of the current methods and metrics\nused to evaluate the performance of Large Language Models (LLMs) in code\ngeneration tasks. With the rapid growth in demand for automated software\ndevelopment, LLMs have demonstrated significant potential in the field of code\ngeneration. The paper begins by reviewing the historical development of LLMs\nand their applications in code generation. Next, it details various methods and\nmetrics for assessing the code generation capabilities of LLMs, including code\ncorrectness, efficiency, readability, and evaluation methods based on expert\nreview and user experience. The paper also evaluates the widely used benchmark\ndatasets, identifying their limitations and proposing directions for future\nimprovements. Specifically, the paper analyzes the performance of code\ngeneration models across different tasks by combining multiple evaluation\nmetrics, such as code compilation/interpretation success rates, unit test pass\nrates, and performance and efficiency metrics, to comprehensively assess the\npractical application of LLMs in code generation. Finally, the paper discusses\nthe challenges faced in evaluating LLMs in code generation, particularly how to\nensure the comprehensiveness and accuracy of evaluation methods and how to\nadapt to the evolving practices of software development. These analyses and\ndiscussions provide valuable insights for further optimizing and improving the\napplication of LLMs in code generation tasks."
                },
                "authors": [
                    {
                        "name": "Liguo Chen"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Hongrui Jia"
                    },
                    {
                        "name": "Zhengran Zeng"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yijiang Xu"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Qing Gao"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Shikun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shikun Zhang"
                },
                "author": "Shikun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02425v1",
                "updated": "2025-03-04T09:12:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    12,
                    11,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T09:12:11Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    12,
                    11,
                    1,
                    63,
                    0
                ],
                "title": "Real-time station monitor and stationtest pipelines for LOFAR 2.01",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time station monitor and stationtest pipelines for LOFAR 2.01"
                },
                "summary": "LOFAR is a low-frequency array distributed across several European countries.\nEach LOFAR station contains thousands of antennas and associated electronics,\nmaking monitoring and thorough testing of those components essential to\nensuring station reliability. This paper discusses various anomalies that may\narise in LOFAR antennas, tile elements, modems, and summators. We also\nintroduce two diagnostic pipelines designed to detect these anomalies: a\nreal-time station monitoring system and an offline stationtest system. These\npipelines provide valuable insights into the operational status of each\nantenna, issuing alerts to minimize observational disruptions while maximizing\nstation uptime, reliability, and sensitivity. By enhancing the efficiency and\nstability of LOFAR stations, they also serve as a foundation for future\nlarge-scale arrays like SKA-Low. The experience gained from their development\nand deployment will contribute to the construction and maintenance of SKA-Low,\nimproving monitoring and diagnostic capabilities for large-scale antenna\nnetworks. Ultimately, these systems play a crucial role in ensuring continuous\nobservations and maintaining data integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOFAR is a low-frequency array distributed across several European countries.\nEach LOFAR station contains thousands of antennas and associated electronics,\nmaking monitoring and thorough testing of those components essential to\nensuring station reliability. This paper discusses various anomalies that may\narise in LOFAR antennas, tile elements, modems, and summators. We also\nintroduce two diagnostic pipelines designed to detect these anomalies: a\nreal-time station monitoring system and an offline stationtest system. These\npipelines provide valuable insights into the operational status of each\nantenna, issuing alerts to minimize observational disruptions while maximizing\nstation uptime, reliability, and sensitivity. By enhancing the efficiency and\nstability of LOFAR stations, they also serve as a foundation for future\nlarge-scale arrays like SKA-Low. The experience gained from their development\nand deployment will contribute to the construction and maintenance of SKA-Low,\nimproving monitoring and diagnostic capabilities for large-scale antenna\nnetworks. Ultimately, these systems play a crucial role in ensuring continuous\nobservations and maintaining data integrity."
                },
                "authors": [
                    {
                        "name": "Jun Wanga"
                    },
                    {
                        "name": "M. J. Nordenb"
                    },
                    {
                        "name": "P. Donker"
                    }
                ],
                "author_detail": {
                    "name": "P. Donker"
                },
                "author": "P. Donker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02316v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02316v4",
                "updated": "2025-03-04T09:10:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    10,
                    59,
                    1,
                    63,
                    0
                ],
                "published": "2024-11-04T17:40:39Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    40,
                    39,
                    0,
                    309,
                    0
                ],
                "title": "Evaluating Creative Short Story Generation in Humans and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Creative Short Story Generation in Humans and Large Language\n  Models"
                },
                "summary": "Story-writing is a fundamental aspect of human imagination, relying heavily\non creativity to produce narratives that are novel, effective, and surprising.\nWhile large language models (LLMs) have demonstrated the ability to generate\nhigh-quality stories, their creative story-writing capabilities remain\nunder-explored. In this work, we conduct a systematic analysis of creativity in\nshort story generation across 60 LLMs and 60 people using a five-sentence\ncreative story-writing task. We use measures to automatically evaluate model-\nand human-generated stories across several dimensions of creativity, including\nnovelty, surprise, diversity, and linguistic complexity. We also collect\ncreativity ratings and Turing Test classifications from non-expert and expert\nhuman raters and LLMs. Automated metrics show that LLMs generate stylistically\ncomplex stories, but tend to fall short in terms of novelty, surprise and\ndiversity when compared to average human writers. Expert ratings generally\ncoincide with automated metrics. However, LLMs and non-experts rate LLM stories\nto be more creative than human-generated stories. We discuss why and how these\ndifferences in ratings occur, and their implications for both human and\nartificial creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Story-writing is a fundamental aspect of human imagination, relying heavily\non creativity to produce narratives that are novel, effective, and surprising.\nWhile large language models (LLMs) have demonstrated the ability to generate\nhigh-quality stories, their creative story-writing capabilities remain\nunder-explored. In this work, we conduct a systematic analysis of creativity in\nshort story generation across 60 LLMs and 60 people using a five-sentence\ncreative story-writing task. We use measures to automatically evaluate model-\nand human-generated stories across several dimensions of creativity, including\nnovelty, surprise, diversity, and linguistic complexity. We also collect\ncreativity ratings and Turing Test classifications from non-expert and expert\nhuman raters and LLMs. Automated metrics show that LLMs generate stylistically\ncomplex stories, but tend to fall short in terms of novelty, surprise and\ndiversity when compared to average human writers. Expert ratings generally\ncoincide with automated metrics. However, LLMs and non-experts rate LLM stories\nto be more creative than human-generated stories. We discuss why and how these\ndifferences in ratings occur, and their implications for both human and\nartificial creativity."
                },
                "authors": [
                    {
                        "name": "Mete Ismayilzada"
                    },
                    {
                        "name": "Claire Stevenson"
                    },
                    {
                        "name": "Lonneke van der Plas"
                    }
                ],
                "author_detail": {
                    "name": "Lonneke van der Plas"
                },
                "author": "Lonneke van der Plas",
                "arxiv_comment": "Submitted to ICCC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02316v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02316v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02420v1",
                "updated": "2025-03-04T09:05:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    5,
                    1,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T09:05:01Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    5,
                    1,
                    1,
                    63,
                    0
                ],
                "title": "Exploring Model Quantization in GenAI-based Image Inpainting and\n  Detection of Arable Plants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Model Quantization in GenAI-based Image Inpainting and\n  Detection of Arable Plants"
                },
                "summary": "Deep learning-based weed control systems often suffer from limited training\ndata diversity and constrained on-board computation, impacting their real-world\nperformance. To overcome these challenges, we propose a framework that\nleverages Stable Diffusion-based inpainting to augment training data\nprogressively in 10% increments -- up to an additional 200%, thus enhancing\nboth the volume and diversity of samples. Our approach is evaluated on two\nstate-of-the-art object detection models, YOLO11(l) and RT-DETR(l), using the\nmAP50 metric to assess detection performance. We explore quantization\nstrategies (FP16 and INT8) for both the generative inpainting and detection\nmodels to strike a balance between inference speed and accuracy. Deployment of\nthe downstream models on the Jetson Orin Nano demonstrates the practical\nviability of our framework in resource-constrained environments, ultimately\nimproving detection accuracy and computational efficiency in intelligent weed\nmanagement systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning-based weed control systems often suffer from limited training\ndata diversity and constrained on-board computation, impacting their real-world\nperformance. To overcome these challenges, we propose a framework that\nleverages Stable Diffusion-based inpainting to augment training data\nprogressively in 10% increments -- up to an additional 200%, thus enhancing\nboth the volume and diversity of samples. Our approach is evaluated on two\nstate-of-the-art object detection models, YOLO11(l) and RT-DETR(l), using the\nmAP50 metric to assess detection performance. We explore quantization\nstrategies (FP16 and INT8) for both the generative inpainting and detection\nmodels to strike a balance between inference speed and accuracy. Deployment of\nthe downstream models on the Jetson Orin Nano demonstrates the practical\nviability of our framework in resource-constrained environments, ultimately\nimproving detection accuracy and computational efficiency in intelligent weed\nmanagement systems."
                },
                "authors": [
                    {
                        "name": "Sourav Modak"
                    },
                    {
                        "name": "Ahmet Oğuz Saltık"
                    },
                    {
                        "name": "Anthony Stein"
                    }
                ],
                "author_detail": {
                    "name": "Anthony Stein"
                },
                "author": "Anthony Stein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01253v2",
                "updated": "2025-03-04T08:59:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    59,
                    26,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-03T07:29:46Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    7,
                    29,
                    46,
                    0,
                    62,
                    0
                ],
                "title": "NM-SpMM: Accelerating Matrix Multiplication Using N:M Sparsity with\n  GPGPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NM-SpMM: Accelerating Matrix Multiplication Using N:M Sparsity with\n  GPGPU"
                },
                "summary": "Deep learning demonstrates effectiveness across a wide range of tasks.\nHowever, the dense and over-parameterized nature of these models results in\nsignificant resource consumption during deployment. In response to this issue,\nweight pruning, particularly through N:M sparsity matrix multiplication, offers\nan efficient solution by transforming dense operations into semi-sparse ones.\nN:M sparsity provides an option for balancing performance and model accuracy,\nbut introduces more complex programming and optimization challenges. To address\nthese issues, we design a systematic top-down performance analysis model for\nN:M sparsity. Meanwhile, NM-SpMM is proposed as an efficient general N:M\nsparsity implementation. Based on our performance analysis, NM-SpMM employs a\nhierarchical blocking mechanism as a general optimization to enhance data\nlocality, while memory access optimization and pipeline design are introduced\nas sparsity-aware optimization, allowing it to achieve close-to-theoretical\npeak performance across different sparsity levels. Experimental results show\nthat NM-SpMM is 2.1x faster than nmSPARSE (the state-of-the-art for general N:M\nsparsity) and 1.4x to 6.3x faster than cuBLAS's dense GEMM operations, closely\napproaching the theoretical maximum speedup resulting from the reduction in\ncomputation due to sparsity. NM-SpMM is open source and publicly available at\nhttps://github.com/M-H482/NM-SpMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning demonstrates effectiveness across a wide range of tasks.\nHowever, the dense and over-parameterized nature of these models results in\nsignificant resource consumption during deployment. In response to this issue,\nweight pruning, particularly through N:M sparsity matrix multiplication, offers\nan efficient solution by transforming dense operations into semi-sparse ones.\nN:M sparsity provides an option for balancing performance and model accuracy,\nbut introduces more complex programming and optimization challenges. To address\nthese issues, we design a systematic top-down performance analysis model for\nN:M sparsity. Meanwhile, NM-SpMM is proposed as an efficient general N:M\nsparsity implementation. Based on our performance analysis, NM-SpMM employs a\nhierarchical blocking mechanism as a general optimization to enhance data\nlocality, while memory access optimization and pipeline design are introduced\nas sparsity-aware optimization, allowing it to achieve close-to-theoretical\npeak performance across different sparsity levels. Experimental results show\nthat NM-SpMM is 2.1x faster than nmSPARSE (the state-of-the-art for general N:M\nsparsity) and 1.4x to 6.3x faster than cuBLAS's dense GEMM operations, closely\napproaching the theoretical maximum speedup resulting from the reduction in\ncomputation due to sparsity. NM-SpMM is open source and publicly available at\nhttps://github.com/M-H482/NM-SpMM."
                },
                "authors": [
                    {
                        "name": "Cong Ma"
                    },
                    {
                        "name": "Du Wu"
                    },
                    {
                        "name": "Zhelang Deng"
                    },
                    {
                        "name": "Jiang Chen"
                    },
                    {
                        "name": "Xiaowen Huang"
                    },
                    {
                        "name": "Jintao Meng"
                    },
                    {
                        "name": "Wenxi Zhu"
                    },
                    {
                        "name": "Bingqiang Wang"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Minwen Deng"
                    },
                    {
                        "name": "Yanjie Wei"
                    },
                    {
                        "name": "Shengzhong Feng"
                    },
                    {
                        "name": "Yi Pan"
                    }
                ],
                "author_detail": {
                    "name": "Yi Pan"
                },
                "author": "Yi Pan",
                "arxiv_comment": "12 pages, 10 figures, accepted at IPDPS 2025. Code:\n  https://github.com/M-H482/NM-SpMM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; D.1.3; G.1.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02401v1",
                "updated": "2025-03-04T08:43:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    43,
                    19,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:43:19Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    43,
                    19,
                    1,
                    63,
                    0
                ],
                "title": "Hierarchical Re-ranker Retriever (HRR)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Re-ranker Retriever (HRR)"
                },
                "summary": "Retrieving the right level of context for a given query is a perennial\nchallenge in information retrieval - too large a chunk dilutes semantic\nspecificity, while chunks that are too small lack broader context. This paper\nintroduces the Hierarchical Re-ranker Retriever (HRR), a framework designed to\nachieve both fine-grained and high-level context retrieval for large language\nmodel (LLM) applications. In HRR, documents are split into sentence-level and\nintermediate-level (512 tokens) chunks to maximize vector-search quality for\nboth short and broad queries. We then employ a reranker that operates on these\n512-token chunks, ensuring an optimal balance neither too coarse nor too fine\nfor robust relevance scoring. Finally, top-ranked intermediate chunks are\nmapped to parent chunks (2048 tokens) to provide an LLM with sufficiently large\ncontext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving the right level of context for a given query is a perennial\nchallenge in information retrieval - too large a chunk dilutes semantic\nspecificity, while chunks that are too small lack broader context. This paper\nintroduces the Hierarchical Re-ranker Retriever (HRR), a framework designed to\nachieve both fine-grained and high-level context retrieval for large language\nmodel (LLM) applications. In HRR, documents are split into sentence-level and\nintermediate-level (512 tokens) chunks to maximize vector-search quality for\nboth short and broad queries. We then employ a reranker that operates on these\n512-token chunks, ensuring an optimal balance neither too coarse nor too fine\nfor robust relevance scoring. Finally, top-ranked intermediate chunks are\nmapped to parent chunks (2048 tokens) to provide an LLM with sufficiently large\ncontext."
                },
                "authors": [
                    {
                        "name": "Ashish Singh"
                    },
                    {
                        "name": "Priti Mohapatra"
                    }
                ],
                "author_detail": {
                    "name": "Priti Mohapatra"
                },
                "author": "Priti Mohapatra",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02400v1",
                "updated": "2025-03-04T08:43:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    43,
                    16,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:43:16Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    43,
                    16,
                    1,
                    63,
                    0
                ],
                "title": "Promptware Engineering: Software Engineering for LLM Prompt Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Promptware Engineering: Software Engineering for LLM Prompt Development"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into software\napplications, with prompts serving as the primary 'programming' interface to\nguide their behavior. As a result, a new software paradigm, promptware, has\nemerged, using natural language prompts to interact with LLMs and enabling\ncomplex tasks without traditional coding. Unlike traditional software, which\nrelies on formal programming languages and deterministic runtime environments,\npromptware is based on ambiguous, unstructured, and context-dependent natural\nlanguage and operates on LLMs as runtime environments, which are probabilistic\nand non-deterministic. These fundamental differences introduce unique\nchallenges in prompt development. In practice, prompt development is largely ad\nhoc and experimental, relying on a time-consuming trial-and-error process - a\nchallenge we term the 'promptware crisis.' To address this, we propose\npromptware engineering, a new methodology that adapts established software\nengineering principles to the process of prompt development. Building on\ndecades of success in traditional software engineering, we envision a\nsystematic framework that includes prompt requirements engineering, design,\nimplementation, testing, debugging, and evolution. Unlike traditional software\nengineering, our framework is specifically tailored to the unique\ncharacteristics of prompt development. This paper outlines a comprehensive\nroadmap for promptware engineering, identifying key research directions and\noffering actionable insights to advance LLM-based software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into software\napplications, with prompts serving as the primary 'programming' interface to\nguide their behavior. As a result, a new software paradigm, promptware, has\nemerged, using natural language prompts to interact with LLMs and enabling\ncomplex tasks without traditional coding. Unlike traditional software, which\nrelies on formal programming languages and deterministic runtime environments,\npromptware is based on ambiguous, unstructured, and context-dependent natural\nlanguage and operates on LLMs as runtime environments, which are probabilistic\nand non-deterministic. These fundamental differences introduce unique\nchallenges in prompt development. In practice, prompt development is largely ad\nhoc and experimental, relying on a time-consuming trial-and-error process - a\nchallenge we term the 'promptware crisis.' To address this, we propose\npromptware engineering, a new methodology that adapts established software\nengineering principles to the process of prompt development. Building on\ndecades of success in traditional software engineering, we envision a\nsystematic framework that includes prompt requirements engineering, design,\nimplementation, testing, debugging, and evolution. Unlike traditional software\nengineering, our framework is specifically tailored to the unique\ncharacteristics of prompt development. This paper outlines a comprehensive\nroadmap for promptware engineering, identifying key research directions and\noffering actionable insights to advance LLM-based software development."
                },
                "authors": [
                    {
                        "name": "Zhenpeng Chen"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Jie M. Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02394v1",
                "updated": "2025-03-04T08:35:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    35,
                    1,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:35:01Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    35,
                    1,
                    1,
                    63,
                    0
                ],
                "title": "BHViT: Binarized Hybrid Vision Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BHViT: Binarized Hybrid Vision Transformer"
                },
                "summary": "Model binarization has made significant progress in enabling real-time and\nenergy-efficient computation for convolutional neural networks (CNN), offering\na potential solution to the deployment challenges faced by Vision Transformers\n(ViTs) on edge devices. However, due to the structural differences between CNN\nand Transformer architectures, simply applying binary CNN strategies to the ViT\nmodels will lead to a significant performance drop. To tackle this challenge,\nwe propose BHViT, a binarization-friendly hybrid ViT architecture and its full\nbinarization model with the guidance of three important observations.\nInitially, BHViT utilizes the local information interaction and hierarchical\nfeature aggregation technique from coarse to fine levels to address redundant\ncomputations stemming from excessive tokens. Then, a novel module based on\nshift operations is proposed to enhance the performance of the binary\nMultilayer Perceptron (MLP) module without significantly increasing\ncomputational overhead. In addition, an innovative attention matrix\nbinarization method based on quantization decomposition is proposed to evaluate\nthe token's importance in the binarized attention matrix. Finally, we propose a\nregularization loss to address the inadequate optimization caused by the\nincompatibility between the weight oscillation in the binary layers and the\nAdam Optimizer. Extensive experimental results demonstrate that our proposed\nalgorithm achieves SOTA performance among binary ViT methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model binarization has made significant progress in enabling real-time and\nenergy-efficient computation for convolutional neural networks (CNN), offering\na potential solution to the deployment challenges faced by Vision Transformers\n(ViTs) on edge devices. However, due to the structural differences between CNN\nand Transformer architectures, simply applying binary CNN strategies to the ViT\nmodels will lead to a significant performance drop. To tackle this challenge,\nwe propose BHViT, a binarization-friendly hybrid ViT architecture and its full\nbinarization model with the guidance of three important observations.\nInitially, BHViT utilizes the local information interaction and hierarchical\nfeature aggregation technique from coarse to fine levels to address redundant\ncomputations stemming from excessive tokens. Then, a novel module based on\nshift operations is proposed to enhance the performance of the binary\nMultilayer Perceptron (MLP) module without significantly increasing\ncomputational overhead. In addition, an innovative attention matrix\nbinarization method based on quantization decomposition is proposed to evaluate\nthe token's importance in the binarized attention matrix. Finally, we propose a\nregularization loss to address the inadequate optimization caused by the\nincompatibility between the weight oscillation in the binary layers and the\nAdam Optimizer. Extensive experimental results demonstrate that our proposed\nalgorithm achieves SOTA performance among binary ViT methods."
                },
                "authors": [
                    {
                        "name": "Tian Gao"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zhiyuan Zhang"
                    },
                    {
                        "name": "Huajun Liu"
                    },
                    {
                        "name": "Kaijie Yin"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Hui Kong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Kong"
                },
                "author": "Hui Kong",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02393v1",
                "updated": "2025-03-04T08:31:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    31,
                    12,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:31:12Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    31,
                    12,
                    1,
                    63,
                    0
                ],
                "title": "Vision-Language Model IP Protection via Prompt-based Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Model IP Protection via Prompt-based Learning"
                },
                "summary": "Vision-language models (VLMs) like CLIP (Contrastive Language-Image\nPre-Training) have seen remarkable success in visual recognition, highlighting\nthe increasing need to safeguard the intellectual property (IP) of well-trained\nmodels. Effective IP protection extends beyond ensuring authorized usage; it\nalso necessitates restricting model deployment to authorized data domains,\nparticularly when the model is fine-tuned for specific target domains. However,\ncurrent IP protection methods often rely solely on the visual backbone, which\nmay lack sufficient semantic richness. To bridge this gap, we introduce\nIP-CLIP, a lightweight IP protection strategy tailored to CLIP, employing a\nprompt-based learning approach. By leveraging the frozen visual backbone of\nCLIP, we extract both image style and content information, incorporating them\ninto the learning of IP prompt. This strategy acts as a robust barrier,\neffectively preventing the unauthorized transfer of features from authorized\ndomains to unauthorized ones. Additionally, we propose a style-enhancement\nbranch that constructs feature banks for both authorized and unauthorized\ndomains. This branch integrates self-enhanced and cross-domain features,\nfurther strengthening IP-CLIP's capability to block features from unauthorized\ndomains. Finally, we present new three metrics designed to better balance the\nperformance degradation of authorized and unauthorized domains. Comprehensive\nexperiments in various scenarios demonstrate its promising potential for\napplication in IP protection tasks for VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) like CLIP (Contrastive Language-Image\nPre-Training) have seen remarkable success in visual recognition, highlighting\nthe increasing need to safeguard the intellectual property (IP) of well-trained\nmodels. Effective IP protection extends beyond ensuring authorized usage; it\nalso necessitates restricting model deployment to authorized data domains,\nparticularly when the model is fine-tuned for specific target domains. However,\ncurrent IP protection methods often rely solely on the visual backbone, which\nmay lack sufficient semantic richness. To bridge this gap, we introduce\nIP-CLIP, a lightweight IP protection strategy tailored to CLIP, employing a\nprompt-based learning approach. By leveraging the frozen visual backbone of\nCLIP, we extract both image style and content information, incorporating them\ninto the learning of IP prompt. This strategy acts as a robust barrier,\neffectively preventing the unauthorized transfer of features from authorized\ndomains to unauthorized ones. Additionally, we propose a style-enhancement\nbranch that constructs feature banks for both authorized and unauthorized\ndomains. This branch integrates self-enhanced and cross-domain features,\nfurther strengthening IP-CLIP's capability to block features from unauthorized\ndomains. Finally, we present new three metrics designed to better balance the\nperformance degradation of authorized and unauthorized domains. Comprehensive\nexperiments in various scenarios demonstrate its promising potential for\napplication in IP protection tasks for VLMs."
                },
                "authors": [
                    {
                        "name": "Lianyu Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Huazhu Fu"
                    },
                    {
                        "name": "Daoqiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Daoqiang Zhang"
                },
                "author": "Daoqiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02390v1",
                "updated": "2025-03-04T08:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    28,
                    4,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    28,
                    4,
                    1,
                    63,
                    0
                ],
                "title": "ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for\n  Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for\n  Reasoning Tasks"
                },
                "summary": "Multi-agent systems have emerged as a promising approach for enhancing the\nreasoning capabilities of large language models in complex problem-solving.\nHowever, current MAS frameworks are limited by poor flexibility and\nscalability, with underdeveloped optimization strategies. To address these\nchallenges, we propose ReSo, which integrates task graph generation with a\nreward-driven two-stage agent selection process. The core of ReSo is the\nproposed Collaborative Reward Model, which can provide fine-grained reward\nsignals for MAS cooperation for optimization. We also introduce an automated\ndata synthesis framework for generating MAS benchmarks, without human\nannotations. Experimentally, ReSo matches or outperforms existing methods. ReSo\nachieves \\textbf{33.7\\%} and \\textbf{32.3\\%} accuracy on Math-MAS and\nSciBench-MAS SciBench, while other methods completely fail. Code is available\nat: \\href{https://github.com/hengzzzhou/ReSo}{ReSo}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems have emerged as a promising approach for enhancing the\nreasoning capabilities of large language models in complex problem-solving.\nHowever, current MAS frameworks are limited by poor flexibility and\nscalability, with underdeveloped optimization strategies. To address these\nchallenges, we propose ReSo, which integrates task graph generation with a\nreward-driven two-stage agent selection process. The core of ReSo is the\nproposed Collaborative Reward Model, which can provide fine-grained reward\nsignals for MAS cooperation for optimization. We also introduce an automated\ndata synthesis framework for generating MAS benchmarks, without human\nannotations. Experimentally, ReSo matches or outperforms existing methods. ReSo\nachieves \\textbf{33.7\\%} and \\textbf{32.3\\%} accuracy on Math-MAS and\nSciBench-MAS SciBench, while other methods completely fail. Code is available\nat: \\href{https://github.com/hengzzzhou/ReSo}{ReSo}"
                },
                "authors": [
                    {
                        "name": "Heng Zhou"
                    },
                    {
                        "name": "Hejia Geng"
                    },
                    {
                        "name": "Xiangyuan Xue"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00203v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00203v2",
                "updated": "2025-03-04T08:23:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    23,
                    10,
                    1,
                    63,
                    0
                ],
                "published": "2025-02-28T21:39:22Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    21,
                    39,
                    22,
                    4,
                    59,
                    0
                ],
                "title": "Llamarine: Open-source Maritime Industry-specific Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llamarine: Open-source Maritime Industry-specific Large Language Model"
                },
                "summary": "Large Language Models (LLMs) have demonstrated substantial potential in\naddressing complex reasoning tasks, yet their general-purpose nature often\nlimits their effectiveness in specialized domains such as maritime navigation.\nTo bridge this gap, we introduce Llamarine, the first open-source LLM designed\nspecifically for maritime navigation. Llamarine 1.0 is developed through\ncontinued pretraining and fine-tuning on a high-quality corpus comprising\nmaritime textbooks, research publications, and web text from Wikipedia. This\ndomain-specific training enables the model to acquire expert-level knowledge in\nnavigational principles, collision avoidance, route optimization, and\nregulatory compliance. Our key contributions include (a) the curation of a\ncomprehensive maritime dataset from authoritative sources, ensuring depth and\nreliability in the model's knowledge base; (b) the development of a\nfoundational model capable of reasoning about complex navigational challenges\nwith greater accuracy than general-purpose LLMs; and (c) the establishment of a\nbenchmark to evaluate performance in maritime-specific decision-making tasks.\nExperimental results demonstrate that Llamarine outperforms both\ngeneral-purpose and commercial LLMs in critical navigation-related tasks, such\nas trajectory planning, risk assessment, and compliance with maritime\nregulations. By providing an open-source foundation model trained exclusively\non high-quality maritime literature, Llamarine paves the way for AI-driven\nadvancements in maritime safety, efficiency, and operational decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated substantial potential in\naddressing complex reasoning tasks, yet their general-purpose nature often\nlimits their effectiveness in specialized domains such as maritime navigation.\nTo bridge this gap, we introduce Llamarine, the first open-source LLM designed\nspecifically for maritime navigation. Llamarine 1.0 is developed through\ncontinued pretraining and fine-tuning on a high-quality corpus comprising\nmaritime textbooks, research publications, and web text from Wikipedia. This\ndomain-specific training enables the model to acquire expert-level knowledge in\nnavigational principles, collision avoidance, route optimization, and\nregulatory compliance. Our key contributions include (a) the curation of a\ncomprehensive maritime dataset from authoritative sources, ensuring depth and\nreliability in the model's knowledge base; (b) the development of a\nfoundational model capable of reasoning about complex navigational challenges\nwith greater accuracy than general-purpose LLMs; and (c) the establishment of a\nbenchmark to evaluate performance in maritime-specific decision-making tasks.\nExperimental results demonstrate that Llamarine outperforms both\ngeneral-purpose and commercial LLMs in critical navigation-related tasks, such\nas trajectory planning, risk assessment, and compliance with maritime\nregulations. By providing an open-source foundation model trained exclusively\non high-quality maritime literature, Llamarine paves the way for AI-driven\nadvancements in maritime safety, efficiency, and operational decision-making."
                },
                "authors": [
                    {
                        "name": "William Nguyen"
                    },
                    {
                        "name": "An Phan"
                    },
                    {
                        "name": "Konobu Kimura"
                    },
                    {
                        "name": "Hitoshi Maeno"
                    },
                    {
                        "name": "Mika Tanaka"
                    },
                    {
                        "name": "Quynh Le"
                    },
                    {
                        "name": "William Poucher"
                    },
                    {
                        "name": "Christopher Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Nguyen"
                },
                "author": "Christopher Nguyen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00203v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00203v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02382v1",
                "updated": "2025-03-04T08:18:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    18,
                    46,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:18:46Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    18,
                    46,
                    1,
                    63,
                    0
                ],
                "title": "An Efficient and Precise Training Data Construction Framework for\n  Process-supervised Reward Model in Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient and Precise Training Data Construction Framework for\n  Process-supervised Reward Model in Mathematical Reasoning"
                },
                "summary": "Enhancing the mathematical reasoning capabilities of Large Language Models\n(LLMs) is of great scientific and practical significance. Researchers typically\nemploy process-supervised reward models (PRMs) to guide the reasoning process,\neffectively improving the models' reasoning abilities. However, existing\nmethods for constructing process supervision training data, such as manual\nannotation and per-step Monte Carlo estimation, are often costly or suffer from\npoor quality. To address these challenges, this paper introduces a framework\ncalled EpicPRM, which annotates each intermediate reasoning step based on its\nquantified contribution and uses an adaptive binary search algorithm to enhance\nboth annotation precision and efficiency. Using this approach, we efficiently\nconstruct a high-quality process supervision training dataset named Epic50k,\nconsisting of 50k annotated intermediate steps. Compared to other publicly\navailable datasets, the PRM trained on Epic50k demonstrates significantly\nsuperior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the mathematical reasoning capabilities of Large Language Models\n(LLMs) is of great scientific and practical significance. Researchers typically\nemploy process-supervised reward models (PRMs) to guide the reasoning process,\neffectively improving the models' reasoning abilities. However, existing\nmethods for constructing process supervision training data, such as manual\nannotation and per-step Monte Carlo estimation, are often costly or suffer from\npoor quality. To address these challenges, this paper introduces a framework\ncalled EpicPRM, which annotates each intermediate reasoning step based on its\nquantified contribution and uses an adaptive binary search algorithm to enhance\nboth annotation precision and efficiency. Using this approach, we efficiently\nconstruct a high-quality process supervision training dataset named Epic50k,\nconsisting of 50k annotated intermediate steps. Compared to other publicly\navailable datasets, the PRM trained on Epic50k demonstrates significantly\nsuperior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM."
                },
                "authors": [
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Qianlong Du"
                    },
                    {
                        "name": "Fuwei Cui"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02374v1",
                "updated": "2025-03-04T08:01:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    1,
                    34,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:01:34Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    1,
                    34,
                    1,
                    63,
                    0
                ],
                "title": "MedEthicEval: Evaluating Large Language Models Based on Chinese Medical\n  Ethics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedEthicEval: Evaluating Large Language Models Based on Chinese Medical\n  Ethics"
                },
                "summary": "Large language models (LLMs) demonstrate significant potential in advancing\nmedical applications, yet their capabilities in addressing medical ethics\nchallenges remain underexplored. This paper introduces MedEthicEval, a novel\nbenchmark designed to systematically evaluate LLMs in the domain of medical\nethics. Our framework encompasses two key components: knowledge, assessing the\nmodels' grasp of medical ethics principles, and application, focusing on their\nability to apply these principles across diverse scenarios. To support this\nbenchmark, we consulted with medical ethics researchers and developed three\ndatasets addressing distinct ethical challenges: blatant violations of medical\nethics, priority dilemmas with clear inclinations, and equilibrium dilemmas\nwithout obvious resolutions. MedEthicEval serves as a critical tool for\nunderstanding LLMs' ethical reasoning in healthcare, paving the way for their\nresponsible and effective use in medical contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate significant potential in advancing\nmedical applications, yet their capabilities in addressing medical ethics\nchallenges remain underexplored. This paper introduces MedEthicEval, a novel\nbenchmark designed to systematically evaluate LLMs in the domain of medical\nethics. Our framework encompasses two key components: knowledge, assessing the\nmodels' grasp of medical ethics principles, and application, focusing on their\nability to apply these principles across diverse scenarios. To support this\nbenchmark, we consulted with medical ethics researchers and developed three\ndatasets addressing distinct ethical challenges: blatant violations of medical\nethics, priority dilemmas with clear inclinations, and equilibrium dilemmas\nwithout obvious resolutions. MedEthicEval serves as a critical tool for\nunderstanding LLMs' ethical reasoning in healthcare, paving the way for their\nresponsible and effective use in medical contexts."
                },
                "authors": [
                    {
                        "name": "Haoan Jin"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Hanhui Xu"
                    },
                    {
                        "name": "Kenny Q. Zhu"
                    },
                    {
                        "name": "Mengyue Wu"
                    }
                ],
                "author_detail": {
                    "name": "Mengyue Wu"
                },
                "author": "Mengyue Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]