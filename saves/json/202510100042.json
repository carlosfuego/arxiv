[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.07318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07318v1",
                "updated": "2025-10-08T17:59:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Hippocampus Networks for Efficient Long-Context Modeling"
                },
                "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN."
                },
                "authors": [
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Shu Zhong"
                    },
                    {
                        "name": "Qinghao Ye"
                    },
                    {
                        "name": "Xuehan Xiong"
                    },
                    {
                        "name": "Lai Wei"
                    }
                ],
                "author_detail": {
                    "name": "Lai Wei"
                },
                "author": "Lai Wei",
                "arxiv_comment": "Code: https://github.com/ByteDance-Seed/AHN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07297v1",
                "updated": "2025-10-08T17:51:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    51,
                    34,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:51:34Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    51,
                    34,
                    2,
                    281,
                    0
                ],
                "title": "Agentic generative AI for media content discovery at the national\n  football league",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic generative AI for media content discovery at the national\n  football league"
                },
                "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines."
                },
                "authors": [
                    {
                        "name": "Henry Wang"
                    },
                    {
                        "name": "Md Sirajus Salekin"
                    },
                    {
                        "name": "Jake Lee"
                    },
                    {
                        "name": "Ross Claytor"
                    },
                    {
                        "name": "Shinan Zhang"
                    },
                    {
                        "name": "Michael Chi"
                    }
                ],
                "author_detail": {
                    "name": "Michael Chi"
                },
                "author": "Michael Chi",
                "arxiv_comment": "13 pages, 7 figures, International Sports Analytics Conference and\n  Exhibition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07293v1",
                "updated": "2025-10-08T17:50:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:50:16Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs"
                },
                "summary": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks."
                },
                "authors": [
                    {
                        "name": "Peize He"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Jiajie Huang"
                    },
                    {
                        "name": "Zehui Lei"
                    },
                    {
                        "name": "Zhuangcheng Gu"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Jiabing Yang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Zhifei Liu"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "26 pages, 23 figures, the code is available at\n  \\url{https://github.com/DabDans/AudioMarathon}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15347v2",
                "updated": "2025-10-08T00:06:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    6,
                    52,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-21T10:20:46Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025 Workshop on Multi-Turn Interactions in Large Language\n  Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v3",
                "updated": "2025-10-07T22:07:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    22,
                    7,
                    44,
                    1,
                    280,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications"
                },
                "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference"
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "arxiv_comment": "NeurIPS 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06415v1",
                "updated": "2025-10-07T19:50:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T19:50:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "title": "Enhanced Breakdown Voltage in $β$-Ga$_2$O$_3$ Schottky Barrier\n  Diodes via Fast Neutron Irradiation and Electrothermal Annealing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Breakdown Voltage in $β$-Ga$_2$O$_3$ Schottky Barrier\n  Diodes via Fast Neutron Irradiation and Electrothermal Annealing"
                },
                "summary": "This study investigates the impact of fast neutron irradiation and\npost-radiation electro-thermal annealing on the electrical performance of\n$\\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV\nneutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,\nincluding a drastic reduction in on-current and an increase in on-resistance.\nElectrothermal testing, conducted through simultaneous current-voltage (J-V)\nmeasurements and thermal annealing, resulted in significant recovery. After\nfour cycles of electro-thermal testing, the devices demonstrated significant\nimprovements in performance, with a substantial recovery of on-current and a\nreduction in on-resistance compared to the post-radiation condition,\napproaching pre-radiation levels. Most recovery occurred during the first two\ncycles, with diminishing improvements in later cycles, indicating that most\nthermally recoverable traps were mitigated early. Capacitance-voltage (C-V)\nmeasurements revealed a substantial reduction in carrier concentration,\ndecreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first\nelectro-thermal testing cycle, indicating an over 82% reduction. Following the\nthird cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,\nreflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited\na remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a\n~325% improvement) after the first electro-thermal testing, attributed to the\nreduction in carrier concentration by compensating radiation-induced traps.\nSubsequent testing reduced breakdown voltage slightly to 940 V due to partial\nrecovery of carrier concentration, but it remained significantly higher than\npre-radiation levels, highlighting the promise of $\\beta$-Ga$_2$O$_3$ power\ndevices for high-power applications in radiation-intense environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the impact of fast neutron irradiation and\npost-radiation electro-thermal annealing on the electrical performance of\n$\\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV\nneutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,\nincluding a drastic reduction in on-current and an increase in on-resistance.\nElectrothermal testing, conducted through simultaneous current-voltage (J-V)\nmeasurements and thermal annealing, resulted in significant recovery. After\nfour cycles of electro-thermal testing, the devices demonstrated significant\nimprovements in performance, with a substantial recovery of on-current and a\nreduction in on-resistance compared to the post-radiation condition,\napproaching pre-radiation levels. Most recovery occurred during the first two\ncycles, with diminishing improvements in later cycles, indicating that most\nthermally recoverable traps were mitigated early. Capacitance-voltage (C-V)\nmeasurements revealed a substantial reduction in carrier concentration,\ndecreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first\nelectro-thermal testing cycle, indicating an over 82% reduction. Following the\nthird cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,\nreflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited\na remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a\n~325% improvement) after the first electro-thermal testing, attributed to the\nreduction in carrier concentration by compensating radiation-induced traps.\nSubsequent testing reduced breakdown voltage slightly to 940 V due to partial\nrecovery of carrier concentration, but it remained significantly higher than\npre-radiation levels, highlighting the promise of $\\beta$-Ga$_2$O$_3$ power\ndevices for high-power applications in radiation-intense environments."
                },
                "authors": [
                    {
                        "name": "Saleh Ahmed Khan"
                    },
                    {
                        "name": "Sudipto Saha"
                    },
                    {
                        "name": "Ahmed Ibreljic"
                    },
                    {
                        "name": "Stephen Margiotta"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Walid Amir"
                    },
                    {
                        "name": "Surajit Chakraborty"
                    },
                    {
                        "name": "Uttam Singisetti"
                    },
                    {
                        "name": "A F M Anhar Uddin Bhuiyan"
                    }
                ],
                "author_detail": {
                    "name": "A F M Anhar Uddin Bhuiyan"
                },
                "author": "A F M Anhar Uddin Bhuiyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06175v1",
                "updated": "2025-10-07T17:35:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:35:28Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "title": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization"
                },
                "summary": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Zhengyang Tong"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05686v1",
                "updated": "2025-10-07T08:43:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T08:43:07Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "title": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment"
                },
                "summary": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%)."
                },
                "authors": [
                    {
                        "name": "José Gómez-delaHiz"
                    },
                    {
                        "name": "Mohamed Faten Zhani"
                    },
                    {
                        "name": "Jaime Galán-Jiménez"
                    },
                    {
                        "name": "John Kaippallimalil"
                    }
                ],
                "author_detail": {
                    "name": "John Kaippallimalil"
                },
                "author": "John Kaippallimalil",
                "arxiv_comment": "10 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05529v1",
                "updated": "2025-10-07T02:39:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    39,
                    35,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T02:39:35Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    39,
                    35,
                    1,
                    280,
                    0
                ],
                "title": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model\n  Inference"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires caching a\ngrowing list of past key-value (KV) pairs, making long-context inference a\nmemory-bound problem. While recent methods have explored quantizing the cache,\nevicting tokens, or using binary sketches for keys (e.g., Loki), these\napproaches often provide an incomplete solution by leaving one component (like\nvalues) uncompressed or by discarding context information. This paper\nintroduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression\nscheme that radically reduces memory usage without sacrificing context. H1B-KV\nrepresents each key vector using a 1-bit binary sketch, enabling\nhardware-friendly bitwise attention, and further compresses value vectors using\n4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter\nLLM to handle an 8k-token context with under 60 MB of cache memory - a 70x\nreduction. We demonstrate that after a lightweight finetuning, H1B-KV matches\nfull-precision performance not only on perplexity benchmarks but also on\ncomplex downstream tasks like mathematical reasoning (GSM8K), multi-task\nunderstanding (MMLU), and code generation (HumanEval). Our results show H1B-KV\nsignificantly outperforms leading quantization (KIVI), token eviction\n(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,\nestablishing it as a robust solution for deploying LLMs in memory-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires caching a\ngrowing list of past key-value (KV) pairs, making long-context inference a\nmemory-bound problem. While recent methods have explored quantizing the cache,\nevicting tokens, or using binary sketches for keys (e.g., Loki), these\napproaches often provide an incomplete solution by leaving one component (like\nvalues) uncompressed or by discarding context information. This paper\nintroduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression\nscheme that radically reduces memory usage without sacrificing context. H1B-KV\nrepresents each key vector using a 1-bit binary sketch, enabling\nhardware-friendly bitwise attention, and further compresses value vectors using\n4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter\nLLM to handle an 8k-token context with under 60 MB of cache memory - a 70x\nreduction. We demonstrate that after a lightweight finetuning, H1B-KV matches\nfull-precision performance not only on perplexity benchmarks but also on\ncomplex downstream tasks like mathematical reasoning (GSM8K), multi-task\nunderstanding (MMLU), and code generation (HumanEval). Our results show H1B-KV\nsignificantly outperforms leading quantization (KIVI), token eviction\n(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,\nestablishing it as a robust solution for deploying LLMs in memory-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Harshil Vejendla"
                    }
                ],
                "author_detail": {
                    "name": "Harshil Vejendla"
                },
                "author": "Harshil Vejendla",
                "arxiv_comment": "MIT URTC 2025 Technical Paper (Oral), 5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05476v1",
                "updated": "2025-10-07T00:32:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T00:32:45Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "title": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided\n  Inter-Node Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided\n  Inter-Node Communications"
                },
                "summary": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages."
                },
                "authors": [
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Jongryool Kim"
                    },
                    {
                        "name": "Byungil Koh"
                    },
                    {
                        "name": "Hoshik Kim"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_doi": "10.1145/3712285.3759816",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759816",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.05476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05373v1",
                "updated": "2025-10-06T21:08:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    21,
                    8,
                    11,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T21:08:11Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    21,
                    8,
                    11,
                    0,
                    279,
                    0
                ],
                "title": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear\n  Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear\n  Correction"
                },
                "summary": "Quantizing the key-value (KV) cache is a promising strategy for improving the\ninference efficiency of large language models (LLMs). However, aggressive\nquantization to very low precision (e.g., 2 bits) introduces significant errors\nin the stored key and value tensors, which propagate through the dot-product\nattention mechanism and ultimately degrade generation quality. To address this,\nwe propose KVLinC, a framework to mitigate attention errors introduced by KV\ncache quantization in the extreme low-precision regime. KVLinC combines a\nHadamard rotation, which reduces quantization error in values, with lightweight\nlinear correction adapters that explicitly compensate for errors introduced by\nquantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3\nmodel families, KVLinC consistently matches or surpasses strong baselines while\nachieving higher KV-cache compression. Furthermore, we implement a custom\nattention kernel that results in upto 2.55x faster inference compared to Flash\nAttention baseline, enabling efficient long-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing the key-value (KV) cache is a promising strategy for improving the\ninference efficiency of large language models (LLMs). However, aggressive\nquantization to very low precision (e.g., 2 bits) introduces significant errors\nin the stored key and value tensors, which propagate through the dot-product\nattention mechanism and ultimately degrade generation quality. To address this,\nwe propose KVLinC, a framework to mitigate attention errors introduced by KV\ncache quantization in the extreme low-precision regime. KVLinC combines a\nHadamard rotation, which reduces quantization error in values, with lightweight\nlinear correction adapters that explicitly compensate for errors introduced by\nquantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3\nmodel families, KVLinC consistently matches or surpasses strong baselines while\nachieving higher KV-cache compression. Furthermore, we implement a custom\nattention kernel that results in upto 2.55x faster inference compared to Flash\nAttention baseline, enabling efficient long-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "14 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05367v1",
                "updated": "2025-10-06T20:54:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    20,
                    54,
                    44,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T20:54:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    20,
                    54,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation"
                },
                "summary": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache ."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Kaiyuan Deng"
                    },
                    {
                        "name": "Yushu Wu"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Bo Hui"
                    }
                ],
                "author_detail": {
                    "name": "Bo Hui"
                },
                "author": "Bo Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v2",
                "updated": "2025-10-06T17:09:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    9,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation. See code and resources at\nhttps://hdlm-colm.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation. See code and resources at\nhttps://hdlm-colm.github.io/"
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-André Noël"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-André Noël"
                },
                "author": "Pierre-André Noël",
                "arxiv_comment": "Published as a conference paper at COLM 2025 Website:\n  https://hdlm-colm.github.io/",
                "arxiv_journal_ref": "Second Conference on Language Modeling,\n  https://openreview.net/forum?id=rgq9BFXSFl (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19341v2",
                "updated": "2025-10-06T13:23:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    23,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-16T09:14:15Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks"
                },
                "summary": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework."
                },
                "authors": [
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Peng Qin"
                    },
                    {
                        "name": "Yueyue Zhang"
                    },
                    {
                        "name": "Pao Cheng"
                    },
                    {
                        "name": "Jun Lu"
                    },
                    {
                        "name": "Yifei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Wang"
                },
                "author": "Yifei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04646v1",
                "updated": "2025-10-06T09:49:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T09:49:14Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "title": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation"
                },
                "summary": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup."
                },
                "authors": [
                    {
                        "name": "Johanna Sommer"
                    },
                    {
                        "name": "John Rachwan"
                    },
                    {
                        "name": "Nils Fleischmann"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Bertrand Charpentier"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Charpentier"
                },
                "author": "Bertrand Charpentier",
                "arxiv_comment": "Accepted at the AI for Science Workshop @ NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v1",
                "updated": "2025-10-06T06:30:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04492v1",
                "updated": "2025-10-06T05:04:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T05:04:57Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "title": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks"
                },
                "summary": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Sumei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Sumei Sun"
                },
                "author": "Sumei Sun",
                "arxiv_comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v3",
                "updated": "2025-10-06T04:28:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    28,
                    5,
                    0,
                    279,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04476v1",
                "updated": "2025-10-06T04:24:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    24,
                    23,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T04:24:23Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    24,
                    23,
                    0,
                    279,
                    0
                ],
                "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space"
                },
                "summary": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x."
                },
                "authors": [
                    {
                        "name": "Tomas Figliolia"
                    },
                    {
                        "name": "Nicholas Alonso"
                    },
                    {
                        "name": "Rishi Iyer"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Beren Millidge"
                    }
                ],
                "author_detail": {
                    "name": "Beren Millidge"
                },
                "author": "Beren Millidge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18149v2",
                "updated": "2025-10-06T02:46:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    2,
                    46,
                    1,
                    0,
                    279,
                    0
                ],
                "published": "2024-03-26T23:17:05Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    23,
                    17,
                    5,
                    1,
                    86,
                    0
                ],
                "title": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC"
                },
                "summary": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Khai Nguyen"
                    },
                    {
                        "name": "Sam Schoedel"
                    },
                    {
                        "name": "Elakhya Nedumaran"
                    },
                    {
                        "name": "Moises Mata"
                    },
                    {
                        "name": "Brian Plancher"
                    },
                    {
                        "name": "Zachary Manchester"
                    }
                ],
                "author_detail": {
                    "name": "Zachary Manchester"
                },
                "author": "Zachary Manchester",
                "arxiv_comment": "First three authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v6",
                "updated": "2025-10-05T22:17:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    22,
                    17,
                    34,
                    6,
                    278,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00384v2",
                "updated": "2025-10-05T21:29:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    21,
                    29,
                    28,
                    6,
                    278,
                    0
                ],
                "published": "2025-05-31T04:27:22Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    27,
                    22,
                    5,
                    151,
                    0
                ],
                "title": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory"
                },
                "summary": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v2",
                "updated": "2025-10-05T18:13:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    18,
                    13,
                    39,
                    6,
                    278,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model has emerged as one of the most widely\nadopted generative models. However, serving diffusion models at the granularity\nof entire images introduces significant challenges, particularly under\nmulti-resolution workloads. First, image-level serving obstructs batching\nacross requests. Second, heterogeneous resolutions exhibit distinct locality\ncharacteristics, making it difficult to apply a uniform cache policy\neffectively.\n  To address these challenges, we present PatchedServe, a Patch Management\nFramework for SLO-Optimized Hybrid-Resolution Diffusion Serving. PatchedServe\nis the first SLO-optimized T2I diffusion serving framework designed to handle\nheterogeneous resolutions. Specifically, it incorporates a novel patch-based\nprocessing workflow that substantially improves throughput for\nhybrid-resolution inputs. Moreover, PatchedServe devises a patch-level cache\nreuse policy to fully exploit diffusion redundancies and integrates an\nSLO-aware scheduling algorithm with lightweight online latency prediction to\nimprove responsiveness. Our evaluation demonstrates that PatchedServe achieves\n30.1 % higher SLO satisfaction than the state-of-the-art diffusion serving\nsystem, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model has emerged as one of the most widely\nadopted generative models. However, serving diffusion models at the granularity\nof entire images introduces significant challenges, particularly under\nmulti-resolution workloads. First, image-level serving obstructs batching\nacross requests. Second, heterogeneous resolutions exhibit distinct locality\ncharacteristics, making it difficult to apply a uniform cache policy\neffectively.\n  To address these challenges, we present PatchedServe, a Patch Management\nFramework for SLO-Optimized Hybrid-Resolution Diffusion Serving. PatchedServe\nis the first SLO-optimized T2I diffusion serving framework designed to handle\nheterogeneous resolutions. Specifically, it incorporates a novel patch-based\nprocessing workflow that substantially improves throughput for\nhybrid-resolution inputs. Moreover, PatchedServe devises a patch-level cache\nreuse policy to fully exploit diffusion redundancies and integrates an\nSLO-aware scheduling algorithm with lightweight online latency prediction to\nimprove responsiveness. Our evaluation demonstrates that PatchedServe achieves\n30.1 % higher SLO satisfaction than the state-of-the-art diffusion serving\nsystem, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04188v1",
                "updated": "2025-10-05T13:01:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    13,
                    1,
                    8,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T13:01:08Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    13,
                    1,
                    8,
                    6,
                    278,
                    0
                ],
                "title": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for\n  Diffusion Transformers"
                },
                "summary": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Guantao Chen"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05176v1",
                "updated": "2025-10-05T12:09:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    9,
                    14,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T12:09:14Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    9,
                    14,
                    6,
                    278,
                    0
                ],
                "title": "PatternKV: Flattening KV Representation Expands Quantization Headroom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatternKV: Flattening KV Representation Expands Quantization Headroom"
                },
                "summary": "KV cache in autoregressive LLMs eliminates redundant recomputation but has\nemerged as the dominant memory and bandwidth bottleneck during inference,\nnotably with long contexts and test-time scaling. KV quantization is a key\nlever for reducing cache cost, but accuracy drops sharply as the native KV\ndistribution lacks flatness and thus maintains a wide quantization range. Prior\nwork focuses on isolating outliers, which caps their error but fails to flatten\nthe overall distribution, leaving performance fragile under low-bit settings.\nIn this work, we show that the K cache maintains a stable structure that\nevolves gradually with context, while the V cache carries latent semantic\nregularities. Building on these insights, we propose PatternKV, a\npattern-aligned residual quantization scheme. It mines representative pattern\nvectors online, aligns each KV vector to its nearest pattern, and quantizes\nonly the residual. This reshaping of the KV distribution flattens the\nquantization target and narrows its range, thereby improving the fidelity of\nlow-bit KV quantization. Across long-context and test-time scaling settings on\nmultiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08%\naverage 4-bit drop relative to FP16, improves test-time scaling accuracy by 10%\non average, and raises throughput by 1.4x while supporting 1.25x larger\nbatches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache in autoregressive LLMs eliminates redundant recomputation but has\nemerged as the dominant memory and bandwidth bottleneck during inference,\nnotably with long contexts and test-time scaling. KV quantization is a key\nlever for reducing cache cost, but accuracy drops sharply as the native KV\ndistribution lacks flatness and thus maintains a wide quantization range. Prior\nwork focuses on isolating outliers, which caps their error but fails to flatten\nthe overall distribution, leaving performance fragile under low-bit settings.\nIn this work, we show that the K cache maintains a stable structure that\nevolves gradually with context, while the V cache carries latent semantic\nregularities. Building on these insights, we propose PatternKV, a\npattern-aligned residual quantization scheme. It mines representative pattern\nvectors online, aligns each KV vector to its nearest pattern, and quantizes\nonly the residual. This reshaping of the KV distribution flattens the\nquantization target and narrows its range, thereby improving the fidelity of\nlow-bit KV quantization. Across long-context and test-time scaling settings on\nmultiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08%\naverage 4-bit drop relative to FP16, improves test-time scaling accuracy by 10%\non average, and raises throughput by 1.4x while supporting 1.25x larger\nbatches."
                },
                "authors": [
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Jiayi Shi"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Chuyi Tan"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04153v1",
                "updated": "2025-10-05T11:09:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    11,
                    9,
                    10,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T11:09:10Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    11,
                    9,
                    10,
                    6,
                    278,
                    0
                ],
                "title": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation"
                },
                "summary": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost."
                },
                "authors": [
                    {
                        "name": "Haoqi Wu"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Qiang Yan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yan"
                },
                "author": "Qiang Yan",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v3",
                "updated": "2025-10-05T08:34:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    8,
                    34,
                    30,
                    6,
                    278,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xudong Wang"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04033v1",
                "updated": "2025-10-05T04:52:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    4,
                    52,
                    26,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T04:52:26Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    4,
                    52,
                    26,
                    6,
                    278,
                    0
                ],
                "title": "A global log for medical AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A global log for medical AI"
                },
                "summary": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology."
                },
                "authors": [
                    {
                        "name": "Ayush Noori"
                    },
                    {
                        "name": "Adam Rodman"
                    },
                    {
                        "name": "Alan Karthikesalingam"
                    },
                    {
                        "name": "Bilal A. Mateen"
                    },
                    {
                        "name": "Christopher A. Longhurst"
                    },
                    {
                        "name": "Daniel Yang"
                    },
                    {
                        "name": "Dave deBronkart"
                    },
                    {
                        "name": "Gauden Galea"
                    },
                    {
                        "name": "Harold F. Wolf III"
                    },
                    {
                        "name": "Jacob Waxman"
                    },
                    {
                        "name": "Joshua C. Mandel"
                    },
                    {
                        "name": "Juliana Rotich"
                    },
                    {
                        "name": "Kenneth D. Mandl"
                    },
                    {
                        "name": "Maryam Mustafa"
                    },
                    {
                        "name": "Melissa Miles"
                    },
                    {
                        "name": "Nigam H. Shah"
                    },
                    {
                        "name": "Peter Lee"
                    },
                    {
                        "name": "Robert Korom"
                    },
                    {
                        "name": "Scott Mahoney"
                    },
                    {
                        "name": "Seth Hain"
                    },
                    {
                        "name": "Tien Yin Wong"
                    },
                    {
                        "name": "Trevor Mundel"
                    },
                    {
                        "name": "Vivek Natarajan"
                    },
                    {
                        "name": "Noa Dagan"
                    },
                    {
                        "name": "David A. Clifton"
                    },
                    {
                        "name": "Ran D. Balicer"
                    },
                    {
                        "name": "Isaac S. Kohane"
                    },
                    {
                        "name": "Marinka Zitnik"
                    }
                ],
                "author_detail": {
                    "name": "Marinka Zitnik"
                },
                "author": "Marinka Zitnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03851v1",
                "updated": "2025-10-04T15:52:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    52,
                    31,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T15:52:31Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    52,
                    31,
                    5,
                    277,
                    0
                ],
                "title": "Algorithm Generation via Creative Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Generation via Creative Ideation"
                },
                "summary": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%)."
                },
                "authors": [
                    {
                        "name": "Ruiying Ma"
                    },
                    {
                        "name": "Chieh-Jan Mike Liang"
                    },
                    {
                        "name": "Yanjie Gao"
                    },
                    {
                        "name": "Francis Y. Yan"
                    }
                ],
                "author_detail": {
                    "name": "Francis Y. Yan"
                },
                "author": "Francis Y. Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03834v1",
                "updated": "2025-10-04T15:25:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    25,
                    4,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T15:25:04Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    25,
                    4,
                    5,
                    277,
                    0
                ],
                "title": "Hybrid MBE Route to Adsorption-Controlled Growth of BaTiO3 Membranes\n  with Robust Polarization Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid MBE Route to Adsorption-Controlled Growth of BaTiO3 Membranes\n  with Robust Polarization Switching"
                },
                "summary": "Freestanding ferroelectric membranes are promising for flexible electronics,\nnonvolatile memory, photonics, and spintronics, but their synthesis is\nchallenged by the need for reproducibility with precise stoichiometric control.\nHere, we demonstrate the adsorption-controlled growth of single-crystalline,\nepitaxial BaTiO3 films by hybrid molecular beam epitaxy (MBE) on a binary oxide\nsacrificial layer. Using a simple water-droplet lift-off method, we obtained\nsubmillimeter- to millimeter-sized membranes that retained crystallinity, as\nconfirmed by high-resolution X-ray diffraction, and exhibited robust tetragonal\nsymmetry by Raman spectroscopy. Impedance spectroscopy confirmed a high\ndielectric constant of 1340, reflecting the robust dielectric response of the\nmembranes. Ferroelectric functionality was revealed by piezoresponse force\nmicroscopy (PFM) and further verified by polarization-electric field (P-E) loop\nmeasurements with Positive-Up-Negative-Down (PUND). The P-E loops exhibited a\nremnant polarization of 5 microC cm-2 and a coercive field of 63 kV cm-1. These\nresults were interpreted in relation to c- and a-domain configurations. These\nresults establish hybrid MBE as a generalizable route for producing\nstoichiometry-controlled ferroelectric membranes, enabling their integration\ninto next-generation flexible and multifunctional quantum oxide devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Freestanding ferroelectric membranes are promising for flexible electronics,\nnonvolatile memory, photonics, and spintronics, but their synthesis is\nchallenged by the need for reproducibility with precise stoichiometric control.\nHere, we demonstrate the adsorption-controlled growth of single-crystalline,\nepitaxial BaTiO3 films by hybrid molecular beam epitaxy (MBE) on a binary oxide\nsacrificial layer. Using a simple water-droplet lift-off method, we obtained\nsubmillimeter- to millimeter-sized membranes that retained crystallinity, as\nconfirmed by high-resolution X-ray diffraction, and exhibited robust tetragonal\nsymmetry by Raman spectroscopy. Impedance spectroscopy confirmed a high\ndielectric constant of 1340, reflecting the robust dielectric response of the\nmembranes. Ferroelectric functionality was revealed by piezoresponse force\nmicroscopy (PFM) and further verified by polarization-electric field (P-E) loop\nmeasurements with Positive-Up-Negative-Down (PUND). The P-E loops exhibited a\nremnant polarization of 5 microC cm-2 and a coercive field of 63 kV cm-1. These\nresults were interpreted in relation to c- and a-domain configurations. These\nresults establish hybrid MBE as a generalizable route for producing\nstoichiometry-controlled ferroelectric membranes, enabling their integration\ninto next-generation flexible and multifunctional quantum oxide devices."
                },
                "authors": [
                    {
                        "name": "S. Choo"
                    },
                    {
                        "name": "S. Varshney"
                    },
                    {
                        "name": "J. Shah"
                    },
                    {
                        "name": "A. K. Manjeshwar"
                    },
                    {
                        "name": "D. K. Lee"
                    },
                    {
                        "name": "K. A. Mkhoyan"
                    },
                    {
                        "name": "R. D. James"
                    },
                    {
                        "name": "B. Jalan"
                    }
                ],
                "author_detail": {
                    "name": "B. Jalan"
                },
                "author": "B. Jalan",
                "arxiv_comment": "22 pages 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03712v1",
                "updated": "2025-10-04T07:22:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T07:22:39Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems"
                },
                "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."
                },
                "authors": [
                    {
                        "name": "Jahidul Arafat"
                    },
                    {
                        "name": "Kh. M. Moniruzzaman"
                    },
                    {
                        "name": "Shamim Hossain"
                    },
                    {
                        "name": "Fariha Tasmin"
                    },
                    {
                        "name": "Kamrujjaman"
                    },
                    {
                        "name": "Ahsan Habib Tareq"
                    }
                ],
                "author_detail": {
                    "name": "Ahsan Habib Tareq"
                },
                "author": "Ahsan Habib Tareq",
                "arxiv_comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M15, 90B25, 68T05, 90C29",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; C.2.4; D.2.5; D.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v3",
                "updated": "2025-10-04T05:59:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    5,
                    59,
                    1,
                    5,
                    277,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v3",
                "updated": "2025-10-04T05:28:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    5,
                    28,
                    39,
                    5,
                    277,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v2",
                "updated": "2025-10-04T03:45:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    3,
                    45,
                    40,
                    5,
                    277,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "Taming Latency-Memory Trade-Off in MoE-Based LLM Serving via\n  Fine-Grained Expert Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Latency-Memory Trade-Off in MoE-Based LLM Serving via\n  Fine-Grained Expert Offloading"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs.\n  To tame the latency-memory trade-off in MoE serving, we present FineMoE, a\nfine-grained expert offloading system for MoE serving that achieves low\ninference latency with memory efficiency. We design FineMoE to extract\nfine-grained expert selection patterns from MoE models and semantic hints from\ninput prompts to efficiently guide expert prefetching, caching, and offloading\ndecisions. FineMoE is prototyped on top of HuggingFace Transformers and\ndeployed on a six-GPU testbed. Experiments with open-source MoE models and\nreal-world workloads show that FineMoE reduces inference latency by 47% and\nimproves expert hit rate by 39% over state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs.\n  To tame the latency-memory trade-off in MoE serving, we present FineMoE, a\nfine-grained expert offloading system for MoE serving that achieves low\ninference latency with memory efficiency. We design FineMoE to extract\nfine-grained expert selection patterns from MoE models and semantic hints from\ninput prompts to efficiently guide expert prefetching, caching, and offloading\ndecisions. FineMoE is prototyped on top of HuggingFace Transformers and\ndeployed on a six-GPU testbed. Experiments with open-source MoE models and\nreal-world workloads show that FineMoE reduces inference latency by 47% and\nimproves expert hit rate by 39% over state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03215v1",
                "updated": "2025-10-03T17:52:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:52:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models"
                },
                "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Zihan Min"
                    },
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Jichao Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03198v1",
                "updated": "2025-10-03T17:35:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:35:16Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    16,
                    4,
                    276,
                    0
                ],
                "title": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft"
                },
                "summary": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences."
                },
                "authors": [
                    {
                        "name": "Junchao Huang"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Boyao Han"
                    },
                    {
                        "name": "Shaoshuai Shi"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Li Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Li Jiang"
                },
                "author": "Li Jiang",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v2",
                "updated": "2025-10-03T15:37:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    37,
                    19,
                    4,
                    276,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures; Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02866v1",
                "updated": "2025-10-03T10:06:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    6,
                    44,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T10:06:44Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    6,
                    44,
                    4,
                    276,
                    0
                ],
                "title": "Life Estimation of HVDC Cable Insulation under Load Cycles: from\n  Macroscopic to Microscopic Charge Conduction Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Life Estimation of HVDC Cable Insulation under Load Cycles: from\n  Macroscopic to Microscopic Charge Conduction Modelling"
                },
                "summary": "This paper goes one step forward in the life estimation of HVDC cable\ninsulation under load cycles by introducing for the first time a microscopic\nmodel of charge conduction and transport i.e., Bipolar Charge Transport BCT\nmodel for electric field calculation inside the insulation thickness. The paper\nfirstly includes the development and the validation of BCT model with that\nfound in literature. Then, the parameters of the developed BCT model are\noptimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed\nby the integration of the developed, validated and optimized model into the\nelectric field calculation for life estimation of a 500 kV DC-XLPE insulated\ncable subjected to Type Test load cycles according to Cigre Techical Brochure\n852. The developed microscopic model is compared to the macroscopic models\nalready found in the literature. The microscopic model shows a comparable\nelectric field inversion similarly to macroscopic models. However, the behavior\nof the microscopic model is noticed to be different under heating and cooling\nload cycles. In hot cable, the maximum electric field stabilizes at different\namplitude and position inside the insulation thickness in both models. This\ninvestigation has been carried out in the framework of the HEU-NEWGEN research\nproject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper goes one step forward in the life estimation of HVDC cable\ninsulation under load cycles by introducing for the first time a microscopic\nmodel of charge conduction and transport i.e., Bipolar Charge Transport BCT\nmodel for electric field calculation inside the insulation thickness. The paper\nfirstly includes the development and the validation of BCT model with that\nfound in literature. Then, the parameters of the developed BCT model are\noptimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed\nby the integration of the developed, validated and optimized model into the\nelectric field calculation for life estimation of a 500 kV DC-XLPE insulated\ncable subjected to Type Test load cycles according to Cigre Techical Brochure\n852. The developed microscopic model is compared to the macroscopic models\nalready found in the literature. The microscopic model shows a comparable\nelectric field inversion similarly to macroscopic models. However, the behavior\nof the microscopic model is noticed to be different under heating and cooling\nload cycles. In hot cable, the maximum electric field stabilizes at different\namplitude and position inside the insulation thickness in both models. This\ninvestigation has been carried out in the framework of the HEU-NEWGEN research\nproject."
                },
                "authors": [
                    {
                        "name": "Bassel Diban"
                    },
                    {
                        "name": "Giovanni Mazzanti"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Mazzanti"
                },
                "author": "Giovanni Mazzanti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02758v1",
                "updated": "2025-10-03T06:43:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:43:24Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "title": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling"
                },
                "summary": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Chuheng Du"
                    },
                    {
                        "name": "Renyuan Liu"
                    },
                    {
                        "name": "Shuochao Yao"
                    },
                    {
                        "name": "Dingtian Yan"
                    },
                    {
                        "name": "Jiang Liao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Accepted by EuroSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02750v1",
                "updated": "2025-10-03T06:27:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:27:33Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "title": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models"
                },
                "summary": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks."
                },
                "authors": [
                    {
                        "name": "Lihua Zhou"
                    },
                    {
                        "name": "Mao Ye"
                    },
                    {
                        "name": "Shuaifeng Li"
                    },
                    {
                        "name": "Nianxin Li"
                    },
                    {
                        "name": "Jinlin Wu"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Lei Deng"
                    },
                    {
                        "name": "Hongbin Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02084v2",
                "updated": "2025-10-03T05:10:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    5,
                    10,
                    2,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-02T14:50:50Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    50,
                    50,
                    3,
                    275,
                    0
                ],
                "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting"
                },
                "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries."
                },
                "authors": [
                    {
                        "name": "Kuiye Ding"
                    },
                    {
                        "name": "Fanda Fan"
                    },
                    {
                        "name": "Zheya Wang"
                    },
                    {
                        "name": "Hongxiao Li"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Chunjie Luo"
                    },
                    {
                        "name": "Jianfeng Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Zhan"
                },
                "author": "Jianfeng Zhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25188v2",
                "updated": "2025-10-03T00:40:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    0,
                    40,
                    49,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-29T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    59,
                    54,
                    0,
                    272,
                    0
                ],
                "title": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Learnable Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Learnable Parallel Decoding"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache."
                },
                "authors": [
                    {
                        "name": "Wenrui Bao"
                    },
                    {
                        "name": "Zhiben Chen"
                    },
                    {
                        "name": "Dan Xu"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02613v1",
                "updated": "2025-10-02T23:16:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    23,
                    16,
                    35,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T23:16:35Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    23,
                    16,
                    35,
                    3,
                    275,
                    0
                ],
                "title": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts\n  Models"
                },
                "summary": "Mixture-of-Experts (MoE) models promise efficient scaling of large language\nmodels (LLMs) by activating only a small subset of experts per token, but their\nparallelized inference pipelines make elastic serving challenging. Existing\nstrategies fall short: horizontal scaling provisions entire replicas of the\ncurrent configuration, often tens to hundreds of accelerators, leading to\ncoarse granularity, long provisioning delays, and costly overprovisioning.\nVertical scaling offers finer adjustments but typically requires instance\nrestarts, incurring downtime. These limitations make current approaches\nill-suited for the bursty, short-lived traffic patterns common in cloud\ndeployments.\n  We present ElasticMoE, an elastic scaling framework for MoE LLMs that\nachieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE\ndecouples inference execution from memory operations, enabling scaling steps to\nproceed concurrently with serving. An HBM Management Module (HMM) reuses\nweights and KV caches via zero-copy remapping, while high-bandwidth\npeer-to-peer transfers bring newly added accelerators online without\ninterrupting service. A virtual memory based expert redistribution mechanism\nmigrates MoE experts without costly buffer reallocations, reducing peak memory\nusage during expert parallelism reconfiguration.\n  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that\nElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput\nduring scaling, and significantly improves SLO attainment compared to\nbaselines. By enabling fine-grained, concurrent scaling with minimal\ndisruption, ElasticMoE advances the practicality of deploying massive MoE LLMs\nin dynamic cloud environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models promise efficient scaling of large language\nmodels (LLMs) by activating only a small subset of experts per token, but their\nparallelized inference pipelines make elastic serving challenging. Existing\nstrategies fall short: horizontal scaling provisions entire replicas of the\ncurrent configuration, often tens to hundreds of accelerators, leading to\ncoarse granularity, long provisioning delays, and costly overprovisioning.\nVertical scaling offers finer adjustments but typically requires instance\nrestarts, incurring downtime. These limitations make current approaches\nill-suited for the bursty, short-lived traffic patterns common in cloud\ndeployments.\n  We present ElasticMoE, an elastic scaling framework for MoE LLMs that\nachieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE\ndecouples inference execution from memory operations, enabling scaling steps to\nproceed concurrently with serving. An HBM Management Module (HMM) reuses\nweights and KV caches via zero-copy remapping, while high-bandwidth\npeer-to-peer transfers bring newly added accelerators online without\ninterrupting service. A virtual memory based expert redistribution mechanism\nmigrates MoE experts without costly buffer reallocations, reducing peak memory\nusage during expert parallelism reconfiguration.\n  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that\nElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput\nduring scaling, and significantly improves SLO attainment compared to\nbaselines. By enabling fine-grained, concurrent scaling with minimal\ndisruption, ElasticMoE advances the practicality of deploying massive MoE LLMs\nin dynamic cloud environments."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Haley Li"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qintao Zhang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "arxiv_affiliation": "Huawei Technologies Canada",
                "author": "Zhenan Fan",
                "arxiv_comment": "19 pages, 15 figures, Under Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v5",
                "updated": "2025-10-02T19:25:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    19,
                    25,
                    29,
                    3,
                    275,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence after the aLoRA is invoked. This change crucially allows\naLoRA to accept the base model's KV cache of the input string, meaning that\naLoRA can be instantly activated whenever needed in a chain without recomputing\nthe prior keys and values. This enables building what we call intrinsics, i.e.\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\ntrain a set of aLoRA-based intrinsics models, demonstrating competitive\naccuracy with standard LoRA while significantly improving inference efficiency.\nWe contributed our Activated LoRA implementation to the Huggingface PEFT\nlibrary https://github.com/huggingface/peft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence after the aLoRA is invoked. This change crucially allows\naLoRA to accept the base model's KV cache of the input string, meaning that\naLoRA can be instantly activated whenever needed in a chain without recomputing\nthe prior keys and values. This enables building what we call intrinsics, i.e.\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\ntrain a set of aLoRA-based intrinsics models, demonstrating competitive\naccuracy with standard LoRA while significantly improving inference efficiency.\nWe contributed our Activated LoRA implementation to the Huggingface PEFT\nlibrary https://github.com/huggingface/peft."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v4",
                "updated": "2025-10-02T19:09:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    19,
                    9,
                    19,
                    3,
                    275,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v2",
                "updated": "2025-10-02T18:38:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    18,
                    38,
                    0,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "arxiv_comment": "project page: https://soroush-mim.github.io/projects/evict3r/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v3",
                "updated": "2025-10-02T18:20:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    18,
                    20,
                    18,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02312v1",
                "updated": "2025-10-02T17:59:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation"
                },
                "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference."
                },
                "authors": [
                    {
                        "name": "Anna Kuzina"
                    },
                    {
                        "name": "Maciej Pioro"
                    },
                    {
                        "name": "Paul N. Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03346v1",
                "updated": "2025-10-02T16:01:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    1,
                    54,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:01:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    1,
                    54,
                    3,
                    275,
                    0
                ],
                "title": "KVComm: Enabling Efficient LLM Communication through Selective KV\n  Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComm: Enabling Efficient LLM Communication through Selective KV\n  Sharing"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-agent\nsystems, where effective inter-model communication is crucial. Existing\ncommunication protocols either rely on natural language, incurring high\ninference costs and information loss, or on hidden states, which suffer from\ninformation concentration bias and inefficiency. To address these limitations,\nwe propose KVComm, a novel communication framework that enables efficient\ncommunication between LLMs through selective sharing of KV pairs. KVComm\nleverages the rich information encoded in the KV pairs while avoiding the\npitfalls of hidden states. We introduce a KV layer-wise selection strategy\nbased on attention importance scores with a Gaussian prior to identify the most\ninformative KV pairs for communication. Extensive experiments across diverse\ntasks and model pairs demonstrate that KVComm achieves comparable performance\nto the upper-bound method, which directly merges inputs to one model without\nany communication, while transmitting as few as 30\\% of layers' KV pairs. Our\nstudy highlights the potential of KV pairs as an effective medium for inter-LLM\ncommunication, paving the way for scalable and efficient multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-agent\nsystems, where effective inter-model communication is crucial. Existing\ncommunication protocols either rely on natural language, incurring high\ninference costs and information loss, or on hidden states, which suffer from\ninformation concentration bias and inefficiency. To address these limitations,\nwe propose KVComm, a novel communication framework that enables efficient\ncommunication between LLMs through selective sharing of KV pairs. KVComm\nleverages the rich information encoded in the KV pairs while avoiding the\npitfalls of hidden states. We introduce a KV layer-wise selection strategy\nbased on attention importance scores with a Gaussian prior to identify the most\ninformative KV pairs for communication. Extensive experiments across diverse\ntasks and model pairs demonstrate that KVComm achieves comparable performance\nto the upper-bound method, which directly merges inputs to one model without\nany communication, while transmitting as few as 30\\% of layers' KV pairs. Our\nstudy highlights the potential of KV pairs as an effective medium for inter-LLM\ncommunication, paving the way for scalable and efficient multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Xiangyu Shi"
                    },
                    {
                        "name": "Marco Chiesa"
                    },
                    {
                        "name": "Gerald Q. Maguire Jr."
                    },
                    {
                        "name": "Dejan Kostic"
                    }
                ],
                "author_detail": {
                    "name": "Dejan Kostic"
                },
                "author": "Dejan Kostic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v2",
                "updated": "2025-10-02T14:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    42,
                    41,
                    3,
                    275,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine caching timings and adopting handcrafted rules for\nmulti-step cache utilization. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail to cope\nwith diverse samples. In this paper, a strong sample-specific correlation is\nrevealed between the variation patterns of the shallow-layer feature\ndifferences in the diffusion model and those of deep-layer features. Moreover,\nwe have observed that the features from different model layers form similar\ntrajectories. Based on these observations, we present DiCache, a novel\ntraining-free adaptive caching strategy for accelerating diffusion models at\nruntime, answering both when and how to cache within a unified framework.\nSpecifically, DiCache is composed of two principal components: (1) Online Probe\nProfiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly\nindicator for the caching error in real time, enabling the model to dynamically\ncustomize the caching schedule for each sample. (2) Dynamic Cache Trajectory\nAlignment adaptively approximates the deep-layer feature output from multi-step\nhistorical caches based on the shallow-layer feature trajectory, facilitating\nhigher visual quality. Extensive experiments validate DiCache's capability in\nachieving higher efficiency and improved fidelity over state-of-the-art\napproaches on various leading diffusion models including WAN 2.1, HunyuanVideo\nand Flux.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine caching timings and adopting handcrafted rules for\nmulti-step cache utilization. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail to cope\nwith diverse samples. In this paper, a strong sample-specific correlation is\nrevealed between the variation patterns of the shallow-layer feature\ndifferences in the diffusion model and those of deep-layer features. Moreover,\nwe have observed that the features from different model layers form similar\ntrajectories. Based on these observations, we present DiCache, a novel\ntraining-free adaptive caching strategy for accelerating diffusion models at\nruntime, answering both when and how to cache within a unified framework.\nSpecifically, DiCache is composed of two principal components: (1) Online Probe\nProfiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly\nindicator for the caching error in real time, enabling the model to dynamically\ncustomize the caching schedule for each sample. (2) Dynamic Cache Trajectory\nAlignment adaptively approximates the deep-layer feature output from multi-step\nhistorical caches based on the shallow-layer feature trajectory, facilitating\nhigher visual quality. Extensive experiments validate DiCache's capability in\nachieving higher efficiency and improved fidelity over state-of-the-art\napproaches on various leading diffusion models including WAN 2.1, HunyuanVideo\nand Flux."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "Project Page: https://bujiazi.github.io/dicache.github.io/ Code:\n  https://github.com/Bujiazi/DiCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v3",
                "updated": "2025-10-02T14:09:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    9,
                    3,
                    3,
                    275,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "arxiv_journal_ref": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01884v1",
                "updated": "2025-10-02T10:49:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    49,
                    54,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T10:49:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    49,
                    54,
                    3,
                    275,
                    0
                ],
                "title": "Study of the $^{20}$Ne($p,γ$)$^{21}$Na reaction at LUNA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the $^{20}$Ne($p,γ$)$^{21}$Na reaction at LUNA"
                },
                "summary": "The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al\nisotopes. The $^{20}$Ne($p,\\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the\nfirst and slowest reaction of the NeNa cycle and it controls the speed at which\nthe entire cycle proceeds. At the state of the art, the uncertainty on the\n20Ne(p,{\\gamma})21Na reaction rate affects the production of the elements in\nthe NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK,\nthe rate is dominated by the 366 keV resonance corresponding to the excited\nstate of EX = 2797.5 keV and by the direct capture component. The present study\nfocus on the study of the 366 keV resonance and the direct capture below 400\nkeV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction has been measured using the intense\nproton beam delivered by the LUNA 400 kV accelerator and a windowless\ndifferential-pumping gas target. The products of the reaction are detected with\ntwo high-purity germanium detectors. The experimental details and preliminary\nresults on the 366 keV resonance and on the direct capture component at very\nlow energies will be shown, together with their possible impact on the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al\nisotopes. The $^{20}$Ne($p,\\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the\nfirst and slowest reaction of the NeNa cycle and it controls the speed at which\nthe entire cycle proceeds. At the state of the art, the uncertainty on the\n20Ne(p,{\\gamma})21Na reaction rate affects the production of the elements in\nthe NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK,\nthe rate is dominated by the 366 keV resonance corresponding to the excited\nstate of EX = 2797.5 keV and by the direct capture component. The present study\nfocus on the study of the 366 keV resonance and the direct capture below 400\nkeV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction has been measured using the intense\nproton beam delivered by the LUNA 400 kV accelerator and a windowless\ndifferential-pumping gas target. The products of the reaction are detected with\ntwo high-purity germanium detectors. The experimental details and preliminary\nresults on the 366 keV resonance and on the direct capture component at very\nlow energies will be shown, together with their possible impact on the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction rate."
                },
                "authors": [
                    {
                        "name": "A. Caciolli"
                    }
                ],
                "author_detail": {
                    "name": "A. Caciolli"
                },
                "arxiv_affiliation": "on behalf of the LUNA collaboration",
                "author": "A. Caciolli",
                "arxiv_doi": "10.1051/epjconf/202429207005",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/epjconf/202429207005",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.01884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "EPJ Web Conf., 292 (2024) 07005",
                "arxiv_primary_category": {
                    "term": "nucl-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20211v2",
                "updated": "2025-10-02T04:11:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    4,
                    11,
                    7,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-26T16:52:40Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    52,
                    40,
                    0,
                    146,
                    0
                ],
                "title": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection"
                },
                "summary": "Fine-tuning large foundation models is essential for building expert models\ntailored to specialized tasks and domains, but fully updating billions of\nparameters is computationally prohibitive. Reducing the number of trainable\nparameters using parameter-efficient fine-tuning is therefore crucial not only\nto reduce training costs but also to mitigate storage, caching, and serving\noverheads during deployment. Prior works, such as Singular Vectors-guided\nFine-Tuning, have shown that exploiting the geometry of pre-trained weights can\nsignificantly improve parameter-efficiency, but they lack a solid theoretical\nfoundation. In this paper, we introduce Parameter-efficient Fine-tuning with\nColumn Space Projection (PiCa), a novel theoretically grounded PEFT method. We\nprove that projecting gradients onto the principal column space of pre-trained\nweights provides an effective inductive bias for adaptation and further enhance\nparameter efficiency through a novel weight-sharing strategy. Across diverse\nNLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines\nunder comparable or smaller parameter budgets, demonstrating both theoretical\nrigor and practical effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large foundation models is essential for building expert models\ntailored to specialized tasks and domains, but fully updating billions of\nparameters is computationally prohibitive. Reducing the number of trainable\nparameters using parameter-efficient fine-tuning is therefore crucial not only\nto reduce training costs but also to mitigate storage, caching, and serving\noverheads during deployment. Prior works, such as Singular Vectors-guided\nFine-Tuning, have shown that exploiting the geometry of pre-trained weights can\nsignificantly improve parameter-efficiency, but they lack a solid theoretical\nfoundation. In this paper, we introduce Parameter-efficient Fine-tuning with\nColumn Space Projection (PiCa), a novel theoretically grounded PEFT method. We\nprove that projecting gradients onto the principal column space of pre-trained\nweights provides an effective inductive bias for adaptation and further enhance\nparameter efficiency through a novel weight-sharing strategy. Across diverse\nNLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines\nunder comparable or smaller parameter budgets, demonstrating both theoretical\nrigor and practical effectiveness."
                },
                "authors": [
                    {
                        "name": "Junseo Hwang"
                    },
                    {
                        "name": "Wonguk Cho"
                    },
                    {
                        "name": "Taesup Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taesup Kim"
                },
                "author": "Taesup Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07447v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07447v4",
                "updated": "2025-10-01T20:30:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    20,
                    30,
                    18,
                    2,
                    274,
                    0
                ],
                "published": "2024-11-12T00:10:34Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    0,
                    10,
                    34,
                    1,
                    317,
                    0
                ],
                "title": "Faster LLM Inference using DBMS-Inspired Preemption and Cache\n  Replacement Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster LLM Inference using DBMS-Inspired Preemption and Cache\n  Replacement Policies"
                },
                "summary": "LLMs are increasingly used world-wide from daily tasks to agentic systems and\ndata analytics, requiring significant GPU resources. LLM inference systems,\nhowever, are slow compared to database systems, and inference performance and\nmechanism have been often regarded as a black box, limiting the expansion of\nthe use of LLMs inside databases and other performance-critical applications.\nThis paper first analyzes the LLM inference performance and focuses on a data\nmanagement issue inside LLM inference. We find that inference systems lack an\nadequate resource cost model and optimization strategy to schedule requests\nwith their intermediate results in a cache reside in GPU memory when executing\nmultiple concurrent inference requests. We adapt classic database techniques by\nbuilding cost models for concurrent inference requests and a new cache\nreplacement policy tailored for LLM inference, which can substantially save GPU\ncosts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used world-wide from daily tasks to agentic systems and\ndata analytics, requiring significant GPU resources. LLM inference systems,\nhowever, are slow compared to database systems, and inference performance and\nmechanism have been often regarded as a black box, limiting the expansion of\nthe use of LLMs inside databases and other performance-critical applications.\nThis paper first analyzes the LLM inference performance and focuses on a data\nmanagement issue inside LLM inference. We find that inference systems lack an\nadequate resource cost model and optimization strategy to schedule requests\nwith their intermediate results in a cache reside in GPU memory when executing\nmultiple concurrent inference requests. We adapt classic database techniques by\nbuilding cost models for concurrent inference requests and a new cache\nreplacement policy tailored for LLM inference, which can substantially save GPU\ncosts."
                },
                "authors": [
                    {
                        "name": "Kyoungmin Kim"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Kijae Hong"
                    },
                    {
                        "name": "Anastasia Ailamaki"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Ailamaki"
                },
                "author": "Anastasia Ailamaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07447v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07447v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v2",
                "updated": "2025-10-01T19:06:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    19,
                    6,
                    10,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v2",
                "updated": "2025-10-01T18:55:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    55,
                    20,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01336v1",
                "updated": "2025-10-01T18:04:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    4,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T18:04:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    4,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiSpec: Hierarchical Speculative Decoding for LLMs"
                },
                "summary": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Sujay Sanghavi"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00948v1",
                "updated": "2025-10-01T14:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    21,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T14:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    21,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution"
                },
                "summary": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR."
                },
                "authors": [
                    {
                        "name": "Ziqing Zhang"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Xi Li"
                    },
                    {
                        "name": "Yucong Chen"
                    },
                    {
                        "name": "Bingnan Duan"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhang"
                },
                "author": "Yulun Zhang",
                "arxiv_comment": "Code will be available at https://github.com/Kai-Liu001/InfVSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26432v2",
                "updated": "2025-10-01T11:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    26,
                    36,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size"
                },
                "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs."
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Yuto Karashima"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00636v1",
                "updated": "2025-10-01T08:12:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    12,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T08:12:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    12,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution"
                },
                "summary": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Maximilian Jeblick"
                    },
                    {
                        "name": "Simon Jégou"
                    }
                ],
                "author_detail": {
                    "name": "Simon Jégou"
                },
                "author": "Simon Jégou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v1",
                "updated": "2025-10-01T06:38:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Panagiotis Karras"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Jignesh M. Patel"
                },
                "author": "Jignesh M. Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00536v1",
                "updated": "2025-10-01T05:37:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    37,
                    54,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T05:37:54Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    37,
                    54,
                    2,
                    274,
                    0
                ],
                "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness"
                },
                "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance."
                },
                "authors": [
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Haoyi Qiu"
                    },
                    {
                        "name": "Yutong Dai"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25454v2",
                "updated": "2025-10-01T05:09:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    9,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T20:00:29Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    0,
                    29,
                    0,
                    272,
                    0
                ],
                "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search"
                },
                "summary": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation."
                },
                "authors": [
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Aaron Tu"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01290v1",
                "updated": "2025-10-01T04:09:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    9,
                    2,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T04:09:02Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    9,
                    2,
                    2,
                    274,
                    0
                ],
                "title": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning\n  Models"
                },
                "summary": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Akshat Ramachandran"
                    },
                    {
                        "name": "Marina Neseem"
                    },
                    {
                        "name": "Charbel Sakr"
                    },
                    {
                        "name": "Rangharajan Venkatesan"
                    },
                    {
                        "name": "Brucek Khailany"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01289v1",
                "updated": "2025-10-01T02:56:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    56,
                    59,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T02:56:59Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    56,
                    59,
                    2,
                    274,
                    0
                ],
                "title": "Detailed Derivation of the Scalar Explicit Expressions Governing the\n  Electric Field, Current Density, and Volumetric Power Density in the Four\n  Types of Linear Divergent MHD Channels Under a Unidirectional Applied\n  Magnetic Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detailed Derivation of the Scalar Explicit Expressions Governing the\n  Electric Field, Current Density, and Volumetric Power Density in the Four\n  Types of Linear Divergent MHD Channels Under a Unidirectional Applied\n  Magnetic Field"
                },
                "summary": "The current study belongs to the field of applied mathematics in plasma\nphysics and electric power, where mathematical analysis of the algebraic\nequations governing the electric field vector, and the electric-current density\nfield vector within a Magnetohydrodynamic (MHD) linear two-dimensional\ndivergent supersonic channel is utilized to derive analytical expressions for\nthese important fields, as well as closed-form equations for the volumetric\npower density (output electric power per unit volume of the plasma channel).\nThe expressions presented here describe analytically the operation of the MHD\nchannel as an electric power source within an Open-Cycle Magnetohydrodynamic\n(OCMHD) generator. The four common types of the MHD linear channels are covered\nhere: namely, (1) continuous-electrode Faraday channel, (2) linear Hall\nchannel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode\nchannel. The mathematical results, their detailed derivation, and the companion\ngraphical illustrations aid in making a proper decision regarding which channel\ntype is the most suitable for a given application.Under typical operational\nconditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000\nm/s plasma speed, as well as an optimized load factor of 0.5, we estimate the\nfollowing numerical values (unsigned magnitudes) for the continuous-electrode\nFaraday channel (with a Hall parameter of 1): useful electric field (across the\nexternal electric load): 5 kV/m, useful electric current-density (between the\nterminal electrodes within the channel): 12.5 kA/m2 , volumetric power density\n(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric\nefficiency (for the electric field or voltage): 50%. For the Halllinear channel\n(with a Hall parameter of 5), these quantitative performance values become25\nkV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current study belongs to the field of applied mathematics in plasma\nphysics and electric power, where mathematical analysis of the algebraic\nequations governing the electric field vector, and the electric-current density\nfield vector within a Magnetohydrodynamic (MHD) linear two-dimensional\ndivergent supersonic channel is utilized to derive analytical expressions for\nthese important fields, as well as closed-form equations for the volumetric\npower density (output electric power per unit volume of the plasma channel).\nThe expressions presented here describe analytically the operation of the MHD\nchannel as an electric power source within an Open-Cycle Magnetohydrodynamic\n(OCMHD) generator. The four common types of the MHD linear channels are covered\nhere: namely, (1) continuous-electrode Faraday channel, (2) linear Hall\nchannel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode\nchannel. The mathematical results, their detailed derivation, and the companion\ngraphical illustrations aid in making a proper decision regarding which channel\ntype is the most suitable for a given application.Under typical operational\nconditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000\nm/s plasma speed, as well as an optimized load factor of 0.5, we estimate the\nfollowing numerical values (unsigned magnitudes) for the continuous-electrode\nFaraday channel (with a Hall parameter of 1): useful electric field (across the\nexternal electric load): 5 kV/m, useful electric current-density (between the\nterminal electrodes within the channel): 12.5 kA/m2 , volumetric power density\n(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric\nefficiency (for the electric field or voltage): 50%. For the Halllinear channel\n(with a Hall parameter of 5), these quantitative performance values become25\nkV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%."
                },
                "authors": [
                    {
                        "name": "Osama A. Marzouk"
                    }
                ],
                "author_detail": {
                    "name": "Osama A. Marzouk"
                },
                "author": "Osama A. Marzouk",
                "arxiv_doi": "10.37256/cm.6420256918",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.37256/cm.6420256918",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.01289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "41 pages, 8 figures, 4 tables, published journal article,\n  peer-reviewed, open access",
                "arxiv_journal_ref": "Contemporary Mathematics. volume 6, issue 4, pages 4060-4100,\n  https://ojs.wiserpub.com/index.php/CM/article/view/6918 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00A79, 03H10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05131v1",
                "updated": "2025-10-01T01:28:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    28,
                    59,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T01:28:59Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    28,
                    59,
                    2,
                    274,
                    0
                ],
                "title": "Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task\n  Discovery"
                },
                "summary": "Head Start programs utilizing GoEngage face significant challenges when new\nor rotating staff attempt to locate appropriate Tasks (modules) on the platform\nhomepage. These difficulties arise from domain-specific jargon (e.g., IFPA,\nDRDP), system-specific nomenclature (e.g., Application Pool), and the inherent\nlimitations of lexical search in handling typos and varied word ordering. We\npropose a pragmatic hybrid semantic search system that synergistically combines\nlightweight typo-tolerant lexical retrieval, embedding-based vector similarity,\nand constrained large language model (LLM) re-ranking. Our approach leverages\nthe organization's existing Task Repository and Knowledge Base infrastructure\nwhile ensuring trustworthiness through low false-positive rates, evolvability\nto accommodate terminological changes, and economic efficiency via intelligent\ncaching, shortlist generation, and graceful degradation mechanisms. We provide\na comprehensive framework detailing required resources, a phased implementation\nstrategy with concrete milestones, an offline evaluation protocol utilizing\ncurated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online\nmeasurement methodology incorporating query success metrics, zero-result rates,\nand dwell-time proxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head Start programs utilizing GoEngage face significant challenges when new\nor rotating staff attempt to locate appropriate Tasks (modules) on the platform\nhomepage. These difficulties arise from domain-specific jargon (e.g., IFPA,\nDRDP), system-specific nomenclature (e.g., Application Pool), and the inherent\nlimitations of lexical search in handling typos and varied word ordering. We\npropose a pragmatic hybrid semantic search system that synergistically combines\nlightweight typo-tolerant lexical retrieval, embedding-based vector similarity,\nand constrained large language model (LLM) re-ranking. Our approach leverages\nthe organization's existing Task Repository and Knowledge Base infrastructure\nwhile ensuring trustworthiness through low false-positive rates, evolvability\nto accommodate terminological changes, and economic efficiency via intelligent\ncaching, shortlist generation, and graceful degradation mechanisms. We provide\na comprehensive framework detailing required resources, a phased implementation\nstrategy with concrete milestones, an offline evaluation protocol utilizing\ncurated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online\nmeasurement methodology incorporating query success metrics, zero-result rates,\nand dwell-time proxies."
                },
                "authors": [
                    {
                        "name": "Bowen Wei"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Wei"
                },
                "author": "Bowen Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02388v1",
                "updated": "2025-09-30T22:19:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    22,
                    19,
                    44,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T22:19:44Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    22,
                    19,
                    44,
                    1,
                    273,
                    0
                ],
                "title": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance on general\nQuestion Answering (QA), yet they often struggle in domain-specific scenarios\nwhere accurate and up-to-date information is required. Retrieval-Augmented\nGeneration (RAG) addresses this limitation by enriching LLMs with external\nknowledge, but existing systems primarily rely on unstructured documents, while\nlargely overlooking relational databases, which provide precise, timely, and\nefficiently queryable factual information, serving as indispensable\ninfrastructure in domains such as finance, healthcare, and scientific research.\nMotivated by this gap, we conduct a systematic analysis that reveals three\ncentral observations: (i) databases and documents offer complementary strengths\nacross queries, (ii) naively combining both sources introduces noise and cost\nwithout consistent accuracy gains, and (iii) selecting the most suitable source\nfor each query is crucial to balance effectiveness and efficiency. We further\nobserve that query types show consistent regularities in their alignment with\nretrieval paths, suggesting that routing decisions can be effectively guided by\nsystematic rules that capture these patterns. Building on these insights, we\npropose a rule-driven routing framework. A routing agent scores candidate\naugmentation paths based on explicit rules and selects the most suitable one; a\nrule-making expert agent refines the rules over time using QA feedback to\nmaintain adaptability; and a path-level meta-cache reuses past routing\ndecisions for semantically similar queries to reduce latency and cost.\nExperiments on three QA benchmarks demonstrate that our framework consistently\noutperforms static strategies and learned routing baselines, achieving higher\naccuracy while maintaining moderate computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance on general\nQuestion Answering (QA), yet they often struggle in domain-specific scenarios\nwhere accurate and up-to-date information is required. Retrieval-Augmented\nGeneration (RAG) addresses this limitation by enriching LLMs with external\nknowledge, but existing systems primarily rely on unstructured documents, while\nlargely overlooking relational databases, which provide precise, timely, and\nefficiently queryable factual information, serving as indispensable\ninfrastructure in domains such as finance, healthcare, and scientific research.\nMotivated by this gap, we conduct a systematic analysis that reveals three\ncentral observations: (i) databases and documents offer complementary strengths\nacross queries, (ii) naively combining both sources introduces noise and cost\nwithout consistent accuracy gains, and (iii) selecting the most suitable source\nfor each query is crucial to balance effectiveness and efficiency. We further\nobserve that query types show consistent regularities in their alignment with\nretrieval paths, suggesting that routing decisions can be effectively guided by\nsystematic rules that capture these patterns. Building on these insights, we\npropose a rule-driven routing framework. A routing agent scores candidate\naugmentation paths based on explicit rules and selects the most suitable one; a\nrule-making expert agent refines the rules over time using QA feedback to\nmaintain adaptability; and a path-level meta-cache reuses past routing\ndecisions for semantically similar queries to reduce latency and cost.\nExperiments on three QA benchmarks demonstrate that our framework consistently\noutperforms static strategies and learned routing baselines, achieving higher\naccuracy while maintaining moderate computational cost."
                },
                "authors": [
                    {
                        "name": "Haoyue Bai"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Shengyu Chen"
                    },
                    {
                        "name": "Zhengzhang Chen"
                    },
                    {
                        "name": "Lu-An Tang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00294v1",
                "updated": "2025-09-30T21:28:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    28,
                    4,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T21:28:04Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    28,
                    4,
                    1,
                    273,
                    0
                ],
                "title": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for\n  Diffusion Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for\n  Diffusion Large Language Models"
                },
                "summary": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of\nlanguage modeling beyond autoregressive next-token prediction. Thanks to their\nbidirectional attention mechanism, DLLMs are more capable of capturing the\nconnection of context, and thus show unique advantages in challenges like the\nfamous \"reversal curse\" or learning under data-constrained scenarios. However,\nthis bidirectional nature also brings an obstacle that DLLMs are not inherently\ncompatible with KV Cache, and consequently, the inference efficiency is not\ncompetitive compared with autoregressive models. Taking advantage of their\ninherent capability of multi-token prediction, existing parallel decoding\nalgorithms can speed up the DLLM inference, but at the cost of non-negligible\nperformance degradation. To overcome this challenge, we introduce Free\nDraft-and-Verification (Freedave), a novel fast sampling algorithm tailored for\nDLLMs that achieves lossless parallel decoding. Specifically, we propose a\npipeline of parallel-decoded candidate generation and verification, which is\nguaranteed to reproduce the same sequence generated by static sampling, without\nintroducing extra model forward calls. By applying Freedave, the throughput of\nDLLMs can be boosted up to $2.8\\times$ without performance degradation on math\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of\nlanguage modeling beyond autoregressive next-token prediction. Thanks to their\nbidirectional attention mechanism, DLLMs are more capable of capturing the\nconnection of context, and thus show unique advantages in challenges like the\nfamous \"reversal curse\" or learning under data-constrained scenarios. However,\nthis bidirectional nature also brings an obstacle that DLLMs are not inherently\ncompatible with KV Cache, and consequently, the inference efficiency is not\ncompetitive compared with autoregressive models. Taking advantage of their\ninherent capability of multi-token prediction, existing parallel decoding\nalgorithms can speed up the DLLM inference, but at the cost of non-negligible\nperformance degradation. To overcome this challenge, we introduce Free\nDraft-and-Verification (Freedave), a novel fast sampling algorithm tailored for\nDLLMs that achieves lossless parallel decoding. Specifically, we propose a\npipeline of parallel-decoded candidate generation and verification, which is\nguaranteed to reproduce the same sequence generated by static sampling, without\nintroducing extra model forward calls. By applying Freedave, the throughput of\nDLLMs can be boosted up to $2.8\\times$ without performance degradation on math\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Shutong Wu"
                    },
                    {
                        "name": "Jiawei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhang"
                },
                "author": "Jiawei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00231v1",
                "updated": "2025-09-30T19:55:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    55,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T19:55:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    55,
                    26,
                    1,
                    273,
                    0
                ],
                "title": "The Pitfalls of KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pitfalls of KV Cache Compression"
                },
                "summary": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks."
                },
                "authors": [
                    {
                        "name": "Alex Chen"
                    },
                    {
                        "name": "Renato Geh"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Daniel Israel"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Israel"
                },
                "author": "Daniel Israel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00184v1",
                "updated": "2025-09-30T19:03:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    3,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T19:03:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    3,
                    26,
                    1,
                    273,
                    0
                ],
                "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls"
                },
                "summary": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Bai"
                    },
                    {
                        "name": "Itamar Pres"
                    },
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Chenhao Tan"
                    },
                    {
                        "name": "Stuart Shieber"
                    },
                    {
                        "name": "Fernanda Viégas"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Andrew Lee"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lee"
                },
                "author": "Andrew Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v1",
                "updated": "2025-09-30T17:15:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v2",
                "updated": "2025-09-30T16:42:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    42,
                    50,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v3",
                "updated": "2025-09-30T15:44:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    44,
                    29,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_doi": "10.1109/TED.2025.3617043",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TED.2025.3617043",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to IEEE Trans. Elec. Dev. Work enabled in part by NanoIC\n  pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26328v1",
                "updated": "2025-09-30T14:40:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:40:18Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM v2: Efficient Block-Diffusion LLM"
                },
                "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v4",
                "updated": "2025-09-30T14:13:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    13,
                    20,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by NeurIPS 2025. Code at https://github.com/NVlabs/Long-RL\n  and model at https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v2",
                "updated": "2025-09-30T09:10:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    9,
                    10,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Lin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shi"
                },
                "author": "Lin Shi",
                "arxiv_comment": "Accepted by ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v2",
                "updated": "2025-09-30T02:51:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    51,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "NeurIPS 2025 Oral. Code: https://github.com/snu-mllab/KVzip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25681v1",
                "updated": "2025-09-30T02:36:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T02:36:11Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought"
                },
                "summary": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Yi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Xu"
                },
                "author": "Yi Xu",
                "arxiv_comment": "technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25401v1",
                "updated": "2025-09-29T18:57:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T18:57:14Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers"
                },
                "summary": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality."
                },
                "authors": [
                    {
                        "name": "Liang Qiao"
                    },
                    {
                        "name": "Yue Dai"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Hongyu Kan"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25155v1",
                "updated": "2025-09-29T17:55:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:55:43Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "title": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units"
                },
                "summary": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts."
                },
                "authors": [
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rakshith Jayanth"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IEEE HiPC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v2",
                "updated": "2025-09-29T15:20:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    20,
                    29,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "EMNLP 2025; 15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v2",
                "updated": "2025-09-29T15:15:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    49,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24832v1",
                "updated": "2025-09-29T14:16:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching"
                },
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "arxiv_comment": "11 figures, 14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24791v1",
                "updated": "2025-09-29T13:45:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:45:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "title": "Vision Function Layer in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Function Layer in Multimodal LLMs"
                },
                "summary": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models."
                },
                "authors": [
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang",
                "arxiv_comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v3",
                "updated": "2025-09-29T12:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    34,
                    50,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v1",
                "updated": "2025-09-29T12:28:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24626v1",
                "updated": "2025-09-29T11:35:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T11:35:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving"
                },
                "summary": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "14 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24407v1",
                "updated": "2025-09-29T07:54:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T07:54:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network"
                },
                "summary": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter."
                },
                "authors": [
                    {
                        "name": "Karl C. Linne"
                    },
                    {
                        "name": "Yuanyuan Li"
                    },
                    {
                        "name": "Debashri Roy"
                    },
                    {
                        "name": "Kaushik Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Chowdhury"
                },
                "arxiv_affiliation": "Kai Li",
                "author": "Kaushik Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v2",
                "updated": "2025-09-29T05:12:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    5,
                    12,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v2",
                "updated": "2025-09-29T02:46:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    2,
                    46,
                    45,
                    0,
                    272,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24178v1",
                "updated": "2025-09-29T01:52:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T01:52:10Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "title": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring"
                },
                "summary": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhou"
                    },
                    {
                        "name": "Steve Majerus"
                    },
                    {
                        "name": "Gourav Datta"
                    }
                ],
                "author_detail": {
                    "name": "Gourav Datta"
                },
                "author": "Gourav Datta",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24088v1",
                "updated": "2025-09-28T21:47:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T21:47:20Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Moyan Li"
                    },
                    {
                        "name": "Shaoyuan Xu"
                    },
                    {
                        "name": "Jinmiao Fu"
                    },
                    {
                        "name": "Xinhai Hou"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Bryan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wang"
                },
                "author": "Bryan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24007v1",
                "updated": "2025-09-28T17:59:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T17:59:15Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "title": "Sequential Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Diffusion Language Models"
                },
                "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM"
                },
                "authors": [
                    {
                        "name": "Yangzhou Liu"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Xiaobo Liang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Yanting Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "14 pages, 5 figures, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23928v1",
                "updated": "2025-09-28T15:05:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T15:05:21Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models"
                },
                "summary": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference."
                },
                "authors": [
                    {
                        "name": "Zhinan Xie"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Jian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jian Cheng"
                },
                "author": "Jian Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02361v1",
                "updated": "2025-09-28T11:04:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    11,
                    4,
                    0,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T11:04:00Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    11,
                    4,
                    0,
                    6,
                    271,
                    0
                ],
                "title": "ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs\n  Inference"
                },
                "summary": "Transformer-based large models excel in natural language processing and\ncomputer vision, but face severe computational inefficiencies due to the\nself-attention's quadratic complexity with input tokens. Recently, researchers\nhave proposed a series of methods based on block selection and compression to\nalleviate this problem, but they either have issues with semantic\nincompleteness or poor training-inference efficiency. To comprehensively\naddress these challenges, we propose ChunkLLM, a lightweight and pluggable\ntraining framework. Specifically, we introduce two components: QK Adapter\n(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each\nTransformer layer, serving dual purposes of feature compression and chunk\nattention acquisition. The latter operates at the bottommost layer of the\nmodel, functioning to detect chunk boundaries by leveraging contextual semantic\ninformation. During the training phase, the parameters of the backbone remain\nfrozen, with only the QK Adapter and Chunk Adapter undergoing training.\nNotably, we design an attention distillation method for training the QK\nAdapter, which enhances the recall rate of key chunks. During the inference\nphase, chunk selection is triggered exclusively when the current token is\ndetected as a chunk boundary, thereby accelerating model inference.\nExperimental evaluations are conducted on a diverse set of long-text and\nshort-text benchmark datasets spanning multiple tasks. ChunkLLM not only\nattains comparable performance on short-text benchmarks but also maintains\n98.64% of the performance on long-context benchmarks while preserving a 48.58%\nkey-value cache retention rate. Particularly, ChunkLLM attains a maximum\nspeedup of 4.48x in comparison to the vanilla Transformer in the processing of\n120K long texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large models excel in natural language processing and\ncomputer vision, but face severe computational inefficiencies due to the\nself-attention's quadratic complexity with input tokens. Recently, researchers\nhave proposed a series of methods based on block selection and compression to\nalleviate this problem, but they either have issues with semantic\nincompleteness or poor training-inference efficiency. To comprehensively\naddress these challenges, we propose ChunkLLM, a lightweight and pluggable\ntraining framework. Specifically, we introduce two components: QK Adapter\n(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each\nTransformer layer, serving dual purposes of feature compression and chunk\nattention acquisition. The latter operates at the bottommost layer of the\nmodel, functioning to detect chunk boundaries by leveraging contextual semantic\ninformation. During the training phase, the parameters of the backbone remain\nfrozen, with only the QK Adapter and Chunk Adapter undergoing training.\nNotably, we design an attention distillation method for training the QK\nAdapter, which enhances the recall rate of key chunks. During the inference\nphase, chunk selection is triggered exclusively when the current token is\ndetected as a chunk boundary, thereby accelerating model inference.\nExperimental evaluations are conducted on a diverse set of long-text and\nshort-text benchmark datasets spanning multiple tasks. ChunkLLM not only\nattains comparable performance on short-text benchmarks but also maintains\n98.64% of the performance on long-context benchmarks while preserving a 48.58%\nkey-value cache retention rate. Particularly, ChunkLLM attains a maximum\nspeedup of 4.48x in comparison to the vanilla Transformer in the processing of\n120K long texts."
                },
                "authors": [
                    {
                        "name": "Haojie Ouyang"
                    },
                    {
                        "name": "Jianwei Lv"
                    },
                    {
                        "name": "Lei Ren"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Xiaojie Wang"
                    },
                    {
                        "name": "Fangxiang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Fangxiang Feng"
                },
                "author": "Fangxiang Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v2",
                "updated": "2025-09-28T08:32:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    8,
                    32,
                    26,
                    6,
                    271,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23601v1",
                "updated": "2025-09-28T03:12:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T03:12:43Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration"
                },
                "summary": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba."
                },
                "authors": [
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Chen Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lyu"
                },
                "author": "Chen Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v2",
                "updated": "2025-09-27T20:13:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    20,
                    13,
                    25,
                    5,
                    270,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Fei Yi"
                    },
                    {
                        "name": "Weidi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Zeng"
                },
                "author": "Weidi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23179v1",
                "updated": "2025-09-27T08:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T08:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "title": "A Near-Cache Architectural Framework for Cryptographic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Near-Cache Architectural Framework for Cryptographic Computing"
                },
                "summary": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath."
                },
                "authors": [
                    {
                        "name": "Jingyao Zhang"
                    },
                    {
                        "name": "Elaheh Sadredini"
                    }
                ],
                "author_detail": {
                    "name": "Elaheh Sadredini"
                },
                "author": "Elaheh Sadredini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v4",
                "updated": "2025-09-27T07:41:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    7,
                    41,
                    38,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "Runtime Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runtime Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.07318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07318v1",
                "updated": "2025-10-08T17:59:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Hippocampus Networks for Efficient Long-Context Modeling"
                },
                "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN."
                },
                "authors": [
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Shu Zhong"
                    },
                    {
                        "name": "Qinghao Ye"
                    },
                    {
                        "name": "Xuehan Xiong"
                    },
                    {
                        "name": "Lai Wei"
                    }
                ],
                "author_detail": {
                    "name": "Lai Wei"
                },
                "author": "Lai Wei",
                "arxiv_comment": "Code: https://github.com/ByteDance-Seed/AHN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07315v1",
                "updated": "2025-10-08T17:59:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    19,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:59:19Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    19,
                    2,
                    281,
                    0
                ],
                "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vibe Checker: Aligning Code Evaluation with Human Preference"
                },
                "summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding."
                },
                "authors": [
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Xiang Zhou"
                    },
                    {
                        "name": "Ting-Yun Chang"
                    },
                    {
                        "name": "Qingze Wang"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Xiance Si"
                    },
                    {
                        "name": "Dan Garrette"
                    },
                    {
                        "name": "Shyam Upadhyay"
                    },
                    {
                        "name": "Jeremiah Liu"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Benoit Schillings"
                    },
                    {
                        "name": "Jiao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiao Sun"
                },
                "author": "Jiao Sun",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07312v1",
                "updated": "2025-10-08T17:58:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    58,
                    41,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:58:41Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    58,
                    41,
                    2,
                    281,
                    0
                ],
                "title": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement\n  Learning"
                },
                "summary": "Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Alesia Ivanova"
                    },
                    {
                        "name": "Ziyang Cai"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Riashat Islam"
                    },
                    {
                        "name": "Shital Shah"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Charles London"
                    }
                ],
                "author_detail": {
                    "name": "Charles London"
                },
                "author": "Charles London",
                "arxiv_comment": "Preprint, 31 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12726v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12726v3",
                "updated": "2025-10-08T17:57:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    57,
                    43,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-18T08:49:29Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    8,
                    49,
                    29,
                    0,
                    230,
                    0
                ],
                "title": "DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM\n  Reasoning"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage tasks but still struggle with complex, multi-step reasoning,\nparticularly across diverse disciplines. Existing reasoning datasets often lack\ndisciplinary breadth, reasoning depth, and diversity, and lack guiding\nprinciples for question synthesis. We propose DESIGNER: a DESIGN-logic-guidEd\nReasoning data synthesis pipeline that leverages naturally available, extensive\nraw documents (e.g., book corpus and web corpus) to generate multidisciplinary\nchallenging questions. We introduce the concept of \"design logic\" and instruct\nLLMs to mimic human educators' question-creation process, enabling automated\nsynthesis of large-scale, high-difficulty questions. We use LLMs to\nreverse-engineer and abstract over 120,000 design logics from existing\nquestions across various disciplines. By matching these design logics with\nsource documents, we are able to create reasoning questions that far surpass\nthe difficulty and diversity of existing datasets. Using this pipeline, we\nsynthesized two large-scale reasoning datasets that span 75 disciplines:\nDLR-Book (3.04 million questions from the book corpus) and DLR-Web (1.66\nmillion questions from the web corpus). Data analysis indicates that the\nquestions synthesized by our method exhibit greater difficulty and diversity\ncompared to those in the baseline datasets. We validate our synthesized data\nthrough supervised fine-tuning (SFT) on the Qwen3 and Llama3 model families.\nOur data substantially enhances their multidisciplinary reasoning capabilities,\noutperforming existing datasets. Notably, after SFT on our datasets, the base\nversions of these models even surpass their official instruction-tuned\ncounterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage tasks but still struggle with complex, multi-step reasoning,\nparticularly across diverse disciplines. Existing reasoning datasets often lack\ndisciplinary breadth, reasoning depth, and diversity, and lack guiding\nprinciples for question synthesis. We propose DESIGNER: a DESIGN-logic-guidEd\nReasoning data synthesis pipeline that leverages naturally available, extensive\nraw documents (e.g., book corpus and web corpus) to generate multidisciplinary\nchallenging questions. We introduce the concept of \"design logic\" and instruct\nLLMs to mimic human educators' question-creation process, enabling automated\nsynthesis of large-scale, high-difficulty questions. We use LLMs to\nreverse-engineer and abstract over 120,000 design logics from existing\nquestions across various disciplines. By matching these design logics with\nsource documents, we are able to create reasoning questions that far surpass\nthe difficulty and diversity of existing datasets. Using this pipeline, we\nsynthesized two large-scale reasoning datasets that span 75 disciplines:\nDLR-Book (3.04 million questions from the book corpus) and DLR-Web (1.66\nmillion questions from the web corpus). Data analysis indicates that the\nquestions synthesized by our method exhibit greater difficulty and diversity\ncompared to those in the baseline datasets. We validate our synthesized data\nthrough supervised fine-tuning (SFT) on the Qwen3 and Llama3 model families.\nOur data substantially enhances their multidisciplinary reasoning capabilities,\noutperforming existing datasets. Notably, after SFT on our datasets, the base\nversions of these models even surpass their official instruction-tuned\ncounterparts."
                },
                "authors": [
                    {
                        "name": "Weize Liu"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Yijia Luo"
                    },
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yanan Li"
                    },
                    {
                        "name": "Xiguo Hu"
                    },
                    {
                        "name": "Zhiqi Bai"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12726v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12726v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07309v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07309v2",
                "updated": "2025-10-09T02:27:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    2,
                    27,
                    56,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-08T17:57:35Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    57,
                    35,
                    2,
                    281,
                    0
                ],
                "title": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the\n  Business Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the\n  Business Domain"
                },
                "summary": "In the business domain, where data-driven decision making is crucial,\ntext-to-SQL is fundamental for easy natural language access to structured data.\nWhile recent LLMs have achieved strong performance in code generation, existing\ntext-to-SQL benchmarks remain focused on factual retrieval of past records. We\nintroduce CORGI, a new benchmark specifically designed for real-world business\ncontexts. CORGI is composed of synthetic databases inspired by enterprises such\nas Doordash, Airbnb, and Lululemon. It provides questions across four\nincreasingly complex categories of business queries: descriptive, explanatory,\npredictive, and recommendational. This challenge calls for causal reasoning,\ntemporal forecasting, and strategic recommendation, reflecting multi-level and\nmulti-step agentic intelligence. We find that LLM performance drops on\nhigh-level questions, struggling to make accurate predictions and offer\nactionable plans. Based on execution success rate, the CORGI benchmark is about\n21% more difficult than the BIRD benchmark. This highlights the gap between\npopular LLMs and the need for real-world business intelligence. We release a\npublic dataset and evaluation framework, and a website for public submissions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the business domain, where data-driven decision making is crucial,\ntext-to-SQL is fundamental for easy natural language access to structured data.\nWhile recent LLMs have achieved strong performance in code generation, existing\ntext-to-SQL benchmarks remain focused on factual retrieval of past records. We\nintroduce CORGI, a new benchmark specifically designed for real-world business\ncontexts. CORGI is composed of synthetic databases inspired by enterprises such\nas Doordash, Airbnb, and Lululemon. It provides questions across four\nincreasingly complex categories of business queries: descriptive, explanatory,\npredictive, and recommendational. This challenge calls for causal reasoning,\ntemporal forecasting, and strategic recommendation, reflecting multi-level and\nmulti-step agentic intelligence. We find that LLM performance drops on\nhigh-level questions, struggling to make accurate predictions and offer\nactionable plans. Based on execution success rate, the CORGI benchmark is about\n21% more difficult than the BIRD benchmark. This highlights the gap between\npopular LLMs and the need for real-world business intelligence. We release a\npublic dataset and evaluation framework, and a website for public submissions."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Ran Tao"
                    },
                    {
                        "name": "Derek Hommel"
                    },
                    {
                        "name": "Yusuf Denizay Dönder"
                    },
                    {
                        "name": "Sungyong Chang"
                    },
                    {
                        "name": "David Mimno"
                    },
                    {
                        "name": "Unso Eun Seo Jo"
                    }
                ],
                "author_detail": {
                    "name": "Unso Eun Seo Jo"
                },
                "author": "Unso Eun Seo Jo",
                "arxiv_comment": "20 pages, 6 figures, under review for ACL ARR; typos corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07309v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07309v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07307v1",
                "updated": "2025-10-08T17:57:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    57,
                    19,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:57:19Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    57,
                    19,
                    2,
                    281,
                    0
                ],
                "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline"
                },
                "summary": "While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality."
                },
                "authors": [
                    {
                        "name": "Rushi Qiang"
                    },
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Sherry Yang"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06635v2",
                "updated": "2025-10-08T17:56:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    56,
                    19,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-08T18:32:52Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    18,
                    32,
                    52,
                    4,
                    220,
                    0
                ],
                "title": "Valid Inference with Imperfect Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Valid Inference with Imperfect Synthetic Data"
                },
                "summary": "Predictions and generations from large language models are increasingly being\nexplored as an aid in limited data regimes, such as in computational social\nscience and human subjects research. While prior technical work has mainly\nexplored the potential to use model-predicted labels for unlabeled data in a\nprincipled manner, there is increasing interest in using large language models\nto generate entirely new synthetic samples (e.g., synthetic simulations), such\nas in responses to surveys. However, it remains unclear by what means\npractitioners can combine such data with real data and yet produce\nstatistically valid conclusions upon them. In this paper, we introduce a new\nestimator based on generalized method of moments, providing a\nhyperparameter-free solution with strong theoretical guarantees to address this\nchallenge. Intriguingly, we find that interactions between the moment residuals\nof synthetic data and those of real data (i.e., when they are predictive of\neach other) can greatly improve estimates of the target parameter. We validate\nthe finite-sample performance of our estimator across different tasks in\ncomputational social science applications, demonstrating large empirical gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictions and generations from large language models are increasingly being\nexplored as an aid in limited data regimes, such as in computational social\nscience and human subjects research. While prior technical work has mainly\nexplored the potential to use model-predicted labels for unlabeled data in a\nprincipled manner, there is increasing interest in using large language models\nto generate entirely new synthetic samples (e.g., synthetic simulations), such\nas in responses to surveys. However, it remains unclear by what means\npractitioners can combine such data with real data and yet produce\nstatistically valid conclusions upon them. In this paper, we introduce a new\nestimator based on generalized method of moments, providing a\nhyperparameter-free solution with strong theoretical guarantees to address this\nchallenge. Intriguingly, we find that interactions between the moment residuals\nof synthetic data and those of real data (i.e., when they are predictive of\neach other) can greatly improve estimates of the target parameter. We validate\nthe finite-sample performance of our estimator across different tasks in\ncomputational social science applications, demonstrating large empirical gains."
                },
                "authors": [
                    {
                        "name": "Yewon Byun"
                    },
                    {
                        "name": "Shantanu Gupta"
                    },
                    {
                        "name": "Zachary C. Lipton"
                    },
                    {
                        "name": "Rachel Leah Childers"
                    },
                    {
                        "name": "Bryan Wilder"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wilder"
                },
                "author": "Bryan Wilder",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05468v2",
                "updated": "2025-10-08T17:55:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    55,
                    2,
                    2,
                    281,
                    0
                ],
                "published": "2025-01-05T17:53:00Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    17,
                    53,
                    0,
                    6,
                    5,
                    0
                ],
                "title": "LatteReview: A Multi-Agent Framework for Systematic Review Automation\n  Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LatteReview: A Multi-Agent Framework for Systematic Review Automation\n  Using Large Language Models"
                },
                "summary": "Systematic literature reviews and meta-analyses are essential for\nsynthesizing research insights, but they remain time-intensive and\nlabor-intensive due to the iterative processes of screening, evaluation, and\ndata extraction. This paper introduces and evaluates LatteReview, a\nPython-based framework that leverages large language models (LLMs) and\nmulti-agent systems to automate key elements of the systematic review process.\nDesigned to streamline workflows while maintaining rigor, LatteReview utilizes\nmodular agents for tasks such as title and abstract screening, relevance\nscoring, and structured data extraction. These agents operate within\norchestrated workflows, supporting sequential and parallel review rounds,\ndynamic decision-making, and iterative refinement based on user feedback.\nLatteReview's architecture integrates LLM providers, enabling compatibility\nwith both cloud-based and locally hosted models. The framework supports\nfeatures such as Retrieval-Augmented Generation (RAG) for incorporating\nexternal context, multimodal reviews, Pydantic-based validation for structured\ninputs and outputs, and asynchronous programming for handling large-scale\ndatasets. The framework is available on the GitHub repository, with detailed\ndocumentation and an installable package.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic literature reviews and meta-analyses are essential for\nsynthesizing research insights, but they remain time-intensive and\nlabor-intensive due to the iterative processes of screening, evaluation, and\ndata extraction. This paper introduces and evaluates LatteReview, a\nPython-based framework that leverages large language models (LLMs) and\nmulti-agent systems to automate key elements of the systematic review process.\nDesigned to streamline workflows while maintaining rigor, LatteReview utilizes\nmodular agents for tasks such as title and abstract screening, relevance\nscoring, and structured data extraction. These agents operate within\norchestrated workflows, supporting sequential and parallel review rounds,\ndynamic decision-making, and iterative refinement based on user feedback.\nLatteReview's architecture integrates LLM providers, enabling compatibility\nwith both cloud-based and locally hosted models. The framework supports\nfeatures such as Retrieval-Augmented Generation (RAG) for incorporating\nexternal context, multimodal reviews, Pydantic-based validation for structured\ninputs and outputs, and asynchronous programming for handling large-scale\ndatasets. The framework is available on the GitHub repository, with detailed\ndocumentation and an installable package."
                },
                "authors": [
                    {
                        "name": "Pouria Rouzrokh"
                    },
                    {
                        "name": "Bardia Khosravi"
                    },
                    {
                        "name": "Parsa Rouzrokh"
                    },
                    {
                        "name": "Moein Shariatnia"
                    }
                ],
                "author_detail": {
                    "name": "Moein Shariatnia"
                },
                "author": "Moein Shariatnia",
                "arxiv_comment": "31 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07293v1",
                "updated": "2025-10-08T17:50:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:50:16Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs"
                },
                "summary": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks."
                },
                "authors": [
                    {
                        "name": "Peize He"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Jiajie Huang"
                    },
                    {
                        "name": "Zehui Lei"
                    },
                    {
                        "name": "Zhuangcheng Gu"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Jiabing Yang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Zhifei Liu"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "26 pages, 23 figures, the code is available at\n  \\url{https://github.com/DabDans/AudioMarathon}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00320v2",
                "updated": "2025-10-08T17:49:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    49,
                    53,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-31T00:10:18Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    10,
                    18,
                    5,
                    151,
                    0
                ],
                "title": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in\n  AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in\n  AI Agents"
                },
                "summary": "Recent progress in reasoning with large language models (LLMs), such as\nDeepSeek-R1, demonstrates impressive capabilities in domains like mathematics\nand coding, by exhibiting complex cognitive behaviors such as verification,\ngoal decomposition, and self-reflection. However, it is unclear what behavior\nis effective and what behavior is missing for long-horizon AI agents tasks. In\nthis work, we propose Dyna-Think, a thinking framework that integrates planning\nwith an internal world model with reasoning and acting to enhance AI agent\nperformance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning\n(DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with\nDyna-Think, DIT reconstructs the thinking process of R1 to focus on performing\nworld model simulation relevant to the proposed (and planned) action, and\ntrains the policy using this reconstructed data. To enhance Dyna-Think, DDT\nuses a two-stage training process to first improve the agent's world modeling\nability via objectives such as state prediction or critique generation, and\nthen improve the agent's action via policy training. We evaluate our methods on\nOSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the\nagent's in-domain and out-of-domain performance, achieving similar best-of-n\nperformance compared to R1 while generating 2x less tokens on average. Our\nextensive empirical studies reveal that 1) using critique generation for world\nmodel training is effective to improve policy performance; and 2) AI agents\nwith better performance correlate with better world modeling abilities. We\nbelieve our results suggest a promising research direction to integrate world\nmodel simulation into AI agents to enhance their reasoning, planning, and\nacting capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in reasoning with large language models (LLMs), such as\nDeepSeek-R1, demonstrates impressive capabilities in domains like mathematics\nand coding, by exhibiting complex cognitive behaviors such as verification,\ngoal decomposition, and self-reflection. However, it is unclear what behavior\nis effective and what behavior is missing for long-horizon AI agents tasks. In\nthis work, we propose Dyna-Think, a thinking framework that integrates planning\nwith an internal world model with reasoning and acting to enhance AI agent\nperformance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning\n(DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with\nDyna-Think, DIT reconstructs the thinking process of R1 to focus on performing\nworld model simulation relevant to the proposed (and planned) action, and\ntrains the policy using this reconstructed data. To enhance Dyna-Think, DDT\nuses a two-stage training process to first improve the agent's world modeling\nability via objectives such as state prediction or critique generation, and\nthen improve the agent's action via policy training. We evaluate our methods on\nOSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the\nagent's in-domain and out-of-domain performance, achieving similar best-of-n\nperformance compared to R1 while generating 2x less tokens on average. Our\nextensive empirical studies reveal that 1) using critique generation for world\nmodel training is effective to improve policy performance; and 2) AI agents\nwith better performance correlate with better world modeling abilities. We\nbelieve our results suggest a promising research direction to integrate world\nmodel simulation into AI agents to enhance their reasoning, planning, and\nacting capabilities."
                },
                "authors": [
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Baolin Peng"
                    },
                    {
                        "name": "Ruize Xu"
                    },
                    {
                        "name": "Michel Galley"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Suman Nath"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Zhou Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Yu"
                },
                "author": "Zhou Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06048v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06048v2",
                "updated": "2025-10-08T17:49:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    49,
                    49,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-07T15:42:33Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    42,
                    33,
                    1,
                    280,
                    0
                ],
                "title": "BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection\n  in Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection\n  in Language Model Pretraining"
                },
                "summary": "Effective data selection is essential for pretraining large language models\n(LLMs), enhancing efficiency and improving generalization to downstream tasks.\nHowever, existing approaches often require leveraging external pretrained\nmodels, making it difficult to disentangle the effects of data selection from\nthose of the external pretrained models. In addition, they often overlook the\nlong-term impact of selected data if the model is trained to convergence,\nprimarily due to the prohibitive cost of full-scale LLM pretraining. In this\npaper, we introduce BLISS (\\textbf{B}ileve\\textbf{L} \\textbf{I}nfluence\n\\textbf{S}coring method for data \\textbf{S}election): a lightweight data\nselection method that operates entirely \\emph{from scratch}, without relying on\nany external pretrained oracle models, while explicitly accounting for the\nlong-term impact of selected data. BLISS leverages a small proxy model as a\nsurrogate for the LLM and employs a score model to estimate the long-term\ninfluence of training samples if the proxy model is trained to convergence. We\nformulate data selection as a bilevel optimization problem, where the\nupper-level objective optimizes the score model to assign importance weights to\ntraining samples, ensuring that minimizing the lower-level objective (i.e.,\ntraining the proxy model over the weighted training loss until convergence)\nleads to best validation performance. Once optimized, the trained score model\npredicts influence scores for the dataset, enabling efficient selection of\nhigh-quality samples for LLM pretraining. We validate BLISS by pretraining\n410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4\ndataset. Notably, under the 1B model setting, BLISS achieves $1.7\\times$\nspeedup in reaching the same performance as the state-of-the-art method,\ndemonstrating superior performance across multiple downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective data selection is essential for pretraining large language models\n(LLMs), enhancing efficiency and improving generalization to downstream tasks.\nHowever, existing approaches often require leveraging external pretrained\nmodels, making it difficult to disentangle the effects of data selection from\nthose of the external pretrained models. In addition, they often overlook the\nlong-term impact of selected data if the model is trained to convergence,\nprimarily due to the prohibitive cost of full-scale LLM pretraining. In this\npaper, we introduce BLISS (\\textbf{B}ileve\\textbf{L} \\textbf{I}nfluence\n\\textbf{S}coring method for data \\textbf{S}election): a lightweight data\nselection method that operates entirely \\emph{from scratch}, without relying on\nany external pretrained oracle models, while explicitly accounting for the\nlong-term impact of selected data. BLISS leverages a small proxy model as a\nsurrogate for the LLM and employs a score model to estimate the long-term\ninfluence of training samples if the proxy model is trained to convergence. We\nformulate data selection as a bilevel optimization problem, where the\nupper-level objective optimizes the score model to assign importance weights to\ntraining samples, ensuring that minimizing the lower-level objective (i.e.,\ntraining the proxy model over the weighted training loss until convergence)\nleads to best validation performance. Once optimized, the trained score model\npredicts influence scores for the dataset, enabling efficient selection of\nhigh-quality samples for LLM pretraining. We validate BLISS by pretraining\n410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4\ndataset. Notably, under the 1B model setting, BLISS achieves $1.7\\times$\nspeedup in reaching the same performance as the state-of-the-art method,\ndemonstrating superior performance across multiple downstream tasks."
                },
                "authors": [
                    {
                        "name": "Jie Hao"
                    },
                    {
                        "name": "Rui Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Huixia Wang"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Mingrui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingrui Liu"
                },
                "author": "Mingrui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06048v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06048v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07290v2",
                "updated": "2025-10-09T02:09:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    2,
                    9,
                    12,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-08T17:46:27Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    46,
                    27,
                    2,
                    281,
                    0
                ],
                "title": "On the Convergence of Moral Self-Correction in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Convergence of Moral Self-Correction in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are able to improve their responses when\ninstructed to do so, a capability known as self-correction. When instructions\nprovide only a general and abstract goal without specific details about\npotential issues in the response, LLMs must rely on their internal knowledge to\nimprove response quality, a process referred to as intrinsic self-correction.\nThe empirical success of intrinsic self-correction is evident in various\napplications, but how and why it is effective remains unknown. Focusing on\nmoral self-correction in LLMs, we reveal a key characteristic of intrinsic\nself-correction: performance convergence through multi-round interactions; and\nprovide a mechanistic analysis of this convergence behavior. Based on our\nexperimental results and analysis, we uncover the underlying mechanism of\nconvergence: consistently injected self-correction instructions activate moral\nconcepts that reduce model uncertainty, leading to converged performance as the\nactivated moral concepts stabilize over successive rounds. This paper\ndemonstrates the strong potential of moral self-correction by showing that it\nexhibits a desirable property of converged performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are able to improve their responses when\ninstructed to do so, a capability known as self-correction. When instructions\nprovide only a general and abstract goal without specific details about\npotential issues in the response, LLMs must rely on their internal knowledge to\nimprove response quality, a process referred to as intrinsic self-correction.\nThe empirical success of intrinsic self-correction is evident in various\napplications, but how and why it is effective remains unknown. Focusing on\nmoral self-correction in LLMs, we reveal a key characteristic of intrinsic\nself-correction: performance convergence through multi-round interactions; and\nprovide a mechanistic analysis of this convergence behavior. Based on our\nexperimental results and analysis, we uncover the underlying mechanism of\nconvergence: consistently injected self-correction instructions activate moral\nconcepts that reduce model uncertainty, leading to converged performance as the\nactivated moral concepts stabilize over successive rounds. This paper\ndemonstrates the strong potential of moral self-correction by showing that it\nexhibits a desirable property of converged performance."
                },
                "authors": [
                    {
                        "name": "Guangliang Liu"
                    },
                    {
                        "name": "Haitao Mao"
                    },
                    {
                        "name": "Bochuan Cao"
                    },
                    {
                        "name": "Zhiyu Xue"
                    },
                    {
                        "name": "Xitong Zhang"
                    },
                    {
                        "name": "Rongrong Wang"
                    },
                    {
                        "name": "Kristen Marie Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Marie Johnson"
                },
                "author": "Kristen Marie Johnson",
                "arxiv_comment": "19pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07284v1",
                "updated": "2025-10-08T17:44:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    44,
                    59,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:44:59Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    44,
                    59,
                    2,
                    281,
                    0
                ],
                "title": "Online Rubrics Elicitation from Pairwise Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Rubrics Elicitation from Pairwise Comparisons"
                },
                "summary": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers\nwhere verifiable rewards are not applicable and human preferences provide\ncoarse signals. Prior work shows that reinforcement learning with rubric-based\nrewards leads to consistent gains in LLM post-training. Most existing\napproaches rely on rubrics that remain static over the course of training. Such\nstatic rubrics, however, are vulnerable to reward-hacking type behaviors and\nfail to capture emergent desiderata that arise during training. We introduce\nOnline Rubrics Elicitation (OnlineRubrics), a method that dynamically curates\nevaluation criteria in an online manner through pairwise comparisons of\nresponses from current and reference policies. This online process enables\ncontinuous identification and mitigation of errors as training proceeds.\nEmpirically, this approach yields consistent improvements of up to 8% over\ntraining exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as\nwell as the validation sets of expert questions and rubrics. We qualitatively\nanalyze the elicited criteria and identify prominent themes such as\ntransparency, practicality, organization, and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers\nwhere verifiable rewards are not applicable and human preferences provide\ncoarse signals. Prior work shows that reinforcement learning with rubric-based\nrewards leads to consistent gains in LLM post-training. Most existing\napproaches rely on rubrics that remain static over the course of training. Such\nstatic rubrics, however, are vulnerable to reward-hacking type behaviors and\nfail to capture emergent desiderata that arise during training. We introduce\nOnline Rubrics Elicitation (OnlineRubrics), a method that dynamically curates\nevaluation criteria in an online manner through pairwise comparisons of\nresponses from current and reference policies. This online process enables\ncontinuous identification and mitigation of errors as training proceeds.\nEmpirically, this approach yields consistent improvements of up to 8% over\ntraining exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as\nwell as the validation sets of expert questions and rubrics. We qualitatively\nanalyze the elicited criteria and identify prominent themes such as\ntransparency, practicality, organization, and reasoning."
                },
                "authors": [
                    {
                        "name": "MohammadHossein Rezaei"
                    },
                    {
                        "name": "Robert Vacareanu"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Clinton Wang"
                    },
                    {
                        "name": "Yunzhong He"
                    },
                    {
                        "name": "Afra Feyza Akyürek"
                    }
                ],
                "author_detail": {
                    "name": "Afra Feyza Akyürek"
                },
                "author": "Afra Feyza Akyürek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07283v1",
                "updated": "2025-10-08T17:43:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    43,
                    36,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:43:36Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    43,
                    36,
                    2,
                    281,
                    0
                ],
                "title": "Content-Adaptive Inference for State-of-the-art Learned Video\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-Adaptive Inference for State-of-the-art Learned Video\n  Compression"
                },
                "summary": "While the BD-rate performance of recent learned video codec models in both\nlow-delay and random-access modes exceed that of respective modes of\ntraditional codecs on average over common benchmarks, the performance\nimprovements for individual videos with complex/large motions is much smaller\ncompared to scenes with simple motion. This is related to the inability of a\nlearned encoder model to generalize to motion vector ranges that have not been\nseen in the training set, which causes loss of performance in both coding of\nflow fields as well as frame prediction and coding. As a remedy, we propose a\ngeneric (model-agnostic) framework to control the scale of motion vectors in a\nscene during inference (encoding) to approximately match the range of motion\nvectors in the test and training videos by adaptively downsampling frames. This\nresults in down-scaled motion vectors enabling: i) better flow estimation;\nhence, frame prediction and ii) more efficient flow compression. We show that\nthe proposed framework for content-adaptive inference improves the BD-rate\nperformance of already state-of-the-art low-delay video codec DCVC-FM by up to\n41\\% on individual videos without any model fine tuning. We present ablation\nstudies to show measures of motion and scene complexity can be used to predict\nthe effectiveness of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the BD-rate performance of recent learned video codec models in both\nlow-delay and random-access modes exceed that of respective modes of\ntraditional codecs on average over common benchmarks, the performance\nimprovements for individual videos with complex/large motions is much smaller\ncompared to scenes with simple motion. This is related to the inability of a\nlearned encoder model to generalize to motion vector ranges that have not been\nseen in the training set, which causes loss of performance in both coding of\nflow fields as well as frame prediction and coding. As a remedy, we propose a\ngeneric (model-agnostic) framework to control the scale of motion vectors in a\nscene during inference (encoding) to approximately match the range of motion\nvectors in the test and training videos by adaptively downsampling frames. This\nresults in down-scaled motion vectors enabling: i) better flow estimation;\nhence, frame prediction and ii) more efficient flow compression. We show that\nthe proposed framework for content-adaptive inference improves the BD-rate\nperformance of already state-of-the-art low-delay video codec DCVC-FM by up to\n41\\% on individual videos without any model fine tuning. We present ablation\nstudies to show measures of motion and scene complexity can be used to predict\nthe effectiveness of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Ahmet Bilican"
                    },
                    {
                        "name": "M. Akın Yılmaz"
                    },
                    {
                        "name": "A. Murat Tekalp"
                    }
                ],
                "author_detail": {
                    "name": "A. Murat Tekalp"
                },
                "author": "A. Murat Tekalp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05753v2",
                "updated": "2025-10-08T17:41:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    41,
                    41,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-07T10:21:05Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    10,
                    21,
                    5,
                    1,
                    280,
                    0
                ],
                "title": "Empirical Comparison of Membership Inference Attacks in Deep Transfer\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Comparison of Membership Inference Attacks in Deep Transfer\n  Learning"
                },
                "summary": "With the emergence of powerful large-scale foundation models, the training\nparadigm is increasingly shifting from from-scratch training to transfer\nlearning. This enables high utility training with small, domain-specific\ndatasets typical in sensitive applications. Membership inference attacks (MIAs)\nprovide an empirical estimate of the privacy leakage by machine learning\nmodels. Yet, prior assessments of MIAs against models fine-tuned with transfer\nlearning rely on a small subset of possible attacks. We address this by\ncomparing performance of diverse MIAs in transfer learning settings to help\npractitioners identify the most efficient attacks for privacy risk evaluation.\nWe find that attack efficacy decreases with the increase in training data for\nscore-based MIAs. We find that there is no one MIA which captures all privacy\nrisks in models trained with transfer learning. While the Likelihood Ratio\nAttack (LiRA) demonstrates superior performance across most experimental\nscenarios, the Inverse Hessian Attack (IHA) proves to be more effective against\nmodels fine-tuned on PatchCamelyon dataset in high data regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of powerful large-scale foundation models, the training\nparadigm is increasingly shifting from from-scratch training to transfer\nlearning. This enables high utility training with small, domain-specific\ndatasets typical in sensitive applications. Membership inference attacks (MIAs)\nprovide an empirical estimate of the privacy leakage by machine learning\nmodels. Yet, prior assessments of MIAs against models fine-tuned with transfer\nlearning rely on a small subset of possible attacks. We address this by\ncomparing performance of diverse MIAs in transfer learning settings to help\npractitioners identify the most efficient attacks for privacy risk evaluation.\nWe find that attack efficacy decreases with the increase in training data for\nscore-based MIAs. We find that there is no one MIA which captures all privacy\nrisks in models trained with transfer learning. While the Likelihood Ratio\nAttack (LiRA) demonstrates superior performance across most experimental\nscenarios, the Inverse Hessian Attack (IHA) proves to be more effective against\nmodels fine-tuned on PatchCamelyon dataset in high data regime."
                },
                "authors": [
                    {
                        "name": "Yuxuan Bai"
                    },
                    {
                        "name": "Gauri Pradhan"
                    },
                    {
                        "name": "Marlon Tobaben"
                    },
                    {
                        "name": "Antti Honkela"
                    }
                ],
                "author_detail": {
                    "name": "Antti Honkela"
                },
                "author": "Antti Honkela",
                "arxiv_comment": "30 pages, 13 figures, published in TMLR\n  https://openreview.net/forum?id=UligTUCgdt",
                "arxiv_journal_ref": "Transactions on Machine Learning Research, ISSN 2835-8856, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20757v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20757v2",
                "updated": "2025-10-08T17:36:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    36,
                    57,
                    2,
                    281,
                    0
                ],
                "published": "2025-03-26T17:46:08Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    46,
                    8,
                    2,
                    85,
                    0
                ],
                "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search"
                },
                "summary": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models."
                },
                "authors": [
                    {
                        "name": "Yunhai Hu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20757v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20757v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16185v2",
                "updated": "2025-10-08T17:29:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    29,
                    7,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-22T07:59:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    59,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding\n  on Indic Subjects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding\n  on Indic Subjects"
                },
                "summary": "Large language models have been widely evaluated on tasks such as\ncomprehension, summarization, code generation, etc. However, their performance\non graduate-level, culturally grounded questions in the Indian context remains\nlargely unexplored. Existing Indian benchmarks emphasise basic fact-orientated\nqueries that offer limited assessment of a deeper disciplinary understanding\ntailored to the Indian setting. In this paper, we present ParamBench,\nconsisting of more than 17K questions in the Hindi language, comprising\nquestionnaires from 21 diverse subjects. These questions are primarily derived\nfrom a nationwide graduate-level entrance examination covering topics such as\nhistory, music, instruments, yoga, literature, philosophy, law, etc.~\nspecifically for the Indian context. Additionally, we assess the ability of\nLLMs to handle diverse question formats - such as list-based matching,\nassertion-reason pairs, and sequence ordering - alongside conventional\nmultiple-choice questions. We evaluated the performance of more than 16 open\nsource LLMs on this benchmark, observing that Gemma3-27B attains the highest\noverall accuracy of 56.4\\%. Furthermore, subject-wise analysis indicates that\neven for the best-performing LLMs, performance remains weak on topics such as\nmusic, classical instruments, and law, underscoring persistent challenges in\nculturally grounded reasoning. The dataset and source code is present at\nhttps://github.com/ayushbits/ParamBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been widely evaluated on tasks such as\ncomprehension, summarization, code generation, etc. However, their performance\non graduate-level, culturally grounded questions in the Indian context remains\nlargely unexplored. Existing Indian benchmarks emphasise basic fact-orientated\nqueries that offer limited assessment of a deeper disciplinary understanding\ntailored to the Indian setting. In this paper, we present ParamBench,\nconsisting of more than 17K questions in the Hindi language, comprising\nquestionnaires from 21 diverse subjects. These questions are primarily derived\nfrom a nationwide graduate-level entrance examination covering topics such as\nhistory, music, instruments, yoga, literature, philosophy, law, etc.~\nspecifically for the Indian context. Additionally, we assess the ability of\nLLMs to handle diverse question formats - such as list-based matching,\nassertion-reason pairs, and sequence ordering - alongside conventional\nmultiple-choice questions. We evaluated the performance of more than 16 open\nsource LLMs on this benchmark, observing that Gemma3-27B attains the highest\noverall accuracy of 56.4\\%. Furthermore, subject-wise analysis indicates that\neven for the best-performing LLMs, performance remains weak on topics such as\nmusic, classical instruments, and law, underscoring persistent challenges in\nculturally grounded reasoning. The dataset and source code is present at\nhttps://github.com/ayushbits/ParamBench."
                },
                "authors": [
                    {
                        "name": "Ayush Maheshwari"
                    },
                    {
                        "name": "Kaushal Sharma"
                    },
                    {
                        "name": "Vivek Patel"
                    },
                    {
                        "name": "Aditya Maheshwari"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Maheshwari"
                },
                "author": "Aditya Maheshwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.10354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.10354v2",
                "updated": "2025-10-08T17:26:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    26,
                    33,
                    2,
                    281,
                    0
                ],
                "published": "2023-06-17T13:55:54Z",
                "published_parsed": [
                    2023,
                    6,
                    17,
                    13,
                    55,
                    54,
                    5,
                    168,
                    0
                ],
                "title": "LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event\n  Boundary Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event\n  Boundary Captioning"
                },
                "summary": "Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)\ncompetition is detailed in this paper. Unlike conventional video captioning\ntasks, GEBC demands that the captioning model possess an understanding of\nimmediate changes in status around the designated video boundary, making it a\ndifficult task. This paper proposes an effective model LLMVA-GEBC (Large\nLanguage Model with Video Adapter for Generic Event Boundary Captioning): (1)\nWe utilize a pretrained LLM for generating human-like captions with high\nquality. (2) To adapt the model to the GEBC task, we take the video Q-former as\nan adapter and train it with the frozen visual feature extractors and LLM. Our\nproposed method achieved a 76.14 score on the test set and won the first place\nin the challenge. Our code is available at\nhttps://github.com/zjr2000/LLMVA-GEBC .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)\ncompetition is detailed in this paper. Unlike conventional video captioning\ntasks, GEBC demands that the captioning model possess an understanding of\nimmediate changes in status around the designated video boundary, making it a\ndifficult task. This paper proposes an effective model LLMVA-GEBC (Large\nLanguage Model with Video Adapter for Generic Event Boundary Captioning): (1)\nWe utilize a pretrained LLM for generating human-like captions with high\nquality. (2) To adapt the model to the GEBC task, we take the video Q-former as\nan adapter and train it with the frozen visual feature extractors and LLM. Our\nproposed method achieved a 76.14 score on the test set and won the first place\nin the challenge. Our code is available at\nhttps://github.com/zjr2000/LLMVA-GEBC ."
                },
                "authors": [
                    {
                        "name": "Yolo Yunlong Tang"
                    },
                    {
                        "name": "Jinrui Zhang"
                    },
                    {
                        "name": "Xiangchen Wang"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Feng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zheng"
                },
                "author": "Feng Zheng",
                "arxiv_comment": "Winner solution to Generic Event Boundary Captioning task in LOVEU\n  Challenge (CVPR 2023 workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.10354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.10354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12353v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12353v3",
                "updated": "2025-10-08T17:22:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    22,
                    42,
                    2,
                    281,
                    0
                ],
                "published": "2024-04-18T17:32:46Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    17,
                    32,
                    46,
                    3,
                    109,
                    0
                ],
                "title": "V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt\n  Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt\n  Instruction Tuning"
                },
                "summary": "Video summarization aims to create short, accurate, and cohesive summaries of\nlonger videos. Despite the existence of various video summarization datasets, a\nnotable limitation is their limited amount of source videos, which hampers the\neffective training of advanced large vision-language models (VLMs).\nAdditionally, most existing datasets are created for video-to-video\nsummarization, overlooking the contemporary need for multimodal video content\nsummarization. Recent efforts have been made to expand from unimodal to\nmultimodal video summarization, categorizing the task into three sub-tasks\nbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), and\na combination of video and text summarization (V2VT). However, the textual\nsummaries in previous multimodal datasets are inadequate. To address these\nissues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset\nfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from\n40 to 940 seconds and an average summarization ratio of 16.39%. Each video\nsummary in Instruct-V2Xum is paired with a textual summary that references\nspecific frame indexes, facilitating the generation of aligned video and\ntextual summaries. In addition, we propose a new video summarization framework\nnamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the\nfirst framework that unifies different video summarization tasks into one large\nlanguage model's (LLM) text decoder and achieves task-controllable video\nsummarization with temporal prompts and task instructions. Experiments show\nthat V2Xum-LLaMA outperforms strong baseline models on multiple video\nsummarization tasks. Furthermore, we propose an enhanced evaluation metric for\nV2V and V2VT summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video summarization aims to create short, accurate, and cohesive summaries of\nlonger videos. Despite the existence of various video summarization datasets, a\nnotable limitation is their limited amount of source videos, which hampers the\neffective training of advanced large vision-language models (VLMs).\nAdditionally, most existing datasets are created for video-to-video\nsummarization, overlooking the contemporary need for multimodal video content\nsummarization. Recent efforts have been made to expand from unimodal to\nmultimodal video summarization, categorizing the task into three sub-tasks\nbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), and\na combination of video and text summarization (V2VT). However, the textual\nsummaries in previous multimodal datasets are inadequate. To address these\nissues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset\nfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from\n40 to 940 seconds and an average summarization ratio of 16.39%. Each video\nsummary in Instruct-V2Xum is paired with a textual summary that references\nspecific frame indexes, facilitating the generation of aligned video and\ntextual summaries. In addition, we propose a new video summarization framework\nnamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the\nfirst framework that unifies different video summarization tasks into one large\nlanguage model's (LLM) text decoder and achieves task-controllable video\nsummarization with temporal prompts and task instructions. Experiments show\nthat V2Xum-LLaMA outperforms strong baseline models on multiple video\nsummarization tasks. Furthermore, we propose an enhanced evaluation metric for\nV2V and V2VT summarization tasks."
                },
                "authors": [
                    {
                        "name": "Hang Hua"
                    },
                    {
                        "name": "Yolo Yunlong Tang"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Jiebo Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jiebo Luo"
                },
                "author": "Jiebo Luo",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12353v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12353v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07259v1",
                "updated": "2025-10-08T17:21:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    21,
                    43,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:21:43Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    21,
                    43,
                    2,
                    281,
                    0
                ],
                "title": "The cosmic web's Lyman-$α$ glow at $z \\approx 2.5$; varying\n  hydrodynamic models, dust, and wide-field, narrow-band imaging detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cosmic web's Lyman-$α$ glow at $z \\approx 2.5$; varying\n  hydrodynamic models, dust, and wide-field, narrow-band imaging detection"
                },
                "summary": "The diffuse glow of the cosmic web in Lyman-$\\alpha$ emission has long been\npredicted, yet remained elusive to direct wide field detection. We present\ntheoretical calculations that, when compared with recent observations made\nusing the Condor Array Telescope in New Mexico reported in Lanzetta et al.\n2024, point to its discovery at $z \\approx 2.5$. Synthetic Lyman-$\\alpha$\nsurface brightness maps are constructed from five state-of-the-art hydrodynamic\nsimulations (Illustris-TNG, SIMBA, EAGLE, CROCODILE, and Sherwood),\nincorporating dust attenuation, star formation, collisional excitation, and\nrecombination physics. Our cosmic web Lyman-$\\alpha$ surface brightness\npredictions are consistent with the UV excess detected at high significance in\nthe recent deep, wide field, narrow-band imaging Condor data. The calculations\npresented here thus demonstrate that diffuse Lyman-$\\alpha$ emission is\nobservable with current (and next-generation) wide field low surface brightness\nfacilities, opening the path to direct cartographic mapping of the cosmic web.\nThese findings mark a turning point: for the first time, cosmology moves beyond\ninference from absorption and high-density peaks, into panoramic imaging of the\nfaint intergalactic scaffolding that underpins structure formation in the\nUniverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diffuse glow of the cosmic web in Lyman-$\\alpha$ emission has long been\npredicted, yet remained elusive to direct wide field detection. We present\ntheoretical calculations that, when compared with recent observations made\nusing the Condor Array Telescope in New Mexico reported in Lanzetta et al.\n2024, point to its discovery at $z \\approx 2.5$. Synthetic Lyman-$\\alpha$\nsurface brightness maps are constructed from five state-of-the-art hydrodynamic\nsimulations (Illustris-TNG, SIMBA, EAGLE, CROCODILE, and Sherwood),\nincorporating dust attenuation, star formation, collisional excitation, and\nrecombination physics. Our cosmic web Lyman-$\\alpha$ surface brightness\npredictions are consistent with the UV excess detected at high significance in\nthe recent deep, wide field, narrow-band imaging Condor data. The calculations\npresented here thus demonstrate that diffuse Lyman-$\\alpha$ emission is\nobservable with current (and next-generation) wide field low surface brightness\nfacilities, opening the path to direct cartographic mapping of the cosmic web.\nThese findings mark a turning point: for the first time, cosmology moves beyond\ninference from absorption and high-density peaks, into panoramic imaging of the\nfaint intergalactic scaffolding that underpins structure formation in the\nUniverse."
                },
                "authors": [
                    {
                        "name": "Oleksii Sokoliuk"
                    },
                    {
                        "name": "John K. Webb"
                    },
                    {
                        "name": "Kenneth M. Lanzetta"
                    },
                    {
                        "name": "Michael M. Shara"
                    },
                    {
                        "name": "Stefan Gromoll"
                    },
                    {
                        "name": "James S. Bolton"
                    },
                    {
                        "name": "Robert F. Carswell"
                    },
                    {
                        "name": "Gaspar Galaz"
                    },
                    {
                        "name": "Cédric Ledoux"
                    },
                    {
                        "name": "Gaspare Lo Curto"
                    },
                    {
                        "name": "Alain Smette"
                    },
                    {
                        "name": "David Valls-Gabaud"
                    },
                    {
                        "name": "Anja von der Linden"
                    },
                    {
                        "name": "Frederick M. Walter"
                    },
                    {
                        "name": "Joris Witstok"
                    }
                ],
                "author_detail": {
                    "name": "Joris Witstok"
                },
                "author": "Joris Witstok",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07257v1",
                "updated": "2025-10-08T17:20:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    20,
                    53,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:20:53Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    20,
                    53,
                    2,
                    281,
                    0
                ],
                "title": "Test-Time Graph Search for Goal-Conditioned Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Graph Search for Goal-Conditioned Reinforcement Learning"
                },
                "summary": "Offline goal-conditioned reinforcement learning (GCRL) trains policies that\nreach user-specified goals at test time, providing a simple, unsupervised,\ndomain-agnostic way to extract diverse behaviors from unlabeled, reward-free\ndatasets. Nonetheless, long-horizon decision making remains difficult for GCRL\nagents due to temporal credit assignment and error accumulation, and the\noffline setting amplifies these effects. To alleviate this issue, we introduce\nTest-Time Graph Search (TTGS), a lightweight planning approach to solve the\nGCRL task. TTGS accepts any state-space distance or cost signal, builds a\nweighted graph over dataset states, and performs fast search to assemble a\nsequence of subgoals that a frozen policy executes. When the base learner is\nvalue-based, the distance is derived directly from the learned goal-conditioned\nvalue function, so no handcrafted metric is needed. TTGS requires no changes to\ntraining, no additional supervision, no online interaction, and no privileged\ninformation, and it runs entirely at inference. On the OGBench benchmark, TTGS\nimproves success rates of multiple base learners on challenging locomotion\ntasks, demonstrating the benefit of simple metric-guided test-time planning for\noffline GCRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline goal-conditioned reinforcement learning (GCRL) trains policies that\nreach user-specified goals at test time, providing a simple, unsupervised,\ndomain-agnostic way to extract diverse behaviors from unlabeled, reward-free\ndatasets. Nonetheless, long-horizon decision making remains difficult for GCRL\nagents due to temporal credit assignment and error accumulation, and the\noffline setting amplifies these effects. To alleviate this issue, we introduce\nTest-Time Graph Search (TTGS), a lightweight planning approach to solve the\nGCRL task. TTGS accepts any state-space distance or cost signal, builds a\nweighted graph over dataset states, and performs fast search to assemble a\nsequence of subgoals that a frozen policy executes. When the base learner is\nvalue-based, the distance is derived directly from the learned goal-conditioned\nvalue function, so no handcrafted metric is needed. TTGS requires no changes to\ntraining, no additional supervision, no online interaction, and no privileged\ninformation, and it runs entirely at inference. On the OGBench benchmark, TTGS\nimproves success rates of multiple base learners on challenging locomotion\ntasks, demonstrating the benefit of simple metric-guided test-time planning for\noffline GCRL."
                },
                "authors": [
                    {
                        "name": "Evgenii Opryshko"
                    },
                    {
                        "name": "Junwei Quan"
                    },
                    {
                        "name": "Claas Voelcker"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Igor Gilitschenski"
                    }
                ],
                "author_detail": {
                    "name": "Igor Gilitschenski"
                },
                "author": "Igor Gilitschenski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07255v1",
                "updated": "2025-10-08T17:20:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    20,
                    27,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:20:27Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    20,
                    27,
                    2,
                    281,
                    0
                ],
                "title": "ARGscape: A modular, interactive tool for manipulation of spatiotemporal\n  ancestral recombination graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARGscape: A modular, interactive tool for manipulation of spatiotemporal\n  ancestral recombination graphs"
                },
                "summary": "Ancestral recombination graphs (ARGs) encode the complete genealogical\nhistory of a population of recombining lineages. ARGs, and their succinct\nrepresentation, tree sequences, are increasingly central to modern population\ngenetics methods, yet building an intuition for ARGs remains challenging. This\nis particularly true when analyzing ancestry in a geographic context, as there\nis a critical lack of dedicated, interactive tools capable of visualizing ARGs\nas spatiotemporal objects. To address this gap, we introduce ARGscape, an\ninteractive platform for simulating, analyzing, and visualizing ARGs across\nspace and time. ARGscape provides a user-friendly graphical interface featuring\ndynamic 2- and 3-dimensional visualizations to explore ARGs through space and\ntime, as well as a novel \"spatial diff\" visualization for quantitative\ncomparison of geographic inference methods. ARGscape is an innovative, unified\nframework that seamlessly integrates leading command-line, Python, and R-based\ntools for ARG simulation, manipulation, and use in spatiotemporal inference\ninto both graphical and command-line interfaces. By integrating these various\nfunctionalities, ARGscape facilitates novel data exploration and hypothesis\ngeneration, while lowering the barrier to entry for spatiotemporal ARG analysis\nin both research and education use-cases. ARGscape is built with a Python\nFastAPI backend and a React/TypeScript frontend. It is freely available as a\nlive demo at https://www.argscape.com and as a Python package on PyPI (pip\ninstall argscape). The source code and documentation are available on GitHub at\nhttps://github.com/chris-a-talbot/argscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ancestral recombination graphs (ARGs) encode the complete genealogical\nhistory of a population of recombining lineages. ARGs, and their succinct\nrepresentation, tree sequences, are increasingly central to modern population\ngenetics methods, yet building an intuition for ARGs remains challenging. This\nis particularly true when analyzing ancestry in a geographic context, as there\nis a critical lack of dedicated, interactive tools capable of visualizing ARGs\nas spatiotemporal objects. To address this gap, we introduce ARGscape, an\ninteractive platform for simulating, analyzing, and visualizing ARGs across\nspace and time. ARGscape provides a user-friendly graphical interface featuring\ndynamic 2- and 3-dimensional visualizations to explore ARGs through space and\ntime, as well as a novel \"spatial diff\" visualization for quantitative\ncomparison of geographic inference methods. ARGscape is an innovative, unified\nframework that seamlessly integrates leading command-line, Python, and R-based\ntools for ARG simulation, manipulation, and use in spatiotemporal inference\ninto both graphical and command-line interfaces. By integrating these various\nfunctionalities, ARGscape facilitates novel data exploration and hypothesis\ngeneration, while lowering the barrier to entry for spatiotemporal ARG analysis\nin both research and education use-cases. ARGscape is built with a Python\nFastAPI backend and a React/TypeScript frontend. It is freely available as a\nlive demo at https://www.argscape.com and as a Python package on PyPI (pip\ninstall argscape). The source code and documentation are available on GitHub at\nhttps://github.com/chris-a-talbot/argscape."
                },
                "authors": [
                    {
                        "name": "Christopher Talbot"
                    },
                    {
                        "name": "Gideon Bradburd"
                    }
                ],
                "author_detail": {
                    "name": "Gideon Bradburd"
                },
                "author": "Gideon Bradburd",
                "arxiv_comment": "28 pages; 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16276v3",
                "updated": "2025-10-08T17:18:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    18,
                    11,
                    2,
                    281,
                    0
                ],
                "published": "2024-03-24T19:50:49Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    19,
                    50,
                    49,
                    6,
                    84,
                    0
                ],
                "title": "Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal\n  Understanding"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language and multimodal domains. By fine-tuning multimodal LLMs with\ntemporal annotations from well-annotated datasets, e.g., dense video captioning\ndatasets, their temporal understanding capacity in video-language tasks can be\nobtained. However, there is a notable lack of untrimmed audio-visual video\ndatasets with precise temporal annotations for events. This deficiency hinders\nLLMs from learning the alignment between time, audio-visual events, and text\ntokens, thus impairing their ability to temporally localize audio-visual events\nin videos. To address this gap, we introduce PU-VALOR, a comprehensive\naudio-visual dataset comprising over 114,000 pseudo-untrimmed videos with\ndetailed temporal annotations. PU-VALOR is derived from the large-scale but\ncoarse-annotated audio-visual dataset VALOR, through a subtle method involving\nevent-based video clustering, random temporal scaling, and permutation. By\nfine-tuning a multimodal LLM on PU-VALOR, we developed AVicuna, a model capable\nof aligning audio-visual events with temporal intervals and corresponding text\ntokens. AVicuna excels in temporal localization and time-aware dialogue\ncapabilities. Our experiments demonstrate that AVicuna effectively handles\ntemporal understanding in audio-visual videos and achieves state-of-the-art\nperformance on open-ended video QA, audio-visual QA, and audio-visual event\ndense localization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language and multimodal domains. By fine-tuning multimodal LLMs with\ntemporal annotations from well-annotated datasets, e.g., dense video captioning\ndatasets, their temporal understanding capacity in video-language tasks can be\nobtained. However, there is a notable lack of untrimmed audio-visual video\ndatasets with precise temporal annotations for events. This deficiency hinders\nLLMs from learning the alignment between time, audio-visual events, and text\ntokens, thus impairing their ability to temporally localize audio-visual events\nin videos. To address this gap, we introduce PU-VALOR, a comprehensive\naudio-visual dataset comprising over 114,000 pseudo-untrimmed videos with\ndetailed temporal annotations. PU-VALOR is derived from the large-scale but\ncoarse-annotated audio-visual dataset VALOR, through a subtle method involving\nevent-based video clustering, random temporal scaling, and permutation. By\nfine-tuning a multimodal LLM on PU-VALOR, we developed AVicuna, a model capable\nof aligning audio-visual events with temporal intervals and corresponding text\ntokens. AVicuna excels in temporal localization and time-aware dialogue\ncapabilities. Our experiments demonstrate that AVicuna effectively handles\ntemporal understanding in audio-visual videos and achieves state-of-the-art\nperformance on open-ended video QA, audio-visual QA, and audio-visual event\ndense localization tasks."
                },
                "authors": [
                    {
                        "name": "Yolo Yunlong Tang"
                    },
                    {
                        "name": "Daiki Shimada"
                    },
                    {
                        "name": "Jing Bi"
                    },
                    {
                        "name": "Mingqian Feng"
                    },
                    {
                        "name": "Hang Hua"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07249v1",
                "updated": "2025-10-08T17:16:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    16,
                    9,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:16:09Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    16,
                    9,
                    2,
                    281,
                    0
                ],
                "title": "TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video\n  Generation"
                },
                "summary": "In this work, we present TalkCuts, a large-scale dataset designed to\nfacilitate the study of multi-shot human speech video generation. Unlike\nexisting datasets that focus on single-shot, static viewpoints, TalkCuts offers\n164k clips totaling over 500 hours of high-quality human speech videos with\ndiverse camera shots, including close-up, half-body, and full-body views. The\ndataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X\nmotion annotations, covering over 10k identities, enabling multimodal learning\nand evaluation. As a first attempt to showcase the value of the dataset, we\npresent Orator, an LLM-guided multi-modal generation framework as a simple\nbaseline, where the language model functions as a multi-faceted director,\norchestrating detailed specifications for camera transitions, speaker\ngesticulations, and vocal modulation. This architecture enables the synthesis\nof coherent long-form videos through our integrated multi-modal video\ngeneration module. Extensive experiments in both pose-guided and audio-driven\nsettings show that training on TalkCuts significantly enhances the\ncinematographic coherence and visual appeal of generated multi-shot speech\nvideos. We believe TalkCuts provides a strong foundation for future work in\ncontrollable, multi-shot speech video generation and broader multimodal\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present TalkCuts, a large-scale dataset designed to\nfacilitate the study of multi-shot human speech video generation. Unlike\nexisting datasets that focus on single-shot, static viewpoints, TalkCuts offers\n164k clips totaling over 500 hours of high-quality human speech videos with\ndiverse camera shots, including close-up, half-body, and full-body views. The\ndataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X\nmotion annotations, covering over 10k identities, enabling multimodal learning\nand evaluation. As a first attempt to showcase the value of the dataset, we\npresent Orator, an LLM-guided multi-modal generation framework as a simple\nbaseline, where the language model functions as a multi-faceted director,\norchestrating detailed specifications for camera transitions, speaker\ngesticulations, and vocal modulation. This architecture enables the synthesis\nof coherent long-form videos through our integrated multi-modal video\ngeneration module. Extensive experiments in both pose-guided and audio-driven\nsettings show that training on TalkCuts significantly enhances the\ncinematographic coherence and visual appeal of generated multi-shot speech\nvideos. We believe TalkCuts provides a strong foundation for future work in\ncontrollable, multi-shot speech video generation and broader multimodal\nlearning."
                },
                "authors": [
                    {
                        "name": "Jiaben Chen"
                    },
                    {
                        "name": "Zixin Wang"
                    },
                    {
                        "name": "Ailing Zeng"
                    },
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Xueyang Yu"
                    },
                    {
                        "name": "Siyuan Cen"
                    },
                    {
                        "name": "Julian Tanke"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Koichi Saito"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "Project page: https://talkcuts.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12009v2",
                "updated": "2025-10-08T17:14:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    14,
                    59,
                    2,
                    281,
                    0
                ],
                "published": "2024-08-21T21:40:30Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    21,
                    40,
                    30,
                    2,
                    234,
                    0
                ],
                "title": "CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for\n  Saliency Prediction with Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for\n  Saliency Prediction with Diffusion"
                },
                "summary": "Video saliency prediction aims to identify the regions in a video that\nattract human attention and gaze, driven by bottom-up features from the video\nand top-down processes like memory and cognition. Among these top-down\ninfluences, language plays a crucial role in guiding attention by shaping how\nvisual information is interpreted. Existing methods primarily focus on modeling\nperceptual information while neglecting the reasoning process facilitated by\nlanguage, where ranking cues are crucial outcomes of this process and practical\nguidance for saliency prediction. In this paper, we propose CaRDiff (Caption,\nRank, and generate with Diffusion), a framework that imitates the process by\nintegrating a multimodal large language model (MLLM), a grounding module, and a\ndiffusion model, to enhance video saliency prediction. Specifically, we\nintroduce a novel prompting method VSOR-CoT (Video Salient Object Ranking Chain\nof Thought), which utilizes an MLLM with a grounding module to caption video\ncontent and infer salient objects along with their rankings and positions. This\nprocess derives ranking maps that can be sufficiently leveraged by the\ndiffusion model to decode the saliency maps for the given video accurately.\nExtensive experiments show the effectiveness of VSOR-CoT in improving the\nperformance of video saliency prediction. The proposed CaRDiff performs better\nthan state-of-the-art models on the MVS dataset and demonstrates cross-dataset\ncapabilities on the DHF1k dataset through zero-shot evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video saliency prediction aims to identify the regions in a video that\nattract human attention and gaze, driven by bottom-up features from the video\nand top-down processes like memory and cognition. Among these top-down\ninfluences, language plays a crucial role in guiding attention by shaping how\nvisual information is interpreted. Existing methods primarily focus on modeling\nperceptual information while neglecting the reasoning process facilitated by\nlanguage, where ranking cues are crucial outcomes of this process and practical\nguidance for saliency prediction. In this paper, we propose CaRDiff (Caption,\nRank, and generate with Diffusion), a framework that imitates the process by\nintegrating a multimodal large language model (MLLM), a grounding module, and a\ndiffusion model, to enhance video saliency prediction. Specifically, we\nintroduce a novel prompting method VSOR-CoT (Video Salient Object Ranking Chain\nof Thought), which utilizes an MLLM with a grounding module to caption video\ncontent and infer salient objects along with their rankings and positions. This\nprocess derives ranking maps that can be sufficiently leveraged by the\ndiffusion model to decode the saliency maps for the given video accurately.\nExtensive experiments show the effectiveness of VSOR-CoT in improving the\nperformance of video saliency prediction. The proposed CaRDiff performs better\nthan state-of-the-art models on the MVS dataset and demonstrates cross-dataset\ncapabilities on the DHF1k dataset through zero-shot evaluation."
                },
                "authors": [
                    {
                        "name": "Yolo Yunlong Tang"
                    },
                    {
                        "name": "Gen Zhan"
                    },
                    {
                        "name": "Li Yang"
                    },
                    {
                        "name": "Yiting Liao"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09225v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09225v4",
                "updated": "2025-10-08T17:12:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    12,
                    48,
                    2,
                    281,
                    0
                ],
                "published": "2024-02-14T15:09:01Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    15,
                    9,
                    1,
                    2,
                    45,
                    0
                ],
                "title": "Is My Data in Your AI? Membership Inference Test (MINT) applied to Face\n  Biometrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is My Data in Your AI? Membership Inference Test (MINT) applied to Face\n  Biometrics"
                },
                "summary": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90\\% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90\\% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Daniel DeAlcala"
                    },
                    {
                        "name": "Aythami Morales"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Javier Ortega-Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Javier Ortega-Garcia"
                },
                "author": "Javier Ortega-Garcia",
                "arxiv_doi": "10.1109/ACCESS.2025.3608951",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3608951",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.09225v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09225v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages main text + 2 pages references and 1 pages appendix",
                "arxiv_journal_ref": "IEEE Access, vol. 13, pp. 163805-163819, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07243v1",
                "updated": "2025-10-08T17:10:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    10,
                    47,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:10:47Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    10,
                    47,
                    2,
                    281,
                    0
                ],
                "title": "LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM\n  Evaluation"
                },
                "summary": "Evaluating large language model (LLM) outputs in the legal domain presents\nunique challenges due to the complex and nuanced nature of legal analysis.\nCurrent evaluation approaches either depend on reference data, which is costly\nto produce, or use standardized assessment methods, both of which have\nsignificant limitations for legal applications.\n  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its\nreliability and effectiveness in legal contexts depend heavily on evaluation\nprocesses unique to the legal industry and how trustworthy the evaluation\nappears to the human legal expert. This is where existing evaluation methods\ncurrently fail and exhibit considerable variability.\n  This paper aims to close the gap: a) we break down lengthy responses into\n'Legal Data Points' (LDPs), self-contained units of information, and introduce\na novel, reference-free evaluation methodology that reflects how lawyers\nevaluate legal answers; b) we demonstrate that our method outperforms a variety\nof baselines on both our proprietary dataset and an open-source dataset\n(LegalBench); c) we show how our method correlates more closely with human\nexpert evaluations and helps improve inter-annotator agreement; and finally d)\nwe open source our Legal Data Points for a subset of LegalBench used in our\nexperiments, allowing the research community to replicate our results and\nadvance research in this vital area of LLM evaluation on legal\nquestion-answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language model (LLM) outputs in the legal domain presents\nunique challenges due to the complex and nuanced nature of legal analysis.\nCurrent evaluation approaches either depend on reference data, which is costly\nto produce, or use standardized assessment methods, both of which have\nsignificant limitations for legal applications.\n  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its\nreliability and effectiveness in legal contexts depend heavily on evaluation\nprocesses unique to the legal industry and how trustworthy the evaluation\nappears to the human legal expert. This is where existing evaluation methods\ncurrently fail and exhibit considerable variability.\n  This paper aims to close the gap: a) we break down lengthy responses into\n'Legal Data Points' (LDPs), self-contained units of information, and introduce\na novel, reference-free evaluation methodology that reflects how lawyers\nevaluate legal answers; b) we demonstrate that our method outperforms a variety\nof baselines on both our proprietary dataset and an open-source dataset\n(LegalBench); c) we show how our method correlates more closely with human\nexpert evaluations and helps improve inter-annotator agreement; and finally d)\nwe open source our Legal Data Points for a subset of LegalBench used in our\nexperiments, allowing the research community to replicate our results and\nadvance research in this vital area of LLM evaluation on legal\nquestion-answering."
                },
                "authors": [
                    {
                        "name": "Joseph Enguehard"
                    },
                    {
                        "name": "Morgane Van Ermengem"
                    },
                    {
                        "name": "Kate Atkinson"
                    },
                    {
                        "name": "Sujeong Cha"
                    },
                    {
                        "name": "Arijit Ghosh Chowdhury"
                    },
                    {
                        "name": "Prashanth Kallur Ramaswamy"
                    },
                    {
                        "name": "Jeremy Roghair"
                    },
                    {
                        "name": "Hannah R Marlowe"
                    },
                    {
                        "name": "Carina Suzana Negreanu"
                    },
                    {
                        "name": "Kitty Boxall"
                    },
                    {
                        "name": "Diana Mincu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Mincu"
                },
                "author": "Diana Mincu",
                "arxiv_comment": "Published in Natural Legal Language Processing - EMNLP Workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07242v1",
                "updated": "2025-10-08T17:09:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    9,
                    41,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:09:41Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    9,
                    41,
                    2,
                    281,
                    0
                ],
                "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense"
                },
                "summary": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning."
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Jason E Weston"
                    },
                    {
                        "name": "Ping Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ping Yu"
                },
                "author": "Ping Yu",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10974v3",
                "updated": "2025-10-08T17:06:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    6,
                    47,
                    2,
                    281,
                    0
                ],
                "published": "2025-06-12T17:59:32Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    17,
                    59,
                    32,
                    3,
                    163,
                    0
                ],
                "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science"
                },
                "summary": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.\nCode is at https://github.com/innovatingAI/AutoMind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.\nCode is at https://github.com/innovatingAI/AutoMind."
                },
                "authors": [
                    {
                        "name": "Yixin Ou"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Jingsheng Zheng"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Zhuoyun Yu"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Yuren Mao"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Ongoing work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07239v1",
                "updated": "2025-10-08T17:06:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    6,
                    20,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:06:20Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    6,
                    20,
                    2,
                    281,
                    0
                ],
                "title": "Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided\n  LoRA Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided\n  LoRA Experts"
                },
                "summary": "Automated red-teaming has emerged as a scalable approach for auditing Large\nLanguage Models (LLMs) prior to deployment, yet existing approaches lack\nmechanisms to efficiently adapt to model-specific vulnerabilities at inference.\nWe introduce Red-Bandit, a red-teaming framework that adapts online to identify\nand exploit model failure modes under distinct attack styles (e.g.,\nmanipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA\nexperts, each specialized for a particular attack style, using reinforcement\nlearning that rewards the generation of unsafe prompts via a rule-based safety\nmodel. At inference, a multi-armed bandit policy dynamically selects among\nthese attack-style experts based on the target model's response safety,\nbalancing exploration and exploitation. Red-Bandit achieves state-of-the-art\nresults on AdvBench under sufficient exploration (ASR@10), while producing more\nhuman-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy\nserves as a diagnostic tool for uncovering model-specific vulnerabilities by\nindicating which attack styles most effectively elicit unsafe behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated red-teaming has emerged as a scalable approach for auditing Large\nLanguage Models (LLMs) prior to deployment, yet existing approaches lack\nmechanisms to efficiently adapt to model-specific vulnerabilities at inference.\nWe introduce Red-Bandit, a red-teaming framework that adapts online to identify\nand exploit model failure modes under distinct attack styles (e.g.,\nmanipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA\nexperts, each specialized for a particular attack style, using reinforcement\nlearning that rewards the generation of unsafe prompts via a rule-based safety\nmodel. At inference, a multi-armed bandit policy dynamically selects among\nthese attack-style experts based on the target model's response safety,\nbalancing exploration and exploitation. Red-Bandit achieves state-of-the-art\nresults on AdvBench under sufficient exploration (ASR@10), while producing more\nhuman-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy\nserves as a diagnostic tool for uncovering model-specific vulnerabilities by\nindicating which attack styles most effectively elicit unsafe behaviors."
                },
                "authors": [
                    {
                        "name": "Christos Ziakas"
                    },
                    {
                        "name": "Nicholas Loo"
                    },
                    {
                        "name": "Nishita Jain"
                    },
                    {
                        "name": "Alessandra Russo"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Russo"
                },
                "author": "Alessandra Russo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07238v1",
                "updated": "2025-10-08T17:06:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    6,
                    7,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:06:07Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    6,
                    7,
                    2,
                    281,
                    0
                ],
                "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model\n  Factuality Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Benchmarks Age: Temporal Misalignment through Large Language Model\n  Factuality Evaluation"
                },
                "summary": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge."
                },
                "authors": [
                    {
                        "name": "Xunyi Jiang"
                    },
                    {
                        "name": "Dingyi Chang"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Xin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xu"
                },
                "author": "Xin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07233v1",
                "updated": "2025-10-08T17:02:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    2,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:02:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    2,
                    4,
                    2,
                    281,
                    0
                ],
                "title": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document\n  Understanding"
                },
                "summary": "Question answering over visually rich documents (VRDs) requires reasoning not\nonly over isolated content but also over documents' structural organization and\ncross-page dependencies. However, conventional retrieval-augmented generation\n(RAG) methods encode content in isolated chunks during ingestion, losing\nstructural and cross-page dependencies, and retrieve a fixed number of pages at\ninference, regardless of the specific demands of the question or context. This\noften results in incomplete evidence retrieval and degraded answer quality for\nmulti-page reasoning tasks. To address these limitations, we propose LAD-RAG, a\nnovel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs\na symbolic document graph that captures layout structure and cross-page\ndependencies, adding it alongside standard neural embeddings to yield a more\nholistic representation of the document. During inference, an LLM agent\ndynamically interacts with the neural and symbolic indices to adaptively\nretrieve the necessary evidence based on the query. Experiments on\nMMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG\nimproves retrieval, achieving over 90% perfect recall on average without any\ntop-k tuning, and outperforming baseline retrievers by up to 20% in recall at\ncomparable noise levels, yielding higher QA accuracy with minimal latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering over visually rich documents (VRDs) requires reasoning not\nonly over isolated content but also over documents' structural organization and\ncross-page dependencies. However, conventional retrieval-augmented generation\n(RAG) methods encode content in isolated chunks during ingestion, losing\nstructural and cross-page dependencies, and retrieve a fixed number of pages at\ninference, regardless of the specific demands of the question or context. This\noften results in incomplete evidence retrieval and degraded answer quality for\nmulti-page reasoning tasks. To address these limitations, we propose LAD-RAG, a\nnovel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs\na symbolic document graph that captures layout structure and cross-page\ndependencies, adding it alongside standard neural embeddings to yield a more\nholistic representation of the document. During inference, an LLM agent\ndynamically interacts with the neural and symbolic indices to adaptively\nretrieve the necessary evidence based on the query. Experiments on\nMMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG\nimproves retrieval, achieving over 90% perfect recall on average without any\ntop-k tuning, and outperforming baseline retrievers by up to 20% in recall at\ncomparable noise levels, yielding higher QA accuracy with minimal latency."
                },
                "authors": [
                    {
                        "name": "Zhivar Sourati"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Marianne Menglin Liu"
                    },
                    {
                        "name": "Yazhe Hu"
                    },
                    {
                        "name": "Mengqing Guo"
                    },
                    {
                        "name": "Sujeeth Bharadwaj"
                    },
                    {
                        "name": "Kyu Han"
                    },
                    {
                        "name": "Tao Sheng"
                    },
                    {
                        "name": "Sujith Ravi"
                    },
                    {
                        "name": "Morteza Dehghani"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07231v1",
                "updated": "2025-10-08T17:00:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    0,
                    49,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:00:49Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    0,
                    49,
                    2,
                    281,
                    0
                ],
                "title": "Benchmarking LLM Causal Reasoning with Scientifically Validated\n  Relationships",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM Causal Reasoning with Scientifically Validated\n  Relationships"
                },
                "summary": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications."
                },
                "authors": [
                    {
                        "name": "Donggyu Lee"
                    },
                    {
                        "name": "Sungwon Park"
                    },
                    {
                        "name": "Yerin Hwang"
                    },
                    {
                        "name": "Hyunwoo Oh"
                    },
                    {
                        "name": "Hyoshin Kim"
                    },
                    {
                        "name": "Jungwon Kim"
                    },
                    {
                        "name": "Meeyoung Cha"
                    },
                    {
                        "name": "Sangyoon Park"
                    },
                    {
                        "name": "Jihee Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jihee Kim"
                },
                "author": "Jihee Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07230v1",
                "updated": "2025-10-08T17:00:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    0,
                    25,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:00:25Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    0,
                    25,
                    2,
                    281,
                    0
                ],
                "title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM\n  Agent in Online Shopping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM\n  Agent in Online Shopping"
                },
                "summary": "Simulating step-wise human behavior with Large Language Models (LLMs) has\nbecome an emerging research direction, enabling applications in various\npractical domains. While prior methods, including prompting, supervised\nfine-tuning (SFT), and reinforcement learning (RL), have shown promise in\nmodeling step-wise behavior, they primarily learn a population-level policy\nwithout conditioning on a user's persona, yielding generic rather than\npersonalized simulations. In this work, we pose a critical question: how can\nLLM agents better simulate personalized user behavior? We introduce\nCustomer-R1, an RL-based method for personalized, step-wise user behavior\nsimulation in online shopping environments. Our policy is conditioned on an\nexplicit persona, and we optimize next-step rationale and action generation via\naction correctness reward signals. Experiments on the OPeRA dataset emonstrate\nthat Customer-R1 not only significantly outperforms prompting and SFT-based\nbaselines in next-action prediction tasks, but also better matches users'\naction distribution, indicating higher fidelity in personalized behavior\nsimulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating step-wise human behavior with Large Language Models (LLMs) has\nbecome an emerging research direction, enabling applications in various\npractical domains. While prior methods, including prompting, supervised\nfine-tuning (SFT), and reinforcement learning (RL), have shown promise in\nmodeling step-wise behavior, they primarily learn a population-level policy\nwithout conditioning on a user's persona, yielding generic rather than\npersonalized simulations. In this work, we pose a critical question: how can\nLLM agents better simulate personalized user behavior? We introduce\nCustomer-R1, an RL-based method for personalized, step-wise user behavior\nsimulation in online shopping environments. Our policy is conditioned on an\nexplicit persona, and we optimize next-step rationale and action generation via\naction correctness reward signals. Experiments on the OPeRA dataset emonstrate\nthat Customer-R1 not only significantly outperforms prompting and SFT-based\nbaselines in next-action prediction tasks, but also better matches users'\naction distribution, indicating higher fidelity in personalized behavior\nsimulation."
                },
                "authors": [
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Yimeng Zhang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07227v1",
                "updated": "2025-10-08T16:57:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    57,
                    46,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:57:46Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    57,
                    46,
                    2,
                    281,
                    0
                ],
                "title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where to Begin: Efficient Pretraining via Subnetwork Selection and\n  Distillation"
                },
                "summary": "Small Language models (SLMs) offer an efficient and accessible alternative to\nLarge Language Models (LLMs), delivering strong performance while using far\nfewer resources. We introduce a simple and effective framework for pretraining\nSLMs that brings together three complementary ideas. First, we identify\nstructurally sparse sub-network initializations that consistently outperform\nrandomly initialized models of similar size under the same compute budget.\nSecond, we use evolutionary search to automatically discover high-quality\nsub-network initializations, providing better starting points for pretraining.\nThird, we apply knowledge distillation from larger teacher models to speed up\ntraining and improve generalization. Together, these components make SLM\npretraining substantially more efficient: our best model, discovered using\nevolutionary search and initialized with LLM weights, matches the validation\nperplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining\ntokens. We release all code and models at\nhttps://github.com/whittle-org/whittle/, offering a practical and reproducible\npath toward cost-efficient small language model development at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language models (SLMs) offer an efficient and accessible alternative to\nLarge Language Models (LLMs), delivering strong performance while using far\nfewer resources. We introduce a simple and effective framework for pretraining\nSLMs that brings together three complementary ideas. First, we identify\nstructurally sparse sub-network initializations that consistently outperform\nrandomly initialized models of similar size under the same compute budget.\nSecond, we use evolutionary search to automatically discover high-quality\nsub-network initializations, providing better starting points for pretraining.\nThird, we apply knowledge distillation from larger teacher models to speed up\ntraining and improve generalization. Together, these components make SLM\npretraining substantially more efficient: our best model, discovered using\nevolutionary search and initialized with LLM weights, matches the validation\nperplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining\ntokens. We release all code and models at\nhttps://github.com/whittle-org/whittle/, offering a practical and reproducible\npath toward cost-efficient small language model development at scale."
                },
                "authors": [
                    {
                        "name": "Arjun Krishnakumar"
                    },
                    {
                        "name": "Rhea Sanjay Sukthanker"
                    },
                    {
                        "name": "Hannan Javed Mahadik"
                    },
                    {
                        "name": "Gabriela Kadlecová"
                    },
                    {
                        "name": "Vladyslav Moroshan"
                    },
                    {
                        "name": "Timur Carstensen"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Aaron Klein"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Klein"
                },
                "author": "Aaron Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19807v3",
                "updated": "2025-10-08T16:56:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    56,
                    59,
                    2,
                    281,
                    0
                ],
                "published": "2025-06-24T17:17:17Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    17,
                    17,
                    1,
                    175,
                    0
                ],
                "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality"
                },
                "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL."
                },
                "authors": [
                    {
                        "name": "Baochang Ren"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16600v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16600v5",
                "updated": "2025-10-08T16:56:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    56,
                    12,
                    2,
                    281,
                    0
                ],
                "published": "2025-02-23T15:00:53Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    15,
                    0,
                    53,
                    6,
                    54,
                    0
                ],
                "title": "Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics\n  and Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics\n  and Generalization"
                },
                "summary": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs."
                },
                "authors": [
                    {
                        "name": "Guangliang Liu"
                    },
                    {
                        "name": "Zimo Qi"
                    },
                    {
                        "name": "Xitong Zhang"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Kristen Marie Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Marie Johnson"
                },
                "author": "Kristen Marie Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16600v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16600v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19828v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19828v4",
                "updated": "2025-10-08T16:54:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    54,
                    13,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-27T12:26:55Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    26,
                    55,
                    2,
                    239,
                    0
                ],
                "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize\n  Memories via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize\n  Memories via Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking a learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns structured operations, including ADD,\nUPDATE, DELETE, and NOOP; and an Answer Agent that pre-selects and reasons over\nrelevant entries. Both agents are fine-tuned with outcome-driven RL (PPO and\nGRPO), enabling adaptive memory management with minimal supervision. With only\n152 training QA pairs, Memory-R1 outperforms strong baselines and generalizes\nacross diverse question types, three benchmarks (LoCoMo, MSC, LongMemEval), and\nmultiple model scales (3B-14B).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking a learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns structured operations, including ADD,\nUPDATE, DELETE, and NOOP; and an Answer Agent that pre-selects and reasons over\nrelevant entries. Both agents are fine-tuned with outcome-driven RL (PPO and\nGRPO), enabling adaptive memory management with minimal supervision. With only\n152 training QA pairs, Memory-R1 outperforms strong baselines and generalizes\nacross diverse question types, three benchmarks (LoCoMo, MSC, LongMemEval), and\nmultiple model scales (3B-14B)."
                },
                "authors": [
                    {
                        "name": "Sikuan Yan"
                    },
                    {
                        "name": "Xiufeng Yang"
                    },
                    {
                        "name": "Zuchao Huang"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Zifeng Ding"
                    },
                    {
                        "name": "Zonggen Li"
                    },
                    {
                        "name": "Xiaowen Ma"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    },
                    {
                        "name": "Hinrich Schütze"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19828v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19828v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16834v3",
                "updated": "2025-10-08T16:40:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    40,
                    37,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-22T16:05:02Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    5,
                    2,
                    3,
                    142,
                    0
                ],
                "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning\n  Trajectory Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning\n  Trajectory Synthesis"
                },
                "summary": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher."
                },
                "authors": [
                    {
                        "name": "Shuang Sun"
                    },
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Fei Bai"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07203v1",
                "updated": "2025-10-08T16:35:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    35,
                    53,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:35:53Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    35,
                    53,
                    2,
                    281,
                    0
                ],
                "title": "Sunflower: A New Approach To Expanding Coverage of African Languages in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sunflower: A New Approach To Expanding Coverage of African Languages in\n  Large Language Models"
                },
                "summary": "There are more than 2000 living languages in Africa, most of which have been\nbypassed by advances in language technology. Current leading LLMs exhibit\nstrong performance on a number of the most common languages (e.g. Swahili or\nYoruba), but prioritise support for the languages with the most speakers first,\nresulting in piecemeal ability across disparate languages. We contend that a\nregionally focussed approach is more efficient, and present a case study for\nUganda, a country with high linguistic diversity. We describe the development\nof Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the\nart comprehension in the majority of all Ugandan languages. These models are\nopen source and can be used to reduce language barriers in a number of\nimportant practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are more than 2000 living languages in Africa, most of which have been\nbypassed by advances in language technology. Current leading LLMs exhibit\nstrong performance on a number of the most common languages (e.g. Swahili or\nYoruba), but prioritise support for the languages with the most speakers first,\nresulting in piecemeal ability across disparate languages. We contend that a\nregionally focussed approach is more efficient, and present a case study for\nUganda, a country with high linguistic diversity. We describe the development\nof Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the\nart comprehension in the majority of all Ugandan languages. These models are\nopen source and can be used to reduce language barriers in a number of\nimportant practical applications."
                },
                "authors": [
                    {
                        "name": "Benjamin Akera"
                    },
                    {
                        "name": "Evelyn Nafula Ouma"
                    },
                    {
                        "name": "Gilbert Yiga"
                    },
                    {
                        "name": "Patrick Walukagga"
                    },
                    {
                        "name": "Phionah Natukunda"
                    },
                    {
                        "name": "Trevor Saaka"
                    },
                    {
                        "name": "Solomon Nsumba"
                    },
                    {
                        "name": "Lilian Teddy Nabukeera"
                    },
                    {
                        "name": "Joel Muhanguzi"
                    },
                    {
                        "name": "Imran Sekalala"
                    },
                    {
                        "name": "Nimpamya Janat Namara"
                    },
                    {
                        "name": "Engineer Bainomugisha"
                    },
                    {
                        "name": "Ernest Mwebaze"
                    },
                    {
                        "name": "John Quinn"
                    }
                ],
                "author_detail": {
                    "name": "John Quinn"
                },
                "author": "John Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07836v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07836v4",
                "updated": "2025-10-08T16:28:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    28,
                    29,
                    2,
                    281,
                    0
                ],
                "published": "2025-04-10T15:13:00Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    13,
                    0,
                    3,
                    100,
                    0
                ],
                "title": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by\n  Exploring Positional Relations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AerialVG: A Challenging Benchmark for Aerial Visual Grounding by\n  Exploring Positional Relations"
                },
                "summary": "Visual grounding (VG) aims to localize target objects in an image based on\nnatural language descriptions. In this paper, we propose AerialVG, a new task\nfocusing on visual grounding from aerial views. Compared to traditional VG,\nAerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is\ninsufficient to distinguish among multiple visually similar objects, and\npositional relations should be emphasized. Besides, existing VG models struggle\nwhen applied to aerial imagery, where high-resolution images cause significant\ndifficulties. To address these challenges, we introduce the first AerialVG\ndataset, consisting of 5K real-world aerial images, 50K manually annotated\ndescriptions, and 103K objects. Particularly, each annotation in AerialVG\ndataset contains multiple target objects annotated with relative spatial\nrelations, requiring models to perform comprehensive spatial reasoning.\nFurthermore, we propose an innovative model especially for the AerialVG task,\nwhere a Hierarchical Cross-Attention is devised to focus on target regions, and\na Relation-Aware Grounding module is designed to infer positional relations.\nExperimental results validate the effectiveness of our dataset and method,\nhighlighting the importance of spatial reasoning in aerial visual grounding.\nThe code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding (VG) aims to localize target objects in an image based on\nnatural language descriptions. In this paper, we propose AerialVG, a new task\nfocusing on visual grounding from aerial views. Compared to traditional VG,\nAerialVG poses new challenges, \\emph{e.g.}, appearance-based grounding is\ninsufficient to distinguish among multiple visually similar objects, and\npositional relations should be emphasized. Besides, existing VG models struggle\nwhen applied to aerial imagery, where high-resolution images cause significant\ndifficulties. To address these challenges, we introduce the first AerialVG\ndataset, consisting of 5K real-world aerial images, 50K manually annotated\ndescriptions, and 103K objects. Particularly, each annotation in AerialVG\ndataset contains multiple target objects annotated with relative spatial\nrelations, requiring models to perform comprehensive spatial reasoning.\nFurthermore, we propose an innovative model especially for the AerialVG task,\nwhere a Hierarchical Cross-Attention is devised to focus on target regions, and\na Relation-Aware Grounding module is designed to infer positional relations.\nExperimental results validate the effectiveness of our dataset and method,\nhighlighting the importance of spatial reasoning in aerial visual grounding.\nThe code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Junli Liu"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yiwen Tang"
                    },
                    {
                        "name": "Yiting Zhang"
                    },
                    {
                        "name": "Chi Yan"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Xuelong Li"
                    },
                    {
                        "name": "Bin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Zhao"
                },
                "author": "Bin Zhao",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07836v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07836v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07195v1",
                "updated": "2025-10-08T16:26:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    26,
                    50,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:26:50Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    26,
                    50,
                    2,
                    281,
                    0
                ],
                "title": "Accelerating Inference for Multilayer Neural Networks with Quantum\n  Computers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Inference for Multilayer Neural Networks with Quantum\n  Computers"
                },
                "summary": "Fault-tolerant Quantum Processing Units (QPUs) promise to deliver exponential\nspeed-ups in select computational tasks, yet their integration into modern deep\nlearning pipelines remains unclear. In this work, we take a step towards\nbridging this gap by presenting the first fully-coherent quantum implementation\nof a multilayer neural network with non-linear activation functions. Our\nconstructions mirror widely used deep learning architectures based on ResNet,\nand consist of residual blocks with multi-filter 2D convolutions, sigmoid\nactivations, skip-connections, and layer normalizations. We analyse the\ncomplexity of inference for networks under three quantum data access regimes.\nWithout any assumptions, we establish a quadratic speedup over classical\nmethods for shallow bilinear-style networks. With efficient quantum access to\nthe weights, we obtain a quartic speedup over classical methods. With efficient\nquantum access to both the inputs and the network weights, we prove that a\nnetwork with an $N$-dimensional vectorized input, $k$ residual block layers,\nand a final residual-linear-pooling layer can be implemented with an error of\n$\\epsilon$ with $O(\\text{polylog}(N/\\epsilon)^k)$ inference cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault-tolerant Quantum Processing Units (QPUs) promise to deliver exponential\nspeed-ups in select computational tasks, yet their integration into modern deep\nlearning pipelines remains unclear. In this work, we take a step towards\nbridging this gap by presenting the first fully-coherent quantum implementation\nof a multilayer neural network with non-linear activation functions. Our\nconstructions mirror widely used deep learning architectures based on ResNet,\nand consist of residual blocks with multi-filter 2D convolutions, sigmoid\nactivations, skip-connections, and layer normalizations. We analyse the\ncomplexity of inference for networks under three quantum data access regimes.\nWithout any assumptions, we establish a quadratic speedup over classical\nmethods for shallow bilinear-style networks. With efficient quantum access to\nthe weights, we obtain a quartic speedup over classical methods. With efficient\nquantum access to both the inputs and the network weights, we prove that a\nnetwork with an $N$-dimensional vectorized input, $k$ residual block layers,\nand a final residual-linear-pooling layer can be implemented with an error of\n$\\epsilon$ with $O(\\text{polylog}(N/\\epsilon)^k)$ inference cost."
                },
                "authors": [
                    {
                        "name": "Arthur G. Rattew"
                    },
                    {
                        "name": "Po-Wei Huang"
                    },
                    {
                        "name": "Naixu Guo"
                    },
                    {
                        "name": "Lirandë Pira"
                    },
                    {
                        "name": "Patrick Rebentrost"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Rebentrost"
                },
                "author": "Patrick Rebentrost",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07192v1",
                "updated": "2025-10-08T16:25:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    25,
                    5,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:25:05Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    25,
                    5,
                    2,
                    281,
                    0
                ],
                "title": "Poisoning Attacks on LLMs Require a Near-constant Number of Poison\n  Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poisoning Attacks on LLMs Require a Near-constant Number of Poison\n  Samples"
                },
                "summary": "Poisoning attacks can compromise the safety of large language models (LLMs)\nby injecting malicious documents into their training data. Existing work has\nstudied pretraining poisoning assuming adversaries control a percentage of the\ntraining corpus. However, for large models, even small percentages translate to\nimpractically large amounts of data. This work demonstrates for the first time\nthat poisoning attacks instead require a near-constant number of documents\nregardless of dataset size. We conduct the largest pretraining poisoning\nexperiments to date, pretraining models from 600M to 13B parameters on\nchinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned\ndocuments similarly compromise models across all model and dataset sizes,\ndespite the largest models training on more than 20 times more clean data. We\nalso run smaller-scale experiments to ablate factors that could influence\nattack success, including broader ratios of poisoned to clean data and\nnon-random distributions of poisoned samples. Finally, we demonstrate the same\ndynamics for poisoning during fine-tuning. Altogether, our results suggest that\ninjecting backdoors through data poisoning may be easier for large models than\npreviously believed as the number of poisons required does not scale up with\nmodel size, highlighting the need for more research on defences to mitigate\nthis risk in future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poisoning attacks can compromise the safety of large language models (LLMs)\nby injecting malicious documents into their training data. Existing work has\nstudied pretraining poisoning assuming adversaries control a percentage of the\ntraining corpus. However, for large models, even small percentages translate to\nimpractically large amounts of data. This work demonstrates for the first time\nthat poisoning attacks instead require a near-constant number of documents\nregardless of dataset size. We conduct the largest pretraining poisoning\nexperiments to date, pretraining models from 600M to 13B parameters on\nchinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned\ndocuments similarly compromise models across all model and dataset sizes,\ndespite the largest models training on more than 20 times more clean data. We\nalso run smaller-scale experiments to ablate factors that could influence\nattack success, including broader ratios of poisoned to clean data and\nnon-random distributions of poisoned samples. Finally, we demonstrate the same\ndynamics for poisoning during fine-tuning. Altogether, our results suggest that\ninjecting backdoors through data poisoning may be easier for large models than\npreviously believed as the number of poisons required does not scale up with\nmodel size, highlighting the need for more research on defences to mitigate\nthis risk in future models."
                },
                "authors": [
                    {
                        "name": "Alexandra Souly"
                    },
                    {
                        "name": "Javier Rando"
                    },
                    {
                        "name": "Ed Chapman"
                    },
                    {
                        "name": "Xander Davies"
                    },
                    {
                        "name": "Burak Hasircioglu"
                    },
                    {
                        "name": "Ezzeldin Shereen"
                    },
                    {
                        "name": "Carlos Mougan"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Erik Jones"
                    },
                    {
                        "name": "Chris Hicks"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Robert Kirk"
                    }
                ],
                "author_detail": {
                    "name": "Robert Kirk"
                },
                "author": "Robert Kirk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.17432v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.17432v7",
                "updated": "2025-10-08T16:24:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    24,
                    55,
                    2,
                    281,
                    0
                ],
                "published": "2023-12-29T01:56:17Z",
                "published_parsed": [
                    2023,
                    12,
                    29,
                    1,
                    56,
                    17,
                    4,
                    363,
                    0
                ],
                "title": "Video Understanding with Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Understanding with Large Language Models: A Survey"
                },
                "summary": "With the burgeoning growth of online video platforms and the escalating\nvolume of video content, the demand for proficient video understanding tools\nhas intensified markedly. Given the remarkable capabilities of large language\nmodels (LLMs) in language and multimodal tasks, this survey provides a detailed\noverview of recent advancements in video understanding that harness the power\nof LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\nadvanced, particularly their ability for open-ended multi-granularity (general,\ntemporal, and spatiotemporal) reasoning combined with commonsense knowledge,\nsuggesting a promising path for future video understanding. We examine the\nunique characteristics and capabilities of Vid-LLMs, categorizing the\napproaches into three main types: Video Analyzer x LLM, Video Embedder x LLM,\nand (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based\non the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as\nText Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this\nsurvey presents a comprehensive study of the tasks, datasets, benchmarks, and\nevaluation methodologies for Vid-LLMs. Additionally, it explores the expansive\napplications of Vid-LLMs across various domains, highlighting their remarkable\nscalability and versatility in real-world video understanding challenges.\nFinally, it summarizes the limitations of existing Vid-LLMs and outlines\ndirections for future research. For more information, readers are recommended\nto visit the repository at\nhttps://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning growth of online video platforms and the escalating\nvolume of video content, the demand for proficient video understanding tools\nhas intensified markedly. Given the remarkable capabilities of large language\nmodels (LLMs) in language and multimodal tasks, this survey provides a detailed\noverview of recent advancements in video understanding that harness the power\nof LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\nadvanced, particularly their ability for open-ended multi-granularity (general,\ntemporal, and spatiotemporal) reasoning combined with commonsense knowledge,\nsuggesting a promising path for future video understanding. We examine the\nunique characteristics and capabilities of Vid-LLMs, categorizing the\napproaches into three main types: Video Analyzer x LLM, Video Embedder x LLM,\nand (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based\non the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as\nText Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this\nsurvey presents a comprehensive study of the tasks, datasets, benchmarks, and\nevaluation methodologies for Vid-LLMs. Additionally, it explores the expansive\napplications of Vid-LLMs across various domains, highlighting their remarkable\nscalability and versatility in real-world video understanding challenges.\nFinally, it summarizes the limitations of existing Vid-LLMs and outlines\ndirections for future research. For more information, readers are recommended\nto visit the repository at\nhttps://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding."
                },
                "authors": [
                    {
                        "name": "Yolo Yunlong Tang"
                    },
                    {
                        "name": "Jing Bi"
                    },
                    {
                        "name": "Siting Xu"
                    },
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Daoan Zhang"
                    },
                    {
                        "name": "Jie An"
                    },
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Rongyi Zhu"
                    },
                    {
                        "name": "Ali Vosoughi"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Pinxin Liu"
                    },
                    {
                        "name": "Mingqian Feng"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "Accepted to IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.17432v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.17432v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07190v1",
                "updated": "2025-10-08T16:24:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    24,
                    22,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:24:22Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    24,
                    22,
                    2,
                    281,
                    0
                ],
                "title": "MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized\n  Multi-view Performer Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized\n  Multi-view Performer Synthesis"
                },
                "summary": "Recent breakthroughs in video generation, powered by large-scale datasets and\ndiffusion techniques, have shown that video diffusion models can function as\nimplicit 4D novel view synthesizers. Nevertheless, current methods primarily\nconcentrate on redirecting camera trajectory within the front view while\nstruggling to generate 360-degree viewpoint changes. In this paper, we focus on\nhuman-centric subdomain and present MV-Performer, an innovative framework for\ncreating synchronized novel view videos from monocular full-body captures. To\nachieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset\nand incorporate an informative condition signal. Specifically, we use the\ncamera-dependent normal maps rendered from oriented partial point clouds, which\neffectively alleviate the ambiguity between seen and unseen observations. To\nmaintain synchronization in the generated videos, we propose a multi-view\nhuman-centric video diffusion model that fuses information from the reference\nvideo, partial rendering, and different viewpoints. Additionally, we provide a\nrobust inference procedure for in-the-wild video cases, which greatly mitigates\nthe artifacts induced by imperfect monocular depth estimation. Extensive\nexperiments on three datasets demonstrate our MV-Performer's state-of-the-art\neffectiveness and robustness, setting a strong model for human-centric 4D novel\nview synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in video generation, powered by large-scale datasets and\ndiffusion techniques, have shown that video diffusion models can function as\nimplicit 4D novel view synthesizers. Nevertheless, current methods primarily\nconcentrate on redirecting camera trajectory within the front view while\nstruggling to generate 360-degree viewpoint changes. In this paper, we focus on\nhuman-centric subdomain and present MV-Performer, an innovative framework for\ncreating synchronized novel view videos from monocular full-body captures. To\nachieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset\nand incorporate an informative condition signal. Specifically, we use the\ncamera-dependent normal maps rendered from oriented partial point clouds, which\neffectively alleviate the ambiguity between seen and unseen observations. To\nmaintain synchronization in the generated videos, we propose a multi-view\nhuman-centric video diffusion model that fuses information from the reference\nvideo, partial rendering, and different viewpoints. Additionally, we provide a\nrobust inference procedure for in-the-wild video cases, which greatly mitigates\nthe artifacts induced by imperfect monocular depth estimation. Extensive\nexperiments on three datasets demonstrate our MV-Performer's state-of-the-art\neffectiveness and robustness, setting a strong model for human-centric 4D novel\nview synthesis."
                },
                "authors": [
                    {
                        "name": "Yihao Zhi"
                    },
                    {
                        "name": "Chenghong Li"
                    },
                    {
                        "name": "Hongjie Liao"
                    },
                    {
                        "name": "Xihe Yang"
                    },
                    {
                        "name": "Zhengwentai Sun"
                    },
                    {
                        "name": "Jiahao Chang"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wensen Feng"
                    },
                    {
                        "name": "Xiaoguang Han"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Han"
                },
                "author": "Xiaoguang Han",
                "arxiv_comment": "Accepted by SIGGRAPH Asia 2025 conference track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07189v1",
                "updated": "2025-10-08T16:24:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    24,
                    9,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:24:09Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    24,
                    9,
                    2,
                    281,
                    0
                ],
                "title": "Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe"
                },
                "summary": "Although Large Language Models (LLMs) show promising solutions to automated\ncode generation, they often produce insecure code that threatens software\nsecurity. Current approaches (e.g., SafeCoder) to improve secure code\ngeneration suffer from limited and imbalanced datasets, reducing their\neffectiveness and generalizability. In this work, we present Secure-Instruct, a\nnovel framework that automatically synthesizes high-quality vulnerable and\nsecure code examples, generates fine-tuning instructions, and instruction-tunes\nLLMs to align task description and secure code generation abilities. We\nevaluate Secure-Instruct on four representative LLMs using two benchmarks: our\nown CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44\nCWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning\ndataset, while CWEval covers 31 CWEs with 119 manually verified\nsecurity-critical tasks. We find that Secure-Instruct improves not only the\nsecurity but also the functional correctness of the generated code. On\nCWEBench, Secure-Instruct substantially improves secure code generation, giving\na 14.3% average increase in secure ratio over the pretrained models and\noutperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%\nincrease for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained\nmodels, and surpasses SafeCoder by 15.8% and 6.8% respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) show promising solutions to automated\ncode generation, they often produce insecure code that threatens software\nsecurity. Current approaches (e.g., SafeCoder) to improve secure code\ngeneration suffer from limited and imbalanced datasets, reducing their\neffectiveness and generalizability. In this work, we present Secure-Instruct, a\nnovel framework that automatically synthesizes high-quality vulnerable and\nsecure code examples, generates fine-tuning instructions, and instruction-tunes\nLLMs to align task description and secure code generation abilities. We\nevaluate Secure-Instruct on four representative LLMs using two benchmarks: our\nown CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44\nCWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning\ndataset, while CWEval covers 31 CWEs with 119 manually verified\nsecurity-critical tasks. We find that Secure-Instruct improves not only the\nsecurity but also the functional correctness of the generated code. On\nCWEBench, Secure-Instruct substantially improves secure code generation, giving\na 14.3% average increase in secure ratio over the pretrained models and\noutperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%\nincrease for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained\nmodels, and surpasses SafeCoder by 15.8% and 6.8% respectively."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Fazle Rabbi"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jinqiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiu Yang"
                },
                "author": "Jinqiu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03122v2",
                "updated": "2025-10-08T16:23:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    23,
                    32,
                    2,
                    281,
                    0
                ],
                "published": "2025-09-03T08:22:04Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    22,
                    4,
                    2,
                    246,
                    0
                ],
                "title": "From Injection to Defense: Constructing Edit-Based Fingerprints for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Injection to Defense: Constructing Edit-Based Fingerprints for\n  Large Language Models"
                },
                "summary": "Fingerprinting is critical for maintaining traceability and protecting the\nintellectual property (IP) of developers, as LLMs deployed in web applications\nare susceptible to unauthorized redistribution and misuse via fine-tuning or\nblack-box deployment. However, current backdoor-based fingerprinting methods\nface a fundamental trade-off: fingerprints embedded as garbled text are easily\ndetected and filtered, whereas those crafted as coherent natural language are\nprone to being triggered unintentionally. To overcome these limitations, we\npropose RFEdit, a knowledge-editing framework that embeds a rule-based\nmultilingual natural language fingerprint (MNLF) by modifying a sparse subset\nof model weights. This approach enables efficient and robust fingerprint\ninjection with minimal impact on unrelated knowledge in LLMs. Our RFEdit\nframework is further safeguarded by Fingerprint Subspace-aware Fine-Tuning\n(FSFT), which mitigates fingerprint degradation during legitimate fine-tuning\nby restricting parameter updates to the fingerprint subspace. This approach\npreserves fingerprint integrity while enhancing downstream task performance of\nLLMs. These advances establish a comprehensive pipeline from fingerprint\ninjection to defense, achieving high detection effectiveness, robustness\nagainst adversarial manipulations, harmlessness to model utility, and\npersistence under fine-tuning. Extensive experiments demonstrate that RFEdit\nmaintains robustness under quantization and pruning. Additionally, fingerprint\neffectiveness is generally improved by more than 10\\% when combined with FSFT\nfor math and alpaca downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprinting is critical for maintaining traceability and protecting the\nintellectual property (IP) of developers, as LLMs deployed in web applications\nare susceptible to unauthorized redistribution and misuse via fine-tuning or\nblack-box deployment. However, current backdoor-based fingerprinting methods\nface a fundamental trade-off: fingerprints embedded as garbled text are easily\ndetected and filtered, whereas those crafted as coherent natural language are\nprone to being triggered unintentionally. To overcome these limitations, we\npropose RFEdit, a knowledge-editing framework that embeds a rule-based\nmultilingual natural language fingerprint (MNLF) by modifying a sparse subset\nof model weights. This approach enables efficient and robust fingerprint\ninjection with minimal impact on unrelated knowledge in LLMs. Our RFEdit\nframework is further safeguarded by Fingerprint Subspace-aware Fine-Tuning\n(FSFT), which mitigates fingerprint degradation during legitimate fine-tuning\nby restricting parameter updates to the fingerprint subspace. This approach\npreserves fingerprint integrity while enhancing downstream task performance of\nLLMs. These advances establish a comprehensive pipeline from fingerprint\ninjection to defense, achieving high detection effectiveness, robustness\nagainst adversarial manipulations, harmlessness to model utility, and\npersistence under fine-tuning. Extensive experiments demonstrate that RFEdit\nmaintains robustness under quantization and pruning. Additionally, fingerprint\neffectiveness is generally improved by more than 10\\% when combined with FSFT\nfor math and alpaca downstream tasks."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Dongsheng Shi"
                    },
                    {
                        "name": "Yongyi Cui"
                    },
                    {
                        "name": "Gerard de Melo"
                    },
                    {
                        "name": "Linlin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Linlin Wang"
                },
                "author": "Linlin Wang",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07187v1",
                "updated": "2025-10-08T16:23:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    23,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:23:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    23,
                    4,
                    2,
                    281,
                    0
                ],
                "title": "Fomalhaut's debris disc is not dominated by primordial Plutos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fomalhaut's debris disc is not dominated by primordial Plutos"
                },
                "summary": "A key challenge in debris-disc science is that we do not know the masses of\ndebris discs, nor the sizes of the largest debris bodies. This is because\nmodern observations can only detect objects up to centimetre sizes, whilst\nlarger planetesimals, which dominate disc mass, remain hidden. We must\ntherefore use other arguments, such as dynamics, to indirectly infer disc\nmasses and body sizes. This paper presents a new method, applicable to narrow\ndebris discs like Fomalhaut. We argue that such discs cannot be too massive,\nnor the largest bodies too large, otherwise they would self-scatter and the\ndisc would be much broader than observed. Using n-body dynamics and collisional\ntheory, we demonstrate that the mass of Fomalhaut's disc cannot be dominated by\nprimordial Plutos. Instead, if the mass is dominated by primordial bodies, then\nthey should have radii below $300^{+80}_{-70}$ km ($0.3 \\pm 0.1$ RPluto) and\nabove $5^{+20}_{-4}$ km. Such bodies would each have less than 1 per cent the\nmass of Pluto. Our conclusions are robust to additional physics, including\nshepherding planets and collisional damping. Our results provide independent,\ndynamical support for the idea that the masses of bright debris discs are\ndominated by objects smaller than Pluto.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in debris-disc science is that we do not know the masses of\ndebris discs, nor the sizes of the largest debris bodies. This is because\nmodern observations can only detect objects up to centimetre sizes, whilst\nlarger planetesimals, which dominate disc mass, remain hidden. We must\ntherefore use other arguments, such as dynamics, to indirectly infer disc\nmasses and body sizes. This paper presents a new method, applicable to narrow\ndebris discs like Fomalhaut. We argue that such discs cannot be too massive,\nnor the largest bodies too large, otherwise they would self-scatter and the\ndisc would be much broader than observed. Using n-body dynamics and collisional\ntheory, we demonstrate that the mass of Fomalhaut's disc cannot be dominated by\nprimordial Plutos. Instead, if the mass is dominated by primordial bodies, then\nthey should have radii below $300^{+80}_{-70}$ km ($0.3 \\pm 0.1$ RPluto) and\nabove $5^{+20}_{-4}$ km. Such bodies would each have less than 1 per cent the\nmass of Pluto. Our conclusions are robust to additional physics, including\nshepherding planets and collisional damping. Our results provide independent,\ndynamical support for the idea that the masses of bright debris discs are\ndominated by objects smaller than Pluto."
                },
                "authors": [
                    {
                        "name": "Tim D. Pearce"
                    },
                    {
                        "name": "Torsten Löhne"
                    },
                    {
                        "name": "Alexander V. Krivov"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Krivov"
                },
                "author": "Alexander V. Krivov",
                "arxiv_comment": "Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07186v1",
                "updated": "2025-10-08T16:22:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    22,
                    53,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:22:53Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    22,
                    53,
                    2,
                    281,
                    0
                ],
                "title": "Renormalization of Interacting Random Graph Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Renormalization of Interacting Random Graph Models"
                },
                "summary": "Random graphs offer a useful mathematical representation of a variety of real\nworld complex networks. Exponential random graphs, for example, are\nparticularly suited towards generating random graphs constrained to have\nspecified statistical moments. In this investigation, we elaborate on a\ngeneralization of the former where link probabilities are conditioned on the\nappearance of other links, corresponding to the introduction of interactions in\nan effective generalized statistical mechanical formalism. When restricted to\nthe simplest non-trivial case of pairwise interactions, one can derive a closed\nform renormalization group transformation for maximum coordination number two\non the corresponding line graph. Higher coordination numbers do not admit exact\nclosed form renormalization group transformations, a feature that paraphrases\nthe usual absence of exact transformations in two or more dimensional lattice\nsystems. We introduce disorder and study the induced renormalization group flow\non its probability assignments, highlighting its formal equivalence to time\nreversed anisotropic drift-diffusion on the statistical manifold associated\nwith the effective Hamiltonian. We discuss the implications of our findings,\nstressing the long wavelength irrelevance of certain classes of pair-wise\nconditioning on random graphs, and conclude with possible applications. These\ninclude modeling the scaling behavior of preferential effects on social\nnetworks, opinion dynamics, and reinforcement effects on neural networks, as\nwell as how our findings offer a systematic framework to deal with data\nlimitations in inference and reconstruction problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random graphs offer a useful mathematical representation of a variety of real\nworld complex networks. Exponential random graphs, for example, are\nparticularly suited towards generating random graphs constrained to have\nspecified statistical moments. In this investigation, we elaborate on a\ngeneralization of the former where link probabilities are conditioned on the\nappearance of other links, corresponding to the introduction of interactions in\nan effective generalized statistical mechanical formalism. When restricted to\nthe simplest non-trivial case of pairwise interactions, one can derive a closed\nform renormalization group transformation for maximum coordination number two\non the corresponding line graph. Higher coordination numbers do not admit exact\nclosed form renormalization group transformations, a feature that paraphrases\nthe usual absence of exact transformations in two or more dimensional lattice\nsystems. We introduce disorder and study the induced renormalization group flow\non its probability assignments, highlighting its formal equivalence to time\nreversed anisotropic drift-diffusion on the statistical manifold associated\nwith the effective Hamiltonian. We discuss the implications of our findings,\nstressing the long wavelength irrelevance of certain classes of pair-wise\nconditioning on random graphs, and conclude with possible applications. These\ninclude modeling the scaling behavior of preferential effects on social\nnetworks, opinion dynamics, and reinforcement effects on neural networks, as\nwell as how our findings offer a systematic framework to deal with data\nlimitations in inference and reconstruction problems."
                },
                "authors": [
                    {
                        "name": "Alessio Catanzaro"
                    },
                    {
                        "name": "Diego Garlaschelli"
                    },
                    {
                        "name": "Subodh P. Patil"
                    }
                ],
                "author_detail": {
                    "name": "Subodh P. Patil"
                },
                "author": "Subodh P. Patil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07182v2",
                "updated": "2025-10-09T04:57:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    4,
                    57,
                    59,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-08T16:20:49Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    20,
                    49,
                    2,
                    281,
                    0
                ],
                "title": "Bridged Clustering for Representation Learning: Semi-Supervised Sparse\n  Bridging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridged Clustering for Representation Learning: Semi-Supervised Sparse\n  Bridging"
                },
                "summary": "We introduce Bridged Clustering, a semi-supervised framework to learn\npredictors from any unpaired input $X$ and output $Y$ dataset. Our method first\nclusters $X$ and $Y$ independently, then learns a sparse, interpretable bridge\nbetween clusters using only a few paired examples. At inference, a new input\n$x$ is assigned to its nearest input cluster, and the centroid of the linked\noutput cluster is returned as the prediction $\\hat{y}$. Unlike traditional SSL,\nBridged Clustering explicitly leverages output-only data, and unlike dense\ntransport-based methods, it maintains a sparse and interpretable alignment.\nThrough theoretical analysis, we show that with bounded mis-clustering and\nmis-bridging rates, our algorithm becomes an effective and efficient predictor.\nEmpirically, our method is competitive with SOTA methods while remaining\nsimple, model-agnostic, and highly label-efficient in low-supervision settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Bridged Clustering, a semi-supervised framework to learn\npredictors from any unpaired input $X$ and output $Y$ dataset. Our method first\nclusters $X$ and $Y$ independently, then learns a sparse, interpretable bridge\nbetween clusters using only a few paired examples. At inference, a new input\n$x$ is assigned to its nearest input cluster, and the centroid of the linked\noutput cluster is returned as the prediction $\\hat{y}$. Unlike traditional SSL,\nBridged Clustering explicitly leverages output-only data, and unlike dense\ntransport-based methods, it maintains a sparse and interpretable alignment.\nThrough theoretical analysis, we show that with bounded mis-clustering and\nmis-bridging rates, our algorithm becomes an effective and efficient predictor.\nEmpirically, our method is competitive with SOTA methods while remaining\nsimple, model-agnostic, and highly label-efficient in low-supervision settings."
                },
                "authors": [
                    {
                        "name": "Patrick Peixuan Ye"
                    },
                    {
                        "name": "Chen Shani"
                    },
                    {
                        "name": "Ellen Vitercik"
                    }
                ],
                "author_detail": {
                    "name": "Ellen Vitercik"
                },
                "author": "Ellen Vitercik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07178v1",
                "updated": "2025-10-08T16:17:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    17,
                    13,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:17:13Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    17,
                    13,
                    2,
                    281,
                    0
                ],
                "title": "Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish\n  the Possible from the Impossible",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish\n  the Possible from the Impossible"
                },
                "summary": "Are large language models (LLMs) sensitive to the distinction between humanly\npossible languages and humanly impossible languages? This question is taken by\nmany to bear on whether LLMs and humans share the same innate learning biases.\nPrevious work has attempted to answer it in the positive by comparing LLM\nlearning curves on existing language datasets and on \"impossible\" datasets\nderived from them via various perturbation functions. Using the same\nmethodology, we examine this claim on a wider set of languages and impossible\nperturbations. We find that in most cases, GPT-2 learns each language and its\nimpossible counterpart equally easily, in contrast to previous claims. We also\napply a more lenient condition by testing whether GPT-2 provides any kind of\nseparation between the whole set of natural languages and the whole set of\nimpossible languages. By considering cross-linguistic variance in various\nmetrics computed on the perplexity curves, we show that GPT-2 provides no\nsystematic separation between the possible and the impossible. Taken together,\nthese perspectives show that LLMs do not share the human innate biases that\nshape linguistic typology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are large language models (LLMs) sensitive to the distinction between humanly\npossible languages and humanly impossible languages? This question is taken by\nmany to bear on whether LLMs and humans share the same innate learning biases.\nPrevious work has attempted to answer it in the positive by comparing LLM\nlearning curves on existing language datasets and on \"impossible\" datasets\nderived from them via various perturbation functions. Using the same\nmethodology, we examine this claim on a wider set of languages and impossible\nperturbations. We find that in most cases, GPT-2 learns each language and its\nimpossible counterpart equally easily, in contrast to previous claims. We also\napply a more lenient condition by testing whether GPT-2 provides any kind of\nseparation between the whole set of natural languages and the whole set of\nimpossible languages. By considering cross-linguistic variance in various\nmetrics computed on the perplexity curves, we show that GPT-2 provides no\nsystematic separation between the possible and the impossible. Taken together,\nthese perspectives show that LLMs do not share the human innate biases that\nshape linguistic typology."
                },
                "authors": [
                    {
                        "name": "Imry Ziv"
                    },
                    {
                        "name": "Nur Lan"
                    },
                    {
                        "name": "Emmanuel Chemla"
                    },
                    {
                        "name": "Roni Katzir"
                    }
                ],
                "author_detail": {
                    "name": "Roni Katzir"
                },
                "author": "Roni Katzir",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07177v1",
                "updated": "2025-10-08T16:16:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    16,
                    46,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:16:46Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    16,
                    46,
                    2,
                    281,
                    0
                ],
                "title": "CARPAS: Towards Content-Aware Refinement of Provided Aspects for\n  Summarization in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARPAS: Towards Content-Aware Refinement of Provided Aspects for\n  Summarization in Large Language Models"
                },
                "summary": "Aspect-based summarization has attracted significant attention for its\nability to generate more fine-grained and user-aligned summaries. While most\nexisting approaches assume a set of predefined aspects as input, real-world\nscenarios often present challenges where these given aspects may be incomplete,\nirrelevant, or entirely missing from the document. Users frequently expect\nsystems to adaptively refine or filter the provided aspects based on the actual\ncontent. In this paper, we initiate this novel task setting, termed\nContent-Aware Refinement of Provided Aspects for Summarization (CARPAS), with\nthe aim of dynamically adjusting the provided aspects based on the document\ncontext before summarizing. We construct three new datasets to facilitate our\npilot experiments, and by using LLMs with four representative prompting\nstrategies in this task, we find that LLMs tend to predict an overly\ncomprehensive set of aspects, which often results in excessively long and\nmisaligned summaries. Building on this observation, we propose a preliminary\nsubtask to predict the number of relevant aspects, and demonstrate that the\npredicted number can serve as effective guidance for the LLMs, reducing the\ninference difficulty, and enabling them to focus on the most pertinent aspects.\nOur extensive experiments show that the proposed approach significantly\nimproves performance across all datasets. Moreover, our deeper analyses uncover\nLLMs' compliance when the requested number of aspects differs from their own\nestimations, establishing a crucial insight for the deployment of LLMs in\nsimilar real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based summarization has attracted significant attention for its\nability to generate more fine-grained and user-aligned summaries. While most\nexisting approaches assume a set of predefined aspects as input, real-world\nscenarios often present challenges where these given aspects may be incomplete,\nirrelevant, or entirely missing from the document. Users frequently expect\nsystems to adaptively refine or filter the provided aspects based on the actual\ncontent. In this paper, we initiate this novel task setting, termed\nContent-Aware Refinement of Provided Aspects for Summarization (CARPAS), with\nthe aim of dynamically adjusting the provided aspects based on the document\ncontext before summarizing. We construct three new datasets to facilitate our\npilot experiments, and by using LLMs with four representative prompting\nstrategies in this task, we find that LLMs tend to predict an overly\ncomprehensive set of aspects, which often results in excessively long and\nmisaligned summaries. Building on this observation, we propose a preliminary\nsubtask to predict the number of relevant aspects, and demonstrate that the\npredicted number can serve as effective guidance for the LLMs, reducing the\ninference difficulty, and enabling them to focus on the most pertinent aspects.\nOur extensive experiments show that the proposed approach significantly\nimproves performance across all datasets. Moreover, our deeper analyses uncover\nLLMs' compliance when the requested number of aspects differs from their own\nestimations, establishing a crucial insight for the deployment of LLMs in\nsimilar real-world applications."
                },
                "authors": [
                    {
                        "name": "Yong-En Tian"
                    },
                    {
                        "name": "Yu-Chien Tang"
                    },
                    {
                        "name": "An-Zi Yen"
                    },
                    {
                        "name": "Wen-Chih Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Chih Peng"
                },
                "author": "Wen-Chih Peng",
                "arxiv_comment": "22 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07176v1",
                "updated": "2025-10-08T16:16:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    16,
                    23,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:16:23Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    16,
                    23,
                    2,
                    281,
                    0
                ],
                "title": "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of\n  Privacy Risks in LLM Agent Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of\n  Privacy Risks in LLM Agent Interactions"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed as agents that\norchestrate tasks and integrate external tools to execute complex workflows. We\ndemonstrate that these interactive behaviors leave distinctive fingerprints in\nencrypted traffic exchanged between users and LLM agents. By analyzing traffic\npatterns associated with agent workflows and tool invocations, adversaries can\ninfer agent activities, distinguish specific agents, and even profile sensitive\nuser attributes. To highlight this risk, we develop AgentPrint, which achieves\nan F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3\naccuracy in user attribute inference for simulated- and real-user settings,\nrespectively. These results uncover an overlooked risk: the very interactivity\nthat empowers LLM agents also exposes user privacy, underscoring the urgent\nneed for technical countermeasures alongside regulatory and policy safeguards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed as agents that\norchestrate tasks and integrate external tools to execute complex workflows. We\ndemonstrate that these interactive behaviors leave distinctive fingerprints in\nencrypted traffic exchanged between users and LLM agents. By analyzing traffic\npatterns associated with agent workflows and tool invocations, adversaries can\ninfer agent activities, distinguish specific agents, and even profile sensitive\nuser attributes. To highlight this risk, we develop AgentPrint, which achieves\nan F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3\naccuracy in user attribute inference for simulated- and real-user settings,\nrespectively. These results uncover an overlooked risk: the very interactivity\nthat empowers LLM agents also exposes user privacy, underscoring the urgent\nneed for technical countermeasures alongside regulatory and policy safeguards."
                },
                "authors": [
                    {
                        "name": "Yixiang Zhang"
                    },
                    {
                        "name": "Xinhao Deng"
                    },
                    {
                        "name": "Zhongyi Gu"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Jianping Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Wu"
                },
                "author": "Jianping Wu",
                "arxiv_comment": "26 pages with 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07175v1",
                "updated": "2025-10-08T16:16:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    16,
                    20,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:16:20Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    16,
                    20,
                    2,
                    281,
                    0
                ],
                "title": "Quantifying Data Contamination in Psychometric Evaluations of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Data Contamination in Psychometric Evaluations of LLMs"
                },
                "summary": "Recent studies apply psychometric questionnaires to Large Language Models\n(LLMs) to assess high-level psychological constructs such as values,\npersonality, moral foundations, and dark traits. Although prior work has raised\nconcerns about possible data contamination from psychometric inventories, which\nmay threaten the reliability of such evaluations, there has been no systematic\nattempt to quantify the extent of this contamination. To address this gap, we\npropose a framework to systematically measure data contamination in\npsychometric evaluations of LLMs, evaluating three aspects: (1) item\nmemorization, (2) evaluation memorization, and (3) target score matching.\nApplying this framework to 21 models from major families and four widely used\npsychometric inventories, we provide evidence that popular inventories such as\nthe Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)\nexhibit strong contamination, where models not only memorize items but can also\nadjust their responses to achieve specific target scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies apply psychometric questionnaires to Large Language Models\n(LLMs) to assess high-level psychological constructs such as values,\npersonality, moral foundations, and dark traits. Although prior work has raised\nconcerns about possible data contamination from psychometric inventories, which\nmay threaten the reliability of such evaluations, there has been no systematic\nattempt to quantify the extent of this contamination. To address this gap, we\npropose a framework to systematically measure data contamination in\npsychometric evaluations of LLMs, evaluating three aspects: (1) item\nmemorization, (2) evaluation memorization, and (3) target score matching.\nApplying this framework to 21 models from major families and four widely used\npsychometric inventories, we provide evidence that popular inventories such as\nthe Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)\nexhibit strong contamination, where models not only memorize items but can also\nadjust their responses to achieve specific target scores."
                },
                "authors": [
                    {
                        "name": "Jongwook Han"
                    },
                    {
                        "name": "Woojung Song"
                    },
                    {
                        "name": "Jonggeun Lee"
                    },
                    {
                        "name": "Yohan Jo"
                    }
                ],
                "author_detail": {
                    "name": "Yohan Jo"
                },
                "author": "Yohan Jo",
                "arxiv_comment": "12 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07174v1",
                "updated": "2025-10-08T16:15:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    15,
                    55,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:15:55Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    15,
                    55,
                    2,
                    281,
                    0
                ],
                "title": "Archival Inference for Eccentric Stellar-Mass Binary Black Holes in\n  Space-Based Gravitational Wave Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Archival Inference for Eccentric Stellar-Mass Binary Black Holes in\n  Space-Based Gravitational Wave Observations"
                },
                "summary": "Space-based gravitational-wave observatories will detect the early inspiral\nof stellar-mass binary black holes and can track their eccentricity evolution.\nHowever, untargeted searches in the space band are computationally demanding\nand require relatively high detection thresholds (signal-to-noise ratio $\\sim\n15$). Information from ground-based detections can significantly shrink the\nparameter space for space-band analyses and thereby substantially reduce the\ndetection threshold. We present a Bayesian inference pipeline for\nground-triggered archival space-band analyses that includes eccentricity. Using\nground-informed priors, we demonstrate that with one year of LISA or TianQin\ndata a GW190521-like source with signal-to-noise ratio $\\sim 7$ can be\ndistinguished and tightly constrained. In this setup, space observations\nsharpened the redshifted chirp mass from $\\mathcal{O}(10^{-3})M_\\odot$ to\n$\\mathcal{O}(10^{-5})M_\\odot$, and constrain the eccentricity to\n$\\mathcal{O}(10^{-5})$ around the injected value $e_{0.01\\mathrm{Hz}}=0.1$.\nThese results demonstrate that inference of eccentric stellar-mass binary black\nholes in noisy space-band data is practically feasible, supports an expanded\nyield of multiband detections, and strengthens prospects for future\nastrophysical and gravitational tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space-based gravitational-wave observatories will detect the early inspiral\nof stellar-mass binary black holes and can track their eccentricity evolution.\nHowever, untargeted searches in the space band are computationally demanding\nand require relatively high detection thresholds (signal-to-noise ratio $\\sim\n15$). Information from ground-based detections can significantly shrink the\nparameter space for space-band analyses and thereby substantially reduce the\ndetection threshold. We present a Bayesian inference pipeline for\nground-triggered archival space-band analyses that includes eccentricity. Using\nground-informed priors, we demonstrate that with one year of LISA or TianQin\ndata a GW190521-like source with signal-to-noise ratio $\\sim 7$ can be\ndistinguished and tightly constrained. In this setup, space observations\nsharpened the redshifted chirp mass from $\\mathcal{O}(10^{-3})M_\\odot$ to\n$\\mathcal{O}(10^{-5})M_\\odot$, and constrain the eccentricity to\n$\\mathcal{O}(10^{-5})$ around the injected value $e_{0.01\\mathrm{Hz}}=0.1$.\nThese results demonstrate that inference of eccentric stellar-mass binary black\nholes in noisy space-band data is practically feasible, supports an expanded\nyield of multiband detections, and strengthens prospects for future\nastrophysical and gravitational tests."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Michael J. Williams"
                    },
                    {
                        "name": "Ian Harry"
                    },
                    {
                        "name": "Yi-Ming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Ming Hu"
                },
                "author": "Yi-Ming Hu",
                "arxiv_comment": "12 pages, 4 figures, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07173v1",
                "updated": "2025-10-08T16:15:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    15,
                    6,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:15:06Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    15,
                    6,
                    2,
                    281,
                    0
                ],
                "title": "NurseLLM: The First Specialized Language Model for Nursing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NurseLLM: The First Specialized Language Model for Nursing"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\ntransformed medical systems. However, their potential within specialized\ndomains such as nursing remains largely underexplored. In this work, we\nintroduce NurseLLM, the first nursing-specialized LLM tailored for multiple\nchoice question-answering (MCQ) tasks. We develop a multi-stage data generation\npipeline to build the first large scale nursing MCQ dataset to train LLMs on a\nbroad spectrum of nursing topics. We further introduce multiple nursing\nbenchmarks to enable rigorous evaluation. Our extensive experiments demonstrate\nthat NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of\ncomparable size on different benchmarks, underscoring the importance of a\nspecialized LLM for the nursing domain. Finally, we explore the role of\nreasoning and multi-agent collaboration systems in nursing, highlighting their\npromise for future research and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\ntransformed medical systems. However, their potential within specialized\ndomains such as nursing remains largely underexplored. In this work, we\nintroduce NurseLLM, the first nursing-specialized LLM tailored for multiple\nchoice question-answering (MCQ) tasks. We develop a multi-stage data generation\npipeline to build the first large scale nursing MCQ dataset to train LLMs on a\nbroad spectrum of nursing topics. We further introduce multiple nursing\nbenchmarks to enable rigorous evaluation. Our extensive experiments demonstrate\nthat NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of\ncomparable size on different benchmarks, underscoring the importance of a\nspecialized LLM for the nursing domain. Finally, we explore the role of\nreasoning and multi-agent collaboration systems in nursing, highlighting their\npromise for future research and applications."
                },
                "authors": [
                    {
                        "name": "Md Tawkat Islam Khondaker"
                    },
                    {
                        "name": "Julia Harrington"
                    },
                    {
                        "name": "Shady Shehata"
                    }
                ],
                "author_detail": {
                    "name": "Shady Shehata"
                },
                "author": "Shady Shehata",
                "arxiv_comment": "EMNLP 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14634v2",
                "updated": "2025-10-08T16:14:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    14,
                    2,
                    2,
                    281,
                    0
                ],
                "published": "2025-07-19T14:08:56Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    14,
                    8,
                    56,
                    5,
                    200,
                    0
                ],
                "title": "Universality of gravitational radiation from magnetar magnetospheres",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universality of gravitational radiation from magnetar magnetospheres"
                },
                "summary": "The intense magnetic fields inferred from magnetars suggest they may be\nstrong gravitational-wave emitters. Although emissions due to hydromagnetic\ndeformations are more promising from a detection standpoint, exterior fields\nalso contribute a strain. However, numerical evidence suggests that the free\nenergy of stable magnetospheric solutions cannot exceed a few tens of percent\nrelative to the potential state, implying that the magnetospheric contribution\nto the gravitational-wave luminosity cannot differ significantly between\nmodels. This prompts 'universality', in the sense that the strain provides a\ndirect probe of the near-surface field without being muddied by magnetospheric\ncurrents. Using a suite of three-dimensional, force-free, general-relativistic\nsolutions for dipole and dipole-plus-quadrupole fields, we find that\nspace-based interferometers may enable marginal detections out to $\\lesssim$\nkpc distances for slowly-rotating magnetars with fields of $\\gtrsim 10^{15}$ G\nindependently of internal deformations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The intense magnetic fields inferred from magnetars suggest they may be\nstrong gravitational-wave emitters. Although emissions due to hydromagnetic\ndeformations are more promising from a detection standpoint, exterior fields\nalso contribute a strain. However, numerical evidence suggests that the free\nenergy of stable magnetospheric solutions cannot exceed a few tens of percent\nrelative to the potential state, implying that the magnetospheric contribution\nto the gravitational-wave luminosity cannot differ significantly between\nmodels. This prompts 'universality', in the sense that the strain provides a\ndirect probe of the near-surface field without being muddied by magnetospheric\ncurrents. Using a suite of three-dimensional, force-free, general-relativistic\nsolutions for dipole and dipole-plus-quadrupole fields, we find that\nspace-based interferometers may enable marginal detections out to $\\lesssim$\nkpc distances for slowly-rotating magnetars with fields of $\\gtrsim 10^{15}$ G\nindependently of internal deformations."
                },
                "authors": [
                    {
                        "name": "Arthur G. Suvorov"
                    },
                    {
                        "name": "Petros Stefanou"
                    },
                    {
                        "name": "José A. Pons"
                    }
                ],
                "author_detail": {
                    "name": "José A. Pons"
                },
                "author": "José A. Pons",
                "arxiv_doi": "10.1103/cptt-s1kv",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/cptt-s1kv",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.14634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 7 figures. Accepted for publication in PRD",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07172v1",
                "updated": "2025-10-08T16:12:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    12,
                    11,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:12:11Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    12,
                    11,
                    2,
                    281,
                    0
                ],
                "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM\n  Agents"
                },
                "summary": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Kelvin Kiu-Wai Tam"
                    },
                    {
                        "name": "Newt Hue-Nam K. Nguyen"
                    },
                    {
                        "name": "Baixuan Xu"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Jiayang Cheng"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Ginny Y. Wong"
                    },
                    {
                        "name": "Simon See"
                    }
                ],
                "author_detail": {
                    "name": "Simon See"
                },
                "author": "Simon See",
                "arxiv_comment": "60 pages, 18 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05034v2",
                "updated": "2025-10-08T16:08:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    8,
                    31,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-06T17:10:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    10,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models"
                },
                "summary": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training"
                },
                "authors": [
                    {
                        "name": "Yolo Yunlong Tang"
                    },
                    {
                        "name": "Jing Bi"
                    },
                    {
                        "name": "Pinxin Liu"
                    },
                    {
                        "name": "Zhenyu Pan"
                    },
                    {
                        "name": "Zhangyun Tan"
                    },
                    {
                        "name": "Qianxiang Shen"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Hang Hua"
                    },
                    {
                        "name": "Junjia Guo"
                    },
                    {
                        "name": "Yunzhong Xiao"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Xinyi Liu"
                    },
                    {
                        "name": "Yizhi Song"
                    },
                    {
                        "name": "Yuhe Nie"
                    },
                    {
                        "name": "Jia-Xing Zhong"
                    },
                    {
                        "name": "Bozheng Li"
                    },
                    {
                        "name": "Daiqing Qi"
                    },
                    {
                        "name": "Ziyun Zeng"
                    },
                    {
                        "name": "Ali Vosoughi"
                    },
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Daiki Shimada"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "The 1st version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07169v1",
                "updated": "2025-10-08T16:07:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    7,
                    26,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:07:26Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    7,
                    26,
                    2,
                    281,
                    0
                ],
                "title": "More Data or Better Data? A Critical Analysis of Data Selection and\n  Synthesis for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Data or Better Data? A Critical Analysis of Data Selection and\n  Synthesis for Mathematical Reasoning"
                },
                "summary": "The reasoning capabilities of Large Language Models (LLMs) play a critical\nrole in many downstream tasks, yet depend strongly on the quality of training\ndata. Despite various proposed data construction methods, their practical\nutility in real-world pipelines remains underexplored. In this work, we conduct\na comprehensive analysis of open-source datasets and data synthesis techniques\nfor mathematical reasoning, evaluating them under a unified pipeline designed\nto mirror training and deployment scenarios. We further distill effective data\nselection strategies and identify practical methods suitable for industrial\napplications. Our findings highlight that structuring data in more\ninterpretable formats, or distilling from stronger models often outweighs\nsimply scaling up data volume. This study provides actionable guidance for\nintegrating training data to enhance LLM capabilities, supporting both\ncost-effective data curation and scalable model enhancement. We hope this work\nwill inspire further research on how to balance \"more data\" versus \"better\ndata\" for real-world reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning capabilities of Large Language Models (LLMs) play a critical\nrole in many downstream tasks, yet depend strongly on the quality of training\ndata. Despite various proposed data construction methods, their practical\nutility in real-world pipelines remains underexplored. In this work, we conduct\na comprehensive analysis of open-source datasets and data synthesis techniques\nfor mathematical reasoning, evaluating them under a unified pipeline designed\nto mirror training and deployment scenarios. We further distill effective data\nselection strategies and identify practical methods suitable for industrial\napplications. Our findings highlight that structuring data in more\ninterpretable formats, or distilling from stronger models often outweighs\nsimply scaling up data volume. This study provides actionable guidance for\nintegrating training data to enhance LLM capabilities, supporting both\ncost-effective data curation and scalable model enhancement. We hope this work\nwill inspire further research on how to balance \"more data\" versus \"better\ndata\" for real-world reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yike Zhao"
                    },
                    {
                        "name": "Simin Guo"
                    },
                    {
                        "name": "Ziqing Yang"
                    },
                    {
                        "name": "Shifan Han"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Fei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Fei Tan"
                },
                "author": "Fei Tan",
                "arxiv_comment": "12 pages, 3 figures, submitted to EMNLP 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07167v1",
                "updated": "2025-10-08T16:06:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    6,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:06:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    6,
                    4,
                    2,
                    281,
                    0
                ],
                "title": "Reasoning for Hierarchical Text Classification: The Case of Patents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning for Hierarchical Text Classification: The Case of Patents"
                },
                "summary": "Hierarchical text classification (HTC) assigns documents to multiple levels\nof a pre-defined taxonomy. Automated patent subject classification represents\none of the hardest HTC scenarios because of domain knowledge difficulty and a\nhuge number of labels. Prior approaches only output a flat label set, which\noffers little insight into the reason behind predictions. Therefore, we propose\nReasoning for Hierarchical Classification (RHC), a novel framework that\nreformulates HTC as a step-by-step reasoning task to sequentially deduce\nhierarchical labels. RHC trains large language models (LLMs) in two stages: a\ncold-start stage that aligns outputs with chain-of-thought (CoT) reasoning\nformat and a reinforcement learning (RL) stage to enhance multi-step reasoning\nability. RHC demonstrates four advantages in our experiments. (1)\nEffectiveness: RHC surpasses previous baselines and outperforms the supervised\nfine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)\nExplainability: RHC produces natural-language justifications before prediction\nto facilitate human inspection. (3) Scalability: RHC scales favorably with\nmodel size with larger gains compared to standard fine-tuning. (4)\nApplicability: Beyond patents, we further demonstrate that RHC achieves\nstate-of-the-art performance on other widely used HTC benchmarks, which\nhighlights its broad applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical text classification (HTC) assigns documents to multiple levels\nof a pre-defined taxonomy. Automated patent subject classification represents\none of the hardest HTC scenarios because of domain knowledge difficulty and a\nhuge number of labels. Prior approaches only output a flat label set, which\noffers little insight into the reason behind predictions. Therefore, we propose\nReasoning for Hierarchical Classification (RHC), a novel framework that\nreformulates HTC as a step-by-step reasoning task to sequentially deduce\nhierarchical labels. RHC trains large language models (LLMs) in two stages: a\ncold-start stage that aligns outputs with chain-of-thought (CoT) reasoning\nformat and a reinforcement learning (RL) stage to enhance multi-step reasoning\nability. RHC demonstrates four advantages in our experiments. (1)\nEffectiveness: RHC surpasses previous baselines and outperforms the supervised\nfine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)\nExplainability: RHC produces natural-language justifications before prediction\nto facilitate human inspection. (3) Scalability: RHC scales favorably with\nmodel size with larger gains compared to standard fine-tuning. (4)\nApplicability: Beyond patents, we further demonstrate that RHC achieves\nstate-of-the-art performance on other widely used HTC benchmarks, which\nhighlights its broad applicability."
                },
                "authors": [
                    {
                        "name": "Lekang Jiang"
                    },
                    {
                        "name": "Wenjun Sun"
                    },
                    {
                        "name": "Stephan Goetz"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Goetz"
                },
                "author": "Stephan Goetz",
                "arxiv_comment": "15 pages, 10 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25775v3",
                "updated": "2025-10-08T16:05:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    5,
                    52,
                    2,
                    281,
                    0
                ],
                "published": "2025-09-30T04:44:36Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    4,
                    44,
                    36,
                    1,
                    273,
                    0
                ],
                "title": "Autonomy-Aware Clustering: When Local Decisions Supersede Global\n  Prescriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomy-Aware Clustering: When Local Decisions Supersede Global\n  Prescriptions"
                },
                "summary": "Clustering arises in a wide range of problem formulations, yet most existing\napproaches assume that the entities under clustering are passive and strictly\nconform to their assigned groups. In reality, entities often exhibit local\nautonomy, overriding prescribed associations in ways not fully captured by\nfeature representations. Such autonomy can substantially reshape clustering\noutcomes -- altering cluster compositions, geometry, and cardinality -- with\nsignificant downstream effects on inference and decision-making. We introduce\nautonomy-aware clustering, a reinforcement learning (RL) framework that learns\nand accounts for the influence of local autonomy without requiring prior\nknowledge of its form. Our approach integrates RL with a Deterministic\nAnnealing (DA) procedure, where, to determine underlying clusters, DA naturally\npromotes exploration in early stages of annealing and transitions to\nexploitation later. We also show that the annealing procedure exhibits phase\ntransitions that enable design of efficient annealing schedules. To further\nenhance adaptability, we propose the Adaptive Distance Estimation Network\n(ADEN), a transformer-based attention model that learns dependencies between\nentities and cluster representatives within the RL loop, accommodates\nvariable-sized inputs and outputs, and enables knowledge transfer across\ndiverse problem instances. Empirical results show that our framework closely\naligns with underlying data dynamics: even without explicit autonomy models, it\nachieves solutions close to the ground truth (gap ~3-4%), whereas ignoring\nautonomy leads to substantially larger gaps (~35-40%). The code and data are\npublicly available at https://github.com/salar96/AutonomyAwareClustering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering arises in a wide range of problem formulations, yet most existing\napproaches assume that the entities under clustering are passive and strictly\nconform to their assigned groups. In reality, entities often exhibit local\nautonomy, overriding prescribed associations in ways not fully captured by\nfeature representations. Such autonomy can substantially reshape clustering\noutcomes -- altering cluster compositions, geometry, and cardinality -- with\nsignificant downstream effects on inference and decision-making. We introduce\nautonomy-aware clustering, a reinforcement learning (RL) framework that learns\nand accounts for the influence of local autonomy without requiring prior\nknowledge of its form. Our approach integrates RL with a Deterministic\nAnnealing (DA) procedure, where, to determine underlying clusters, DA naturally\npromotes exploration in early stages of annealing and transitions to\nexploitation later. We also show that the annealing procedure exhibits phase\ntransitions that enable design of efficient annealing schedules. To further\nenhance adaptability, we propose the Adaptive Distance Estimation Network\n(ADEN), a transformer-based attention model that learns dependencies between\nentities and cluster representatives within the RL loop, accommodates\nvariable-sized inputs and outputs, and enables knowledge transfer across\ndiverse problem instances. Empirical results show that our framework closely\naligns with underlying data dynamics: even without explicit autonomy models, it\nachieves solutions close to the ground truth (gap ~3-4%), whereas ignoring\nautonomy leads to substantially larger gaps (~35-40%). The code and data are\npublicly available at https://github.com/salar96/AutonomyAwareClustering."
                },
                "authors": [
                    {
                        "name": "Amber Srivastava"
                    },
                    {
                        "name": "Salar Basiri"
                    },
                    {
                        "name": "Srinivasa Salapaka"
                    }
                ],
                "author_detail": {
                    "name": "Srinivasa Salapaka"
                },
                "author": "Srinivasa Salapaka",
                "arxiv_comment": "Preprint. Under review at a peer-reviewed venue. Minor formatting\n  correction: the earlier version included an incorrect conference header,\n  which has been removed. Content unchanged",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07161v1",
                "updated": "2025-10-08T15:59:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    59,
                    11,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:59:11Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    59,
                    11,
                    2,
                    281,
                    0
                ],
                "title": "Integrating Domain Knowledge into Process Discovery Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Domain Knowledge into Process Discovery Using Large Language\n  Models"
                },
                "summary": "Process discovery aims to derive process models from event logs, providing\ninsights into operational behavior and forming a foundation for conformance\nchecking and process improvement. However, models derived solely from event\ndata may not accurately reflect the real process, as event logs are often\nincomplete or affected by noise, and domain knowledge, an important\ncomplementary resource, is typically disregarded. As a result, the discovered\nmodels may lack reliability for downstream tasks. We propose an interactive\nframework that incorporates domain knowledge, expressed in natural language,\ninto the process discovery pipeline using Large Language Models (LLMs). Our\napproach leverages LLMs to extract declarative rules from textual descriptions\nprovided by domain experts. These rules are used to guide the IMr discovery\nalgorithm, which recursively constructs process models by combining insights\nfrom both the event log and the extracted rules, helping to avoid problematic\nprocess structures that contradict domain knowledge. The framework coordinates\ninteractions among the LLM, domain experts, and a set of backend services. We\npresent a fully implemented tool that supports this workflow and conduct an\nextensive evaluation of multiple LLMs and prompt engineering strategies. Our\nempirical study includes a case study based on a real-life event log with the\ninvolvement of domain experts, who assessed the usability and effectiveness of\nthe framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process discovery aims to derive process models from event logs, providing\ninsights into operational behavior and forming a foundation for conformance\nchecking and process improvement. However, models derived solely from event\ndata may not accurately reflect the real process, as event logs are often\nincomplete or affected by noise, and domain knowledge, an important\ncomplementary resource, is typically disregarded. As a result, the discovered\nmodels may lack reliability for downstream tasks. We propose an interactive\nframework that incorporates domain knowledge, expressed in natural language,\ninto the process discovery pipeline using Large Language Models (LLMs). Our\napproach leverages LLMs to extract declarative rules from textual descriptions\nprovided by domain experts. These rules are used to guide the IMr discovery\nalgorithm, which recursively constructs process models by combining insights\nfrom both the event log and the extracted rules, helping to avoid problematic\nprocess structures that contradict domain knowledge. The framework coordinates\ninteractions among the LLM, domain experts, and a set of backend services. We\npresent a fully implemented tool that supports this workflow and conduct an\nextensive evaluation of multiple LLMs and prompt engineering strategies. Our\nempirical study includes a case study based on a real-life event log with the\ninvolvement of domain experts, who assessed the usability and effectiveness of\nthe framework."
                },
                "authors": [
                    {
                        "name": "Ali Norouzifar"
                    },
                    {
                        "name": "Humam Kourani"
                    },
                    {
                        "name": "Marcus Dees"
                    },
                    {
                        "name": "Wil van der Aalst"
                    }
                ],
                "author_detail": {
                    "name": "Wil van der Aalst"
                },
                "author": "Wil van der Aalst",
                "arxiv_comment": "This paper is currently under review for publication in a journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07153v1",
                "updated": "2025-10-08T15:52:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    52,
                    1,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:52:01Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    52,
                    1,
                    2,
                    281,
                    0
                ],
                "title": "Randomization Restrictions: Their Impact on Type I Error When\n  Experimenting with Finite Populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomization Restrictions: Their Impact on Type I Error When\n  Experimenting with Finite Populations"
                },
                "summary": "Participants in clinical trials are often viewed as a unique, finite\npopulation. Yet, statistical analyses often assume that participants were\nrandomly sampled from a larger population. Under Complete Randomization,\nRandomization-Based Inference (RBI; a finite population inference) and Analysis\nof Variance (ANOVA; a random sampling inference) provide asymptotically\nequivalent difference-in-means tests. However, sequentially-enrolling trials\ntypically employ restricted randomization schemes, such as block or Maximum\nTolerable Imbalance (MTI) designs, to reduce the chance of chronological\ntreatment imbalances. The impact of these restrictions on RBI and ANOVA\nconcordance is not well understood. With real-world frames of reference, such\nas rare and ultra-rare diseases, we review full versus random sampling of\nfinite populations and empirically evaluate finite population Type I error when\nusing ANOVA following randomization restrictions. Randomization restrictions\nstrongly impacted ANOVA Type I error, even for trials with 1,000 participants.\nProperly adjusting for restrictions corrected Type I error. We corrected for\nblock randomization, yet leave open how to correct for MTI designs. More\ndirectly, RBI accounts for randomization restrictions while ensuring correct\nfinite population Type I error. Novel contributions are: 1) deepening the\nunderstanding and correction of RBI and ANOVA concordance under block and MTI\nrestrictions and 2) using finite populations to estimate the convergence of\nType I error to a nominal rate. We discuss the challenge of specifying an\nestimand's population and reconciling with sampled trial participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Participants in clinical trials are often viewed as a unique, finite\npopulation. Yet, statistical analyses often assume that participants were\nrandomly sampled from a larger population. Under Complete Randomization,\nRandomization-Based Inference (RBI; a finite population inference) and Analysis\nof Variance (ANOVA; a random sampling inference) provide asymptotically\nequivalent difference-in-means tests. However, sequentially-enrolling trials\ntypically employ restricted randomization schemes, such as block or Maximum\nTolerable Imbalance (MTI) designs, to reduce the chance of chronological\ntreatment imbalances. The impact of these restrictions on RBI and ANOVA\nconcordance is not well understood. With real-world frames of reference, such\nas rare and ultra-rare diseases, we review full versus random sampling of\nfinite populations and empirically evaluate finite population Type I error when\nusing ANOVA following randomization restrictions. Randomization restrictions\nstrongly impacted ANOVA Type I error, even for trials with 1,000 participants.\nProperly adjusting for restrictions corrected Type I error. We corrected for\nblock randomization, yet leave open how to correct for MTI designs. More\ndirectly, RBI accounts for randomization restrictions while ensuring correct\nfinite population Type I error. Novel contributions are: 1) deepening the\nunderstanding and correction of RBI and ANOVA concordance under block and MTI\nrestrictions and 2) using finite populations to estimate the convergence of\nType I error to a nominal rate. We discuss the challenge of specifying an\nestimand's population and reconciling with sampled trial participants."
                },
                "authors": [
                    {
                        "name": "Jonathan J. Chipman"
                    },
                    {
                        "name": "Oleksandr Sverdlov"
                    },
                    {
                        "name": "Diane Uschner"
                    }
                ],
                "author_detail": {
                    "name": "Diane Uschner"
                },
                "author": "Diane Uschner",
                "arxiv_comment": "26 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07147v1",
                "updated": "2025-10-08T15:48:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    48,
                    41,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:48:41Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    48,
                    41,
                    2,
                    281,
                    0
                ],
                "title": "A Multi-Agent Framework for Stateful Inference-Time Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent Framework for Stateful Inference-Time Search"
                },
                "summary": "Recent work explores agentic inference-time techniques to perform structured,\nmulti-step reasoning. However, stateless inference often struggles on\nmulti-step tasks due to the absence of persistent state. Moreover,\ntask-specific fine-tuning or instruction-tuning often achieve surface-level\ncode generation but remain brittle on tasks requiring deeper reasoning and\nlong-horizon dependencies. To address these limitations, we propose stateful\nmulti-agent evolutionary search, a training-free framework that departs from\nprior stateless approaches by combining (i) persistent inference-time state,\n(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate\nits effectiveness in automated unit test generation through the generation of\nedge cases. We generate robust edge cases using an evolutionary search process,\nwhere specialized agents sequentially propose, mutate, and score candidates. A\ncontroller maintains persistent state across generations, while evolutionary\npreservation ensures diversity and exploration across all possible cases. This\nyields a generalist agent capable of discovering robust, high-coverage edge\ncases across unseen codebases. Experiments show our stateful multi-agent\ninference framework achieves substantial gains in coverage over stateless\nsingle-step baselines, evaluated on prevalent unit-testing benchmarks such as\nHumanEval and TestGenEvalMini and using three diverse LLM families - Llama,\nGemma, and GPT. These results indicate that combining persistent inference-time\nstate with evolutionary search materially improves unit-test generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work explores agentic inference-time techniques to perform structured,\nmulti-step reasoning. However, stateless inference often struggles on\nmulti-step tasks due to the absence of persistent state. Moreover,\ntask-specific fine-tuning or instruction-tuning often achieve surface-level\ncode generation but remain brittle on tasks requiring deeper reasoning and\nlong-horizon dependencies. To address these limitations, we propose stateful\nmulti-agent evolutionary search, a training-free framework that departs from\nprior stateless approaches by combining (i) persistent inference-time state,\n(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate\nits effectiveness in automated unit test generation through the generation of\nedge cases. We generate robust edge cases using an evolutionary search process,\nwhere specialized agents sequentially propose, mutate, and score candidates. A\ncontroller maintains persistent state across generations, while evolutionary\npreservation ensures diversity and exploration across all possible cases. This\nyields a generalist agent capable of discovering robust, high-coverage edge\ncases across unseen codebases. Experiments show our stateful multi-agent\ninference framework achieves substantial gains in coverage over stateless\nsingle-step baselines, evaluated on prevalent unit-testing benchmarks such as\nHumanEval and TestGenEvalMini and using three diverse LLM families - Llama,\nGemma, and GPT. These results indicate that combining persistent inference-time\nstate with evolutionary search materially improves unit-test generation."
                },
                "authors": [
                    {
                        "name": "Arshika Lalan"
                    },
                    {
                        "name": "Rajat Ghosh"
                    },
                    {
                        "name": "Aditya Kolsur"
                    },
                    {
                        "name": "Debojyoti Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Debojyoti Dutta"
                },
                "author": "Debojyoti Dutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09278v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09278v3",
                "updated": "2025-10-08T15:47:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    47,
                    39,
                    2,
                    281,
                    0
                ],
                "published": "2024-12-12T13:41:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    41,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Towards a Multimodal Large Language Model with Pixel-Level Insight for\n  Biomedicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Multimodal Large Language Model with Pixel-Level Insight for\n  Biomedicine"
                },
                "summary": "In recent years, Multimodal Large Language Models (MLLM) have achieved\nnotable advancements, demonstrating the feasibility of developing an\nintelligent biomedical assistant. However, current biomedical MLLMs\npredominantly focus on image-level understanding and restrict interactions to\ntextual commands, thus limiting their capability boundaries and the flexibility\nof usage. In this paper, we introduce a novel end-to-end multimodal large\nlanguage model for the biomedical domain, named MedPLIB, which possesses\npixel-level understanding. Excitingly, it supports visual question answering\n(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form\nshapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)\nmulti-stage training strategy, which divides MoE into separate training phases\nfor a visual-language expert model and a pixel-grounding expert model, followed\nby fine-tuning using MoE. This strategy effectively coordinates multitask\nlearning while maintaining the computational cost at inference equivalent to\nthat of a single expert model. To advance the research of biomedical MLLMs, we\nintroduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),\nwhich comprises an array of 8 modalities for complex medical imaging question\nanswering and image region understanding. Experimental results indicate that\nMedPLIB has achieved state-of-the-art outcomes across multiple medical visual\nlanguage tasks. More importantly, in zero-shot evaluations for the pixel\ngrounding task, MedPLIB leads the best small and large models by margins of\n19.7 and 15.6 respectively on the mDice metric. The codes, data, and model\ncheckpoints will be made publicly available at\nhttps://github.com/ShawnHuang497/MedPLIB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Multimodal Large Language Models (MLLM) have achieved\nnotable advancements, demonstrating the feasibility of developing an\nintelligent biomedical assistant. However, current biomedical MLLMs\npredominantly focus on image-level understanding and restrict interactions to\ntextual commands, thus limiting their capability boundaries and the flexibility\nof usage. In this paper, we introduce a novel end-to-end multimodal large\nlanguage model for the biomedical domain, named MedPLIB, which possesses\npixel-level understanding. Excitingly, it supports visual question answering\n(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form\nshapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)\nmulti-stage training strategy, which divides MoE into separate training phases\nfor a visual-language expert model and a pixel-grounding expert model, followed\nby fine-tuning using MoE. This strategy effectively coordinates multitask\nlearning while maintaining the computational cost at inference equivalent to\nthat of a single expert model. To advance the research of biomedical MLLMs, we\nintroduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),\nwhich comprises an array of 8 modalities for complex medical imaging question\nanswering and image region understanding. Experimental results indicate that\nMedPLIB has achieved state-of-the-art outcomes across multiple medical visual\nlanguage tasks. More importantly, in zero-shot evaluations for the pixel\ngrounding task, MedPLIB leads the best small and large models by margins of\n19.7 and 15.6 respectively on the mDice metric. The codes, data, and model\ncheckpoints will be made publicly available at\nhttps://github.com/ShawnHuang497/MedPLIB."
                },
                "authors": [
                    {
                        "name": "Xiaoshuang Huang"
                    },
                    {
                        "name": "Lingdong Shen"
                    },
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Fangxin Shang"
                    },
                    {
                        "name": "Hongxiang Li"
                    },
                    {
                        "name": "Haifeng Huang"
                    },
                    {
                        "name": "Yehui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yehui Yang"
                },
                "author": "Yehui Yang",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09278v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09278v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17218v2",
                "updated": "2025-10-08T15:47:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    47,
                    28,
                    2,
                    281,
                    0
                ],
                "published": "2025-01-28T19:00:00Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    19,
                    0,
                    0,
                    1,
                    28,
                    0
                ],
                "title": "Energy Correlators Beyond Angles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Correlators Beyond Angles"
                },
                "summary": "Energy correlators are theoretically simple and physically intuitive\nobservables that bridge experimental and theoretical particle physics. They\nhave for example enabled the most precise jet substructure determination of the\nstrong coupling constant to date, and recent proposals suggest that they may be\nused to precisely determine of the top quark mass with calculable, small\ntheoretical uncertainties. However, existing energy correlators all measure\ncorrelations in angles between particles, from which other observables such as\nmass must be inferred through potentially complicated procedures. In this work,\nwe generalize energy correlators to enable straightforward measurements of\nnon-angular correlations, which we call Energy Weighted Observable Correlations\n(EWOCs). To enforce collinear safety, EWOCs quantify correlations between\nsubjets rather than particles. The subjet radius can be tuned to control both\nthe physical scales probed by EWOCs and their sensitivity to non-perturbative\nphysics. We focus on the phenomenologically relevant example of the mass EWOC,\nwhich measures mass correlations between pairs of subjets, in the task of\nextracting mass scales from jets. In jet substructure determinations of the\nmass of a hadronically-decaying W boson, we show that the mass EWOC outperforms\nthe angle-based energy correlator, and performs comparably to the soft-drop\ngroomed jet mass. As a first exploration of the theoretical properties of\nEWOCs, we also calculate the mass EWOC on light-quark jets and compare to\nresults obtained with Pythia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy correlators are theoretically simple and physically intuitive\nobservables that bridge experimental and theoretical particle physics. They\nhave for example enabled the most precise jet substructure determination of the\nstrong coupling constant to date, and recent proposals suggest that they may be\nused to precisely determine of the top quark mass with calculable, small\ntheoretical uncertainties. However, existing energy correlators all measure\ncorrelations in angles between particles, from which other observables such as\nmass must be inferred through potentially complicated procedures. In this work,\nwe generalize energy correlators to enable straightforward measurements of\nnon-angular correlations, which we call Energy Weighted Observable Correlations\n(EWOCs). To enforce collinear safety, EWOCs quantify correlations between\nsubjets rather than particles. The subjet radius can be tuned to control both\nthe physical scales probed by EWOCs and their sensitivity to non-perturbative\nphysics. We focus on the phenomenologically relevant example of the mass EWOC,\nwhich measures mass correlations between pairs of subjets, in the task of\nextracting mass scales from jets. In jet substructure determinations of the\nmass of a hadronically-decaying W boson, we show that the mass EWOC outperforms\nthe angle-based energy correlator, and performs comparably to the soft-drop\ngroomed jet mass. As a first exploration of the theoretical properties of\nEWOCs, we also calculate the mass EWOC on light-quark jets and compare to\nresults obtained with Pythia."
                },
                "authors": [
                    {
                        "name": "Samuel Alipour-fard"
                    },
                    {
                        "name": "Wouter J. Waalewijn"
                    }
                ],
                "author_detail": {
                    "name": "Wouter J. Waalewijn"
                },
                "author": "Wouter J. Waalewijn",
                "arxiv_doi": "10.1007/JHEP07(2025)043",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/JHEP07(2025)043",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.17218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 9 figures. Code available at\n  https://github.com/samcaf/ResolvedEnergyCorrelators . v2: journal version",
                "arxiv_journal_ref": "J. High Energ. Phys. 2025, 43 (2025)",
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07143v1",
                "updated": "2025-10-08T15:44:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    44,
                    28,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:44:28Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    44,
                    28,
                    2,
                    281,
                    0
                ],
                "title": "Are We Using the Right Benchmark: An Evaluation Framework for Visual\n  Token Compression Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are We Using the Right Benchmark: An Evaluation Framework for Visual\n  Token Compression Methods"
                },
                "summary": "Recent endeavors to accelerate inference in Multimodal Large Language Models\n(MLLMs) have primarily focused on visual token compression. The effectiveness\nof these methods is typically assessed by measuring the accuracy drop on\nestablished benchmarks, comparing model performance before and after\ncompression. However, these benchmarks are originally designed to assess the\nperception and reasoning capabilities of MLLMs, rather than to evaluate\ncompression techniques. As a result, directly applying them to visual token\ncompression introduces a task mismatch. Strikingly, our investigation reveals\nthat simple image downsampling consistently outperforms many advanced\ncompression methods across multiple widely used benchmarks. Through extensive\nexperiments, we make the following observations: (i) Current benchmarks are\nnoisy for the visual token compression task. (ii) Down-sampling is able to\nserve as a data filter to evaluate the difficulty of samples in the visual\ntoken compression task. Motivated by these findings, we introduce VTC-Bench, an\nevaluation framework that incorporates a data filtering mechanism to denoise\nexisting benchmarks, thereby enabling fairer and more accurate assessment of\nvisual token compression methods. All data and code are available at\nhttps://github.com/Chenfei-Liao/VTC-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent endeavors to accelerate inference in Multimodal Large Language Models\n(MLLMs) have primarily focused on visual token compression. The effectiveness\nof these methods is typically assessed by measuring the accuracy drop on\nestablished benchmarks, comparing model performance before and after\ncompression. However, these benchmarks are originally designed to assess the\nperception and reasoning capabilities of MLLMs, rather than to evaluate\ncompression techniques. As a result, directly applying them to visual token\ncompression introduces a task mismatch. Strikingly, our investigation reveals\nthat simple image downsampling consistently outperforms many advanced\ncompression methods across multiple widely used benchmarks. Through extensive\nexperiments, we make the following observations: (i) Current benchmarks are\nnoisy for the visual token compression task. (ii) Down-sampling is able to\nserve as a data filter to evaluate the difficulty of samples in the visual\ntoken compression task. Motivated by these findings, we introduce VTC-Bench, an\nevaluation framework that incorporates a data filtering mechanism to denoise\nexisting benchmarks, thereby enabling fairer and more accurate assessment of\nvisual token compression methods. All data and code are available at\nhttps://github.com/Chenfei-Liao/VTC-Bench."
                },
                "authors": [
                    {
                        "name": "Chenfei Liao"
                    },
                    {
                        "name": "Wensong Wang"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Xu Zheng"
                    },
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Haocong He"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Lutao Jiang"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Bin Ren"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07141v1",
                "updated": "2025-10-08T15:42:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    42,
                    49,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:42:49Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    42,
                    49,
                    2,
                    281,
                    0
                ],
                "title": "Comparing human and language models sentence processing difficulties on\n  complex structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing human and language models sentence processing difficulties on\n  complex structures"
                },
                "summary": "Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs."
                },
                "authors": [
                    {
                        "name": "Samuel Joseph Amouyal"
                    },
                    {
                        "name": "Aya Meltzer-Asscher"
                    },
                    {
                        "name": "Jonathan Berant"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Berant"
                },
                "author": "Jonathan Berant",
                "arxiv_comment": "Data and code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17967v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17967v4",
                "updated": "2025-10-08T15:33:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    33,
                    25,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-23T14:37:00Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    37,
                    0,
                    4,
                    143,
                    0
                ],
                "title": "FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization\n  of Large Language Models"
                },
                "summary": "Low-rank optimization has emerged as a promising direction in training large\nlanguage models (LLMs) to improve running time and reduce the memory usage of\nadaptive optimizers by constraining learning to a lower-dimensional space.\nPrior work typically projects gradients of linear layers using approaches based\non Singular Value Decomposition (SVD) or QR-decomposition. Applying these\ntechniques individually to each layer in large models is computationally\nexpensive and incurs additional memory costs due to storing the projection\nmatrices. In this work, we propose a computationally efficient and conceptually\nsimple, two-step procedure to approximate SVD/QR-based gradient projections\ninto lower-dimensional spaces by using a predefined orthogonal matrix of the\nDiscrete Cosine Transform (DCT). We dynamically select columns from the DCT\nmatrix based on their alignment with the gradient of each layer. The effective\nprojection matrices are obtained via a simple matmul with the DCT matrix in\n$O(n^3)$ time, followed by a lightweight sorting step to identify the most\nrelevant basis vectors. For large layers, DCT can be computed via Makhoul's\n$N$-point algorithm based on Fast Fourier Transform (FFT) in $O(n^2 \\log(n))$\ntime. Due to the predefined nature of the orthogonal bases, they are computed\nonce at the start of training. Our numerical experiments on both pre-training\nand fine-tuning tasks demonstrate the effectiveness of our dual strategy in\napproximating optimal low-rank projections, obtaining an approach with\nrank-independent running time that matches the performance of costly\nSVD/QR-based methods while achieving faster runtime and reduced memory usage by\nup to $25\\%$ across different model sizes. Our code is available at\n\\href{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers}{\\texttt{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank optimization has emerged as a promising direction in training large\nlanguage models (LLMs) to improve running time and reduce the memory usage of\nadaptive optimizers by constraining learning to a lower-dimensional space.\nPrior work typically projects gradients of linear layers using approaches based\non Singular Value Decomposition (SVD) or QR-decomposition. Applying these\ntechniques individually to each layer in large models is computationally\nexpensive and incurs additional memory costs due to storing the projection\nmatrices. In this work, we propose a computationally efficient and conceptually\nsimple, two-step procedure to approximate SVD/QR-based gradient projections\ninto lower-dimensional spaces by using a predefined orthogonal matrix of the\nDiscrete Cosine Transform (DCT). We dynamically select columns from the DCT\nmatrix based on their alignment with the gradient of each layer. The effective\nprojection matrices are obtained via a simple matmul with the DCT matrix in\n$O(n^3)$ time, followed by a lightweight sorting step to identify the most\nrelevant basis vectors. For large layers, DCT can be computed via Makhoul's\n$N$-point algorithm based on Fast Fourier Transform (FFT) in $O(n^2 \\log(n))$\ntime. Due to the predefined nature of the orthogonal bases, they are computed\nonce at the start of training. Our numerical experiments on both pre-training\nand fine-tuning tasks demonstrate the effectiveness of our dual strategy in\napproximating optimal low-rank projections, obtaining an approach with\nrank-independent running time that matches the performance of costly\nSVD/QR-based methods while achieving faster runtime and reduced memory usage by\nup to $25\\%$ across different model sizes. Our code is available at\n\\href{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers}{\\texttt{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers}}."
                },
                "authors": [
                    {
                        "name": "Ionut-Vlad Modoranu"
                    },
                    {
                        "name": "Mher Safaryan"
                    },
                    {
                        "name": "Erik Schultheis"
                    },
                    {
                        "name": "Max Ryabinin"
                    },
                    {
                        "name": "Artem Chumachenko"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17967v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17967v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07134v1",
                "updated": "2025-10-08T15:29:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    29,
                    17,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:29:17Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    29,
                    17,
                    2,
                    281,
                    0
                ],
                "title": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models\n  for Embodied Visual Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models\n  for Embodied Visual Tracking"
                },
                "summary": "Embodied Visual Tracking (EVT) is a fundamental ability that underpins\npractical applications, such as companion robots, guidance robots and service\nassistants, where continuously following moving targets is essential. Recent\nadvances have enabled language-guided tracking in complex and unstructured\nscenes. However, existing approaches lack explicit spatial reasoning and\neffective temporal memory, causing failures under severe occlusions or in the\npresence of similar-looking distractors. To address these challenges, we\npresent TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances\nembodied visual tracking with two key modules, a spatial reasoning mechanism\nand a Target Identification Memory (TIM). The reasoning module introduces a\nChain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative\nposition and encodes it as a compact polar-coordinate token for action\nprediction. Guided by these spatial priors, the TIM employs a gated update\nstrategy to preserve long-horizon target memory, ensuring spatiotemporal\nconsistency and mitigating target loss during extended occlusions. Extensive\nexperiments show that TrackVLA++ achieves state-of-the-art performance on\npublic benchmarks across both egocentric and multi-camera settings. On the\nchallenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading\napproach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong\nzero-shot generalization, enabling robust real-world tracking in dynamic and\noccluded scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Visual Tracking (EVT) is a fundamental ability that underpins\npractical applications, such as companion robots, guidance robots and service\nassistants, where continuously following moving targets is essential. Recent\nadvances have enabled language-guided tracking in complex and unstructured\nscenes. However, existing approaches lack explicit spatial reasoning and\neffective temporal memory, causing failures under severe occlusions or in the\npresence of similar-looking distractors. To address these challenges, we\npresent TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances\nembodied visual tracking with two key modules, a spatial reasoning mechanism\nand a Target Identification Memory (TIM). The reasoning module introduces a\nChain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative\nposition and encodes it as a compact polar-coordinate token for action\nprediction. Guided by these spatial priors, the TIM employs a gated update\nstrategy to preserve long-horizon target memory, ensuring spatiotemporal\nconsistency and mitigating target loss during extended occlusions. Extensive\nexperiments show that TrackVLA++ achieves state-of-the-art performance on\npublic benchmarks across both egocentric and multi-camera settings. On the\nchallenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading\napproach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong\nzero-shot generalization, enabling robust real-world tracking in dynamic and\noccluded scenarios."
                },
                "authors": [
                    {
                        "name": "Jiahang Liu"
                    },
                    {
                        "name": "Yunpeng Qi"
                    },
                    {
                        "name": "Jiazhao Zhang"
                    },
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Shaoan Wang"
                    },
                    {
                        "name": "Kui Wu"
                    },
                    {
                        "name": "Hanjing Ye"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Zhibo Chen"
                    },
                    {
                        "name": "Fangwei Zhong"
                    },
                    {
                        "name": "Zhizheng Zhang"
                    },
                    {
                        "name": "He Wang"
                    }
                ],
                "author_detail": {
                    "name": "He Wang"
                },
                "author": "He Wang",
                "arxiv_comment": "Project page: https://pku-epic.github.io/TrackVLA-plus-plus-Web/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07132v1",
                "updated": "2025-10-08T15:27:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    27,
                    8,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:27:08Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    27,
                    8,
                    2,
                    281,
                    0
                ],
                "title": "DPMM-CFL: Clustered Federated Learning via Dirichlet Process Mixture\n  Model Nonparametric Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPMM-CFL: Clustered Federated Learning via Dirichlet Process Mixture\n  Model Nonparametric Clustering"
                },
                "summary": "Clustered Federated Learning (CFL) improves performance under non-IID client\nheterogeneity by clustering clients and training one model per cluster, thereby\nbalancing between a global model and fully personalized models. However, most\nCFL methods require the number of clusters K to be fixed a priori, which is\nimpractical when the latent structure is unknown. We propose DPMM-CFL, a CFL\nalgorithm that places a Dirichlet Process (DP) prior over the distribution of\ncluster parameters. This enables nonparametric Bayesian inference to jointly\ninfer both the number of clusters and client assignments, while optimizing\nper-cluster federated objectives. This results in a method where, at each\nround, federated updates and cluster inferences are coupled, as presented in\nthis paper. The algorithm is validated on benchmark datasets under Dirichlet\nand class-split non-IID partitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustered Federated Learning (CFL) improves performance under non-IID client\nheterogeneity by clustering clients and training one model per cluster, thereby\nbalancing between a global model and fully personalized models. However, most\nCFL methods require the number of clusters K to be fixed a priori, which is\nimpractical when the latent structure is unknown. We propose DPMM-CFL, a CFL\nalgorithm that places a Dirichlet Process (DP) prior over the distribution of\ncluster parameters. This enables nonparametric Bayesian inference to jointly\ninfer both the number of clusters and client assignments, while optimizing\nper-cluster federated objectives. This results in a method where, at each\nround, federated updates and cluster inferences are coupled, as presented in\nthis paper. The algorithm is validated on benchmark datasets under Dirichlet\nand class-split non-IID partitions."
                },
                "authors": [
                    {
                        "name": "Mariona Jaramillo-Civill"
                    },
                    {
                        "name": "Peng Wu"
                    },
                    {
                        "name": "Pau Closas"
                    }
                ],
                "author_detail": {
                    "name": "Pau Closas"
                },
                "author": "Pau Closas",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24893v3",
                "updated": "2025-10-08T15:26:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    26,
                    58,
                    2,
                    281,
                    0
                ],
                "published": "2025-09-29T15:03:31Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    3,
                    31,
                    0,
                    272,
                    0
                ],
                "title": "HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss\n  Guided Depth and Bidirectional Warping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HBSplat: Robust Sparse-View Gaussian Reconstruction with Hybrid-Loss\n  Guided Depth and Bidirectional Warping"
                },
                "summary": "Novel View Synthesis (NVS) from sparse views presents a formidable challenge\nin 3D reconstruction, where limited multi-view constraints lead to severe\noverfitting, geometric distortion, and fragmented scenes. While 3D Gaussian\nSplatting (3DGS) delivers real-time, high-fidelity rendering, its performance\ndrastically deteriorates under sparse inputs, plagued by floating artifacts and\nstructural failures. To address these challenges, we introduce HBSplat, a\nunified framework that elevates 3DGS by seamlessly integrating robust\nstructural cues, virtual view constraints, and occluded region completion. Our\ncore contributions are threefold: a Hybrid-Loss Depth Estimation module that\nensures multi-view consistency by leveraging dense matching priors and\nintegrating reprojection, point propagation, and smoothness constraints; a\nBidirectional Warping Virtual View Synthesis method that enforces substantially\nstronger constraints by creating high-fidelity virtual views through\nbidirectional depth-image warping and multi-view fusion; and an Occlusion-Aware\nReconstruction component that recovers occluded areas using a depth-difference\nmask and a learning-based inpainting model. Extensive evaluations on LLFF,\nBlender, and DTU benchmarks validate that HBSplat sets a new state-of-the-art,\nachieving up to 21.13 dB PSNR and 0.189 LPIPS, while maintaining real-time\ninference. Code is available at: https://github.com/eternalland/HBSplat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel View Synthesis (NVS) from sparse views presents a formidable challenge\nin 3D reconstruction, where limited multi-view constraints lead to severe\noverfitting, geometric distortion, and fragmented scenes. While 3D Gaussian\nSplatting (3DGS) delivers real-time, high-fidelity rendering, its performance\ndrastically deteriorates under sparse inputs, plagued by floating artifacts and\nstructural failures. To address these challenges, we introduce HBSplat, a\nunified framework that elevates 3DGS by seamlessly integrating robust\nstructural cues, virtual view constraints, and occluded region completion. Our\ncore contributions are threefold: a Hybrid-Loss Depth Estimation module that\nensures multi-view consistency by leveraging dense matching priors and\nintegrating reprojection, point propagation, and smoothness constraints; a\nBidirectional Warping Virtual View Synthesis method that enforces substantially\nstronger constraints by creating high-fidelity virtual views through\nbidirectional depth-image warping and multi-view fusion; and an Occlusion-Aware\nReconstruction component that recovers occluded areas using a depth-difference\nmask and a learning-based inpainting model. Extensive evaluations on LLFF,\nBlender, and DTU benchmarks validate that HBSplat sets a new state-of-the-art,\nachieving up to 21.13 dB PSNR and 0.189 LPIPS, while maintaining real-time\ninference. Code is available at: https://github.com/eternalland/HBSplat."
                },
                "authors": [
                    {
                        "name": "Yu Ma"
                    },
                    {
                        "name": "Guoliang Wei"
                    },
                    {
                        "name": "Haihong Xiao"
                    },
                    {
                        "name": "Yue Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yue Cheng"
                },
                "author": "Yue Cheng",
                "arxiv_comment": "14 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07131v1",
                "updated": "2025-10-08T15:26:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    26,
                    54,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:26:54Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    26,
                    54,
                    2,
                    281,
                    0
                ],
                "title": "CURLING -- II. Improvement on the $H_{0}$ Inference from Pixelized\n  Cluster Strong Lens Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CURLING -- II. Improvement on the $H_{0}$ Inference from Pixelized\n  Cluster Strong Lens Modeling"
                },
                "summary": "Strongly lensed supernovae (glSNe) provide a powerful, independent method to\nmeasure the Hubble constant, $H_{0}$, through time delays between their\nmultiple images. The accuracy of this measurement depends critically on both\nthe precision of time delay estimation and the robustness of lens modeling. In\nmany current cluster-scale modeling algorithms, all multiple images used for\nmodeling are simplified as point sources to reduce computational costs. In the\nfirst paper of the CURLING program, we demonstrated that such a point-like\napproximation can introduce significant uncertainties and biases in both\nmagnification reconstruction and cosmological inference. In this study, we\nexplore how such simplifications affect $H_0$ measurements from glSNe. We\nsimulate a lensed supernova at $z=1.95$, lensed by a galaxy cluster at\n$z=0.336$, assuming time delays are measured from LSST-like light curves. The\nlens model is constructed using JWST-like imaging data, utilizing both Lenstool\nand a pixelated method developed in CURLING. Under a fiducial cosmology with\n$H_0=70\\rm \\ km \\ s^{-1}\\ Mpc^{-1}$, the Lenstool model yields\n$H_0=69.91^{+6.27}_{-5.50}\\rm \\ km\\ s^{-1}\\ Mpc^{-1}$, whereas the pixelated\nframework improves the precision by over an order of magnitude,\n$H_0=70.39^{+0.82}_{-0.60}\\rm \\ km \\ s^{-1}\\ Mpc^{-1}$. Our results indicate\nthat in the next-generation observations (e.g., JWST), uncertainties from lens\nmodeling dominate the error budget for $H_0$ inference, emphasizing the\nimportance of incorporating the extended surface brightness of multiple images\nto fully leverage the potential of glSNe for cosmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strongly lensed supernovae (glSNe) provide a powerful, independent method to\nmeasure the Hubble constant, $H_{0}$, through time delays between their\nmultiple images. The accuracy of this measurement depends critically on both\nthe precision of time delay estimation and the robustness of lens modeling. In\nmany current cluster-scale modeling algorithms, all multiple images used for\nmodeling are simplified as point sources to reduce computational costs. In the\nfirst paper of the CURLING program, we demonstrated that such a point-like\napproximation can introduce significant uncertainties and biases in both\nmagnification reconstruction and cosmological inference. In this study, we\nexplore how such simplifications affect $H_0$ measurements from glSNe. We\nsimulate a lensed supernova at $z=1.95$, lensed by a galaxy cluster at\n$z=0.336$, assuming time delays are measured from LSST-like light curves. The\nlens model is constructed using JWST-like imaging data, utilizing both Lenstool\nand a pixelated method developed in CURLING. Under a fiducial cosmology with\n$H_0=70\\rm \\ km \\ s^{-1}\\ Mpc^{-1}$, the Lenstool model yields\n$H_0=69.91^{+6.27}_{-5.50}\\rm \\ km\\ s^{-1}\\ Mpc^{-1}$, whereas the pixelated\nframework improves the precision by over an order of magnitude,\n$H_0=70.39^{+0.82}_{-0.60}\\rm \\ km \\ s^{-1}\\ Mpc^{-1}$. Our results indicate\nthat in the next-generation observations (e.g., JWST), uncertainties from lens\nmodeling dominate the error budget for $H_0$ inference, emphasizing the\nimportance of incorporating the extended surface brightness of multiple images\nto fully leverage the potential of glSNe for cosmology."
                },
                "authors": [
                    {
                        "name": "Yushan Xie"
                    },
                    {
                        "name": "Huanyuan Shan"
                    },
                    {
                        "name": "Yiping Shu"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Ji Yao"
                    },
                    {
                        "name": "Ran Li"
                    },
                    {
                        "name": "Xiaoyue Cao"
                    },
                    {
                        "name": "Zizhao He"
                    },
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Eric Jullo"
                    },
                    {
                        "name": "Jean-Paul Kneib"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07130v1",
                "updated": "2025-10-08T15:26:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    26,
                    9,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:26:09Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    26,
                    9,
                    2,
                    281,
                    0
                ],
                "title": "Polka-dotted Stars II: Starspots and obliquities of Kepler-17 and\n  Kepler-63",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polka-dotted Stars II: Starspots and obliquities of Kepler-17 and\n  Kepler-63"
                },
                "summary": "Starspots trace stellar magnetic activity and influence both stellar\nevolution and exoplanet characterization. While occultation-based spot analyses\nhave been applied to individual systems, comparative studies remain limited. We\napply the StarryStarryProcess Bayesian surface-mapping framework to archival\nKepler light curves of two planet hosts, Kepler-63 and Kepler-17, extending the\nvalidation established on TOI-3884 (Paper I). Across both systems, we infer\ncharacteristic spot radii smaller than 10 degrees. The latitudinal spot\ndistributions of these G dwarfs show bimodal belts: Kepler-63 near 30 degrees\nand Kepler-17 near 15 degrees. Our analysis yields stellar obliquity\nmeasurements in excellent agreement with previous studies, validating our\nmethodology and demonstrating that transit-based surface mapping can\nsimultaneously recover planetary parameters, stellar orientations, and magnetic\nmorphologies. Together, these results reveal a range of stellar geometries from\nnearly aligned (Kepler-17) to highly misaligned (Kepler-63).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Starspots trace stellar magnetic activity and influence both stellar\nevolution and exoplanet characterization. While occultation-based spot analyses\nhave been applied to individual systems, comparative studies remain limited. We\napply the StarryStarryProcess Bayesian surface-mapping framework to archival\nKepler light curves of two planet hosts, Kepler-63 and Kepler-17, extending the\nvalidation established on TOI-3884 (Paper I). Across both systems, we infer\ncharacteristic spot radii smaller than 10 degrees. The latitudinal spot\ndistributions of these G dwarfs show bimodal belts: Kepler-63 near 30 degrees\nand Kepler-17 near 15 degrees. Our analysis yields stellar obliquity\nmeasurements in excellent agreement with previous studies, validating our\nmethodology and demonstrating that transit-based surface mapping can\nsimultaneously recover planetary parameters, stellar orientations, and magnetic\nmorphologies. Together, these results reveal a range of stellar geometries from\nnearly aligned (Kepler-17) to highly misaligned (Kepler-63)."
                },
                "authors": [
                    {
                        "name": "Sabina Sagynbayeva"
                    },
                    {
                        "name": "Will M. Farr"
                    }
                ],
                "author_detail": {
                    "name": "Will M. Farr"
                },
                "author": "Will M. Farr",
                "arxiv_comment": "15 pages, 12 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07128v1",
                "updated": "2025-10-08T15:24:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    24,
                    51,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:24:51Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    24,
                    51,
                    2,
                    281,
                    0
                ],
                "title": "jmstate, a Flexible Python Package for Multi-State Joint Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "jmstate, a Flexible Python Package for Multi-State Joint Modeling"
                },
                "summary": "Classical joint modeling approaches often rely on competing risks or\nrecurrent event formulations to account for complex real-world processes\ninvolving evolving longitudinal markers and discrete event occurrences.\nHowever, these frameworks typically capture only limited aspects of the\nunderlying event dynamics.\n  Multi-state joint models offer a more flexible alternative by representing\nfull event histories through a network of possible transitions, including\nrecurrent cycles and terminal absorptions, all potentially influenced by\nlongitudinal covariates.\n  In this paper, we propose a general framework that unifies longitudinal\nbiomarker modeling with multi-state event processes defined on arbitrary\ndirected graphs. Our approach accommodates both Markovian and semi-Markovian\ntransition structures, and extends classical joint models by coupling nonlinear\nmixed-effects longitudinal submodels with multi-state survival processes via\nshared latent structures.\n  We derive the full likelihood and develop scalable inference procedures based\non stochastic gradient descent. Furthermore, we introduce a dynamic prediction\nframework, enabling individualized risk assessments along complex\nstate-transition trajectories.\n  To facilitate reproducibility and dissemination, we provide an open-source\nPython library \\texttt{jmstate} implementing the proposed methodology,\navailable on \\href{https://pypi.org/project/jmstate/}{PyPI}. Simulation\nexperiments and a biomedical case study demonstrate the flexibility and\nperformance of the framework in representing complex longitudinal and\nmulti-state event dynamics. The full Python notebooks used to reproduce the\nexperiments as well as the source code of this paper are available on\n\\href{https://gitlab.com/felixlaplante0/jmstate-paper/}{GitLab}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical joint modeling approaches often rely on competing risks or\nrecurrent event formulations to account for complex real-world processes\ninvolving evolving longitudinal markers and discrete event occurrences.\nHowever, these frameworks typically capture only limited aspects of the\nunderlying event dynamics.\n  Multi-state joint models offer a more flexible alternative by representing\nfull event histories through a network of possible transitions, including\nrecurrent cycles and terminal absorptions, all potentially influenced by\nlongitudinal covariates.\n  In this paper, we propose a general framework that unifies longitudinal\nbiomarker modeling with multi-state event processes defined on arbitrary\ndirected graphs. Our approach accommodates both Markovian and semi-Markovian\ntransition structures, and extends classical joint models by coupling nonlinear\nmixed-effects longitudinal submodels with multi-state survival processes via\nshared latent structures.\n  We derive the full likelihood and develop scalable inference procedures based\non stochastic gradient descent. Furthermore, we introduce a dynamic prediction\nframework, enabling individualized risk assessments along complex\nstate-transition trajectories.\n  To facilitate reproducibility and dissemination, we provide an open-source\nPython library \\texttt{jmstate} implementing the proposed methodology,\navailable on \\href{https://pypi.org/project/jmstate/}{PyPI}. Simulation\nexperiments and a biomedical case study demonstrate the flexibility and\nperformance of the framework in representing complex longitudinal and\nmulti-state event dynamics. The full Python notebooks used to reproduce the\nexperiments as well as the source code of this paper are available on\n\\href{https://gitlab.com/felixlaplante0/jmstate-paper/}{GitLab}."
                },
                "authors": [
                    {
                        "name": "Félix Laplante"
                    },
                    {
                        "name": "Christophe Ambroise"
                    },
                    {
                        "name": "Estelle Kuhn"
                    },
                    {
                        "name": "Sarah Lemler"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Lemler"
                },
                "author": "Sarah Lemler",
                "arxiv_comment": "23 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07126v1",
                "updated": "2025-10-08T15:21:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    21,
                    53,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:21:53Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    21,
                    53,
                    2,
                    281,
                    0
                ],
                "title": "Validation of Various Normalization Methods for Brain Tumor\n  Segmentation: Can Federated Learning Overcome This Heterogeneity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validation of Various Normalization Methods for Brain Tumor\n  Segmentation: Can Federated Learning Overcome This Heterogeneity?"
                },
                "summary": "Deep learning (DL) has been increasingly applied in medical imaging, however,\nit requires large amounts of data, which raises many challenges related to data\nprivacy, storage, and transfer. Federated learning (FL) is a training paradigm\nthat overcomes these issues, though its effectiveness may be reduced when\ndealing with non-independent and identically distributed (non-IID) data. This\nstudy simulates non-IID conditions by applying different MRI intensity\nnormalization techniques to separate data subsets, reflecting a common cause of\nheterogeneity. These subsets are then used for training and testing models for\nbrain tumor segmentation. The findings provide insights into the influence of\nthe MRI intensity normalization methods on segmentation models, both training\nand inference. Notably, the FL methods demonstrated resilience to\ninconsistently normalized data across clients, achieving the 3D Dice score of\n92%, which is comparable to a centralized model (trained using all data). These\nresults indicate that FL is a solution to effectively train high-performing\nmodels without violating data privacy, a crucial concern in medical\napplications. The code is available at:\nhttps://github.com/SanoScience/fl-varying-normalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) has been increasingly applied in medical imaging, however,\nit requires large amounts of data, which raises many challenges related to data\nprivacy, storage, and transfer. Federated learning (FL) is a training paradigm\nthat overcomes these issues, though its effectiveness may be reduced when\ndealing with non-independent and identically distributed (non-IID) data. This\nstudy simulates non-IID conditions by applying different MRI intensity\nnormalization techniques to separate data subsets, reflecting a common cause of\nheterogeneity. These subsets are then used for training and testing models for\nbrain tumor segmentation. The findings provide insights into the influence of\nthe MRI intensity normalization methods on segmentation models, both training\nand inference. Notably, the FL methods demonstrated resilience to\ninconsistently normalized data across clients, achieving the 3D Dice score of\n92%, which is comparable to a centralized model (trained using all data). These\nresults indicate that FL is a solution to effectively train high-performing\nmodels without violating data privacy, a crucial concern in medical\napplications. The code is available at:\nhttps://github.com/SanoScience/fl-varying-normalization."
                },
                "authors": [
                    {
                        "name": "Jan Fiszer"
                    },
                    {
                        "name": "Dominika Ciupek"
                    },
                    {
                        "name": "Maciej Malawski"
                    }
                ],
                "author_detail": {
                    "name": "Maciej Malawski"
                },
                "author": "Maciej Malawski",
                "arxiv_doi": "10.1007/978-3-032-05663-4_12",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-05663-4_12",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.07126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07118v1",
                "updated": "2025-10-08T15:11:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    11,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:11:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    11,
                    4,
                    2,
                    281,
                    0
                ],
                "title": "TRIM: Token-wise Attention-Derived Saliency for Data-Efficient\n  Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIM: Token-wise Attention-Derived Saliency for Data-Efficient\n  Instruction Tuning"
                },
                "summary": "Instruction tuning is essential for aligning large language models (LLMs) to\ndownstream tasks and commonly relies on large, diverse corpora. However, small,\nhigh-quality subsets, known as coresets, can deliver comparable or superior\nresults, though curating them remains challenging. Existing methods often rely\non coarse, sample-level signals like gradients, an approach that is\ncomputationally expensive and overlooks fine-grained features. To address this,\nwe introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a\nforward-only, token-centric framework. Instead of using gradients, TRIM\noperates by matching underlying representational patterns identified via\nattention-based \"fingerprints\" from a handful of target samples. Such an\napproach makes TRIM highly efficient and uniquely sensitive to the structural\nfeatures that define a task. Coresets selected by our method consistently\noutperform state-of-the-art baselines by up to 9% on downstream tasks and even\nsurpass the performance of full-data fine-tuning in some settings. By avoiding\nexpensive backward passes, TRIM achieves this at a fraction of the\ncomputational cost. These findings establish TRIM as a scalable and efficient\nalternative for building high-quality instruction-tuning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is essential for aligning large language models (LLMs) to\ndownstream tasks and commonly relies on large, diverse corpora. However, small,\nhigh-quality subsets, known as coresets, can deliver comparable or superior\nresults, though curating them remains challenging. Existing methods often rely\non coarse, sample-level signals like gradients, an approach that is\ncomputationally expensive and overlooks fine-grained features. To address this,\nwe introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a\nforward-only, token-centric framework. Instead of using gradients, TRIM\noperates by matching underlying representational patterns identified via\nattention-based \"fingerprints\" from a handful of target samples. Such an\napproach makes TRIM highly efficient and uniquely sensitive to the structural\nfeatures that define a task. Coresets selected by our method consistently\noutperform state-of-the-art baselines by up to 9% on downstream tasks and even\nsurpass the performance of full-data fine-tuning in some settings. By avoiding\nexpensive backward passes, TRIM achieves this at a fraction of the\ncomputational cost. These findings establish TRIM as a scalable and efficient\nalternative for building high-quality instruction-tuning datasets."
                },
                "authors": [
                    {
                        "name": "Manish Nagaraj"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Deepak Ravikumar"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10309v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10309v2",
                "updated": "2025-10-08T15:08:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    8,
                    8,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-15T13:55:27Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    55,
                    27,
                    3,
                    135,
                    0
                ],
                "title": "Empirically evaluating commonsense intelligence in large language models\n  with large-scale human judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirically evaluating commonsense intelligence in large language models\n  with large-scale human judgments"
                },
                "summary": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a method\nfor evaluating common sense in artificial intelligence (AI), specifically in\nlarge language models (LLMs), that incorporates empirically observed\nheterogeneity among humans by measuring the correspondence between a model's\njudgment and that of a human population. We first find that, when treated as\nindependent survey respondents, most LLMs remain below the human median in\ntheir individual commonsense competence. Second, when used as simulators of a\nhypothetical population, LLMs correlate with real humans only modestly in the\nextent to which they agree on the same set of statements. In both cases,\nsmaller, open-weight models are surprisingly more competitive than larger,\nproprietary frontier models. Our evaluation framework, which ties commonsense\nintelligence to its cultural basis, contributes to the growing call for\nadapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a method\nfor evaluating common sense in artificial intelligence (AI), specifically in\nlarge language models (LLMs), that incorporates empirically observed\nheterogeneity among humans by measuring the correspondence between a model's\njudgment and that of a human population. We first find that, when treated as\nindependent survey respondents, most LLMs remain below the human median in\ntheir individual commonsense competence. Second, when used as simulators of a\nhypothetical population, LLMs correlate with real humans only modestly in the\nextent to which they agree on the same set of statements. In both cases,\nsmaller, open-weight models are surprisingly more competitive than larger,\nproprietary frontier models. Our evaluation framework, which ties commonsense\nintelligence to its cultural basis, contributes to the growing call for\nadapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge."
                },
                "authors": [
                    {
                        "name": "Tuan Dung Nguyen"
                    },
                    {
                        "name": "Duncan J. Watts"
                    },
                    {
                        "name": "Mark E. Whiting"
                    }
                ],
                "author_detail": {
                    "name": "Mark E. Whiting"
                },
                "author": "Mark E. Whiting",
                "arxiv_comment": "Code and data: https://github.com/Watts-Lab/commonsense-llm-eval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10309v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10309v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07105v1",
                "updated": "2025-10-08T14:59:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    59,
                    24,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:59:24Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    59,
                    24,
                    2,
                    281,
                    0
                ],
                "title": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples\n  via Meta-Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples\n  via Meta-Learning"
                },
                "summary": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity,\nor legitimate disagreement between annotators. In this paper, we outline our\nsystem for modeling human variation. Our system leverages language models'\n(LLMs) in-context learning abilities, along with a two-step meta-learning\ntraining procedure for 1) post-training on many datasets requiring in-context\nlearning and 2) specializing the model via in-context meta-learning to the\nparticular data distribution of interest. We also evaluate the performance of\nour system submission to the Learning With Disagreements (LeWiDi) competition,\nwhere it was the overall winner on both tasks. Additionally, we perform an\nablation study to measure the importance of each system component. We find that\nincluding rater examples in-context is crucial for our system's performance,\ndataset-specific fine-tuning is helpful on the larger datasets, post-training\non other in-context datasets is helpful on one of the competition datasets, and\nthat performance improves with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity,\nor legitimate disagreement between annotators. In this paper, we outline our\nsystem for modeling human variation. Our system leverages language models'\n(LLMs) in-context learning abilities, along with a two-step meta-learning\ntraining procedure for 1) post-training on many datasets requiring in-context\nlearning and 2) specializing the model via in-context meta-learning to the\nparticular data distribution of interest. We also evaluate the performance of\nour system submission to the Learning With Disagreements (LeWiDi) competition,\nwhere it was the overall winner on both tasks. Additionally, we perform an\nablation study to measure the importance of each system component. We find that\nincluding rater examples in-context is crucial for our system's performance,\ndataset-specific fine-tuning is helpful on the larger datasets, post-training\non other in-context datasets is helpful on one of the competition datasets, and\nthat performance improves with model scale."
                },
                "authors": [
                    {
                        "name": "Taylor Sorensen"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "NLPerspectives: The 4th Workshop on Perspectivist Approaches to\n  Natural Language Processing at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07098v1",
                "updated": "2025-10-08T14:56:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    56,
                    42,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:56:42Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    56,
                    42,
                    2,
                    281,
                    0
                ],
                "title": "TALENT: Table VQA via Augmented Language-Enhanced Natural-text\n  Transcription",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TALENT: Table VQA via Augmented Language-Enhanced Natural-text\n  Transcription"
                },
                "summary": "Table Visual Question Answering (Table VQA) is typically addressed by large\nvision-language models (VLMs). While such models can answer directly from\nimages, they often miss fine-grained details unless scaled to very large sizes,\nwhich are computationally prohibitive, especially for mobile deployment. A\nlighter alternative is to have a small VLM perform OCR and then use a large\nlanguage model (LLM) to reason over structured outputs such as Markdown tables.\nHowever, these representations are not naturally optimized for LLMs and still\nintroduce substantial errors. We propose TALENT (Table VQA via Augmented\nLanguage-Enhanced Natural-text Transcription), a lightweight framework that\nleverages dual representations of tables. TALENT prompts a small VLM to produce\nboth OCR text and natural language narration, then combines them with the\nquestion for reasoning by an LLM. This reframes Table VQA as an LLM-centric\nmultimodal reasoning task, where the VLM serves as a perception-narration\nmodule rather than a monolithic solver. Additionally, we construct ReTabVQA, a\nmore challenging Table VQA dataset requiring multi-step quantitative reasoning\nover table images. Experiments show that TALENT enables a small VLM-LLM\ncombination to match or surpass a single large VLM at significantly lower\ncomputational cost on both public datasets and ReTabVQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table Visual Question Answering (Table VQA) is typically addressed by large\nvision-language models (VLMs). While such models can answer directly from\nimages, they often miss fine-grained details unless scaled to very large sizes,\nwhich are computationally prohibitive, especially for mobile deployment. A\nlighter alternative is to have a small VLM perform OCR and then use a large\nlanguage model (LLM) to reason over structured outputs such as Markdown tables.\nHowever, these representations are not naturally optimized for LLMs and still\nintroduce substantial errors. We propose TALENT (Table VQA via Augmented\nLanguage-Enhanced Natural-text Transcription), a lightweight framework that\nleverages dual representations of tables. TALENT prompts a small VLM to produce\nboth OCR text and natural language narration, then combines them with the\nquestion for reasoning by an LLM. This reframes Table VQA as an LLM-centric\nmultimodal reasoning task, where the VLM serves as a perception-narration\nmodule rather than a monolithic solver. Additionally, we construct ReTabVQA, a\nmore challenging Table VQA dataset requiring multi-step quantitative reasoning\nover table images. Experiments show that TALENT enables a small VLM-LLM\ncombination to match or surpass a single large VLM at significantly lower\ncomputational cost on both public datasets and ReTabVQA."
                },
                "authors": [
                    {
                        "name": "Guo Yutong"
                    },
                    {
                        "name": "Wanying Wang"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Zichen Miao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07096v1",
                "updated": "2025-10-08T14:53:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    53,
                    48,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:53:48Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    53,
                    48,
                    2,
                    281,
                    0
                ],
                "title": "Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided\n  Sarcastic Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided\n  Sarcastic Speech Synthesis"
                },
                "summary": "Sarcasm is a subtle form of non-literal language that poses significant\nchallenges for speech synthesis due to its reliance on nuanced semantic,\ncontextual, and prosodic cues. While existing speech synthesis research has\nfocused primarily on broad emotional categories, sarcasm remains largely\nunexplored. In this paper, we propose a Large Language Model (LLM)-enhanced\nRetrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach\ncombines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture\npragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic\nexemplars retrieved via a Retrieval Augmented Generation (RAG) module, which\nprovide expressive reference patterns of sarcastic delivery. Integrated within\na VITS backbone, this dual conditioning enables more natural and contextually\nappropriate sarcastic speech. Experiments demonstrate that our method\noutperforms baselines in both objective measures and subjective evaluations,\nyielding improvements in speech naturalness, sarcastic expressivity, and\ndownstream sarcasm detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sarcasm is a subtle form of non-literal language that poses significant\nchallenges for speech synthesis due to its reliance on nuanced semantic,\ncontextual, and prosodic cues. While existing speech synthesis research has\nfocused primarily on broad emotional categories, sarcasm remains largely\nunexplored. In this paper, we propose a Large Language Model (LLM)-enhanced\nRetrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach\ncombines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture\npragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic\nexemplars retrieved via a Retrieval Augmented Generation (RAG) module, which\nprovide expressive reference patterns of sarcastic delivery. Integrated within\na VITS backbone, this dual conditioning enables more natural and contextually\nappropriate sarcastic speech. Experiments demonstrate that our method\noutperforms baselines in both objective measures and subjective evaluations,\nyielding improvements in speech naturalness, sarcastic expressivity, and\ndownstream sarcasm detection."
                },
                "authors": [
                    {
                        "name": "Zhu Li"
                    },
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Xiyuan Gao"
                    },
                    {
                        "name": "Shekhar Nayak"
                    },
                    {
                        "name": "Matt Coler"
                    }
                ],
                "author_detail": {
                    "name": "Matt Coler"
                },
                "author": "Matt Coler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11329v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11329v3",
                "updated": "2025-10-08T14:49:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    49,
                    25,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-16T14:53:50Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    53,
                    50,
                    4,
                    136,
                    0
                ],
                "title": "TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM\n  Inference"
                },
                "summary": "Distributed inference of large language models (LLMs) can introduce overheads\nof up to 20% even over GPUs connected via high-speed interconnects such as\nNVLink. Multiple techniques have been proposed to mitigate these overheads by\ndecomposing computations into finer-grained tasks and overlapping communication\nwith sub-tasks as they complete. However, fine-grained decomposition of a large\ncomputation into many smaller computations on GPUs results in overheads.\nFurthermore, the communication itself uses many streaming multiprocessors\n(SMs), adding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a\nToken-Splitting technique that divides the tokens in the inference batch into\ntwo approximately equal subsets in a wave-aware manner. The communication of\none subset is then overlapped with the computation of the other. In addition,\nTokenWeave optimizes the order of the layer normalization computation with\nrespect to communication operations and implements a novel fused\nAllReduce--RMSNorm kernel that carefully leverages Multimem instruction support\navailable on Hopper and Blackwell NVIDIA GPUs. These optimizations allow\nTokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover,\nour kernel enables the memory-bound RMSNorm to be overlapped with the other\nbatch's computation, providing additional gains.\n  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher\nthroughput across multiple models and workloads. In several settings,\nTokenWeave results in better performance compared to an equivalent model with\nall communication removed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed inference of large language models (LLMs) can introduce overheads\nof up to 20% even over GPUs connected via high-speed interconnects such as\nNVLink. Multiple techniques have been proposed to mitigate these overheads by\ndecomposing computations into finer-grained tasks and overlapping communication\nwith sub-tasks as they complete. However, fine-grained decomposition of a large\ncomputation into many smaller computations on GPUs results in overheads.\nFurthermore, the communication itself uses many streaming multiprocessors\n(SMs), adding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a\nToken-Splitting technique that divides the tokens in the inference batch into\ntwo approximately equal subsets in a wave-aware manner. The communication of\none subset is then overlapped with the computation of the other. In addition,\nTokenWeave optimizes the order of the layer normalization computation with\nrespect to communication operations and implements a novel fused\nAllReduce--RMSNorm kernel that carefully leverages Multimem instruction support\navailable on Hopper and Blackwell NVIDIA GPUs. These optimizations allow\nTokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover,\nour kernel enables the memory-bound RMSNorm to be overlapped with the other\nbatch's computation, providing additional gains.\n  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher\nthroughput across multiple models and workloads. In several settings,\nTokenWeave results in better performance compared to an equivalent model with\nall communication removed."
                },
                "authors": [
                    {
                        "name": "Raja Gond"
                    },
                    {
                        "name": "Nipun Kwatra"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    }
                ],
                "author_detail": {
                    "name": "Ramachandran Ramjee"
                },
                "author": "Ramachandran Ramjee",
                "arxiv_comment": "14 pages, 16 figures. For source code, see\n  https://github.com/microsoft/tokenweave. In version 2, Figure 6 shows All\n  Reduce bandwidth, not Reduce Scatter. The Multimem Reduce Scatter bandwidth\n  formula differs slightly from the Ring-based version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11329v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11329v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07091v1",
                "updated": "2025-10-08T14:47:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    47,
                    40,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:47:40Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    47,
                    40,
                    2,
                    281,
                    0
                ],
                "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from\n  Planning with Actions to Planning with Schemas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from\n  Planning with Actions to Planning with Schemas"
                },
                "summary": "Enabling LLMs to effectively operate long-horizon task which requires\nlong-term planning and multiple interactions is essential for open-world\nautonomy. Conventional methods adopt planning with actions where a executable\naction list would be provided as reference. However, this action representation\nchoice would be impractical when the environment action space is combinatorial\nexploded (e.g., open-ended real world). This naturally leads to a question: As\nenvironmental action space scales, what is the optimal action representation\nfor long-horizon agents? In this paper, we systematically study the\neffectiveness of two different action representations. The first one is\nconventional planning with actions (PwA) which is predominantly adopted for its\neffectiveness on existing benchmarks. The other one is planning with schemas\n(PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ]\nto [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable\nscalability. This alternative is motivated by its alignment with human\ncognition and its compliance with environment-imposed action format\nrestriction. We propose cognitive bandwidth perspective as a conceptual\nframework to qualitatively understand the differences between these two action\nrepresentations and empirically observe a representation-choice inflection\npoint between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve\nas evidence of the need for scalable representations. We further conduct\ncontrolled experiments to study how the location of this inflection point\ninteracts with different model capacities: stronger planning proficiency shifts\nthe inflection rightward, whereas better schema instantiation shifts it\nleftward. Finally, noting the suboptimal performance of PwS agents, we provide\nan actionable guide for building more capable PwS agents for better scalable\nautonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling LLMs to effectively operate long-horizon task which requires\nlong-term planning and multiple interactions is essential for open-world\nautonomy. Conventional methods adopt planning with actions where a executable\naction list would be provided as reference. However, this action representation\nchoice would be impractical when the environment action space is combinatorial\nexploded (e.g., open-ended real world). This naturally leads to a question: As\nenvironmental action space scales, what is the optimal action representation\nfor long-horizon agents? In this paper, we systematically study the\neffectiveness of two different action representations. The first one is\nconventional planning with actions (PwA) which is predominantly adopted for its\neffectiveness on existing benchmarks. The other one is planning with schemas\n(PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ]\nto [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable\nscalability. This alternative is motivated by its alignment with human\ncognition and its compliance with environment-imposed action format\nrestriction. We propose cognitive bandwidth perspective as a conceptual\nframework to qualitatively understand the differences between these two action\nrepresentations and empirically observe a representation-choice inflection\npoint between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve\nas evidence of the need for scalable representations. We further conduct\ncontrolled experiments to study how the location of this inflection point\ninteracts with different model capacities: stronger planning proficiency shifts\nthe inflection rightward, whereas better schema instantiation shifts it\nleftward. Finally, noting the suboptimal performance of PwS agents, we provide\nan actionable guide for building more capable PwS agents for better scalable\nautonomy."
                },
                "authors": [
                    {
                        "name": "Baixuan Xu"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07083v1",
                "updated": "2025-10-08T14:40:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    40,
                    33,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:40:33Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    40,
                    33,
                    2,
                    281,
                    0
                ],
                "title": "All Claims Are Equal, but Some Claims Are More Equal Than Others:\n  Importance-Sensitive Factuality Evaluation of LLM Generations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All Claims Are Equal, but Some Claims Are More Equal Than Others:\n  Importance-Sensitive Factuality Evaluation of LLM Generations"
                },
                "summary": "Existing methods for evaluating the factuality of large language model (LLM)\nresponses treat all claims as equally important. This results in misleading\nevaluations when vital information is missing or incorrect as it receives the\nsame weight as peripheral details, raising the question: how can we reliably\ndetect such differences when there are errors in key information? Current\napproaches that measure factuality tend to be insensitive to omitted or false\nkey information. To investigate this lack of sensitivity, we construct\nVITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses\ndesigned to omit or falsify key information. Using this dataset, we demonstrate\nthe insensitivities of existing evaluation metrics to key information errors.\nTo address this gap, we introduce VITAL, a set of metrics that provide greater\nsensitivity in measuring the factuality of responses by incorporating the\nrelevance and importance of claims with respect to the query. Our analysis\ndemonstrates that VITAL metrics more reliably detect errors in key information\nthan previous methods. Our dataset, metrics, and analysis provide a foundation\nfor more accurate and robust assessment of LLM factuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods for evaluating the factuality of large language model (LLM)\nresponses treat all claims as equally important. This results in misleading\nevaluations when vital information is missing or incorrect as it receives the\nsame weight as peripheral details, raising the question: how can we reliably\ndetect such differences when there are errors in key information? Current\napproaches that measure factuality tend to be insensitive to omitted or false\nkey information. To investigate this lack of sensitivity, we construct\nVITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses\ndesigned to omit or falsify key information. Using this dataset, we demonstrate\nthe insensitivities of existing evaluation metrics to key information errors.\nTo address this gap, we introduce VITAL, a set of metrics that provide greater\nsensitivity in measuring the factuality of responses by incorporating the\nrelevance and importance of claims with respect to the query. Our analysis\ndemonstrates that VITAL metrics more reliably detect errors in key information\nthan previous methods. Our dataset, metrics, and analysis provide a foundation\nfor more accurate and robust assessment of LLM factuality."
                },
                "authors": [
                    {
                        "name": "Miriam Wanner"
                    },
                    {
                        "name": "Leif Azzopardi"
                    },
                    {
                        "name": "Paul Thomas"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Nick Craswell"
                    }
                ],
                "author_detail": {
                    "name": "Nick Craswell"
                },
                "author": "Nick Craswell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11787v2",
                "updated": "2025-10-08T14:40:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    40,
                    12,
                    2,
                    281,
                    0
                ],
                "published": "2025-09-15T11:16:04Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    16,
                    4,
                    0,
                    258,
                    0
                ],
                "title": "CodeCureAgent: Automatic Classification and Repair of Static Analysis\n  Warnings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeCureAgent: Automatic Classification and Repair of Static Analysis\n  Warnings"
                },
                "summary": "Static analysis tools are widely used to detect bugs, vulnerabilities, and\ncode smells. Traditionally, developers must resolve these warnings manually.\nBecause this process is tedious, developers sometimes ignore warnings, leading\nto an accumulation of warnings and a degradation of code quality. This paper\npresents CodeCureAgent, an approach that harnesses LLM-based agents to\nautomatically analyze, classify, and repair static analysis warnings. Unlike\nprevious work, our method does not follow a predetermined algorithm. Instead,\nwe adopt an agentic framework that iteratively invokes tools to gather\nadditional information from the codebase (e.g., via code search) and edit the\ncodebase to resolve the warning. CodeCureAgent detects and suppresses false\npositives, while fixing true positives when identified. We equip CodeCureAgent\nwith a three-step heuristic to approve patches: (1) build the project, (2)\nverify that the warning disappears without introducing new warnings, and (3)\nrun the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube\nwarnings found in 106 Java projects and covering 291 distinct rules. Our\napproach produces plausible fixes for 96.8% of the warnings, outperforming\nstate-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,\nrespectively. Manual inspection of 291 cases reveals a correct-fix rate of\n86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.\nThe approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end\nprocessing time of about four minutes per warning. We envision CodeCureAgent\nhelping to clean existing codebases and being integrated into CI/CD pipelines\nto prevent the accumulation of static analysis warnings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static analysis tools are widely used to detect bugs, vulnerabilities, and\ncode smells. Traditionally, developers must resolve these warnings manually.\nBecause this process is tedious, developers sometimes ignore warnings, leading\nto an accumulation of warnings and a degradation of code quality. This paper\npresents CodeCureAgent, an approach that harnesses LLM-based agents to\nautomatically analyze, classify, and repair static analysis warnings. Unlike\nprevious work, our method does not follow a predetermined algorithm. Instead,\nwe adopt an agentic framework that iteratively invokes tools to gather\nadditional information from the codebase (e.g., via code search) and edit the\ncodebase to resolve the warning. CodeCureAgent detects and suppresses false\npositives, while fixing true positives when identified. We equip CodeCureAgent\nwith a three-step heuristic to approve patches: (1) build the project, (2)\nverify that the warning disappears without introducing new warnings, and (3)\nrun the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube\nwarnings found in 106 Java projects and covering 291 distinct rules. Our\napproach produces plausible fixes for 96.8% of the warnings, outperforming\nstate-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,\nrespectively. Manual inspection of 291 cases reveals a correct-fix rate of\n86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.\nThe approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end\nprocessing time of about four minutes per warning. We envision CodeCureAgent\nhelping to clean existing codebases and being integrated into CI/CD pipelines\nto prevent the accumulation of static analysis warnings."
                },
                "authors": [
                    {
                        "name": "Pascal Joos"
                    },
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05318v2",
                "updated": "2025-10-08T14:39:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    39,
                    59,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-06T19:31:47Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    19,
                    31,
                    47,
                    0,
                    279,
                    0
                ],
                "title": "BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language\n  Models via Lens of Dynamic Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language\n  Models via Lens of Dynamic Interactions"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nsingle-turn text-to-SQL tasks, but real-world database applications\npredominantly require multi-turn interactions to handle ambiguous queries,\nexecution errors, and evolving user requirements. Existing multi-turn\nbenchmarks fall short by treating conversation histories as static context or\nlimiting evaluation to read-only operations, failing to reflect\nproduction-grade database assistant challenges. We introduce BIRD-INTERACT, a\nbenchmark that restores this realism through: (1) a comprehensive interaction\nenvironment coupling each database with a hierarchical knowledge base, metadata\nfiles, and a function-driven user simulator, enabling models to solicit\nclarifications, retrieve knowledge, and recover from errors without human\nsupervision; (2) two evaluation settings consisting of a pre-defined\nconversational protocol (c-Interact) and an open-ended agentic setting\n(a-Interact) where models autonomously decide when to query the user simulator\nor explore the environment; (3) a challenging task suite covering the full CRUD\nspectrum for business-intelligence and operational use cases, guarded by\nexecutable test cases. Each task features ambiguous and follow-up sub-tasks\nrequiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600\ntasks, up to 11,796 interactions) for comprehensive performance assessment, and\nBIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed\nbehavioral analysis and rapid method development. Our empirical results\nhighlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in\nc-Interact and 17.00% in a-Interact. Analysis via memory grafting and\nInteraction Test-time Scaling validates the importance of effective interaction\nfor complex, dynamic text-to-SQL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance on\nsingle-turn text-to-SQL tasks, but real-world database applications\npredominantly require multi-turn interactions to handle ambiguous queries,\nexecution errors, and evolving user requirements. Existing multi-turn\nbenchmarks fall short by treating conversation histories as static context or\nlimiting evaluation to read-only operations, failing to reflect\nproduction-grade database assistant challenges. We introduce BIRD-INTERACT, a\nbenchmark that restores this realism through: (1) a comprehensive interaction\nenvironment coupling each database with a hierarchical knowledge base, metadata\nfiles, and a function-driven user simulator, enabling models to solicit\nclarifications, retrieve knowledge, and recover from errors without human\nsupervision; (2) two evaluation settings consisting of a pre-defined\nconversational protocol (c-Interact) and an open-ended agentic setting\n(a-Interact) where models autonomously decide when to query the user simulator\nor explore the environment; (3) a challenging task suite covering the full CRUD\nspectrum for business-intelligence and operational use cases, guarded by\nexecutable test cases. Each task features ambiguous and follow-up sub-tasks\nrequiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600\ntasks, up to 11,796 interactions) for comprehensive performance assessment, and\nBIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed\nbehavioral analysis and rapid method development. Our empirical results\nhighlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in\nc-Interact and 17.00% in a-Interact. Analysis via memory grafting and\nInteraction Test-time Scaling validates the importance of effective interaction\nfor complex, dynamic text-to-SQL tasks."
                },
                "authors": [
                    {
                        "name": "Nan Huo"
                    },
                    {
                        "name": "Xiaohan Xu"
                    },
                    {
                        "name": "Jinyang Li"
                    },
                    {
                        "name": "Per Jacobsson"
                    },
                    {
                        "name": "Shipei Lin"
                    },
                    {
                        "name": "Bowen Qin"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Ge Qu"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Linheng Han"
                    },
                    {
                        "name": "Edward Alexander"
                    },
                    {
                        "name": "Xintong Zhu"
                    },
                    {
                        "name": "Rui Qin"
                    },
                    {
                        "name": "Ruihan Yu"
                    },
                    {
                        "name": "Yiyao Jin"
                    },
                    {
                        "name": "Feige Zhou"
                    },
                    {
                        "name": "Weihao Zhong"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Fatma Ozcan"
                    },
                    {
                        "name": "Yannis Papakonstantinou"
                    },
                    {
                        "name": "Reynold Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Reynold Cheng"
                },
                "author": "Reynold Cheng",
                "arxiv_comment": "47 pages, 26 figures, 11 tables. Submitted to arXiv; based on work\n  from The BIRD Team and Google Cloud. Dataset and code available at\n  https://bird-interact.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07081v1",
                "updated": "2025-10-08T14:39:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    39,
                    34,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:39:34Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    39,
                    34,
                    2,
                    281,
                    0
                ],
                "title": "Accelerating Diffusion LLM Inference via Local Determinism Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLM Inference via Local Determinism Propagation"
                },
                "summary": "Diffusion large language models (dLLMs) represent a significant advancement\nin text generation, offering parallel token decoding capabilities. However,\nexisting open-source implementations suffer from quality-speed trade-offs that\nimpede their practical deployment. Conservative sampling strategies typically\ndecode only the most confident token per step to ensure quality (i.e., greedy\ndecoding), at the cost of inference efficiency due to repeated redundant\nrefinement iterations--a phenomenon we term delayed decoding. Through\nsystematic analysis of dLLM decoding dynamics, we characterize this delayed\ndecoding behavior and propose a training-free adaptive parallel decoding\nstrategy, named LocalLeap, to address these inefficiencies. LocalLeap is built\non two fundamental empirical principles: local determinism propagation centered\non high-confidence anchors and progressive spatial consistency decay. By\napplying these principles, LocalLeap identifies anchors and performs localized\nrelaxed parallel decoding within bounded neighborhoods, achieving substantial\ninference step reduction through early commitment of already-determined tokens\nwithout compromising output quality. Comprehensive evaluation on various\nbenchmarks demonstrates that LocalLeap achieves 6.94$\\times$ throughput\nimprovements and reduces decoding steps to just 14.2\\% of the original\nrequirement, achieving these gains with negligible performance impact. The\nsource codes are available at: https://github.com/friedrichor/LocalLeap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) represent a significant advancement\nin text generation, offering parallel token decoding capabilities. However,\nexisting open-source implementations suffer from quality-speed trade-offs that\nimpede their practical deployment. Conservative sampling strategies typically\ndecode only the most confident token per step to ensure quality (i.e., greedy\ndecoding), at the cost of inference efficiency due to repeated redundant\nrefinement iterations--a phenomenon we term delayed decoding. Through\nsystematic analysis of dLLM decoding dynamics, we characterize this delayed\ndecoding behavior and propose a training-free adaptive parallel decoding\nstrategy, named LocalLeap, to address these inefficiencies. LocalLeap is built\non two fundamental empirical principles: local determinism propagation centered\non high-confidence anchors and progressive spatial consistency decay. By\napplying these principles, LocalLeap identifies anchors and performs localized\nrelaxed parallel decoding within bounded neighborhoods, achieving substantial\ninference step reduction through early commitment of already-determined tokens\nwithout compromising output quality. Comprehensive evaluation on various\nbenchmarks demonstrates that LocalLeap achieves 6.94$\\times$ throughput\nimprovements and reduces decoding steps to just 14.2\\% of the original\nrequirement, achieving these gains with negligible performance impact. The\nsource codes are available at: https://github.com/friedrichor/LocalLeap."
                },
                "authors": [
                    {
                        "name": "Fanheng Kong"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Yahui Liu"
                    },
                    {
                        "name": "Zirui Wu"
                    },
                    {
                        "name": "Yu Tian"
                    },
                    {
                        "name": "Victoria W."
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "21 pages, 4 figures. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07077v1",
                "updated": "2025-10-08T14:38:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    38,
                    25,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:38:25Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    38,
                    25,
                    2,
                    281,
                    0
                ],
                "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action Models for Robotics: A Review Towards Real-World\n  Applications"
                },
                "summary": "Amid growing efforts to leverage advances in large language models (LLMs) and\nvision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models\nhave recently gained significant attention. By unifying vision, language, and\naction data at scale, which have traditionally been studied separately, VLA\nmodels aim to learn policies that generalise across diverse tasks, objects,\nembodiments, and environments. This generalisation capability is expected to\nenable robots to solve novel downstream tasks with minimal or no additional\ntask-specific data, facilitating more flexible and scalable real-world\ndeployment. Unlike previous surveys that focus narrowly on action\nrepresentations or high-level model architectures, this work offers a\ncomprehensive, full-stack review, integrating both software and hardware\ncomponents of VLA systems. In particular, this paper provides a systematic\nreview of VLAs, covering their strategy and architectural transition,\narchitectures and building blocks, modality-specific processing techniques, and\nlearning paradigms. In addition, to support the deployment of VLAs in\nreal-world robotic applications, we also review commonly used robot platforms,\ndata collection strategies, publicly available datasets, data augmentation\nmethods, and evaluation benchmarks. Throughout this comprehensive survey, this\npaper aims to offer practical guidance for the robotics community in applying\nVLAs to real-world robotic systems. All references categorized by training\napproach, evaluation method, modality, and dataset are available in the table\non our project website: https://vla-survey.github.io .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amid growing efforts to leverage advances in large language models (LLMs) and\nvision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models\nhave recently gained significant attention. By unifying vision, language, and\naction data at scale, which have traditionally been studied separately, VLA\nmodels aim to learn policies that generalise across diverse tasks, objects,\nembodiments, and environments. This generalisation capability is expected to\nenable robots to solve novel downstream tasks with minimal or no additional\ntask-specific data, facilitating more flexible and scalable real-world\ndeployment. Unlike previous surveys that focus narrowly on action\nrepresentations or high-level model architectures, this work offers a\ncomprehensive, full-stack review, integrating both software and hardware\ncomponents of VLA systems. In particular, this paper provides a systematic\nreview of VLAs, covering their strategy and architectural transition,\narchitectures and building blocks, modality-specific processing techniques, and\nlearning paradigms. In addition, to support the deployment of VLAs in\nreal-world robotic applications, we also review commonly used robot platforms,\ndata collection strategies, publicly available datasets, data augmentation\nmethods, and evaluation benchmarks. Throughout this comprehensive survey, this\npaper aims to offer practical guidance for the robotics community in applying\nVLAs to real-world robotic systems. All references categorized by training\napproach, evaluation method, modality, and dataset are available in the table\non our project website: https://vla-survey.github.io ."
                },
                "authors": [
                    {
                        "name": "Kento Kawaharazuka"
                    },
                    {
                        "name": "Jihoon Oh"
                    },
                    {
                        "name": "Jun Yamada"
                    },
                    {
                        "name": "Ingmar Posner"
                    },
                    {
                        "name": "Yuke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Zhu"
                },
                "author": "Yuke Zhu",
                "arxiv_doi": "10.1109/ACCESS.2025.3609980",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3609980",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.07077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to IEEE Access, website: https://vla-survey.github.io",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07073v1",
                "updated": "2025-10-08T14:35:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    35,
                    9,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:35:09Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    35,
                    9,
                    2,
                    281,
                    0
                ],
                "title": "VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle\n  Routing Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle\n  Routing Problems"
                },
                "summary": "Designing high-performing heuristics for vehicle routing problems (VRPs) is a\ncomplex task that requires both intuition and deep domain knowledge. Large\nlanguage model (LLM)-based code generation has recently shown promise across\nmany domains, but it still falls short of producing heuristics that rival those\ncrafted by human experts. In this paper, we propose VRPAgent, a framework that\nintegrates LLM-generated components into a metaheuristic and refines them\nthrough a novel genetic search. By using the LLM to generate problem-specific\noperators, embedded within a generic metaheuristic framework, VRPAgent keeps\ntasks manageable, guarantees correctness, and still enables the discovery of\nnovel and powerful strategies. Across multiple problems, including the\ncapacitated VRP, the VRP with time windows, and the prize-collecting VRP, our\nmethod discovers heuristic operators that outperform handcrafted methods and\nrecent learning-based approaches while requiring only a single CPU core. To our\nknowledge, \\VRPAgent is the first LLM-based paradigm to advance the\nstate-of-the-art in VRPs, highlighting a promising future for automated\nheuristics discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing high-performing heuristics for vehicle routing problems (VRPs) is a\ncomplex task that requires both intuition and deep domain knowledge. Large\nlanguage model (LLM)-based code generation has recently shown promise across\nmany domains, but it still falls short of producing heuristics that rival those\ncrafted by human experts. In this paper, we propose VRPAgent, a framework that\nintegrates LLM-generated components into a metaheuristic and refines them\nthrough a novel genetic search. By using the LLM to generate problem-specific\noperators, embedded within a generic metaheuristic framework, VRPAgent keeps\ntasks manageable, guarantees correctness, and still enables the discovery of\nnovel and powerful strategies. Across multiple problems, including the\ncapacitated VRP, the VRP with time windows, and the prize-collecting VRP, our\nmethod discovers heuristic operators that outperform handcrafted methods and\nrecent learning-based approaches while requiring only a single CPU core. To our\nknowledge, \\VRPAgent is the first LLM-based paradigm to advance the\nstate-of-the-art in VRPs, highlighting a promising future for automated\nheuristics discovery."
                },
                "authors": [
                    {
                        "name": "André Hottung"
                    },
                    {
                        "name": "Federico Berto"
                    },
                    {
                        "name": "Chuanbo Hua"
                    },
                    {
                        "name": "Nayeli Gast Zepeda"
                    },
                    {
                        "name": "Daniel Wetzel"
                    },
                    {
                        "name": "Michael Römer"
                    },
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Davide Zago"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Jinkyoo Park"
                    },
                    {
                        "name": "Kevin Tierney"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Tierney"
                },
                "author": "Kevin Tierney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.08559v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.08559v3",
                "updated": "2025-10-08T14:32:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    32,
                    17,
                    2,
                    281,
                    0
                ],
                "published": "2023-06-14T15:05:48Z",
                "published_parsed": [
                    2023,
                    6,
                    14,
                    15,
                    5,
                    48,
                    2,
                    165,
                    0
                ],
                "title": "Inference in clustered IV models with many and weak instruments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference in clustered IV models with many and weak instruments"
                },
                "summary": "Data clustering reduces the effective sample size from the number of\nobservations towards the number of clusters. For instrumental variable models\nthis reduced effective sample size makes the instruments more likely to be\nweak, in the sense that they contain little information about the endogenous\nregressor, and many, in the sense that their number is large compared to the\nsample size. Consequently, weak and many instrument problems for estimators and\ntests in instrumental variable models are also more likely. None of the\npreviously developed many and weak instrument robust tests, however, can be\napplied to clustered data as they all require independent observations.\nTherefore, I adapt the many and weak instrument robust jackknife\nAnderson--Rubin and jackknife score tests to clustered data by removing\nclusters rather than individual observations from the statistics. Simulations\nand a revisitation of a study on the effect of queenly reign on war show the\nempirical relevance of the new tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data clustering reduces the effective sample size from the number of\nobservations towards the number of clusters. For instrumental variable models\nthis reduced effective sample size makes the instruments more likely to be\nweak, in the sense that they contain little information about the endogenous\nregressor, and many, in the sense that their number is large compared to the\nsample size. Consequently, weak and many instrument problems for estimators and\ntests in instrumental variable models are also more likely. None of the\npreviously developed many and weak instrument robust tests, however, can be\napplied to clustered data as they all require independent observations.\nTherefore, I adapt the many and weak instrument robust jackknife\nAnderson--Rubin and jackknife score tests to clustered data by removing\nclusters rather than individual observations from the statistics. Simulations\nand a revisitation of a study on the effect of queenly reign on war show the\nempirical relevance of the new tests."
                },
                "authors": [
                    {
                        "name": "Johannes W. Ligtenberg"
                    }
                ],
                "author_detail": {
                    "name": "Johannes W. Ligtenberg"
                },
                "author": "Johannes W. Ligtenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.08559v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.08559v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07067v1",
                "updated": "2025-10-08T14:31:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    31,
                    35,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:31:35Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    31,
                    35,
                    2,
                    281,
                    0
                ],
                "title": "Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied\n  AI Commands on VLA Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied\n  AI Commands on VLA Models"
                },
                "summary": "Vision Language Action (VLA) models are widely used in Embodied AI, enabling\nrobots to interpret and execute language instructions. However, their\nrobustness to natural language variability in real-world scenarios has not been\nthoroughly investigated. In this work, we present a novel systematic study of\nthe robustness of state-of-the-art VLA models under linguistic perturbations.\nSpecifically, we evaluate model performance under two types of instruction\nnoise: (1) human-generated paraphrasing and (2) the addition of irrelevant\ncontext. We further categorize irrelevant contexts into two groups according to\ntheir length and their semantic and lexical proximity to robot commands. In\nthis study, we observe consistent performance degradation as context size\nexpands. We also demonstrate that the model can exhibit relative robustness to\nrandom context, with a performance drop within 10%, while semantically and\nlexically similar context of the same length can trigger a quality decline of\naround 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To\nmitigate this, we propose an LLM-based filtering framework that extracts core\ncommands from noisy inputs. Incorporating our filtering step allows models to\nrecover up to 98.5% of their original performance under noisy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Action (VLA) models are widely used in Embodied AI, enabling\nrobots to interpret and execute language instructions. However, their\nrobustness to natural language variability in real-world scenarios has not been\nthoroughly investigated. In this work, we present a novel systematic study of\nthe robustness of state-of-the-art VLA models under linguistic perturbations.\nSpecifically, we evaluate model performance under two types of instruction\nnoise: (1) human-generated paraphrasing and (2) the addition of irrelevant\ncontext. We further categorize irrelevant contexts into two groups according to\ntheir length and their semantic and lexical proximity to robot commands. In\nthis study, we observe consistent performance degradation as context size\nexpands. We also demonstrate that the model can exhibit relative robustness to\nrandom context, with a performance drop within 10%, while semantically and\nlexically similar context of the same length can trigger a quality decline of\naround 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To\nmitigate this, we propose an LLM-based filtering framework that extracts core\ncommands from noisy inputs. Incorporating our filtering step allows models to\nrecover up to 98.5% of their original performance under noisy conditions."
                },
                "authors": [
                    {
                        "name": "Daria Pugacheva"
                    },
                    {
                        "name": "Andrey Moskalenko"
                    },
                    {
                        "name": "Denis Shepelev"
                    },
                    {
                        "name": "Andrey Kuznetsov"
                    },
                    {
                        "name": "Vlad Shakhuro"
                    },
                    {
                        "name": "Elena Tutubalina"
                    }
                ],
                "author_detail": {
                    "name": "Elena Tutubalina"
                },
                "author": "Elena Tutubalina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07064v1",
                "updated": "2025-10-08T14:28:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    28,
                    53,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:28:53Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    28,
                    53,
                    2,
                    281,
                    0
                ],
                "title": "Prompt Optimization Across Multiple Agents for Representing Diverse\n  Human Populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Optimization Across Multiple Agents for Representing Diverse\n  Human Populations"
                },
                "summary": "The difficulty and expense of obtaining large-scale human responses make\nLarge Language Models (LLMs) an attractive alternative and a promising proxy\nfor human behavior. However, prior work shows that LLMs often produce\nhomogeneous outputs that fail to capture the rich diversity of human\nperspectives and behaviors. Thus, rather than trying to capture this diversity\nwith a single LLM agent, we propose a novel framework to construct a set of\nagents that collectively capture the diversity of a given human population.\nEach agent is an LLM whose behavior is steered by conditioning on a small set\nof human demonstrations (task-response pairs) through in-context learning. The\ncentral challenge is therefore to select a representative set of LLM agents\nfrom the exponentially large space of possible agents. We tackle this selection\nproblem from the lens of submodular optimization. In particular, we develop\nmethods that offer different trade-offs regarding time complexity and\nperformance guarantees. Extensive experiments in crowdsourcing and educational\ndomains demonstrate that our approach constructs agents that more effectively\nrepresent human populations compared to baselines. Moreover, behavioral\nanalyses on new tasks show that these agents reproduce the behavior patterns\nand perspectives of the students and annotators they are designed to represent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The difficulty and expense of obtaining large-scale human responses make\nLarge Language Models (LLMs) an attractive alternative and a promising proxy\nfor human behavior. However, prior work shows that LLMs often produce\nhomogeneous outputs that fail to capture the rich diversity of human\nperspectives and behaviors. Thus, rather than trying to capture this diversity\nwith a single LLM agent, we propose a novel framework to construct a set of\nagents that collectively capture the diversity of a given human population.\nEach agent is an LLM whose behavior is steered by conditioning on a small set\nof human demonstrations (task-response pairs) through in-context learning. The\ncentral challenge is therefore to select a representative set of LLM agents\nfrom the exponentially large space of possible agents. We tackle this selection\nproblem from the lens of submodular optimization. In particular, we develop\nmethods that offer different trade-offs regarding time complexity and\nperformance guarantees. Extensive experiments in crowdsourcing and educational\ndomains demonstrate that our approach constructs agents that more effectively\nrepresent human populations compared to baselines. Moreover, behavioral\nanalyses on new tasks show that these agents reproduce the behavior patterns\nand perspectives of the students and annotators they are designed to represent."
                },
                "authors": [
                    {
                        "name": "Manh Hung Nguyen"
                    },
                    {
                        "name": "Sebastian Tschiatschek"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07061v1",
                "updated": "2025-10-08T14:27:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    27,
                    2,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:27:02Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    27,
                    2,
                    2,
                    281,
                    0
                ],
                "title": "Revisiting Metric Reliability for Fine-grained Evaluation of Machine\n  Translation and Summarization in Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Metric Reliability for Fine-grained Evaluation of Machine\n  Translation and Summarization in Indian Languages"
                },
                "summary": "While automatic metrics drive progress in Machine Translation (MT) and Text\nSummarization (TS), existing metrics have been developed and validated almost\nexclusively for English and other high-resource languages. This narrow focus\nleaves Indian languages, spoken by over 1.5 billion people, largely overlooked,\ncasting doubt on the universality of current evaluation practices. To address\nthis gap, we introduce ITEM, a large-scale benchmark that systematically\nevaluates the alignment of 26 automatic metrics with human judgments across six\nmajor Indian languages, enriched with fine-grained annotations. Our extensive\nevaluation, covering agreement with human judgments, sensitivity to outliers,\nlanguage-specific reliability, inter-metric correlations, and resilience to\ncontrolled perturbations, reveals four central findings: (1) LLM-based\nevaluators show the strongest alignment with human judgments at both segment\nand system levels; (2) outliers exert a significant impact on metric-human\nagreement; (3) in TS, metrics are more effective at capturing content fidelity,\nwhereas in MT, they better reflect fluency; and (4) metrics differ in their\nrobustness and sensitivity when subjected to diverse perturbations.\nCollectively, these findings offer critical guidance for advancing metric\ndesign and evaluation in Indian languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While automatic metrics drive progress in Machine Translation (MT) and Text\nSummarization (TS), existing metrics have been developed and validated almost\nexclusively for English and other high-resource languages. This narrow focus\nleaves Indian languages, spoken by over 1.5 billion people, largely overlooked,\ncasting doubt on the universality of current evaluation practices. To address\nthis gap, we introduce ITEM, a large-scale benchmark that systematically\nevaluates the alignment of 26 automatic metrics with human judgments across six\nmajor Indian languages, enriched with fine-grained annotations. Our extensive\nevaluation, covering agreement with human judgments, sensitivity to outliers,\nlanguage-specific reliability, inter-metric correlations, and resilience to\ncontrolled perturbations, reveals four central findings: (1) LLM-based\nevaluators show the strongest alignment with human judgments at both segment\nand system levels; (2) outliers exert a significant impact on metric-human\nagreement; (3) in TS, metrics are more effective at capturing content fidelity,\nwhereas in MT, they better reflect fluency; and (4) metrics differ in their\nrobustness and sensitivity when subjected to diverse perturbations.\nCollectively, these findings offer critical guidance for advancing metric\ndesign and evaluation in Indian languages."
                },
                "authors": [
                    {
                        "name": "Amir Hossein Yari"
                    },
                    {
                        "name": "Kalmit Kulkarni"
                    },
                    {
                        "name": "Ahmad Raza Khan"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "arxiv_comment": "18 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04023v2",
                "updated": "2025-10-08T14:20:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    20,
                    50,
                    2,
                    281,
                    0
                ],
                "published": "2025-07-05T12:31:17Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    12,
                    31,
                    17,
                    5,
                    186,
                    0
                ],
                "title": "Do LLMs Overthink Basic Math Reasoning? Benchmarking the\n  Accuracy-Efficiency Tradeoff in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Overthink Basic Math Reasoning? Benchmarking the\n  Accuracy-Efficiency Tradeoff in Language Models"
                },
                "summary": "Large language models (LLMs) achieve impressive performance on complex\nmathematical benchmarks yet sometimes fail on basic math reasoning while\ngenerating unnecessarily verbose responses. In this paper, we present a\nsystematic benchmark and comprehensive empirical study to evaluate the\nefficiency of reasoning in LLMs, focusing on the fundamental tradeoff between\naccuracy and overthinking. First, we formalize the accuracy-verbosity tradeoff.\nSecond, we introduce the Overthinking Score, a harmonic-mean metric combining\naccuracy and token-efficiency for holistic model evaluation. Third, we\nestablish an evaluation protocol with dynamically-generated data across 14\nbasic math tasks. Fourth, we conduct a large-scale empirical study evaluating\n53 LLMs, including reasoning and quantized variants across different reasoning\nbudgets. Our findings reveal: 1) model performance on complex benchmarks does\nnot translate directly to basic math reasoning; 2) reasoning models generate\n~18 more tokens while sometimes achieving lower accuracy and exhibit\ncatastrophic collapse when token is constrained, dropping by ~28; 3) the\naccuracy-verbosity relationship is non-monotonic with extended reasoning\nbudgets yielding diminishing returns (GPT-5/o-series models show zero accuracy\ngain from low -> medium -> high reasoning effort). Our findings challenge the\nassumption that longer reasoning in LLMs necessarily improves mathematical\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve impressive performance on complex\nmathematical benchmarks yet sometimes fail on basic math reasoning while\ngenerating unnecessarily verbose responses. In this paper, we present a\nsystematic benchmark and comprehensive empirical study to evaluate the\nefficiency of reasoning in LLMs, focusing on the fundamental tradeoff between\naccuracy and overthinking. First, we formalize the accuracy-verbosity tradeoff.\nSecond, we introduce the Overthinking Score, a harmonic-mean metric combining\naccuracy and token-efficiency for holistic model evaluation. Third, we\nestablish an evaluation protocol with dynamically-generated data across 14\nbasic math tasks. Fourth, we conduct a large-scale empirical study evaluating\n53 LLMs, including reasoning and quantized variants across different reasoning\nbudgets. Our findings reveal: 1) model performance on complex benchmarks does\nnot translate directly to basic math reasoning; 2) reasoning models generate\n~18 more tokens while sometimes achieving lower accuracy and exhibit\ncatastrophic collapse when token is constrained, dropping by ~28; 3) the\naccuracy-verbosity relationship is non-monotonic with extended reasoning\nbudgets yielding diminishing returns (GPT-5/o-series models show zero accuracy\ngain from low -> medium -> high reasoning effort). Our findings challenge the\nassumption that longer reasoning in LLMs necessarily improves mathematical\nreasoning."
                },
                "authors": [
                    {
                        "name": "Gaurav Srivastava"
                    },
                    {
                        "name": "Aafiya Hussain"
                    },
                    {
                        "name": "Sriram Srinivasan"
                    },
                    {
                        "name": "Xuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Wang"
                },
                "author": "Xuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09001v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09001v4",
                "updated": "2025-10-08T14:16:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    16,
                    30,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-13T22:36:07Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    22,
                    36,
                    7,
                    1,
                    133,
                    0
                ],
                "title": "Causal Feedback Discovery using Convergence Cross Mapping on Sea Ice\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Feedback Discovery using Convergence Cross Mapping on Sea Ice\n  Data"
                },
                "summary": "Identifying causal relationships in climate systems remains challenging due\nto nonlinear, coupled dynamics that limit the effectiveness of linear and\nstochastic causal discovery approaches. This study benchmarks Convergence Cross\nMapping (CCM) against Granger causality, PCMCI, and VarLiNGAM using both\nsynthetic datasets with ground truth causal links and 41 years of Arctic\nclimate data (1979--2021). Unlike stochastic models that rely on autoregressive\nresidual dependence, CCM leverages Takens' state-space reconstruction and\ndelay-embedding to reconstruct attractor manifolds from time series. Cross\nmapping between reconstructed manifolds exploits deterministic signatures of\ncausation, enabling the detection of weak and bidirectional causal links that\nlinear models fail to resolve. Results demonstrate that CCM achieves higher\nspecificity and fewer false positives on synthetic benchmarks, while\nmaintaining robustness under observational noise and limited sample lengths. On\nArctic data, CCM reveals significant causal interactions between sea ice extent\nand atmospheric variables like specific humidity, longwave radiation, and\nsurface temperature with a $p$-value of $0.009$, supporting ice-albedo\nfeedbacks and moisture-radiation couplings central to Arctic amplification. In\ncontrast, stochastic approaches miss these nonlinear dependencies or infer\nspurious causal relations. This work establishes CCM as a robust causal\ninference tool for nonlinear climate dynamics and provides the first systematic\nbenchmarking framework for method selection in climate research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying causal relationships in climate systems remains challenging due\nto nonlinear, coupled dynamics that limit the effectiveness of linear and\nstochastic causal discovery approaches. This study benchmarks Convergence Cross\nMapping (CCM) against Granger causality, PCMCI, and VarLiNGAM using both\nsynthetic datasets with ground truth causal links and 41 years of Arctic\nclimate data (1979--2021). Unlike stochastic models that rely on autoregressive\nresidual dependence, CCM leverages Takens' state-space reconstruction and\ndelay-embedding to reconstruct attractor manifolds from time series. Cross\nmapping between reconstructed manifolds exploits deterministic signatures of\ncausation, enabling the detection of weak and bidirectional causal links that\nlinear models fail to resolve. Results demonstrate that CCM achieves higher\nspecificity and fewer false positives on synthetic benchmarks, while\nmaintaining robustness under observational noise and limited sample lengths. On\nArctic data, CCM reveals significant causal interactions between sea ice extent\nand atmospheric variables like specific humidity, longwave radiation, and\nsurface temperature with a $p$-value of $0.009$, supporting ice-albedo\nfeedbacks and moisture-radiation couplings central to Arctic amplification. In\ncontrast, stochastic approaches miss these nonlinear dependencies or infer\nspurious causal relations. This work establishes CCM as a robust causal\ninference tool for nonlinear climate dynamics and provides the first systematic\nbenchmarking framework for method selection in climate research."
                },
                "authors": [
                    {
                        "name": "Francis Nji"
                    },
                    {
                        "name": "Seraj Al Mahmud Mostafa"
                    },
                    {
                        "name": "Jianwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwu Wang"
                },
                "author": "Jianwu Wang",
                "arxiv_comment": "Accepted in ACM Sigspatial Conference, PolDS Workshop, 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09001v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09001v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07048v1",
                "updated": "2025-10-08T14:16:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    16,
                    20,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:16:20Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    16,
                    20,
                    2,
                    281,
                    0
                ],
                "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language\n  Models"
                },
                "summary": "Despite their remarkable natural language understanding capabilities, Large\nLanguage Models (LLMs) have been underutilized for retrieval tasks. We present\nSearch-R3, a novel framework that addresses this limitation by adapting LLMs to\ngenerate search embeddings as a direct output of their reasoning process. Our\napproach exploits LLMs' chain-of-thought capabilities, allowing them to produce\nmore effective embeddings by reasoning step-by-step through complex semantic\nanalyses. We implement this through three complementary mechanisms. (1) a\nsupervised learning stage enables the model's ability to produce quality\nembeddings, (2) a reinforcement learning (RL) methodology that optimizes\nembedding generation alongside reasoning, and (3) a specialized RL environment\nthat efficiently handles evolving embedding representations without requiring\ncomplete corpus re-encoding at each training iteration. Our extensive\nevaluations on diverse benchmarks demonstrate that Search-R3 significantly\noutperforms prior methods by unifying the reasoning and embedding generation\nprocesses. This integrated post-training approach represents a substantial\nadvancement in handling complex knowledge-intensive tasks that require both\nsophisticated reasoning and effective information retrieval. Project page:\nhttps://github.com/ytgui/Search-R3",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable natural language understanding capabilities, Large\nLanguage Models (LLMs) have been underutilized for retrieval tasks. We present\nSearch-R3, a novel framework that addresses this limitation by adapting LLMs to\ngenerate search embeddings as a direct output of their reasoning process. Our\napproach exploits LLMs' chain-of-thought capabilities, allowing them to produce\nmore effective embeddings by reasoning step-by-step through complex semantic\nanalyses. We implement this through three complementary mechanisms. (1) a\nsupervised learning stage enables the model's ability to produce quality\nembeddings, (2) a reinforcement learning (RL) methodology that optimizes\nembedding generation alongside reasoning, and (3) a specialized RL environment\nthat efficiently handles evolving embedding representations without requiring\ncomplete corpus re-encoding at each training iteration. Our extensive\nevaluations on diverse benchmarks demonstrate that Search-R3 significantly\noutperforms prior methods by unifying the reasoning and embedding generation\nprocesses. This integrated post-training approach represents a substantial\nadvancement in handling complex knowledge-intensive tasks that require both\nsophisticated reasoning and effective information retrieval. Project page:\nhttps://github.com/ytgui/Search-R3"
                },
                "authors": [
                    {
                        "name": "Yuntao Gui"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07043v1",
                "updated": "2025-10-08T14:09:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    9,
                    46,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:09:46Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    9,
                    46,
                    2,
                    281,
                    0
                ],
                "title": "COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference\n  Optimization"
                },
                "summary": "Real-world large language model (LLM) agents must master strategic tool use\nand user preference optimization through multi-turn interactions to assist\nusers with complex planning tasks. We introduce COMPASS (Constrained\nOptimization through Multi-turn Planning and Strategic Solutions), a benchmark\nthat evaluates agents on realistic travel-planning scenarios. We cast travel\nplanning as a constrained preference optimization problem, where agents must\nsatisfy hard constraints while simultaneously optimizing soft user preferences.\nTo support this, we build a realistic travel database covering transportation,\naccommodation, and ticketing for 20 U.S. National Parks, along with a\ncomprehensive tool ecosystem that mirrors commercial booking platforms.\nEvaluating state-of-the-art models, we uncover two critical gaps: (i) an\nacceptable-optimal gap, where agents reliably meet constraints but fail to\noptimize preferences, and (ii) a plan-coordination gap, where performance\ncollapses on multi-service (flight and hotel) coordination tasks, especially\nfor open-source models. By grounding reasoning and planning in a practical,\nuser-facing domain, COMPASS provides a benchmark that directly measures an\nagent's ability to optimize user preferences in realistic tasks, bridging\ntheoretical advances with real-world impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world large language model (LLM) agents must master strategic tool use\nand user preference optimization through multi-turn interactions to assist\nusers with complex planning tasks. We introduce COMPASS (Constrained\nOptimization through Multi-turn Planning and Strategic Solutions), a benchmark\nthat evaluates agents on realistic travel-planning scenarios. We cast travel\nplanning as a constrained preference optimization problem, where agents must\nsatisfy hard constraints while simultaneously optimizing soft user preferences.\nTo support this, we build a realistic travel database covering transportation,\naccommodation, and ticketing for 20 U.S. National Parks, along with a\ncomprehensive tool ecosystem that mirrors commercial booking platforms.\nEvaluating state-of-the-art models, we uncover two critical gaps: (i) an\nacceptable-optimal gap, where agents reliably meet constraints but fail to\noptimize preferences, and (ii) a plan-coordination gap, where performance\ncollapses on multi-service (flight and hotel) coordination tasks, especially\nfor open-source models. By grounding reasoning and planning in a practical,\nuser-facing domain, COMPASS provides a benchmark that directly measures an\nagent's ability to optimize user preferences in realistic tasks, bridging\ntheoretical advances with real-world impact."
                },
                "authors": [
                    {
                        "name": "Tian Qin"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Ting-Yao Hu"
                    },
                    {
                        "name": "Raviteja Vemulapalli"
                    },
                    {
                        "name": "Hema Swetha Koppula"
                    },
                    {
                        "name": "Zhiyang Xu"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Mert Cemri"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Meng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Cao"
                },
                "author": "Meng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19366v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19366v3",
                "updated": "2025-10-08T14:06:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    6,
                    26,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-26T18:54:52Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    18,
                    54,
                    52,
                    1,
                    238,
                    0
                ],
                "title": "Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying\n  Hallucinations in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying\n  Hallucinations in Multimodal LLMs"
                },
                "summary": "Hallucinations in LLMs--especially in multimodal settings--undermine\nreliability. We present a rigorous, information-geometric framework in\ndiffusion dynamics that quantifies hallucination in MLLMs: model outputs are\nembedded spectrally on multimodal graph Laplacians, and gaps to a truth\nmanifold define a semantic-distortion metric. We derive Courant--Fischer bounds\non a temperature-dependent hallucination energy and use RKHS eigenmodes to\nobtain modality-aware, interpretable measures that track evolution over prompts\nand time. This reframes hallucination as measurable and bounded, providing a\nprincipled basis for evaluation and mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in LLMs--especially in multimodal settings--undermine\nreliability. We present a rigorous, information-geometric framework in\ndiffusion dynamics that quantifies hallucination in MLLMs: model outputs are\nembedded spectrally on multimodal graph Laplacians, and gaps to a truth\nmanifold define a semantic-distortion metric. We derive Courant--Fischer bounds\non a temperature-dependent hallucination energy and use RKHS eigenmodes to\nobtain modality-aware, interpretable measures that track evolution over prompts\nand time. This reframes hallucination as measurable and bounded, providing a\nprincipled basis for evaluation and mitigation."
                },
                "authors": [
                    {
                        "name": "Supratik Sarkar"
                    },
                    {
                        "name": "Swagatam Das"
                    }
                ],
                "author_detail": {
                    "name": "Swagatam Das"
                },
                "author": "Swagatam Das",
                "arxiv_comment": "29 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19366v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19366v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53B21, 46E22 (Primary), 68R10 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07038v1",
                "updated": "2025-10-08T14:04:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    4,
                    27,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:04:27Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    4,
                    27,
                    2,
                    281,
                    0
                ],
                "title": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive\n  Tool Use with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive\n  Tool Use with Reinforcement Learning"
                },
                "summary": "Recent advances in large language models (LLMs) have popularized test-time\nscaling, where models generate additional reasoning tokens before producing\nfinal answers. These approaches have demonstrated significant performance\nimprovements on benchmarks involving mathematical reasoning. However, language\nmodels relying solely on direct inference still struggle with tasks demanding\nup-to-date knowledge or computational tools such as calculators and code\ninterpreters for complex arithmetic operations. To overcome these limitations,\nwe propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement\nlearning framework that systematically integrates multi-hop reasoning with\nadaptive tool-calling capabilities. Our approach employs a modified version of\nDynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,\nwhich we adapt specifically for tool invocation scenarios, enabling models to\ndynamically interleave complex reasoning with on-demand tool usage (including\nsearch APIs and Python interpreters).\n  To support this research, we introduce two new datasets: TAPO-easy-60K and\nTAPO-hard-18K, specifically designed to train and evaluate both fact-based\nreasoning and mathematical calculation capabilities. Our experiments on\nQwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,\nwith both models achieving state-of-the-art performance on tasks requiring\nexternal knowledge and mathematical computation among methods with comparable\nparameters. Notably, TAPO achieves more efficient tool utilization than\nbaseline methods while preventing excessive calls caused by reward hacking.\nThese results highlight the significant potential of combining advanced\nreasoning with tool usage to enhance model performance in knowledge-intensive\nand computationally demanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have popularized test-time\nscaling, where models generate additional reasoning tokens before producing\nfinal answers. These approaches have demonstrated significant performance\nimprovements on benchmarks involving mathematical reasoning. However, language\nmodels relying solely on direct inference still struggle with tasks demanding\nup-to-date knowledge or computational tools such as calculators and code\ninterpreters for complex arithmetic operations. To overcome these limitations,\nwe propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement\nlearning framework that systematically integrates multi-hop reasoning with\nadaptive tool-calling capabilities. Our approach employs a modified version of\nDynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,\nwhich we adapt specifically for tool invocation scenarios, enabling models to\ndynamically interleave complex reasoning with on-demand tool usage (including\nsearch APIs and Python interpreters).\n  To support this research, we introduce two new datasets: TAPO-easy-60K and\nTAPO-hard-18K, specifically designed to train and evaluate both fact-based\nreasoning and mathematical calculation capabilities. Our experiments on\nQwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,\nwith both models achieving state-of-the-art performance on tasks requiring\nexternal knowledge and mathematical computation among methods with comparable\nparameters. Notably, TAPO achieves more efficient tool utilization than\nbaseline methods while preventing excessive calls caused by reward hacking.\nThese results highlight the significant potential of combining advanced\nreasoning with tool usage to enhance model performance in knowledge-intensive\nand computationally demanding tasks."
                },
                "authors": [
                    {
                        "name": "Wenxun Wu"
                    },
                    {
                        "name": "Yuanyang Li"
                    },
                    {
                        "name": "Guhan Chen"
                    },
                    {
                        "name": "Linyue Wang"
                    },
                    {
                        "name": "Hongyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Chen"
                },
                "author": "Hongyang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.07315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07315v1",
                "updated": "2025-10-08T17:59:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    19,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:59:19Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    19,
                    2,
                    281,
                    0
                ],
                "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vibe Checker: Aligning Code Evaluation with Human Preference"
                },
                "summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding."
                },
                "authors": [
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Xiang Zhou"
                    },
                    {
                        "name": "Ting-Yun Chang"
                    },
                    {
                        "name": "Qingze Wang"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Xiance Si"
                    },
                    {
                        "name": "Dan Garrette"
                    },
                    {
                        "name": "Shyam Upadhyay"
                    },
                    {
                        "name": "Jeremiah Liu"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Benoit Schillings"
                    },
                    {
                        "name": "Jiao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiao Sun"
                },
                "author": "Jiao Sun",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07312v1",
                "updated": "2025-10-08T17:58:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    58,
                    41,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:58:41Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    58,
                    41,
                    2,
                    281,
                    0
                ],
                "title": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement\n  Learning"
                },
                "summary": "Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Alesia Ivanova"
                    },
                    {
                        "name": "Ziyang Cai"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Riashat Islam"
                    },
                    {
                        "name": "Shital Shah"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Charles London"
                    }
                ],
                "author_detail": {
                    "name": "Charles London"
                },
                "author": "Charles London",
                "arxiv_comment": "Preprint, 31 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12726v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12726v3",
                "updated": "2025-10-08T17:57:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    57,
                    43,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-18T08:49:29Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    8,
                    49,
                    29,
                    0,
                    230,
                    0
                ],
                "title": "DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM\n  Reasoning"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage tasks but still struggle with complex, multi-step reasoning,\nparticularly across diverse disciplines. Existing reasoning datasets often lack\ndisciplinary breadth, reasoning depth, and diversity, and lack guiding\nprinciples for question synthesis. We propose DESIGNER: a DESIGN-logic-guidEd\nReasoning data synthesis pipeline that leverages naturally available, extensive\nraw documents (e.g., book corpus and web corpus) to generate multidisciplinary\nchallenging questions. We introduce the concept of \"design logic\" and instruct\nLLMs to mimic human educators' question-creation process, enabling automated\nsynthesis of large-scale, high-difficulty questions. We use LLMs to\nreverse-engineer and abstract over 120,000 design logics from existing\nquestions across various disciplines. By matching these design logics with\nsource documents, we are able to create reasoning questions that far surpass\nthe difficulty and diversity of existing datasets. Using this pipeline, we\nsynthesized two large-scale reasoning datasets that span 75 disciplines:\nDLR-Book (3.04 million questions from the book corpus) and DLR-Web (1.66\nmillion questions from the web corpus). Data analysis indicates that the\nquestions synthesized by our method exhibit greater difficulty and diversity\ncompared to those in the baseline datasets. We validate our synthesized data\nthrough supervised fine-tuning (SFT) on the Qwen3 and Llama3 model families.\nOur data substantially enhances their multidisciplinary reasoning capabilities,\noutperforming existing datasets. Notably, after SFT on our datasets, the base\nversions of these models even surpass their official instruction-tuned\ncounterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage tasks but still struggle with complex, multi-step reasoning,\nparticularly across diverse disciplines. Existing reasoning datasets often lack\ndisciplinary breadth, reasoning depth, and diversity, and lack guiding\nprinciples for question synthesis. We propose DESIGNER: a DESIGN-logic-guidEd\nReasoning data synthesis pipeline that leverages naturally available, extensive\nraw documents (e.g., book corpus and web corpus) to generate multidisciplinary\nchallenging questions. We introduce the concept of \"design logic\" and instruct\nLLMs to mimic human educators' question-creation process, enabling automated\nsynthesis of large-scale, high-difficulty questions. We use LLMs to\nreverse-engineer and abstract over 120,000 design logics from existing\nquestions across various disciplines. By matching these design logics with\nsource documents, we are able to create reasoning questions that far surpass\nthe difficulty and diversity of existing datasets. Using this pipeline, we\nsynthesized two large-scale reasoning datasets that span 75 disciplines:\nDLR-Book (3.04 million questions from the book corpus) and DLR-Web (1.66\nmillion questions from the web corpus). Data analysis indicates that the\nquestions synthesized by our method exhibit greater difficulty and diversity\ncompared to those in the baseline datasets. We validate our synthesized data\nthrough supervised fine-tuning (SFT) on the Qwen3 and Llama3 model families.\nOur data substantially enhances their multidisciplinary reasoning capabilities,\noutperforming existing datasets. Notably, after SFT on our datasets, the base\nversions of these models even surpass their official instruction-tuned\ncounterparts."
                },
                "authors": [
                    {
                        "name": "Weize Liu"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Yijia Luo"
                    },
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yanan Li"
                    },
                    {
                        "name": "Xiguo Hu"
                    },
                    {
                        "name": "Zhiqi Bai"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12726v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12726v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07309v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07309v2",
                "updated": "2025-10-09T02:27:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    2,
                    27,
                    56,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-08T17:57:35Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    57,
                    35,
                    2,
                    281,
                    0
                ],
                "title": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the\n  Business Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the\n  Business Domain"
                },
                "summary": "In the business domain, where data-driven decision making is crucial,\ntext-to-SQL is fundamental for easy natural language access to structured data.\nWhile recent LLMs have achieved strong performance in code generation, existing\ntext-to-SQL benchmarks remain focused on factual retrieval of past records. We\nintroduce CORGI, a new benchmark specifically designed for real-world business\ncontexts. CORGI is composed of synthetic databases inspired by enterprises such\nas Doordash, Airbnb, and Lululemon. It provides questions across four\nincreasingly complex categories of business queries: descriptive, explanatory,\npredictive, and recommendational. This challenge calls for causal reasoning,\ntemporal forecasting, and strategic recommendation, reflecting multi-level and\nmulti-step agentic intelligence. We find that LLM performance drops on\nhigh-level questions, struggling to make accurate predictions and offer\nactionable plans. Based on execution success rate, the CORGI benchmark is about\n21% more difficult than the BIRD benchmark. This highlights the gap between\npopular LLMs and the need for real-world business intelligence. We release a\npublic dataset and evaluation framework, and a website for public submissions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the business domain, where data-driven decision making is crucial,\ntext-to-SQL is fundamental for easy natural language access to structured data.\nWhile recent LLMs have achieved strong performance in code generation, existing\ntext-to-SQL benchmarks remain focused on factual retrieval of past records. We\nintroduce CORGI, a new benchmark specifically designed for real-world business\ncontexts. CORGI is composed of synthetic databases inspired by enterprises such\nas Doordash, Airbnb, and Lululemon. It provides questions across four\nincreasingly complex categories of business queries: descriptive, explanatory,\npredictive, and recommendational. This challenge calls for causal reasoning,\ntemporal forecasting, and strategic recommendation, reflecting multi-level and\nmulti-step agentic intelligence. We find that LLM performance drops on\nhigh-level questions, struggling to make accurate predictions and offer\nactionable plans. Based on execution success rate, the CORGI benchmark is about\n21% more difficult than the BIRD benchmark. This highlights the gap between\npopular LLMs and the need for real-world business intelligence. We release a\npublic dataset and evaluation framework, and a website for public submissions."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Ran Tao"
                    },
                    {
                        "name": "Derek Hommel"
                    },
                    {
                        "name": "Yusuf Denizay Dönder"
                    },
                    {
                        "name": "Sungyong Chang"
                    },
                    {
                        "name": "David Mimno"
                    },
                    {
                        "name": "Unso Eun Seo Jo"
                    }
                ],
                "author_detail": {
                    "name": "Unso Eun Seo Jo"
                },
                "author": "Unso Eun Seo Jo",
                "arxiv_comment": "20 pages, 6 figures, under review for ACL ARR; typos corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07309v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07309v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07307v1",
                "updated": "2025-10-08T17:57:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    57,
                    19,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:57:19Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    57,
                    19,
                    2,
                    281,
                    0
                ],
                "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline"
                },
                "summary": "While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality."
                },
                "authors": [
                    {
                        "name": "Rushi Qiang"
                    },
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Sherry Yang"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05468v2",
                "updated": "2025-10-08T17:55:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    55,
                    2,
                    2,
                    281,
                    0
                ],
                "published": "2025-01-05T17:53:00Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    17,
                    53,
                    0,
                    6,
                    5,
                    0
                ],
                "title": "LatteReview: A Multi-Agent Framework for Systematic Review Automation\n  Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LatteReview: A Multi-Agent Framework for Systematic Review Automation\n  Using Large Language Models"
                },
                "summary": "Systematic literature reviews and meta-analyses are essential for\nsynthesizing research insights, but they remain time-intensive and\nlabor-intensive due to the iterative processes of screening, evaluation, and\ndata extraction. This paper introduces and evaluates LatteReview, a\nPython-based framework that leverages large language models (LLMs) and\nmulti-agent systems to automate key elements of the systematic review process.\nDesigned to streamline workflows while maintaining rigor, LatteReview utilizes\nmodular agents for tasks such as title and abstract screening, relevance\nscoring, and structured data extraction. These agents operate within\norchestrated workflows, supporting sequential and parallel review rounds,\ndynamic decision-making, and iterative refinement based on user feedback.\nLatteReview's architecture integrates LLM providers, enabling compatibility\nwith both cloud-based and locally hosted models. The framework supports\nfeatures such as Retrieval-Augmented Generation (RAG) for incorporating\nexternal context, multimodal reviews, Pydantic-based validation for structured\ninputs and outputs, and asynchronous programming for handling large-scale\ndatasets. The framework is available on the GitHub repository, with detailed\ndocumentation and an installable package.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic literature reviews and meta-analyses are essential for\nsynthesizing research insights, but they remain time-intensive and\nlabor-intensive due to the iterative processes of screening, evaluation, and\ndata extraction. This paper introduces and evaluates LatteReview, a\nPython-based framework that leverages large language models (LLMs) and\nmulti-agent systems to automate key elements of the systematic review process.\nDesigned to streamline workflows while maintaining rigor, LatteReview utilizes\nmodular agents for tasks such as title and abstract screening, relevance\nscoring, and structured data extraction. These agents operate within\norchestrated workflows, supporting sequential and parallel review rounds,\ndynamic decision-making, and iterative refinement based on user feedback.\nLatteReview's architecture integrates LLM providers, enabling compatibility\nwith both cloud-based and locally hosted models. The framework supports\nfeatures such as Retrieval-Augmented Generation (RAG) for incorporating\nexternal context, multimodal reviews, Pydantic-based validation for structured\ninputs and outputs, and asynchronous programming for handling large-scale\ndatasets. The framework is available on the GitHub repository, with detailed\ndocumentation and an installable package."
                },
                "authors": [
                    {
                        "name": "Pouria Rouzrokh"
                    },
                    {
                        "name": "Bardia Khosravi"
                    },
                    {
                        "name": "Parsa Rouzrokh"
                    },
                    {
                        "name": "Moein Shariatnia"
                    }
                ],
                "author_detail": {
                    "name": "Moein Shariatnia"
                },
                "author": "Moein Shariatnia",
                "arxiv_comment": "31 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07300v1",
                "updated": "2025-10-08T17:55:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    55,
                    2,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:55:02Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    55,
                    2,
                    2,
                    281,
                    0
                ],
                "title": "Think Natively: Unlocking Multilingual Reasoning with\n  Consistency-Enhanced Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Natively: Unlocking Multilingual Reasoning with\n  Consistency-Enhanced Reinforcement Learning"
                },
                "summary": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex\nreasoning tasks by adopting the \"think-then-answer\" paradigm, which enhances\nboth accuracy and interpretability. However, current LRMs exhibit two critical\nlimitations when processing non-English languages: (1) They often struggle to\nmaintain input-output language consistency; (2) They generally perform poorly\nwith wrong reasoning paths and lower answer accuracy compared to English. These\nlimitations significantly degrade the user experience for non-English speakers\nand hinder the global deployment of LRMs. To address these limitations, we\npropose M-Thinker, which is trained by the GRPO algorithm that involves a\nLanguage Consistency (LC) reward and a novel Cross-lingual Thinking Alignment\n(CTA) reward. Specifically, the LC reward defines a strict constraint on the\nlanguage consistency between the input, thought, and answer. Besides, the CTA\nreward compares the model's non-English reasoning paths with its English\nreasoning path to transfer its own reasoning capability from English to\nnon-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B\nmodels not only achieve nearly 100% language consistency and superior\nperformance on two multilingual benchmarks (MMATH and PolyMath), but also\nexhibit excellent generalization on out-of-domain languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex\nreasoning tasks by adopting the \"think-then-answer\" paradigm, which enhances\nboth accuracy and interpretability. However, current LRMs exhibit two critical\nlimitations when processing non-English languages: (1) They often struggle to\nmaintain input-output language consistency; (2) They generally perform poorly\nwith wrong reasoning paths and lower answer accuracy compared to English. These\nlimitations significantly degrade the user experience for non-English speakers\nand hinder the global deployment of LRMs. To address these limitations, we\npropose M-Thinker, which is trained by the GRPO algorithm that involves a\nLanguage Consistency (LC) reward and a novel Cross-lingual Thinking Alignment\n(CTA) reward. Specifically, the LC reward defines a strict constraint on the\nlanguage consistency between the input, thought, and answer. Besides, the CTA\nreward compares the model's non-English reasoning paths with its English\nreasoning path to transfer its own reasoning capability from English to\nnon-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B\nmodels not only achieve nearly 100% language consistency and superior\nperformance on two multilingual benchmarks (MMATH and PolyMath), but also\nexhibit excellent generalization on out-of-domain languages."
                },
                "authors": [
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Kaiyu Huang"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "13 pages, 8 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07293v1",
                "updated": "2025-10-08T17:50:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:50:16Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs"
                },
                "summary": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks."
                },
                "authors": [
                    {
                        "name": "Peize He"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Jiajie Huang"
                    },
                    {
                        "name": "Zehui Lei"
                    },
                    {
                        "name": "Zhuangcheng Gu"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Jiabing Yang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Zhifei Liu"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "26 pages, 23 figures, the code is available at\n  \\url{https://github.com/DabDans/AudioMarathon}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00320v2",
                "updated": "2025-10-08T17:49:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    49,
                    53,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-31T00:10:18Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    10,
                    18,
                    5,
                    151,
                    0
                ],
                "title": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in\n  AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in\n  AI Agents"
                },
                "summary": "Recent progress in reasoning with large language models (LLMs), such as\nDeepSeek-R1, demonstrates impressive capabilities in domains like mathematics\nand coding, by exhibiting complex cognitive behaviors such as verification,\ngoal decomposition, and self-reflection. However, it is unclear what behavior\nis effective and what behavior is missing for long-horizon AI agents tasks. In\nthis work, we propose Dyna-Think, a thinking framework that integrates planning\nwith an internal world model with reasoning and acting to enhance AI agent\nperformance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning\n(DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with\nDyna-Think, DIT reconstructs the thinking process of R1 to focus on performing\nworld model simulation relevant to the proposed (and planned) action, and\ntrains the policy using this reconstructed data. To enhance Dyna-Think, DDT\nuses a two-stage training process to first improve the agent's world modeling\nability via objectives such as state prediction or critique generation, and\nthen improve the agent's action via policy training. We evaluate our methods on\nOSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the\nagent's in-domain and out-of-domain performance, achieving similar best-of-n\nperformance compared to R1 while generating 2x less tokens on average. Our\nextensive empirical studies reveal that 1) using critique generation for world\nmodel training is effective to improve policy performance; and 2) AI agents\nwith better performance correlate with better world modeling abilities. We\nbelieve our results suggest a promising research direction to integrate world\nmodel simulation into AI agents to enhance their reasoning, planning, and\nacting capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in reasoning with large language models (LLMs), such as\nDeepSeek-R1, demonstrates impressive capabilities in domains like mathematics\nand coding, by exhibiting complex cognitive behaviors such as verification,\ngoal decomposition, and self-reflection. However, it is unclear what behavior\nis effective and what behavior is missing for long-horizon AI agents tasks. In\nthis work, we propose Dyna-Think, a thinking framework that integrates planning\nwith an internal world model with reasoning and acting to enhance AI agent\nperformance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning\n(DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with\nDyna-Think, DIT reconstructs the thinking process of R1 to focus on performing\nworld model simulation relevant to the proposed (and planned) action, and\ntrains the policy using this reconstructed data. To enhance Dyna-Think, DDT\nuses a two-stage training process to first improve the agent's world modeling\nability via objectives such as state prediction or critique generation, and\nthen improve the agent's action via policy training. We evaluate our methods on\nOSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the\nagent's in-domain and out-of-domain performance, achieving similar best-of-n\nperformance compared to R1 while generating 2x less tokens on average. Our\nextensive empirical studies reveal that 1) using critique generation for world\nmodel training is effective to improve policy performance; and 2) AI agents\nwith better performance correlate with better world modeling abilities. We\nbelieve our results suggest a promising research direction to integrate world\nmodel simulation into AI agents to enhance their reasoning, planning, and\nacting capabilities."
                },
                "authors": [
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Baolin Peng"
                    },
                    {
                        "name": "Ruize Xu"
                    },
                    {
                        "name": "Michel Galley"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Suman Nath"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Zhou Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Yu"
                },
                "author": "Zhou Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06048v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06048v2",
                "updated": "2025-10-08T17:49:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    49,
                    49,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-07T15:42:33Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    15,
                    42,
                    33,
                    1,
                    280,
                    0
                ],
                "title": "BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection\n  in Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection\n  in Language Model Pretraining"
                },
                "summary": "Effective data selection is essential for pretraining large language models\n(LLMs), enhancing efficiency and improving generalization to downstream tasks.\nHowever, existing approaches often require leveraging external pretrained\nmodels, making it difficult to disentangle the effects of data selection from\nthose of the external pretrained models. In addition, they often overlook the\nlong-term impact of selected data if the model is trained to convergence,\nprimarily due to the prohibitive cost of full-scale LLM pretraining. In this\npaper, we introduce BLISS (\\textbf{B}ileve\\textbf{L} \\textbf{I}nfluence\n\\textbf{S}coring method for data \\textbf{S}election): a lightweight data\nselection method that operates entirely \\emph{from scratch}, without relying on\nany external pretrained oracle models, while explicitly accounting for the\nlong-term impact of selected data. BLISS leverages a small proxy model as a\nsurrogate for the LLM and employs a score model to estimate the long-term\ninfluence of training samples if the proxy model is trained to convergence. We\nformulate data selection as a bilevel optimization problem, where the\nupper-level objective optimizes the score model to assign importance weights to\ntraining samples, ensuring that minimizing the lower-level objective (i.e.,\ntraining the proxy model over the weighted training loss until convergence)\nleads to best validation performance. Once optimized, the trained score model\npredicts influence scores for the dataset, enabling efficient selection of\nhigh-quality samples for LLM pretraining. We validate BLISS by pretraining\n410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4\ndataset. Notably, under the 1B model setting, BLISS achieves $1.7\\times$\nspeedup in reaching the same performance as the state-of-the-art method,\ndemonstrating superior performance across multiple downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective data selection is essential for pretraining large language models\n(LLMs), enhancing efficiency and improving generalization to downstream tasks.\nHowever, existing approaches often require leveraging external pretrained\nmodels, making it difficult to disentangle the effects of data selection from\nthose of the external pretrained models. In addition, they often overlook the\nlong-term impact of selected data if the model is trained to convergence,\nprimarily due to the prohibitive cost of full-scale LLM pretraining. In this\npaper, we introduce BLISS (\\textbf{B}ileve\\textbf{L} \\textbf{I}nfluence\n\\textbf{S}coring method for data \\textbf{S}election): a lightweight data\nselection method that operates entirely \\emph{from scratch}, without relying on\nany external pretrained oracle models, while explicitly accounting for the\nlong-term impact of selected data. BLISS leverages a small proxy model as a\nsurrogate for the LLM and employs a score model to estimate the long-term\ninfluence of training samples if the proxy model is trained to convergence. We\nformulate data selection as a bilevel optimization problem, where the\nupper-level objective optimizes the score model to assign importance weights to\ntraining samples, ensuring that minimizing the lower-level objective (i.e.,\ntraining the proxy model over the weighted training loss until convergence)\nleads to best validation performance. Once optimized, the trained score model\npredicts influence scores for the dataset, enabling efficient selection of\nhigh-quality samples for LLM pretraining. We validate BLISS by pretraining\n410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4\ndataset. Notably, under the 1B model setting, BLISS achieves $1.7\\times$\nspeedup in reaching the same performance as the state-of-the-art method,\ndemonstrating superior performance across multiple downstream tasks."
                },
                "authors": [
                    {
                        "name": "Jie Hao"
                    },
                    {
                        "name": "Rui Yu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Huixia Wang"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Mingrui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Mingrui Liu"
                },
                "author": "Mingrui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06048v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06048v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07290v2",
                "updated": "2025-10-09T02:09:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    2,
                    9,
                    12,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-08T17:46:27Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    46,
                    27,
                    2,
                    281,
                    0
                ],
                "title": "On the Convergence of Moral Self-Correction in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Convergence of Moral Self-Correction in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are able to improve their responses when\ninstructed to do so, a capability known as self-correction. When instructions\nprovide only a general and abstract goal without specific details about\npotential issues in the response, LLMs must rely on their internal knowledge to\nimprove response quality, a process referred to as intrinsic self-correction.\nThe empirical success of intrinsic self-correction is evident in various\napplications, but how and why it is effective remains unknown. Focusing on\nmoral self-correction in LLMs, we reveal a key characteristic of intrinsic\nself-correction: performance convergence through multi-round interactions; and\nprovide a mechanistic analysis of this convergence behavior. Based on our\nexperimental results and analysis, we uncover the underlying mechanism of\nconvergence: consistently injected self-correction instructions activate moral\nconcepts that reduce model uncertainty, leading to converged performance as the\nactivated moral concepts stabilize over successive rounds. This paper\ndemonstrates the strong potential of moral self-correction by showing that it\nexhibits a desirable property of converged performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are able to improve their responses when\ninstructed to do so, a capability known as self-correction. When instructions\nprovide only a general and abstract goal without specific details about\npotential issues in the response, LLMs must rely on their internal knowledge to\nimprove response quality, a process referred to as intrinsic self-correction.\nThe empirical success of intrinsic self-correction is evident in various\napplications, but how and why it is effective remains unknown. Focusing on\nmoral self-correction in LLMs, we reveal a key characteristic of intrinsic\nself-correction: performance convergence through multi-round interactions; and\nprovide a mechanistic analysis of this convergence behavior. Based on our\nexperimental results and analysis, we uncover the underlying mechanism of\nconvergence: consistently injected self-correction instructions activate moral\nconcepts that reduce model uncertainty, leading to converged performance as the\nactivated moral concepts stabilize over successive rounds. This paper\ndemonstrates the strong potential of moral self-correction by showing that it\nexhibits a desirable property of converged performance."
                },
                "authors": [
                    {
                        "name": "Guangliang Liu"
                    },
                    {
                        "name": "Haitao Mao"
                    },
                    {
                        "name": "Bochuan Cao"
                    },
                    {
                        "name": "Zhiyu Xue"
                    },
                    {
                        "name": "Xitong Zhang"
                    },
                    {
                        "name": "Rongrong Wang"
                    },
                    {
                        "name": "Kristen Marie Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Marie Johnson"
                },
                "author": "Kristen Marie Johnson",
                "arxiv_comment": "19pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07284v1",
                "updated": "2025-10-08T17:44:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    44,
                    59,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:44:59Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    44,
                    59,
                    2,
                    281,
                    0
                ],
                "title": "Online Rubrics Elicitation from Pairwise Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Rubrics Elicitation from Pairwise Comparisons"
                },
                "summary": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers\nwhere verifiable rewards are not applicable and human preferences provide\ncoarse signals. Prior work shows that reinforcement learning with rubric-based\nrewards leads to consistent gains in LLM post-training. Most existing\napproaches rely on rubrics that remain static over the course of training. Such\nstatic rubrics, however, are vulnerable to reward-hacking type behaviors and\nfail to capture emergent desiderata that arise during training. We introduce\nOnline Rubrics Elicitation (OnlineRubrics), a method that dynamically curates\nevaluation criteria in an online manner through pairwise comparisons of\nresponses from current and reference policies. This online process enables\ncontinuous identification and mitigation of errors as training proceeds.\nEmpirically, this approach yields consistent improvements of up to 8% over\ntraining exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as\nwell as the validation sets of expert questions and rubrics. We qualitatively\nanalyze the elicited criteria and identify prominent themes such as\ntransparency, practicality, organization, and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers\nwhere verifiable rewards are not applicable and human preferences provide\ncoarse signals. Prior work shows that reinforcement learning with rubric-based\nrewards leads to consistent gains in LLM post-training. Most existing\napproaches rely on rubrics that remain static over the course of training. Such\nstatic rubrics, however, are vulnerable to reward-hacking type behaviors and\nfail to capture emergent desiderata that arise during training. We introduce\nOnline Rubrics Elicitation (OnlineRubrics), a method that dynamically curates\nevaluation criteria in an online manner through pairwise comparisons of\nresponses from current and reference policies. This online process enables\ncontinuous identification and mitigation of errors as training proceeds.\nEmpirically, this approach yields consistent improvements of up to 8% over\ntraining exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as\nwell as the validation sets of expert questions and rubrics. We qualitatively\nanalyze the elicited criteria and identify prominent themes such as\ntransparency, practicality, organization, and reasoning."
                },
                "authors": [
                    {
                        "name": "MohammadHossein Rezaei"
                    },
                    {
                        "name": "Robert Vacareanu"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Clinton Wang"
                    },
                    {
                        "name": "Yunzhong He"
                    },
                    {
                        "name": "Afra Feyza Akyürek"
                    }
                ],
                "author_detail": {
                    "name": "Afra Feyza Akyürek"
                },
                "author": "Afra Feyza Akyürek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19732v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19732v4",
                "updated": "2025-10-08T17:43:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    43,
                    7,
                    2,
                    281,
                    0
                ],
                "published": "2025-02-27T03:53:45Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    3,
                    53,
                    45,
                    3,
                    58,
                    0
                ],
                "title": "Speculative Decoding and Beyond: An In-Depth Survey of Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Decoding and Beyond: An In-Depth Survey of Techniques"
                },
                "summary": "Sequential dependencies present a fundamental bottleneck in deploying\nlarge-scale autoregressive models, particularly for real-time applications.\nWhile traditional optimization approaches like pruning and quantization often\ncompromise model quality, recent advances in generation-refinement frameworks\ndemonstrate that this trade-off can be significantly mitigated.\n  This survey presents a comprehensive taxonomy of generation-refinement\nframeworks, analyzing methods across autoregressive sequence tasks. We\ncategorize methods based on their generation strategies (from simple n-gram\nprediction to sophisticated draft models) and refinement mechanisms (including\nsingle-pass verification and iterative approaches). Through systematic analysis\nof both algorithmic innovations and system-level implementations, we examine\ndeployment strategies across computing environments and explore applications\nspanning text, images, and speech generation. This systematic examination of\nboth theoretical frameworks and practical implementations provides a foundation\nfor future research in efficient autoregressive decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential dependencies present a fundamental bottleneck in deploying\nlarge-scale autoregressive models, particularly for real-time applications.\nWhile traditional optimization approaches like pruning and quantization often\ncompromise model quality, recent advances in generation-refinement frameworks\ndemonstrate that this trade-off can be significantly mitigated.\n  This survey presents a comprehensive taxonomy of generation-refinement\nframeworks, analyzing methods across autoregressive sequence tasks. We\ncategorize methods based on their generation strategies (from simple n-gram\nprediction to sophisticated draft models) and refinement mechanisms (including\nsingle-pass verification and iterative approaches). Through systematic analysis\nof both algorithmic innovations and system-level implementations, we examine\ndeployment strategies across computing environments and explore applications\nspanning text, images, and speech generation. This systematic examination of\nboth theoretical frameworks and practical implementations provides a foundation\nfor future research in efficient autoregressive decoding."
                },
                "authors": [
                    {
                        "name": "Yunhai Hu"
                    },
                    {
                        "name": "Zining Liu"
                    },
                    {
                        "name": "Zhenyuan Dong"
                    },
                    {
                        "name": "Tianfan Peng"
                    },
                    {
                        "name": "Bradley McDanel"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19732v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19732v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20757v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20757v2",
                "updated": "2025-10-08T17:36:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    36,
                    57,
                    2,
                    281,
                    0
                ],
                "published": "2025-03-26T17:46:08Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    46,
                    8,
                    2,
                    85,
                    0
                ],
                "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search"
                },
                "summary": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models."
                },
                "authors": [
                    {
                        "name": "Yunhai Hu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20757v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20757v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16185v2",
                "updated": "2025-10-08T17:29:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    29,
                    7,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-22T07:59:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    59,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding\n  on Indic Subjects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding\n  on Indic Subjects"
                },
                "summary": "Large language models have been widely evaluated on tasks such as\ncomprehension, summarization, code generation, etc. However, their performance\non graduate-level, culturally grounded questions in the Indian context remains\nlargely unexplored. Existing Indian benchmarks emphasise basic fact-orientated\nqueries that offer limited assessment of a deeper disciplinary understanding\ntailored to the Indian setting. In this paper, we present ParamBench,\nconsisting of more than 17K questions in the Hindi language, comprising\nquestionnaires from 21 diverse subjects. These questions are primarily derived\nfrom a nationwide graduate-level entrance examination covering topics such as\nhistory, music, instruments, yoga, literature, philosophy, law, etc.~\nspecifically for the Indian context. Additionally, we assess the ability of\nLLMs to handle diverse question formats - such as list-based matching,\nassertion-reason pairs, and sequence ordering - alongside conventional\nmultiple-choice questions. We evaluated the performance of more than 16 open\nsource LLMs on this benchmark, observing that Gemma3-27B attains the highest\noverall accuracy of 56.4\\%. Furthermore, subject-wise analysis indicates that\neven for the best-performing LLMs, performance remains weak on topics such as\nmusic, classical instruments, and law, underscoring persistent challenges in\nculturally grounded reasoning. The dataset and source code is present at\nhttps://github.com/ayushbits/ParamBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been widely evaluated on tasks such as\ncomprehension, summarization, code generation, etc. However, their performance\non graduate-level, culturally grounded questions in the Indian context remains\nlargely unexplored. Existing Indian benchmarks emphasise basic fact-orientated\nqueries that offer limited assessment of a deeper disciplinary understanding\ntailored to the Indian setting. In this paper, we present ParamBench,\nconsisting of more than 17K questions in the Hindi language, comprising\nquestionnaires from 21 diverse subjects. These questions are primarily derived\nfrom a nationwide graduate-level entrance examination covering topics such as\nhistory, music, instruments, yoga, literature, philosophy, law, etc.~\nspecifically for the Indian context. Additionally, we assess the ability of\nLLMs to handle diverse question formats - such as list-based matching,\nassertion-reason pairs, and sequence ordering - alongside conventional\nmultiple-choice questions. We evaluated the performance of more than 16 open\nsource LLMs on this benchmark, observing that Gemma3-27B attains the highest\noverall accuracy of 56.4\\%. Furthermore, subject-wise analysis indicates that\neven for the best-performing LLMs, performance remains weak on topics such as\nmusic, classical instruments, and law, underscoring persistent challenges in\nculturally grounded reasoning. The dataset and source code is present at\nhttps://github.com/ayushbits/ParamBench."
                },
                "authors": [
                    {
                        "name": "Ayush Maheshwari"
                    },
                    {
                        "name": "Kaushal Sharma"
                    },
                    {
                        "name": "Vivek Patel"
                    },
                    {
                        "name": "Aditya Maheshwari"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Maheshwari"
                },
                "author": "Aditya Maheshwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.10354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.10354v2",
                "updated": "2025-10-08T17:26:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    26,
                    33,
                    2,
                    281,
                    0
                ],
                "published": "2023-06-17T13:55:54Z",
                "published_parsed": [
                    2023,
                    6,
                    17,
                    13,
                    55,
                    54,
                    5,
                    168,
                    0
                ],
                "title": "LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event\n  Boundary Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event\n  Boundary Captioning"
                },
                "summary": "Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)\ncompetition is detailed in this paper. Unlike conventional video captioning\ntasks, GEBC demands that the captioning model possess an understanding of\nimmediate changes in status around the designated video boundary, making it a\ndifficult task. This paper proposes an effective model LLMVA-GEBC (Large\nLanguage Model with Video Adapter for Generic Event Boundary Captioning): (1)\nWe utilize a pretrained LLM for generating human-like captions with high\nquality. (2) To adapt the model to the GEBC task, we take the video Q-former as\nan adapter and train it with the frozen visual feature extractors and LLM. Our\nproposed method achieved a 76.14 score on the test set and won the first place\nin the challenge. Our code is available at\nhttps://github.com/zjr2000/LLMVA-GEBC .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)\ncompetition is detailed in this paper. Unlike conventional video captioning\ntasks, GEBC demands that the captioning model possess an understanding of\nimmediate changes in status around the designated video boundary, making it a\ndifficult task. This paper proposes an effective model LLMVA-GEBC (Large\nLanguage Model with Video Adapter for Generic Event Boundary Captioning): (1)\nWe utilize a pretrained LLM for generating human-like captions with high\nquality. (2) To adapt the model to the GEBC task, we take the video Q-former as\nan adapter and train it with the frozen visual feature extractors and LLM. Our\nproposed method achieved a 76.14 score on the test set and won the first place\nin the challenge. Our code is available at\nhttps://github.com/zjr2000/LLMVA-GEBC ."
                },
                "authors": [
                    {
                        "name": "Yolo Yunlong Tang"
                    },
                    {
                        "name": "Jinrui Zhang"
                    },
                    {
                        "name": "Xiangchen Wang"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Feng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zheng"
                },
                "author": "Feng Zheng",
                "arxiv_comment": "Winner solution to Generic Event Boundary Captioning task in LOVEU\n  Challenge (CVPR 2023 workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.10354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.10354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12353v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12353v3",
                "updated": "2025-10-08T17:22:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    22,
                    42,
                    2,
                    281,
                    0
                ],
                "published": "2024-04-18T17:32:46Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    17,
                    32,
                    46,
                    3,
                    109,
                    0
                ],
                "title": "V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt\n  Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt\n  Instruction Tuning"
                },
                "summary": "Video summarization aims to create short, accurate, and cohesive summaries of\nlonger videos. Despite the existence of various video summarization datasets, a\nnotable limitation is their limited amount of source videos, which hampers the\neffective training of advanced large vision-language models (VLMs).\nAdditionally, most existing datasets are created for video-to-video\nsummarization, overlooking the contemporary need for multimodal video content\nsummarization. Recent efforts have been made to expand from unimodal to\nmultimodal video summarization, categorizing the task into three sub-tasks\nbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), and\na combination of video and text summarization (V2VT). However, the textual\nsummaries in previous multimodal datasets are inadequate. To address these\nissues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset\nfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from\n40 to 940 seconds and an average summarization ratio of 16.39%. Each video\nsummary in Instruct-V2Xum is paired with a textual summary that references\nspecific frame indexes, facilitating the generation of aligned video and\ntextual summaries. In addition, we propose a new video summarization framework\nnamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the\nfirst framework that unifies different video summarization tasks into one large\nlanguage model's (LLM) text decoder and achieves task-controllable video\nsummarization with temporal prompts and task instructions. Experiments show\nthat V2Xum-LLaMA outperforms strong baseline models on multiple video\nsummarization tasks. Furthermore, we propose an enhanced evaluation metric for\nV2V and V2VT summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video summarization aims to create short, accurate, and cohesive summaries of\nlonger videos. Despite the existence of various video summarization datasets, a\nnotable limitation is their limited amount of source videos, which hampers the\neffective training of advanced large vision-language models (VLMs).\nAdditionally, most existing datasets are created for video-to-video\nsummarization, overlooking the contemporary need for multimodal video content\nsummarization. Recent efforts have been made to expand from unimodal to\nmultimodal video summarization, categorizing the task into three sub-tasks\nbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), and\na combination of video and text summarization (V2VT). However, the textual\nsummaries in previous multimodal datasets are inadequate. To address these\nissues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset\nfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from\n40 to 940 seconds and an average summarization ratio of 16.39%. Each video\nsummary in Instruct-V2Xum is paired with a textual summary that references\nspecific frame indexes, facilitating the generation of aligned video and\ntextual summaries. In addition, we propose a new video summarization framework\nnamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the\nfirst framework that unifies different video summarization tasks into one large\nlanguage model's (LLM) text decoder and achieves task-controllable video\nsummarization with temporal prompts and task instructions. Experiments show\nthat V2Xum-LLaMA outperforms strong baseline models on multiple video\nsummarization tasks. Furthermore, we propose an enhanced evaluation metric for\nV2V and V2VT summarization tasks."
                },
                "authors": [
                    {
                        "name": "Hang Hua"
                    },
                    {
                        "name": "Yolo Yunlong Tang"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Jiebo Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jiebo Luo"
                },
                "author": "Jiebo Luo",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12353v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12353v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16276v3",
                "updated": "2025-10-08T17:18:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    18,
                    11,
                    2,
                    281,
                    0
                ],
                "published": "2024-03-24T19:50:49Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    19,
                    50,
                    49,
                    6,
                    84,
                    0
                ],
                "title": "Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal\n  Understanding"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language and multimodal domains. By fine-tuning multimodal LLMs with\ntemporal annotations from well-annotated datasets, e.g., dense video captioning\ndatasets, their temporal understanding capacity in video-language tasks can be\nobtained. However, there is a notable lack of untrimmed audio-visual video\ndatasets with precise temporal annotations for events. This deficiency hinders\nLLMs from learning the alignment between time, audio-visual events, and text\ntokens, thus impairing their ability to temporally localize audio-visual events\nin videos. To address this gap, we introduce PU-VALOR, a comprehensive\naudio-visual dataset comprising over 114,000 pseudo-untrimmed videos with\ndetailed temporal annotations. PU-VALOR is derived from the large-scale but\ncoarse-annotated audio-visual dataset VALOR, through a subtle method involving\nevent-based video clustering, random temporal scaling, and permutation. By\nfine-tuning a multimodal LLM on PU-VALOR, we developed AVicuna, a model capable\nof aligning audio-visual events with temporal intervals and corresponding text\ntokens. AVicuna excels in temporal localization and time-aware dialogue\ncapabilities. Our experiments demonstrate that AVicuna effectively handles\ntemporal understanding in audio-visual videos and achieves state-of-the-art\nperformance on open-ended video QA, audio-visual QA, and audio-visual event\ndense localization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language and multimodal domains. By fine-tuning multimodal LLMs with\ntemporal annotations from well-annotated datasets, e.g., dense video captioning\ndatasets, their temporal understanding capacity in video-language tasks can be\nobtained. However, there is a notable lack of untrimmed audio-visual video\ndatasets with precise temporal annotations for events. This deficiency hinders\nLLMs from learning the alignment between time, audio-visual events, and text\ntokens, thus impairing their ability to temporally localize audio-visual events\nin videos. To address this gap, we introduce PU-VALOR, a comprehensive\naudio-visual dataset comprising over 114,000 pseudo-untrimmed videos with\ndetailed temporal annotations. PU-VALOR is derived from the large-scale but\ncoarse-annotated audio-visual dataset VALOR, through a subtle method involving\nevent-based video clustering, random temporal scaling, and permutation. By\nfine-tuning a multimodal LLM on PU-VALOR, we developed AVicuna, a model capable\nof aligning audio-visual events with temporal intervals and corresponding text\ntokens. AVicuna excels in temporal localization and time-aware dialogue\ncapabilities. Our experiments demonstrate that AVicuna effectively handles\ntemporal understanding in audio-visual videos and achieves state-of-the-art\nperformance on open-ended video QA, audio-visual QA, and audio-visual event\ndense localization tasks."
                },
                "authors": [
                    {
                        "name": "Yolo Yunlong Tang"
                    },
                    {
                        "name": "Daiki Shimada"
                    },
                    {
                        "name": "Jing Bi"
                    },
                    {
                        "name": "Mingqian Feng"
                    },
                    {
                        "name": "Hang Hua"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07249v1",
                "updated": "2025-10-08T17:16:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    16,
                    9,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:16:09Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    16,
                    9,
                    2,
                    281,
                    0
                ],
                "title": "TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video\n  Generation"
                },
                "summary": "In this work, we present TalkCuts, a large-scale dataset designed to\nfacilitate the study of multi-shot human speech video generation. Unlike\nexisting datasets that focus on single-shot, static viewpoints, TalkCuts offers\n164k clips totaling over 500 hours of high-quality human speech videos with\ndiverse camera shots, including close-up, half-body, and full-body views. The\ndataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X\nmotion annotations, covering over 10k identities, enabling multimodal learning\nand evaluation. As a first attempt to showcase the value of the dataset, we\npresent Orator, an LLM-guided multi-modal generation framework as a simple\nbaseline, where the language model functions as a multi-faceted director,\norchestrating detailed specifications for camera transitions, speaker\ngesticulations, and vocal modulation. This architecture enables the synthesis\nof coherent long-form videos through our integrated multi-modal video\ngeneration module. Extensive experiments in both pose-guided and audio-driven\nsettings show that training on TalkCuts significantly enhances the\ncinematographic coherence and visual appeal of generated multi-shot speech\nvideos. We believe TalkCuts provides a strong foundation for future work in\ncontrollable, multi-shot speech video generation and broader multimodal\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present TalkCuts, a large-scale dataset designed to\nfacilitate the study of multi-shot human speech video generation. Unlike\nexisting datasets that focus on single-shot, static viewpoints, TalkCuts offers\n164k clips totaling over 500 hours of high-quality human speech videos with\ndiverse camera shots, including close-up, half-body, and full-body views. The\ndataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X\nmotion annotations, covering over 10k identities, enabling multimodal learning\nand evaluation. As a first attempt to showcase the value of the dataset, we\npresent Orator, an LLM-guided multi-modal generation framework as a simple\nbaseline, where the language model functions as a multi-faceted director,\norchestrating detailed specifications for camera transitions, speaker\ngesticulations, and vocal modulation. This architecture enables the synthesis\nof coherent long-form videos through our integrated multi-modal video\ngeneration module. Extensive experiments in both pose-guided and audio-driven\nsettings show that training on TalkCuts significantly enhances the\ncinematographic coherence and visual appeal of generated multi-shot speech\nvideos. We believe TalkCuts provides a strong foundation for future work in\ncontrollable, multi-shot speech video generation and broader multimodal\nlearning."
                },
                "authors": [
                    {
                        "name": "Jiaben Chen"
                    },
                    {
                        "name": "Zixin Wang"
                    },
                    {
                        "name": "Ailing Zeng"
                    },
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Xueyang Yu"
                    },
                    {
                        "name": "Siyuan Cen"
                    },
                    {
                        "name": "Julian Tanke"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Koichi Saito"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "Project page: https://talkcuts.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09225v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09225v4",
                "updated": "2025-10-08T17:12:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    12,
                    48,
                    2,
                    281,
                    0
                ],
                "published": "2024-02-14T15:09:01Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    15,
                    9,
                    1,
                    2,
                    45,
                    0
                ],
                "title": "Is My Data in Your AI? Membership Inference Test (MINT) applied to Face\n  Biometrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is My Data in Your AI? Membership Inference Test (MINT) applied to Face\n  Biometrics"
                },
                "summary": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90\\% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90\\% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Daniel DeAlcala"
                    },
                    {
                        "name": "Aythami Morales"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Javier Ortega-Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Javier Ortega-Garcia"
                },
                "author": "Javier Ortega-Garcia",
                "arxiv_doi": "10.1109/ACCESS.2025.3608951",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3608951",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.09225v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09225v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages main text + 2 pages references and 1 pages appendix",
                "arxiv_journal_ref": "IEEE Access, vol. 13, pp. 163805-163819, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07243v1",
                "updated": "2025-10-08T17:10:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    10,
                    47,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:10:47Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    10,
                    47,
                    2,
                    281,
                    0
                ],
                "title": "LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM\n  Evaluation"
                },
                "summary": "Evaluating large language model (LLM) outputs in the legal domain presents\nunique challenges due to the complex and nuanced nature of legal analysis.\nCurrent evaluation approaches either depend on reference data, which is costly\nto produce, or use standardized assessment methods, both of which have\nsignificant limitations for legal applications.\n  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its\nreliability and effectiveness in legal contexts depend heavily on evaluation\nprocesses unique to the legal industry and how trustworthy the evaluation\nappears to the human legal expert. This is where existing evaluation methods\ncurrently fail and exhibit considerable variability.\n  This paper aims to close the gap: a) we break down lengthy responses into\n'Legal Data Points' (LDPs), self-contained units of information, and introduce\na novel, reference-free evaluation methodology that reflects how lawyers\nevaluate legal answers; b) we demonstrate that our method outperforms a variety\nof baselines on both our proprietary dataset and an open-source dataset\n(LegalBench); c) we show how our method correlates more closely with human\nexpert evaluations and helps improve inter-annotator agreement; and finally d)\nwe open source our Legal Data Points for a subset of LegalBench used in our\nexperiments, allowing the research community to replicate our results and\nadvance research in this vital area of LLM evaluation on legal\nquestion-answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language model (LLM) outputs in the legal domain presents\nunique challenges due to the complex and nuanced nature of legal analysis.\nCurrent evaluation approaches either depend on reference data, which is costly\nto produce, or use standardized assessment methods, both of which have\nsignificant limitations for legal applications.\n  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its\nreliability and effectiveness in legal contexts depend heavily on evaluation\nprocesses unique to the legal industry and how trustworthy the evaluation\nappears to the human legal expert. This is where existing evaluation methods\ncurrently fail and exhibit considerable variability.\n  This paper aims to close the gap: a) we break down lengthy responses into\n'Legal Data Points' (LDPs), self-contained units of information, and introduce\na novel, reference-free evaluation methodology that reflects how lawyers\nevaluate legal answers; b) we demonstrate that our method outperforms a variety\nof baselines on both our proprietary dataset and an open-source dataset\n(LegalBench); c) we show how our method correlates more closely with human\nexpert evaluations and helps improve inter-annotator agreement; and finally d)\nwe open source our Legal Data Points for a subset of LegalBench used in our\nexperiments, allowing the research community to replicate our results and\nadvance research in this vital area of LLM evaluation on legal\nquestion-answering."
                },
                "authors": [
                    {
                        "name": "Joseph Enguehard"
                    },
                    {
                        "name": "Morgane Van Ermengem"
                    },
                    {
                        "name": "Kate Atkinson"
                    },
                    {
                        "name": "Sujeong Cha"
                    },
                    {
                        "name": "Arijit Ghosh Chowdhury"
                    },
                    {
                        "name": "Prashanth Kallur Ramaswamy"
                    },
                    {
                        "name": "Jeremy Roghair"
                    },
                    {
                        "name": "Hannah R Marlowe"
                    },
                    {
                        "name": "Carina Suzana Negreanu"
                    },
                    {
                        "name": "Kitty Boxall"
                    },
                    {
                        "name": "Diana Mincu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Mincu"
                },
                "author": "Diana Mincu",
                "arxiv_comment": "Published in Natural Legal Language Processing - EMNLP Workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07242v1",
                "updated": "2025-10-08T17:09:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    9,
                    41,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:09:41Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    9,
                    41,
                    2,
                    281,
                    0
                ],
                "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense"
                },
                "summary": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning."
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Jason E Weston"
                    },
                    {
                        "name": "Ping Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ping Yu"
                },
                "author": "Ping Yu",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10974v3",
                "updated": "2025-10-08T17:06:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    6,
                    47,
                    2,
                    281,
                    0
                ],
                "published": "2025-06-12T17:59:32Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    17,
                    59,
                    32,
                    3,
                    163,
                    0
                ],
                "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science"
                },
                "summary": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.\nCode is at https://github.com/innovatingAI/AutoMind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.\nCode is at https://github.com/innovatingAI/AutoMind."
                },
                "authors": [
                    {
                        "name": "Yixin Ou"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Jingsheng Zheng"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Zhuoyun Yu"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Yuren Mao"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Ongoing work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07239v1",
                "updated": "2025-10-08T17:06:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    6,
                    20,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:06:20Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    6,
                    20,
                    2,
                    281,
                    0
                ],
                "title": "Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided\n  LoRA Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided\n  LoRA Experts"
                },
                "summary": "Automated red-teaming has emerged as a scalable approach for auditing Large\nLanguage Models (LLMs) prior to deployment, yet existing approaches lack\nmechanisms to efficiently adapt to model-specific vulnerabilities at inference.\nWe introduce Red-Bandit, a red-teaming framework that adapts online to identify\nand exploit model failure modes under distinct attack styles (e.g.,\nmanipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA\nexperts, each specialized for a particular attack style, using reinforcement\nlearning that rewards the generation of unsafe prompts via a rule-based safety\nmodel. At inference, a multi-armed bandit policy dynamically selects among\nthese attack-style experts based on the target model's response safety,\nbalancing exploration and exploitation. Red-Bandit achieves state-of-the-art\nresults on AdvBench under sufficient exploration (ASR@10), while producing more\nhuman-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy\nserves as a diagnostic tool for uncovering model-specific vulnerabilities by\nindicating which attack styles most effectively elicit unsafe behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated red-teaming has emerged as a scalable approach for auditing Large\nLanguage Models (LLMs) prior to deployment, yet existing approaches lack\nmechanisms to efficiently adapt to model-specific vulnerabilities at inference.\nWe introduce Red-Bandit, a red-teaming framework that adapts online to identify\nand exploit model failure modes under distinct attack styles (e.g.,\nmanipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA\nexperts, each specialized for a particular attack style, using reinforcement\nlearning that rewards the generation of unsafe prompts via a rule-based safety\nmodel. At inference, a multi-armed bandit policy dynamically selects among\nthese attack-style experts based on the target model's response safety,\nbalancing exploration and exploitation. Red-Bandit achieves state-of-the-art\nresults on AdvBench under sufficient exploration (ASR@10), while producing more\nhuman-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy\nserves as a diagnostic tool for uncovering model-specific vulnerabilities by\nindicating which attack styles most effectively elicit unsafe behaviors."
                },
                "authors": [
                    {
                        "name": "Christos Ziakas"
                    },
                    {
                        "name": "Nicholas Loo"
                    },
                    {
                        "name": "Nishita Jain"
                    },
                    {
                        "name": "Alessandra Russo"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Russo"
                },
                "author": "Alessandra Russo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07238v1",
                "updated": "2025-10-08T17:06:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    6,
                    7,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:06:07Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    6,
                    7,
                    2,
                    281,
                    0
                ],
                "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model\n  Factuality Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Benchmarks Age: Temporal Misalignment through Large Language Model\n  Factuality Evaluation"
                },
                "summary": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge."
                },
                "authors": [
                    {
                        "name": "Xunyi Jiang"
                    },
                    {
                        "name": "Dingyi Chang"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Xin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xu"
                },
                "author": "Xin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07233v1",
                "updated": "2025-10-08T17:02:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    2,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:02:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    2,
                    4,
                    2,
                    281,
                    0
                ],
                "title": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document\n  Understanding"
                },
                "summary": "Question answering over visually rich documents (VRDs) requires reasoning not\nonly over isolated content but also over documents' structural organization and\ncross-page dependencies. However, conventional retrieval-augmented generation\n(RAG) methods encode content in isolated chunks during ingestion, losing\nstructural and cross-page dependencies, and retrieve a fixed number of pages at\ninference, regardless of the specific demands of the question or context. This\noften results in incomplete evidence retrieval and degraded answer quality for\nmulti-page reasoning tasks. To address these limitations, we propose LAD-RAG, a\nnovel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs\na symbolic document graph that captures layout structure and cross-page\ndependencies, adding it alongside standard neural embeddings to yield a more\nholistic representation of the document. During inference, an LLM agent\ndynamically interacts with the neural and symbolic indices to adaptively\nretrieve the necessary evidence based on the query. Experiments on\nMMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG\nimproves retrieval, achieving over 90% perfect recall on average without any\ntop-k tuning, and outperforming baseline retrievers by up to 20% in recall at\ncomparable noise levels, yielding higher QA accuracy with minimal latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering over visually rich documents (VRDs) requires reasoning not\nonly over isolated content but also over documents' structural organization and\ncross-page dependencies. However, conventional retrieval-augmented generation\n(RAG) methods encode content in isolated chunks during ingestion, losing\nstructural and cross-page dependencies, and retrieve a fixed number of pages at\ninference, regardless of the specific demands of the question or context. This\noften results in incomplete evidence retrieval and degraded answer quality for\nmulti-page reasoning tasks. To address these limitations, we propose LAD-RAG, a\nnovel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs\na symbolic document graph that captures layout structure and cross-page\ndependencies, adding it alongside standard neural embeddings to yield a more\nholistic representation of the document. During inference, an LLM agent\ndynamically interacts with the neural and symbolic indices to adaptively\nretrieve the necessary evidence based on the query. Experiments on\nMMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG\nimproves retrieval, achieving over 90% perfect recall on average without any\ntop-k tuning, and outperforming baseline retrievers by up to 20% in recall at\ncomparable noise levels, yielding higher QA accuracy with minimal latency."
                },
                "authors": [
                    {
                        "name": "Zhivar Sourati"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Marianne Menglin Liu"
                    },
                    {
                        "name": "Yazhe Hu"
                    },
                    {
                        "name": "Mengqing Guo"
                    },
                    {
                        "name": "Sujeeth Bharadwaj"
                    },
                    {
                        "name": "Kyu Han"
                    },
                    {
                        "name": "Tao Sheng"
                    },
                    {
                        "name": "Sujith Ravi"
                    },
                    {
                        "name": "Morteza Dehghani"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07231v1",
                "updated": "2025-10-08T17:00:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    0,
                    49,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:00:49Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    0,
                    49,
                    2,
                    281,
                    0
                ],
                "title": "Benchmarking LLM Causal Reasoning with Scientifically Validated\n  Relationships",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM Causal Reasoning with Scientifically Validated\n  Relationships"
                },
                "summary": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications."
                },
                "authors": [
                    {
                        "name": "Donggyu Lee"
                    },
                    {
                        "name": "Sungwon Park"
                    },
                    {
                        "name": "Yerin Hwang"
                    },
                    {
                        "name": "Hyunwoo Oh"
                    },
                    {
                        "name": "Hyoshin Kim"
                    },
                    {
                        "name": "Jungwon Kim"
                    },
                    {
                        "name": "Meeyoung Cha"
                    },
                    {
                        "name": "Sangyoon Park"
                    },
                    {
                        "name": "Jihee Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jihee Kim"
                },
                "author": "Jihee Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07230v1",
                "updated": "2025-10-08T17:00:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    0,
                    25,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:00:25Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    0,
                    25,
                    2,
                    281,
                    0
                ],
                "title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM\n  Agent in Online Shopping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM\n  Agent in Online Shopping"
                },
                "summary": "Simulating step-wise human behavior with Large Language Models (LLMs) has\nbecome an emerging research direction, enabling applications in various\npractical domains. While prior methods, including prompting, supervised\nfine-tuning (SFT), and reinforcement learning (RL), have shown promise in\nmodeling step-wise behavior, they primarily learn a population-level policy\nwithout conditioning on a user's persona, yielding generic rather than\npersonalized simulations. In this work, we pose a critical question: how can\nLLM agents better simulate personalized user behavior? We introduce\nCustomer-R1, an RL-based method for personalized, step-wise user behavior\nsimulation in online shopping environments. Our policy is conditioned on an\nexplicit persona, and we optimize next-step rationale and action generation via\naction correctness reward signals. Experiments on the OPeRA dataset emonstrate\nthat Customer-R1 not only significantly outperforms prompting and SFT-based\nbaselines in next-action prediction tasks, but also better matches users'\naction distribution, indicating higher fidelity in personalized behavior\nsimulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating step-wise human behavior with Large Language Models (LLMs) has\nbecome an emerging research direction, enabling applications in various\npractical domains. While prior methods, including prompting, supervised\nfine-tuning (SFT), and reinforcement learning (RL), have shown promise in\nmodeling step-wise behavior, they primarily learn a population-level policy\nwithout conditioning on a user's persona, yielding generic rather than\npersonalized simulations. In this work, we pose a critical question: how can\nLLM agents better simulate personalized user behavior? We introduce\nCustomer-R1, an RL-based method for personalized, step-wise user behavior\nsimulation in online shopping environments. Our policy is conditioned on an\nexplicit persona, and we optimize next-step rationale and action generation via\naction correctness reward signals. Experiments on the OPeRA dataset emonstrate\nthat Customer-R1 not only significantly outperforms prompting and SFT-based\nbaselines in next-action prediction tasks, but also better matches users'\naction distribution, indicating higher fidelity in personalized behavior\nsimulation."
                },
                "authors": [
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Yimeng Zhang"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Dakuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dakuo Wang"
                },
                "author": "Dakuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07227v1",
                "updated": "2025-10-08T16:57:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    57,
                    46,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:57:46Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    57,
                    46,
                    2,
                    281,
                    0
                ],
                "title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where to Begin: Efficient Pretraining via Subnetwork Selection and\n  Distillation"
                },
                "summary": "Small Language models (SLMs) offer an efficient and accessible alternative to\nLarge Language Models (LLMs), delivering strong performance while using far\nfewer resources. We introduce a simple and effective framework for pretraining\nSLMs that brings together three complementary ideas. First, we identify\nstructurally sparse sub-network initializations that consistently outperform\nrandomly initialized models of similar size under the same compute budget.\nSecond, we use evolutionary search to automatically discover high-quality\nsub-network initializations, providing better starting points for pretraining.\nThird, we apply knowledge distillation from larger teacher models to speed up\ntraining and improve generalization. Together, these components make SLM\npretraining substantially more efficient: our best model, discovered using\nevolutionary search and initialized with LLM weights, matches the validation\nperplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining\ntokens. We release all code and models at\nhttps://github.com/whittle-org/whittle/, offering a practical and reproducible\npath toward cost-efficient small language model development at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language models (SLMs) offer an efficient and accessible alternative to\nLarge Language Models (LLMs), delivering strong performance while using far\nfewer resources. We introduce a simple and effective framework for pretraining\nSLMs that brings together three complementary ideas. First, we identify\nstructurally sparse sub-network initializations that consistently outperform\nrandomly initialized models of similar size under the same compute budget.\nSecond, we use evolutionary search to automatically discover high-quality\nsub-network initializations, providing better starting points for pretraining.\nThird, we apply knowledge distillation from larger teacher models to speed up\ntraining and improve generalization. Together, these components make SLM\npretraining substantially more efficient: our best model, discovered using\nevolutionary search and initialized with LLM weights, matches the validation\nperplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining\ntokens. We release all code and models at\nhttps://github.com/whittle-org/whittle/, offering a practical and reproducible\npath toward cost-efficient small language model development at scale."
                },
                "authors": [
                    {
                        "name": "Arjun Krishnakumar"
                    },
                    {
                        "name": "Rhea Sanjay Sukthanker"
                    },
                    {
                        "name": "Hannan Javed Mahadik"
                    },
                    {
                        "name": "Gabriela Kadlecová"
                    },
                    {
                        "name": "Vladyslav Moroshan"
                    },
                    {
                        "name": "Timur Carstensen"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Aaron Klein"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Klein"
                },
                "author": "Aaron Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19807v3",
                "updated": "2025-10-08T16:56:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    56,
                    59,
                    2,
                    281,
                    0
                ],
                "published": "2025-06-24T17:17:17Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    17,
                    17,
                    1,
                    175,
                    0
                ],
                "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality"
                },
                "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL."
                },
                "authors": [
                    {
                        "name": "Baochang Ren"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16600v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16600v5",
                "updated": "2025-10-08T16:56:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    56,
                    12,
                    2,
                    281,
                    0
                ],
                "published": "2025-02-23T15:00:53Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    15,
                    0,
                    53,
                    6,
                    54,
                    0
                ],
                "title": "Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics\n  and Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics\n  and Generalization"
                },
                "summary": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs."
                },
                "authors": [
                    {
                        "name": "Guangliang Liu"
                    },
                    {
                        "name": "Zimo Qi"
                    },
                    {
                        "name": "Xitong Zhang"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Kristen Marie Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Marie Johnson"
                },
                "author": "Kristen Marie Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16600v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16600v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07221v1",
                "updated": "2025-10-08T16:55:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    55,
                    28,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:55:28Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    55,
                    28,
                    2,
                    281,
                    0
                ],
                "title": "How much speech data is necessary for ASR in African languages? An\n  evaluation of data scaling in Kinyarwanda and Kikuyu",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How much speech data is necessary for ASR in African languages? An\n  evaluation of data scaling in Kinyarwanda and Kikuyu"
                },
                "summary": "The development of Automatic Speech Recognition (ASR) systems for\nlow-resource African languages remains challenging due to limited transcribed\nspeech data. While recent advances in large multilingual models like OpenAI's\nWhisper offer promising pathways for low-resource ASR development, critical\nquestions persist regarding practical deployment requirements. This paper\naddresses two fundamental concerns for practitioners: determining the minimum\ndata volumes needed for viable performance and characterizing the primary\nfailure modes that emerge in production systems. We evaluate Whisper's\nperformance through comprehensive experiments on two Bantu languages:\nsystematic data scaling analysis on Kinyarwanda using training sets from 1 to\n1,400 hours, and detailed error characterization on Kikuyu using 270 hours of\ntraining data. Our scaling experiments demonstrate that practical ASR\nperformance (WER < 13\\%) becomes achievable with as little as 50 hours of\ntraining data, with substantial improvements continuing through 200 hours (WER\n< 10\\%). Complementing these volume-focused findings, our error analysis\nreveals that data quality issues, particularly noisy ground truth\ntranscriptions, account for 38.6\\% of high-error cases, indicating that careful\ndata curation is as critical as data volume for robust system performance.\nThese results provide actionable benchmarks and deployment guidance for teams\ndeveloping ASR systems across similar low-resource language contexts. We\nrelease accompanying and models see\nhttps://github.com/SunbirdAI/kinyarwanda-whisper-eval",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Automatic Speech Recognition (ASR) systems for\nlow-resource African languages remains challenging due to limited transcribed\nspeech data. While recent advances in large multilingual models like OpenAI's\nWhisper offer promising pathways for low-resource ASR development, critical\nquestions persist regarding practical deployment requirements. This paper\naddresses two fundamental concerns for practitioners: determining the minimum\ndata volumes needed for viable performance and characterizing the primary\nfailure modes that emerge in production systems. We evaluate Whisper's\nperformance through comprehensive experiments on two Bantu languages:\nsystematic data scaling analysis on Kinyarwanda using training sets from 1 to\n1,400 hours, and detailed error characterization on Kikuyu using 270 hours of\ntraining data. Our scaling experiments demonstrate that practical ASR\nperformance (WER < 13\\%) becomes achievable with as little as 50 hours of\ntraining data, with substantial improvements continuing through 200 hours (WER\n< 10\\%). Complementing these volume-focused findings, our error analysis\nreveals that data quality issues, particularly noisy ground truth\ntranscriptions, account for 38.6\\% of high-error cases, indicating that careful\ndata curation is as critical as data volume for robust system performance.\nThese results provide actionable benchmarks and deployment guidance for teams\ndeveloping ASR systems across similar low-resource language contexts. We\nrelease accompanying and models see\nhttps://github.com/SunbirdAI/kinyarwanda-whisper-eval"
                },
                "authors": [
                    {
                        "name": "Benjamin Akera"
                    },
                    {
                        "name": "Evelyn Nafula"
                    },
                    {
                        "name": "Patrick Walukagga"
                    },
                    {
                        "name": "Gilbert Yiga"
                    },
                    {
                        "name": "John Quinn"
                    },
                    {
                        "name": "Ernest Mwebaze"
                    }
                ],
                "author_detail": {
                    "name": "Ernest Mwebaze"
                },
                "author": "Ernest Mwebaze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19828v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19828v4",
                "updated": "2025-10-08T16:54:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    54,
                    13,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-27T12:26:55Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    26,
                    55,
                    2,
                    239,
                    0
                ],
                "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize\n  Memories via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize\n  Memories via Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking a learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns structured operations, including ADD,\nUPDATE, DELETE, and NOOP; and an Answer Agent that pre-selects and reasons over\nrelevant entries. Both agents are fine-tuned with outcome-driven RL (PPO and\nGRPO), enabling adaptive memory management with minimal supervision. With only\n152 training QA pairs, Memory-R1 outperforms strong baselines and generalizes\nacross diverse question types, three benchmarks (LoCoMo, MSC, LongMemEval), and\nmultiple model scales (3B-14B).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking a learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns structured operations, including ADD,\nUPDATE, DELETE, and NOOP; and an Answer Agent that pre-selects and reasons over\nrelevant entries. Both agents are fine-tuned with outcome-driven RL (PPO and\nGRPO), enabling adaptive memory management with minimal supervision. With only\n152 training QA pairs, Memory-R1 outperforms strong baselines and generalizes\nacross diverse question types, three benchmarks (LoCoMo, MSC, LongMemEval), and\nmultiple model scales (3B-14B)."
                },
                "authors": [
                    {
                        "name": "Sikuan Yan"
                    },
                    {
                        "name": "Xiufeng Yang"
                    },
                    {
                        "name": "Zuchao Huang"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Zifeng Ding"
                    },
                    {
                        "name": "Zonggen Li"
                    },
                    {
                        "name": "Xiaowen Ma"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    },
                    {
                        "name": "Hinrich Schütze"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19828v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19828v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07206v1",
                "updated": "2025-10-08T16:42:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    42,
                    20,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:42:20Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    42,
                    20,
                    2,
                    281,
                    0
                ],
                "title": "EigenScore: OOD Detection using Covariance in Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EigenScore: OOD Detection using Covariance in Diffusion Models"
                },
                "summary": "Out-of-distribution (OOD) detection is critical for the safe deployment of\nmachine learning systems in safety-sensitive domains. Diffusion models have\nrecently emerged as powerful generative models, capable of capturing complex\ndata distributions through iterative denoising. Building on this progress,\nrecent work has explored their potential for OOD detection. We propose\nEigenScore, a new OOD detection method that leverages the eigenvalue spectrum\nof the posterior covariance induced by a diffusion model. We argue that\nposterior covariance provides a consistent signal of distribution shift,\nleading to larger trace and leading eigenvalues on OOD inputs, yielding a clear\nspectral signature. We further provide analysis explicitly linking posterior\ncovariance to distribution mismatch, establishing it as a reliable signal for\nOOD detection. To ensure tractability, we adopt a Jacobian-free subspace\niteration method to estimate the leading eigenvalues using only forward\nevaluations of the denoiser. Empirically, EigenScore achieves SOTA performance,\nwith up to 5% AUROC improvement over the best baseline. Notably, it remains\nrobust in near-OOD settings such as CIFAR-10 vs CIFAR-100, where existing\ndiffusion-based methods often fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is critical for the safe deployment of\nmachine learning systems in safety-sensitive domains. Diffusion models have\nrecently emerged as powerful generative models, capable of capturing complex\ndata distributions through iterative denoising. Building on this progress,\nrecent work has explored their potential for OOD detection. We propose\nEigenScore, a new OOD detection method that leverages the eigenvalue spectrum\nof the posterior covariance induced by a diffusion model. We argue that\nposterior covariance provides a consistent signal of distribution shift,\nleading to larger trace and leading eigenvalues on OOD inputs, yielding a clear\nspectral signature. We further provide analysis explicitly linking posterior\ncovariance to distribution mismatch, establishing it as a reliable signal for\nOOD detection. To ensure tractability, we adopt a Jacobian-free subspace\niteration method to estimate the leading eigenvalues using only forward\nevaluations of the denoiser. Empirically, EigenScore achieves SOTA performance,\nwith up to 5% AUROC improvement over the best baseline. Notably, it remains\nrobust in near-OOD settings such as CIFAR-10 vs CIFAR-100, where existing\ndiffusion-based methods often fail."
                },
                "authors": [
                    {
                        "name": "Shirin Shoushtari"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Xiao Shi"
                    },
                    {
                        "name": "M. Salman Asif"
                    },
                    {
                        "name": "Ulugbek S. Kamilov"
                    }
                ],
                "author_detail": {
                    "name": "Ulugbek S. Kamilov"
                },
                "author": "Ulugbek S. Kamilov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16834v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16834v3",
                "updated": "2025-10-08T16:40:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    40,
                    37,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-22T16:05:02Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    5,
                    2,
                    3,
                    142,
                    0
                ],
                "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning\n  Trajectory Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning\n  Trajectory Synthesis"
                },
                "summary": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher."
                },
                "authors": [
                    {
                        "name": "Shuang Sun"
                    },
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Fei Bai"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16834v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16834v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07203v1",
                "updated": "2025-10-08T16:35:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    35,
                    53,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:35:53Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    35,
                    53,
                    2,
                    281,
                    0
                ],
                "title": "Sunflower: A New Approach To Expanding Coverage of African Languages in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sunflower: A New Approach To Expanding Coverage of African Languages in\n  Large Language Models"
                },
                "summary": "There are more than 2000 living languages in Africa, most of which have been\nbypassed by advances in language technology. Current leading LLMs exhibit\nstrong performance on a number of the most common languages (e.g. Swahili or\nYoruba), but prioritise support for the languages with the most speakers first,\nresulting in piecemeal ability across disparate languages. We contend that a\nregionally focussed approach is more efficient, and present a case study for\nUganda, a country with high linguistic diversity. We describe the development\nof Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the\nart comprehension in the majority of all Ugandan languages. These models are\nopen source and can be used to reduce language barriers in a number of\nimportant practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are more than 2000 living languages in Africa, most of which have been\nbypassed by advances in language technology. Current leading LLMs exhibit\nstrong performance on a number of the most common languages (e.g. Swahili or\nYoruba), but prioritise support for the languages with the most speakers first,\nresulting in piecemeal ability across disparate languages. We contend that a\nregionally focussed approach is more efficient, and present a case study for\nUganda, a country with high linguistic diversity. We describe the development\nof Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the\nart comprehension in the majority of all Ugandan languages. These models are\nopen source and can be used to reduce language barriers in a number of\nimportant practical applications."
                },
                "authors": [
                    {
                        "name": "Benjamin Akera"
                    },
                    {
                        "name": "Evelyn Nafula Ouma"
                    },
                    {
                        "name": "Gilbert Yiga"
                    },
                    {
                        "name": "Patrick Walukagga"
                    },
                    {
                        "name": "Phionah Natukunda"
                    },
                    {
                        "name": "Trevor Saaka"
                    },
                    {
                        "name": "Solomon Nsumba"
                    },
                    {
                        "name": "Lilian Teddy Nabukeera"
                    },
                    {
                        "name": "Joel Muhanguzi"
                    },
                    {
                        "name": "Imran Sekalala"
                    },
                    {
                        "name": "Nimpamya Janat Namara"
                    },
                    {
                        "name": "Engineer Bainomugisha"
                    },
                    {
                        "name": "Ernest Mwebaze"
                    },
                    {
                        "name": "John Quinn"
                    }
                ],
                "author_detail": {
                    "name": "John Quinn"
                },
                "author": "John Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07192v1",
                "updated": "2025-10-08T16:25:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    25,
                    5,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:25:05Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    25,
                    5,
                    2,
                    281,
                    0
                ],
                "title": "Poisoning Attacks on LLMs Require a Near-constant Number of Poison\n  Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poisoning Attacks on LLMs Require a Near-constant Number of Poison\n  Samples"
                },
                "summary": "Poisoning attacks can compromise the safety of large language models (LLMs)\nby injecting malicious documents into their training data. Existing work has\nstudied pretraining poisoning assuming adversaries control a percentage of the\ntraining corpus. However, for large models, even small percentages translate to\nimpractically large amounts of data. This work demonstrates for the first time\nthat poisoning attacks instead require a near-constant number of documents\nregardless of dataset size. We conduct the largest pretraining poisoning\nexperiments to date, pretraining models from 600M to 13B parameters on\nchinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned\ndocuments similarly compromise models across all model and dataset sizes,\ndespite the largest models training on more than 20 times more clean data. We\nalso run smaller-scale experiments to ablate factors that could influence\nattack success, including broader ratios of poisoned to clean data and\nnon-random distributions of poisoned samples. Finally, we demonstrate the same\ndynamics for poisoning during fine-tuning. Altogether, our results suggest that\ninjecting backdoors through data poisoning may be easier for large models than\npreviously believed as the number of poisons required does not scale up with\nmodel size, highlighting the need for more research on defences to mitigate\nthis risk in future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poisoning attacks can compromise the safety of large language models (LLMs)\nby injecting malicious documents into their training data. Existing work has\nstudied pretraining poisoning assuming adversaries control a percentage of the\ntraining corpus. However, for large models, even small percentages translate to\nimpractically large amounts of data. This work demonstrates for the first time\nthat poisoning attacks instead require a near-constant number of documents\nregardless of dataset size. We conduct the largest pretraining poisoning\nexperiments to date, pretraining models from 600M to 13B parameters on\nchinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned\ndocuments similarly compromise models across all model and dataset sizes,\ndespite the largest models training on more than 20 times more clean data. We\nalso run smaller-scale experiments to ablate factors that could influence\nattack success, including broader ratios of poisoned to clean data and\nnon-random distributions of poisoned samples. Finally, we demonstrate the same\ndynamics for poisoning during fine-tuning. Altogether, our results suggest that\ninjecting backdoors through data poisoning may be easier for large models than\npreviously believed as the number of poisons required does not scale up with\nmodel size, highlighting the need for more research on defences to mitigate\nthis risk in future models."
                },
                "authors": [
                    {
                        "name": "Alexandra Souly"
                    },
                    {
                        "name": "Javier Rando"
                    },
                    {
                        "name": "Ed Chapman"
                    },
                    {
                        "name": "Xander Davies"
                    },
                    {
                        "name": "Burak Hasircioglu"
                    },
                    {
                        "name": "Ezzeldin Shereen"
                    },
                    {
                        "name": "Carlos Mougan"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Erik Jones"
                    },
                    {
                        "name": "Chris Hicks"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Robert Kirk"
                    }
                ],
                "author_detail": {
                    "name": "Robert Kirk"
                },
                "author": "Robert Kirk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.17432v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.17432v7",
                "updated": "2025-10-08T16:24:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    24,
                    55,
                    2,
                    281,
                    0
                ],
                "published": "2023-12-29T01:56:17Z",
                "published_parsed": [
                    2023,
                    12,
                    29,
                    1,
                    56,
                    17,
                    4,
                    363,
                    0
                ],
                "title": "Video Understanding with Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Understanding with Large Language Models: A Survey"
                },
                "summary": "With the burgeoning growth of online video platforms and the escalating\nvolume of video content, the demand for proficient video understanding tools\nhas intensified markedly. Given the remarkable capabilities of large language\nmodels (LLMs) in language and multimodal tasks, this survey provides a detailed\noverview of recent advancements in video understanding that harness the power\nof LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\nadvanced, particularly their ability for open-ended multi-granularity (general,\ntemporal, and spatiotemporal) reasoning combined with commonsense knowledge,\nsuggesting a promising path for future video understanding. We examine the\nunique characteristics and capabilities of Vid-LLMs, categorizing the\napproaches into three main types: Video Analyzer x LLM, Video Embedder x LLM,\nand (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based\non the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as\nText Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this\nsurvey presents a comprehensive study of the tasks, datasets, benchmarks, and\nevaluation methodologies for Vid-LLMs. Additionally, it explores the expansive\napplications of Vid-LLMs across various domains, highlighting their remarkable\nscalability and versatility in real-world video understanding challenges.\nFinally, it summarizes the limitations of existing Vid-LLMs and outlines\ndirections for future research. For more information, readers are recommended\nto visit the repository at\nhttps://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning growth of online video platforms and the escalating\nvolume of video content, the demand for proficient video understanding tools\nhas intensified markedly. Given the remarkable capabilities of large language\nmodels (LLMs) in language and multimodal tasks, this survey provides a detailed\noverview of recent advancements in video understanding that harness the power\nof LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\nadvanced, particularly their ability for open-ended multi-granularity (general,\ntemporal, and spatiotemporal) reasoning combined with commonsense knowledge,\nsuggesting a promising path for future video understanding. We examine the\nunique characteristics and capabilities of Vid-LLMs, categorizing the\napproaches into three main types: Video Analyzer x LLM, Video Embedder x LLM,\nand (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based\non the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as\nText Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this\nsurvey presents a comprehensive study of the tasks, datasets, benchmarks, and\nevaluation methodologies for Vid-LLMs. Additionally, it explores the expansive\napplications of Vid-LLMs across various domains, highlighting their remarkable\nscalability and versatility in real-world video understanding challenges.\nFinally, it summarizes the limitations of existing Vid-LLMs and outlines\ndirections for future research. For more information, readers are recommended\nto visit the repository at\nhttps://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding."
                },
                "authors": [
                    {
                        "name": "Yolo Yunlong Tang"
                    },
                    {
                        "name": "Jing Bi"
                    },
                    {
                        "name": "Siting Xu"
                    },
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Daoan Zhang"
                    },
                    {
                        "name": "Jie An"
                    },
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Rongyi Zhu"
                    },
                    {
                        "name": "Ali Vosoughi"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Pinxin Liu"
                    },
                    {
                        "name": "Mingqian Feng"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "Accepted to IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.17432v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.17432v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07189v1",
                "updated": "2025-10-08T16:24:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    24,
                    9,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:24:09Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    24,
                    9,
                    2,
                    281,
                    0
                ],
                "title": "Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe"
                },
                "summary": "Although Large Language Models (LLMs) show promising solutions to automated\ncode generation, they often produce insecure code that threatens software\nsecurity. Current approaches (e.g., SafeCoder) to improve secure code\ngeneration suffer from limited and imbalanced datasets, reducing their\neffectiveness and generalizability. In this work, we present Secure-Instruct, a\nnovel framework that automatically synthesizes high-quality vulnerable and\nsecure code examples, generates fine-tuning instructions, and instruction-tunes\nLLMs to align task description and secure code generation abilities. We\nevaluate Secure-Instruct on four representative LLMs using two benchmarks: our\nown CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44\nCWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning\ndataset, while CWEval covers 31 CWEs with 119 manually verified\nsecurity-critical tasks. We find that Secure-Instruct improves not only the\nsecurity but also the functional correctness of the generated code. On\nCWEBench, Secure-Instruct substantially improves secure code generation, giving\na 14.3% average increase in secure ratio over the pretrained models and\noutperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%\nincrease for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained\nmodels, and surpasses SafeCoder by 15.8% and 6.8% respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) show promising solutions to automated\ncode generation, they often produce insecure code that threatens software\nsecurity. Current approaches (e.g., SafeCoder) to improve secure code\ngeneration suffer from limited and imbalanced datasets, reducing their\neffectiveness and generalizability. In this work, we present Secure-Instruct, a\nnovel framework that automatically synthesizes high-quality vulnerable and\nsecure code examples, generates fine-tuning instructions, and instruction-tunes\nLLMs to align task description and secure code generation abilities. We\nevaluate Secure-Instruct on four representative LLMs using two benchmarks: our\nown CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44\nCWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning\ndataset, while CWEval covers 31 CWEs with 119 manually verified\nsecurity-critical tasks. We find that Secure-Instruct improves not only the\nsecurity but also the functional correctness of the generated code. On\nCWEBench, Secure-Instruct substantially improves secure code generation, giving\na 14.3% average increase in secure ratio over the pretrained models and\noutperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%\nincrease for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained\nmodels, and surpasses SafeCoder by 15.8% and 6.8% respectively."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Fazle Rabbi"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Jinqiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiu Yang"
                },
                "author": "Jinqiu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03122v2",
                "updated": "2025-10-08T16:23:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    23,
                    32,
                    2,
                    281,
                    0
                ],
                "published": "2025-09-03T08:22:04Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    22,
                    4,
                    2,
                    246,
                    0
                ],
                "title": "From Injection to Defense: Constructing Edit-Based Fingerprints for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Injection to Defense: Constructing Edit-Based Fingerprints for\n  Large Language Models"
                },
                "summary": "Fingerprinting is critical for maintaining traceability and protecting the\nintellectual property (IP) of developers, as LLMs deployed in web applications\nare susceptible to unauthorized redistribution and misuse via fine-tuning or\nblack-box deployment. However, current backdoor-based fingerprinting methods\nface a fundamental trade-off: fingerprints embedded as garbled text are easily\ndetected and filtered, whereas those crafted as coherent natural language are\nprone to being triggered unintentionally. To overcome these limitations, we\npropose RFEdit, a knowledge-editing framework that embeds a rule-based\nmultilingual natural language fingerprint (MNLF) by modifying a sparse subset\nof model weights. This approach enables efficient and robust fingerprint\ninjection with minimal impact on unrelated knowledge in LLMs. Our RFEdit\nframework is further safeguarded by Fingerprint Subspace-aware Fine-Tuning\n(FSFT), which mitigates fingerprint degradation during legitimate fine-tuning\nby restricting parameter updates to the fingerprint subspace. This approach\npreserves fingerprint integrity while enhancing downstream task performance of\nLLMs. These advances establish a comprehensive pipeline from fingerprint\ninjection to defense, achieving high detection effectiveness, robustness\nagainst adversarial manipulations, harmlessness to model utility, and\npersistence under fine-tuning. Extensive experiments demonstrate that RFEdit\nmaintains robustness under quantization and pruning. Additionally, fingerprint\neffectiveness is generally improved by more than 10\\% when combined with FSFT\nfor math and alpaca downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprinting is critical for maintaining traceability and protecting the\nintellectual property (IP) of developers, as LLMs deployed in web applications\nare susceptible to unauthorized redistribution and misuse via fine-tuning or\nblack-box deployment. However, current backdoor-based fingerprinting methods\nface a fundamental trade-off: fingerprints embedded as garbled text are easily\ndetected and filtered, whereas those crafted as coherent natural language are\nprone to being triggered unintentionally. To overcome these limitations, we\npropose RFEdit, a knowledge-editing framework that embeds a rule-based\nmultilingual natural language fingerprint (MNLF) by modifying a sparse subset\nof model weights. This approach enables efficient and robust fingerprint\ninjection with minimal impact on unrelated knowledge in LLMs. Our RFEdit\nframework is further safeguarded by Fingerprint Subspace-aware Fine-Tuning\n(FSFT), which mitigates fingerprint degradation during legitimate fine-tuning\nby restricting parameter updates to the fingerprint subspace. This approach\npreserves fingerprint integrity while enhancing downstream task performance of\nLLMs. These advances establish a comprehensive pipeline from fingerprint\ninjection to defense, achieving high detection effectiveness, robustness\nagainst adversarial manipulations, harmlessness to model utility, and\npersistence under fine-tuning. Extensive experiments demonstrate that RFEdit\nmaintains robustness under quantization and pruning. Additionally, fingerprint\neffectiveness is generally improved by more than 10\\% when combined with FSFT\nfor math and alpaca downstream tasks."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Xin Yi"
                    },
                    {
                        "name": "Dongsheng Shi"
                    },
                    {
                        "name": "Yongyi Cui"
                    },
                    {
                        "name": "Gerard de Melo"
                    },
                    {
                        "name": "Linlin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Linlin Wang"
                },
                "author": "Linlin Wang",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07178v1",
                "updated": "2025-10-08T16:17:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    17,
                    13,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:17:13Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    17,
                    13,
                    2,
                    281,
                    0
                ],
                "title": "Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish\n  the Possible from the Impossible",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish\n  the Possible from the Impossible"
                },
                "summary": "Are large language models (LLMs) sensitive to the distinction between humanly\npossible languages and humanly impossible languages? This question is taken by\nmany to bear on whether LLMs and humans share the same innate learning biases.\nPrevious work has attempted to answer it in the positive by comparing LLM\nlearning curves on existing language datasets and on \"impossible\" datasets\nderived from them via various perturbation functions. Using the same\nmethodology, we examine this claim on a wider set of languages and impossible\nperturbations. We find that in most cases, GPT-2 learns each language and its\nimpossible counterpart equally easily, in contrast to previous claims. We also\napply a more lenient condition by testing whether GPT-2 provides any kind of\nseparation between the whole set of natural languages and the whole set of\nimpossible languages. By considering cross-linguistic variance in various\nmetrics computed on the perplexity curves, we show that GPT-2 provides no\nsystematic separation between the possible and the impossible. Taken together,\nthese perspectives show that LLMs do not share the human innate biases that\nshape linguistic typology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are large language models (LLMs) sensitive to the distinction between humanly\npossible languages and humanly impossible languages? This question is taken by\nmany to bear on whether LLMs and humans share the same innate learning biases.\nPrevious work has attempted to answer it in the positive by comparing LLM\nlearning curves on existing language datasets and on \"impossible\" datasets\nderived from them via various perturbation functions. Using the same\nmethodology, we examine this claim on a wider set of languages and impossible\nperturbations. We find that in most cases, GPT-2 learns each language and its\nimpossible counterpart equally easily, in contrast to previous claims. We also\napply a more lenient condition by testing whether GPT-2 provides any kind of\nseparation between the whole set of natural languages and the whole set of\nimpossible languages. By considering cross-linguistic variance in various\nmetrics computed on the perplexity curves, we show that GPT-2 provides no\nsystematic separation between the possible and the impossible. Taken together,\nthese perspectives show that LLMs do not share the human innate biases that\nshape linguistic typology."
                },
                "authors": [
                    {
                        "name": "Imry Ziv"
                    },
                    {
                        "name": "Nur Lan"
                    },
                    {
                        "name": "Emmanuel Chemla"
                    },
                    {
                        "name": "Roni Katzir"
                    }
                ],
                "author_detail": {
                    "name": "Roni Katzir"
                },
                "author": "Roni Katzir",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07177v1",
                "updated": "2025-10-08T16:16:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    16,
                    46,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:16:46Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    16,
                    46,
                    2,
                    281,
                    0
                ],
                "title": "CARPAS: Towards Content-Aware Refinement of Provided Aspects for\n  Summarization in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARPAS: Towards Content-Aware Refinement of Provided Aspects for\n  Summarization in Large Language Models"
                },
                "summary": "Aspect-based summarization has attracted significant attention for its\nability to generate more fine-grained and user-aligned summaries. While most\nexisting approaches assume a set of predefined aspects as input, real-world\nscenarios often present challenges where these given aspects may be incomplete,\nirrelevant, or entirely missing from the document. Users frequently expect\nsystems to adaptively refine or filter the provided aspects based on the actual\ncontent. In this paper, we initiate this novel task setting, termed\nContent-Aware Refinement of Provided Aspects for Summarization (CARPAS), with\nthe aim of dynamically adjusting the provided aspects based on the document\ncontext before summarizing. We construct three new datasets to facilitate our\npilot experiments, and by using LLMs with four representative prompting\nstrategies in this task, we find that LLMs tend to predict an overly\ncomprehensive set of aspects, which often results in excessively long and\nmisaligned summaries. Building on this observation, we propose a preliminary\nsubtask to predict the number of relevant aspects, and demonstrate that the\npredicted number can serve as effective guidance for the LLMs, reducing the\ninference difficulty, and enabling them to focus on the most pertinent aspects.\nOur extensive experiments show that the proposed approach significantly\nimproves performance across all datasets. Moreover, our deeper analyses uncover\nLLMs' compliance when the requested number of aspects differs from their own\nestimations, establishing a crucial insight for the deployment of LLMs in\nsimilar real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based summarization has attracted significant attention for its\nability to generate more fine-grained and user-aligned summaries. While most\nexisting approaches assume a set of predefined aspects as input, real-world\nscenarios often present challenges where these given aspects may be incomplete,\nirrelevant, or entirely missing from the document. Users frequently expect\nsystems to adaptively refine or filter the provided aspects based on the actual\ncontent. In this paper, we initiate this novel task setting, termed\nContent-Aware Refinement of Provided Aspects for Summarization (CARPAS), with\nthe aim of dynamically adjusting the provided aspects based on the document\ncontext before summarizing. We construct three new datasets to facilitate our\npilot experiments, and by using LLMs with four representative prompting\nstrategies in this task, we find that LLMs tend to predict an overly\ncomprehensive set of aspects, which often results in excessively long and\nmisaligned summaries. Building on this observation, we propose a preliminary\nsubtask to predict the number of relevant aspects, and demonstrate that the\npredicted number can serve as effective guidance for the LLMs, reducing the\ninference difficulty, and enabling them to focus on the most pertinent aspects.\nOur extensive experiments show that the proposed approach significantly\nimproves performance across all datasets. Moreover, our deeper analyses uncover\nLLMs' compliance when the requested number of aspects differs from their own\nestimations, establishing a crucial insight for the deployment of LLMs in\nsimilar real-world applications."
                },
                "authors": [
                    {
                        "name": "Yong-En Tian"
                    },
                    {
                        "name": "Yu-Chien Tang"
                    },
                    {
                        "name": "An-Zi Yen"
                    },
                    {
                        "name": "Wen-Chih Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Chih Peng"
                },
                "author": "Wen-Chih Peng",
                "arxiv_comment": "22 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07176v1",
                "updated": "2025-10-08T16:16:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    16,
                    23,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:16:23Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    16,
                    23,
                    2,
                    281,
                    0
                ],
                "title": "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of\n  Privacy Risks in LLM Agent Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of\n  Privacy Risks in LLM Agent Interactions"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed as agents that\norchestrate tasks and integrate external tools to execute complex workflows. We\ndemonstrate that these interactive behaviors leave distinctive fingerprints in\nencrypted traffic exchanged between users and LLM agents. By analyzing traffic\npatterns associated with agent workflows and tool invocations, adversaries can\ninfer agent activities, distinguish specific agents, and even profile sensitive\nuser attributes. To highlight this risk, we develop AgentPrint, which achieves\nan F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3\naccuracy in user attribute inference for simulated- and real-user settings,\nrespectively. These results uncover an overlooked risk: the very interactivity\nthat empowers LLM agents also exposes user privacy, underscoring the urgent\nneed for technical countermeasures alongside regulatory and policy safeguards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed as agents that\norchestrate tasks and integrate external tools to execute complex workflows. We\ndemonstrate that these interactive behaviors leave distinctive fingerprints in\nencrypted traffic exchanged between users and LLM agents. By analyzing traffic\npatterns associated with agent workflows and tool invocations, adversaries can\ninfer agent activities, distinguish specific agents, and even profile sensitive\nuser attributes. To highlight this risk, we develop AgentPrint, which achieves\nan F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3\naccuracy in user attribute inference for simulated- and real-user settings,\nrespectively. These results uncover an overlooked risk: the very interactivity\nthat empowers LLM agents also exposes user privacy, underscoring the urgent\nneed for technical countermeasures alongside regulatory and policy safeguards."
                },
                "authors": [
                    {
                        "name": "Yixiang Zhang"
                    },
                    {
                        "name": "Xinhao Deng"
                    },
                    {
                        "name": "Zhongyi Gu"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Jianping Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Wu"
                },
                "author": "Jianping Wu",
                "arxiv_comment": "26 pages with 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07175v1",
                "updated": "2025-10-08T16:16:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    16,
                    20,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:16:20Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    16,
                    20,
                    2,
                    281,
                    0
                ],
                "title": "Quantifying Data Contamination in Psychometric Evaluations of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Data Contamination in Psychometric Evaluations of LLMs"
                },
                "summary": "Recent studies apply psychometric questionnaires to Large Language Models\n(LLMs) to assess high-level psychological constructs such as values,\npersonality, moral foundations, and dark traits. Although prior work has raised\nconcerns about possible data contamination from psychometric inventories, which\nmay threaten the reliability of such evaluations, there has been no systematic\nattempt to quantify the extent of this contamination. To address this gap, we\npropose a framework to systematically measure data contamination in\npsychometric evaluations of LLMs, evaluating three aspects: (1) item\nmemorization, (2) evaluation memorization, and (3) target score matching.\nApplying this framework to 21 models from major families and four widely used\npsychometric inventories, we provide evidence that popular inventories such as\nthe Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)\nexhibit strong contamination, where models not only memorize items but can also\nadjust their responses to achieve specific target scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies apply psychometric questionnaires to Large Language Models\n(LLMs) to assess high-level psychological constructs such as values,\npersonality, moral foundations, and dark traits. Although prior work has raised\nconcerns about possible data contamination from psychometric inventories, which\nmay threaten the reliability of such evaluations, there has been no systematic\nattempt to quantify the extent of this contamination. To address this gap, we\npropose a framework to systematically measure data contamination in\npsychometric evaluations of LLMs, evaluating three aspects: (1) item\nmemorization, (2) evaluation memorization, and (3) target score matching.\nApplying this framework to 21 models from major families and four widely used\npsychometric inventories, we provide evidence that popular inventories such as\nthe Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)\nexhibit strong contamination, where models not only memorize items but can also\nadjust their responses to achieve specific target scores."
                },
                "authors": [
                    {
                        "name": "Jongwook Han"
                    },
                    {
                        "name": "Woojung Song"
                    },
                    {
                        "name": "Jonggeun Lee"
                    },
                    {
                        "name": "Yohan Jo"
                    }
                ],
                "author_detail": {
                    "name": "Yohan Jo"
                },
                "author": "Yohan Jo",
                "arxiv_comment": "12 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07173v1",
                "updated": "2025-10-08T16:15:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    15,
                    6,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:15:06Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    15,
                    6,
                    2,
                    281,
                    0
                ],
                "title": "NurseLLM: The First Specialized Language Model for Nursing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NurseLLM: The First Specialized Language Model for Nursing"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\ntransformed medical systems. However, their potential within specialized\ndomains such as nursing remains largely underexplored. In this work, we\nintroduce NurseLLM, the first nursing-specialized LLM tailored for multiple\nchoice question-answering (MCQ) tasks. We develop a multi-stage data generation\npipeline to build the first large scale nursing MCQ dataset to train LLMs on a\nbroad spectrum of nursing topics. We further introduce multiple nursing\nbenchmarks to enable rigorous evaluation. Our extensive experiments demonstrate\nthat NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of\ncomparable size on different benchmarks, underscoring the importance of a\nspecialized LLM for the nursing domain. Finally, we explore the role of\nreasoning and multi-agent collaboration systems in nursing, highlighting their\npromise for future research and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\ntransformed medical systems. However, their potential within specialized\ndomains such as nursing remains largely underexplored. In this work, we\nintroduce NurseLLM, the first nursing-specialized LLM tailored for multiple\nchoice question-answering (MCQ) tasks. We develop a multi-stage data generation\npipeline to build the first large scale nursing MCQ dataset to train LLMs on a\nbroad spectrum of nursing topics. We further introduce multiple nursing\nbenchmarks to enable rigorous evaluation. Our extensive experiments demonstrate\nthat NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of\ncomparable size on different benchmarks, underscoring the importance of a\nspecialized LLM for the nursing domain. Finally, we explore the role of\nreasoning and multi-agent collaboration systems in nursing, highlighting their\npromise for future research and applications."
                },
                "authors": [
                    {
                        "name": "Md Tawkat Islam Khondaker"
                    },
                    {
                        "name": "Julia Harrington"
                    },
                    {
                        "name": "Shady Shehata"
                    }
                ],
                "author_detail": {
                    "name": "Shady Shehata"
                },
                "author": "Shady Shehata",
                "arxiv_comment": "EMNLP 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07172v1",
                "updated": "2025-10-08T16:12:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    12,
                    11,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:12:11Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    12,
                    11,
                    2,
                    281,
                    0
                ],
                "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM\n  Agents"
                },
                "summary": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Kelvin Kiu-Wai Tam"
                    },
                    {
                        "name": "Newt Hue-Nam K. Nguyen"
                    },
                    {
                        "name": "Baixuan Xu"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Jiayang Cheng"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Ginny Y. Wong"
                    },
                    {
                        "name": "Simon See"
                    }
                ],
                "author_detail": {
                    "name": "Simon See"
                },
                "author": "Simon See",
                "arxiv_comment": "60 pages, 18 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07169v1",
                "updated": "2025-10-08T16:07:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    7,
                    26,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:07:26Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    7,
                    26,
                    2,
                    281,
                    0
                ],
                "title": "More Data or Better Data? A Critical Analysis of Data Selection and\n  Synthesis for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Data or Better Data? A Critical Analysis of Data Selection and\n  Synthesis for Mathematical Reasoning"
                },
                "summary": "The reasoning capabilities of Large Language Models (LLMs) play a critical\nrole in many downstream tasks, yet depend strongly on the quality of training\ndata. Despite various proposed data construction methods, their practical\nutility in real-world pipelines remains underexplored. In this work, we conduct\na comprehensive analysis of open-source datasets and data synthesis techniques\nfor mathematical reasoning, evaluating them under a unified pipeline designed\nto mirror training and deployment scenarios. We further distill effective data\nselection strategies and identify practical methods suitable for industrial\napplications. Our findings highlight that structuring data in more\ninterpretable formats, or distilling from stronger models often outweighs\nsimply scaling up data volume. This study provides actionable guidance for\nintegrating training data to enhance LLM capabilities, supporting both\ncost-effective data curation and scalable model enhancement. We hope this work\nwill inspire further research on how to balance \"more data\" versus \"better\ndata\" for real-world reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning capabilities of Large Language Models (LLMs) play a critical\nrole in many downstream tasks, yet depend strongly on the quality of training\ndata. Despite various proposed data construction methods, their practical\nutility in real-world pipelines remains underexplored. In this work, we conduct\na comprehensive analysis of open-source datasets and data synthesis techniques\nfor mathematical reasoning, evaluating them under a unified pipeline designed\nto mirror training and deployment scenarios. We further distill effective data\nselection strategies and identify practical methods suitable for industrial\napplications. Our findings highlight that structuring data in more\ninterpretable formats, or distilling from stronger models often outweighs\nsimply scaling up data volume. This study provides actionable guidance for\nintegrating training data to enhance LLM capabilities, supporting both\ncost-effective data curation and scalable model enhancement. We hope this work\nwill inspire further research on how to balance \"more data\" versus \"better\ndata\" for real-world reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yike Zhao"
                    },
                    {
                        "name": "Simin Guo"
                    },
                    {
                        "name": "Ziqing Yang"
                    },
                    {
                        "name": "Shifan Han"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Fei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Fei Tan"
                },
                "author": "Fei Tan",
                "arxiv_comment": "12 pages, 3 figures, submitted to EMNLP 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07167v1",
                "updated": "2025-10-08T16:06:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    6,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T16:06:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    6,
                    4,
                    2,
                    281,
                    0
                ],
                "title": "Reasoning for Hierarchical Text Classification: The Case of Patents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning for Hierarchical Text Classification: The Case of Patents"
                },
                "summary": "Hierarchical text classification (HTC) assigns documents to multiple levels\nof a pre-defined taxonomy. Automated patent subject classification represents\none of the hardest HTC scenarios because of domain knowledge difficulty and a\nhuge number of labels. Prior approaches only output a flat label set, which\noffers little insight into the reason behind predictions. Therefore, we propose\nReasoning for Hierarchical Classification (RHC), a novel framework that\nreformulates HTC as a step-by-step reasoning task to sequentially deduce\nhierarchical labels. RHC trains large language models (LLMs) in two stages: a\ncold-start stage that aligns outputs with chain-of-thought (CoT) reasoning\nformat and a reinforcement learning (RL) stage to enhance multi-step reasoning\nability. RHC demonstrates four advantages in our experiments. (1)\nEffectiveness: RHC surpasses previous baselines and outperforms the supervised\nfine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)\nExplainability: RHC produces natural-language justifications before prediction\nto facilitate human inspection. (3) Scalability: RHC scales favorably with\nmodel size with larger gains compared to standard fine-tuning. (4)\nApplicability: Beyond patents, we further demonstrate that RHC achieves\nstate-of-the-art performance on other widely used HTC benchmarks, which\nhighlights its broad applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical text classification (HTC) assigns documents to multiple levels\nof a pre-defined taxonomy. Automated patent subject classification represents\none of the hardest HTC scenarios because of domain knowledge difficulty and a\nhuge number of labels. Prior approaches only output a flat label set, which\noffers little insight into the reason behind predictions. Therefore, we propose\nReasoning for Hierarchical Classification (RHC), a novel framework that\nreformulates HTC as a step-by-step reasoning task to sequentially deduce\nhierarchical labels. RHC trains large language models (LLMs) in two stages: a\ncold-start stage that aligns outputs with chain-of-thought (CoT) reasoning\nformat and a reinforcement learning (RL) stage to enhance multi-step reasoning\nability. RHC demonstrates four advantages in our experiments. (1)\nEffectiveness: RHC surpasses previous baselines and outperforms the supervised\nfine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)\nExplainability: RHC produces natural-language justifications before prediction\nto facilitate human inspection. (3) Scalability: RHC scales favorably with\nmodel size with larger gains compared to standard fine-tuning. (4)\nApplicability: Beyond patents, we further demonstrate that RHC achieves\nstate-of-the-art performance on other widely used HTC benchmarks, which\nhighlights its broad applicability."
                },
                "authors": [
                    {
                        "name": "Lekang Jiang"
                    },
                    {
                        "name": "Wenjun Sun"
                    },
                    {
                        "name": "Stephan Goetz"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Goetz"
                },
                "author": "Stephan Goetz",
                "arxiv_comment": "15 pages, 10 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07161v1",
                "updated": "2025-10-08T15:59:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    59,
                    11,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:59:11Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    59,
                    11,
                    2,
                    281,
                    0
                ],
                "title": "Integrating Domain Knowledge into Process Discovery Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Domain Knowledge into Process Discovery Using Large Language\n  Models"
                },
                "summary": "Process discovery aims to derive process models from event logs, providing\ninsights into operational behavior and forming a foundation for conformance\nchecking and process improvement. However, models derived solely from event\ndata may not accurately reflect the real process, as event logs are often\nincomplete or affected by noise, and domain knowledge, an important\ncomplementary resource, is typically disregarded. As a result, the discovered\nmodels may lack reliability for downstream tasks. We propose an interactive\nframework that incorporates domain knowledge, expressed in natural language,\ninto the process discovery pipeline using Large Language Models (LLMs). Our\napproach leverages LLMs to extract declarative rules from textual descriptions\nprovided by domain experts. These rules are used to guide the IMr discovery\nalgorithm, which recursively constructs process models by combining insights\nfrom both the event log and the extracted rules, helping to avoid problematic\nprocess structures that contradict domain knowledge. The framework coordinates\ninteractions among the LLM, domain experts, and a set of backend services. We\npresent a fully implemented tool that supports this workflow and conduct an\nextensive evaluation of multiple LLMs and prompt engineering strategies. Our\nempirical study includes a case study based on a real-life event log with the\ninvolvement of domain experts, who assessed the usability and effectiveness of\nthe framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process discovery aims to derive process models from event logs, providing\ninsights into operational behavior and forming a foundation for conformance\nchecking and process improvement. However, models derived solely from event\ndata may not accurately reflect the real process, as event logs are often\nincomplete or affected by noise, and domain knowledge, an important\ncomplementary resource, is typically disregarded. As a result, the discovered\nmodels may lack reliability for downstream tasks. We propose an interactive\nframework that incorporates domain knowledge, expressed in natural language,\ninto the process discovery pipeline using Large Language Models (LLMs). Our\napproach leverages LLMs to extract declarative rules from textual descriptions\nprovided by domain experts. These rules are used to guide the IMr discovery\nalgorithm, which recursively constructs process models by combining insights\nfrom both the event log and the extracted rules, helping to avoid problematic\nprocess structures that contradict domain knowledge. The framework coordinates\ninteractions among the LLM, domain experts, and a set of backend services. We\npresent a fully implemented tool that supports this workflow and conduct an\nextensive evaluation of multiple LLMs and prompt engineering strategies. Our\nempirical study includes a case study based on a real-life event log with the\ninvolvement of domain experts, who assessed the usability and effectiveness of\nthe framework."
                },
                "authors": [
                    {
                        "name": "Ali Norouzifar"
                    },
                    {
                        "name": "Humam Kourani"
                    },
                    {
                        "name": "Marcus Dees"
                    },
                    {
                        "name": "Wil van der Aalst"
                    }
                ],
                "author_detail": {
                    "name": "Wil van der Aalst"
                },
                "author": "Wil van der Aalst",
                "arxiv_comment": "This paper is currently under review for publication in a journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07147v1",
                "updated": "2025-10-08T15:48:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    48,
                    41,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:48:41Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    48,
                    41,
                    2,
                    281,
                    0
                ],
                "title": "A Multi-Agent Framework for Stateful Inference-Time Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent Framework for Stateful Inference-Time Search"
                },
                "summary": "Recent work explores agentic inference-time techniques to perform structured,\nmulti-step reasoning. However, stateless inference often struggles on\nmulti-step tasks due to the absence of persistent state. Moreover,\ntask-specific fine-tuning or instruction-tuning often achieve surface-level\ncode generation but remain brittle on tasks requiring deeper reasoning and\nlong-horizon dependencies. To address these limitations, we propose stateful\nmulti-agent evolutionary search, a training-free framework that departs from\nprior stateless approaches by combining (i) persistent inference-time state,\n(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate\nits effectiveness in automated unit test generation through the generation of\nedge cases. We generate robust edge cases using an evolutionary search process,\nwhere specialized agents sequentially propose, mutate, and score candidates. A\ncontroller maintains persistent state across generations, while evolutionary\npreservation ensures diversity and exploration across all possible cases. This\nyields a generalist agent capable of discovering robust, high-coverage edge\ncases across unseen codebases. Experiments show our stateful multi-agent\ninference framework achieves substantial gains in coverage over stateless\nsingle-step baselines, evaluated on prevalent unit-testing benchmarks such as\nHumanEval and TestGenEvalMini and using three diverse LLM families - Llama,\nGemma, and GPT. These results indicate that combining persistent inference-time\nstate with evolutionary search materially improves unit-test generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work explores agentic inference-time techniques to perform structured,\nmulti-step reasoning. However, stateless inference often struggles on\nmulti-step tasks due to the absence of persistent state. Moreover,\ntask-specific fine-tuning or instruction-tuning often achieve surface-level\ncode generation but remain brittle on tasks requiring deeper reasoning and\nlong-horizon dependencies. To address these limitations, we propose stateful\nmulti-agent evolutionary search, a training-free framework that departs from\nprior stateless approaches by combining (i) persistent inference-time state,\n(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate\nits effectiveness in automated unit test generation through the generation of\nedge cases. We generate robust edge cases using an evolutionary search process,\nwhere specialized agents sequentially propose, mutate, and score candidates. A\ncontroller maintains persistent state across generations, while evolutionary\npreservation ensures diversity and exploration across all possible cases. This\nyields a generalist agent capable of discovering robust, high-coverage edge\ncases across unseen codebases. Experiments show our stateful multi-agent\ninference framework achieves substantial gains in coverage over stateless\nsingle-step baselines, evaluated on prevalent unit-testing benchmarks such as\nHumanEval and TestGenEvalMini and using three diverse LLM families - Llama,\nGemma, and GPT. These results indicate that combining persistent inference-time\nstate with evolutionary search materially improves unit-test generation."
                },
                "authors": [
                    {
                        "name": "Arshika Lalan"
                    },
                    {
                        "name": "Rajat Ghosh"
                    },
                    {
                        "name": "Aditya Kolsur"
                    },
                    {
                        "name": "Debojyoti Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Debojyoti Dutta"
                },
                "author": "Debojyoti Dutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07141v1",
                "updated": "2025-10-08T15:42:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    42,
                    49,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:42:49Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    42,
                    49,
                    2,
                    281,
                    0
                ],
                "title": "Comparing human and language models sentence processing difficulties on\n  complex structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing human and language models sentence processing difficulties on\n  complex structures"
                },
                "summary": "Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs."
                },
                "authors": [
                    {
                        "name": "Samuel Joseph Amouyal"
                    },
                    {
                        "name": "Aya Meltzer-Asscher"
                    },
                    {
                        "name": "Jonathan Berant"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Berant"
                },
                "author": "Jonathan Berant",
                "arxiv_comment": "Data and code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17967v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17967v4",
                "updated": "2025-10-08T15:33:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    33,
                    25,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-23T14:37:00Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    14,
                    37,
                    0,
                    4,
                    143,
                    0
                ],
                "title": "FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FFT-based Dynamic Subspace Selection for Low-Rank Adaptive Optimization\n  of Large Language Models"
                },
                "summary": "Low-rank optimization has emerged as a promising direction in training large\nlanguage models (LLMs) to improve running time and reduce the memory usage of\nadaptive optimizers by constraining learning to a lower-dimensional space.\nPrior work typically projects gradients of linear layers using approaches based\non Singular Value Decomposition (SVD) or QR-decomposition. Applying these\ntechniques individually to each layer in large models is computationally\nexpensive and incurs additional memory costs due to storing the projection\nmatrices. In this work, we propose a computationally efficient and conceptually\nsimple, two-step procedure to approximate SVD/QR-based gradient projections\ninto lower-dimensional spaces by using a predefined orthogonal matrix of the\nDiscrete Cosine Transform (DCT). We dynamically select columns from the DCT\nmatrix based on their alignment with the gradient of each layer. The effective\nprojection matrices are obtained via a simple matmul with the DCT matrix in\n$O(n^3)$ time, followed by a lightweight sorting step to identify the most\nrelevant basis vectors. For large layers, DCT can be computed via Makhoul's\n$N$-point algorithm based on Fast Fourier Transform (FFT) in $O(n^2 \\log(n))$\ntime. Due to the predefined nature of the orthogonal bases, they are computed\nonce at the start of training. Our numerical experiments on both pre-training\nand fine-tuning tasks demonstrate the effectiveness of our dual strategy in\napproximating optimal low-rank projections, obtaining an approach with\nrank-independent running time that matches the performance of costly\nSVD/QR-based methods while achieving faster runtime and reduced memory usage by\nup to $25\\%$ across different model sizes. Our code is available at\n\\href{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers}{\\texttt{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank optimization has emerged as a promising direction in training large\nlanguage models (LLMs) to improve running time and reduce the memory usage of\nadaptive optimizers by constraining learning to a lower-dimensional space.\nPrior work typically projects gradients of linear layers using approaches based\non Singular Value Decomposition (SVD) or QR-decomposition. Applying these\ntechniques individually to each layer in large models is computationally\nexpensive and incurs additional memory costs due to storing the projection\nmatrices. In this work, we propose a computationally efficient and conceptually\nsimple, two-step procedure to approximate SVD/QR-based gradient projections\ninto lower-dimensional spaces by using a predefined orthogonal matrix of the\nDiscrete Cosine Transform (DCT). We dynamically select columns from the DCT\nmatrix based on their alignment with the gradient of each layer. The effective\nprojection matrices are obtained via a simple matmul with the DCT matrix in\n$O(n^3)$ time, followed by a lightweight sorting step to identify the most\nrelevant basis vectors. For large layers, DCT can be computed via Makhoul's\n$N$-point algorithm based on Fast Fourier Transform (FFT) in $O(n^2 \\log(n))$\ntime. Due to the predefined nature of the orthogonal bases, they are computed\nonce at the start of training. Our numerical experiments on both pre-training\nand fine-tuning tasks demonstrate the effectiveness of our dual strategy in\napproximating optimal low-rank projections, obtaining an approach with\nrank-independent running time that matches the performance of costly\nSVD/QR-based methods while achieving faster runtime and reduced memory usage by\nup to $25\\%$ across different model sizes. Our code is available at\n\\href{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers}{\\texttt{https://github.com/IST-DASLab/ISTA-DASLab-Optimizers}}."
                },
                "authors": [
                    {
                        "name": "Ionut-Vlad Modoranu"
                    },
                    {
                        "name": "Mher Safaryan"
                    },
                    {
                        "name": "Erik Schultheis"
                    },
                    {
                        "name": "Max Ryabinin"
                    },
                    {
                        "name": "Artem Chumachenko"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17967v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17967v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07118v1",
                "updated": "2025-10-08T15:11:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    11,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T15:11:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    11,
                    4,
                    2,
                    281,
                    0
                ],
                "title": "TRIM: Token-wise Attention-Derived Saliency for Data-Efficient\n  Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIM: Token-wise Attention-Derived Saliency for Data-Efficient\n  Instruction Tuning"
                },
                "summary": "Instruction tuning is essential for aligning large language models (LLMs) to\ndownstream tasks and commonly relies on large, diverse corpora. However, small,\nhigh-quality subsets, known as coresets, can deliver comparable or superior\nresults, though curating them remains challenging. Existing methods often rely\non coarse, sample-level signals like gradients, an approach that is\ncomputationally expensive and overlooks fine-grained features. To address this,\nwe introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a\nforward-only, token-centric framework. Instead of using gradients, TRIM\noperates by matching underlying representational patterns identified via\nattention-based \"fingerprints\" from a handful of target samples. Such an\napproach makes TRIM highly efficient and uniquely sensitive to the structural\nfeatures that define a task. Coresets selected by our method consistently\noutperform state-of-the-art baselines by up to 9% on downstream tasks and even\nsurpass the performance of full-data fine-tuning in some settings. By avoiding\nexpensive backward passes, TRIM achieves this at a fraction of the\ncomputational cost. These findings establish TRIM as a scalable and efficient\nalternative for building high-quality instruction-tuning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is essential for aligning large language models (LLMs) to\ndownstream tasks and commonly relies on large, diverse corpora. However, small,\nhigh-quality subsets, known as coresets, can deliver comparable or superior\nresults, though curating them remains challenging. Existing methods often rely\non coarse, sample-level signals like gradients, an approach that is\ncomputationally expensive and overlooks fine-grained features. To address this,\nwe introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a\nforward-only, token-centric framework. Instead of using gradients, TRIM\noperates by matching underlying representational patterns identified via\nattention-based \"fingerprints\" from a handful of target samples. Such an\napproach makes TRIM highly efficient and uniquely sensitive to the structural\nfeatures that define a task. Coresets selected by our method consistently\noutperform state-of-the-art baselines by up to 9% on downstream tasks and even\nsurpass the performance of full-data fine-tuning in some settings. By avoiding\nexpensive backward passes, TRIM achieves this at a fraction of the\ncomputational cost. These findings establish TRIM as a scalable and efficient\nalternative for building high-quality instruction-tuning datasets."
                },
                "authors": [
                    {
                        "name": "Manish Nagaraj"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Deepak Ravikumar"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10309v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10309v2",
                "updated": "2025-10-08T15:08:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    8,
                    8,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-15T13:55:27Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    55,
                    27,
                    3,
                    135,
                    0
                ],
                "title": "Empirically evaluating commonsense intelligence in large language models\n  with large-scale human judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirically evaluating commonsense intelligence in large language models\n  with large-scale human judgments"
                },
                "summary": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a method\nfor evaluating common sense in artificial intelligence (AI), specifically in\nlarge language models (LLMs), that incorporates empirically observed\nheterogeneity among humans by measuring the correspondence between a model's\njudgment and that of a human population. We first find that, when treated as\nindependent survey respondents, most LLMs remain below the human median in\ntheir individual commonsense competence. Second, when used as simulators of a\nhypothetical population, LLMs correlate with real humans only modestly in the\nextent to which they agree on the same set of statements. In both cases,\nsmaller, open-weight models are surprisingly more competitive than larger,\nproprietary frontier models. Our evaluation framework, which ties commonsense\nintelligence to its cultural basis, contributes to the growing call for\nadapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a method\nfor evaluating common sense in artificial intelligence (AI), specifically in\nlarge language models (LLMs), that incorporates empirically observed\nheterogeneity among humans by measuring the correspondence between a model's\njudgment and that of a human population. We first find that, when treated as\nindependent survey respondents, most LLMs remain below the human median in\ntheir individual commonsense competence. Second, when used as simulators of a\nhypothetical population, LLMs correlate with real humans only modestly in the\nextent to which they agree on the same set of statements. In both cases,\nsmaller, open-weight models are surprisingly more competitive than larger,\nproprietary frontier models. Our evaluation framework, which ties commonsense\nintelligence to its cultural basis, contributes to the growing call for\nadapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge."
                },
                "authors": [
                    {
                        "name": "Tuan Dung Nguyen"
                    },
                    {
                        "name": "Duncan J. Watts"
                    },
                    {
                        "name": "Mark E. Whiting"
                    }
                ],
                "author_detail": {
                    "name": "Mark E. Whiting"
                },
                "author": "Mark E. Whiting",
                "arxiv_comment": "Code and data: https://github.com/Watts-Lab/commonsense-llm-eval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10309v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10309v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07105v1",
                "updated": "2025-10-08T14:59:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    59,
                    24,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:59:24Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    59,
                    24,
                    2,
                    281,
                    0
                ],
                "title": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples\n  via Meta-Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples\n  via Meta-Learning"
                },
                "summary": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity,\nor legitimate disagreement between annotators. In this paper, we outline our\nsystem for modeling human variation. Our system leverages language models'\n(LLMs) in-context learning abilities, along with a two-step meta-learning\ntraining procedure for 1) post-training on many datasets requiring in-context\nlearning and 2) specializing the model via in-context meta-learning to the\nparticular data distribution of interest. We also evaluate the performance of\nour system submission to the Learning With Disagreements (LeWiDi) competition,\nwhere it was the overall winner on both tasks. Additionally, we perform an\nablation study to measure the importance of each system component. We find that\nincluding rater examples in-context is crucial for our system's performance,\ndataset-specific fine-tuning is helpful on the larger datasets, post-training\non other in-context datasets is helpful on one of the competition datasets, and\nthat performance improves with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity,\nor legitimate disagreement between annotators. In this paper, we outline our\nsystem for modeling human variation. Our system leverages language models'\n(LLMs) in-context learning abilities, along with a two-step meta-learning\ntraining procedure for 1) post-training on many datasets requiring in-context\nlearning and 2) specializing the model via in-context meta-learning to the\nparticular data distribution of interest. We also evaluate the performance of\nour system submission to the Learning With Disagreements (LeWiDi) competition,\nwhere it was the overall winner on both tasks. Additionally, we perform an\nablation study to measure the importance of each system component. We find that\nincluding rater examples in-context is crucial for our system's performance,\ndataset-specific fine-tuning is helpful on the larger datasets, post-training\non other in-context datasets is helpful on one of the competition datasets, and\nthat performance improves with model scale."
                },
                "authors": [
                    {
                        "name": "Taylor Sorensen"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "NLPerspectives: The 4th Workshop on Perspectivist Approaches to\n  Natural Language Processing at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07098v1",
                "updated": "2025-10-08T14:56:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    56,
                    42,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:56:42Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    56,
                    42,
                    2,
                    281,
                    0
                ],
                "title": "TALENT: Table VQA via Augmented Language-Enhanced Natural-text\n  Transcription",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TALENT: Table VQA via Augmented Language-Enhanced Natural-text\n  Transcription"
                },
                "summary": "Table Visual Question Answering (Table VQA) is typically addressed by large\nvision-language models (VLMs). While such models can answer directly from\nimages, they often miss fine-grained details unless scaled to very large sizes,\nwhich are computationally prohibitive, especially for mobile deployment. A\nlighter alternative is to have a small VLM perform OCR and then use a large\nlanguage model (LLM) to reason over structured outputs such as Markdown tables.\nHowever, these representations are not naturally optimized for LLMs and still\nintroduce substantial errors. We propose TALENT (Table VQA via Augmented\nLanguage-Enhanced Natural-text Transcription), a lightweight framework that\nleverages dual representations of tables. TALENT prompts a small VLM to produce\nboth OCR text and natural language narration, then combines them with the\nquestion for reasoning by an LLM. This reframes Table VQA as an LLM-centric\nmultimodal reasoning task, where the VLM serves as a perception-narration\nmodule rather than a monolithic solver. Additionally, we construct ReTabVQA, a\nmore challenging Table VQA dataset requiring multi-step quantitative reasoning\nover table images. Experiments show that TALENT enables a small VLM-LLM\ncombination to match or surpass a single large VLM at significantly lower\ncomputational cost on both public datasets and ReTabVQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table Visual Question Answering (Table VQA) is typically addressed by large\nvision-language models (VLMs). While such models can answer directly from\nimages, they often miss fine-grained details unless scaled to very large sizes,\nwhich are computationally prohibitive, especially for mobile deployment. A\nlighter alternative is to have a small VLM perform OCR and then use a large\nlanguage model (LLM) to reason over structured outputs such as Markdown tables.\nHowever, these representations are not naturally optimized for LLMs and still\nintroduce substantial errors. We propose TALENT (Table VQA via Augmented\nLanguage-Enhanced Natural-text Transcription), a lightweight framework that\nleverages dual representations of tables. TALENT prompts a small VLM to produce\nboth OCR text and natural language narration, then combines them with the\nquestion for reasoning by an LLM. This reframes Table VQA as an LLM-centric\nmultimodal reasoning task, where the VLM serves as a perception-narration\nmodule rather than a monolithic solver. Additionally, we construct ReTabVQA, a\nmore challenging Table VQA dataset requiring multi-step quantitative reasoning\nover table images. Experiments show that TALENT enables a small VLM-LLM\ncombination to match or surpass a single large VLM at significantly lower\ncomputational cost on both public datasets and ReTabVQA."
                },
                "authors": [
                    {
                        "name": "Guo Yutong"
                    },
                    {
                        "name": "Wanying Wang"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Zichen Miao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07096v1",
                "updated": "2025-10-08T14:53:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    53,
                    48,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:53:48Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    53,
                    48,
                    2,
                    281,
                    0
                ],
                "title": "Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided\n  Sarcastic Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided\n  Sarcastic Speech Synthesis"
                },
                "summary": "Sarcasm is a subtle form of non-literal language that poses significant\nchallenges for speech synthesis due to its reliance on nuanced semantic,\ncontextual, and prosodic cues. While existing speech synthesis research has\nfocused primarily on broad emotional categories, sarcasm remains largely\nunexplored. In this paper, we propose a Large Language Model (LLM)-enhanced\nRetrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach\ncombines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture\npragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic\nexemplars retrieved via a Retrieval Augmented Generation (RAG) module, which\nprovide expressive reference patterns of sarcastic delivery. Integrated within\na VITS backbone, this dual conditioning enables more natural and contextually\nappropriate sarcastic speech. Experiments demonstrate that our method\noutperforms baselines in both objective measures and subjective evaluations,\nyielding improvements in speech naturalness, sarcastic expressivity, and\ndownstream sarcasm detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sarcasm is a subtle form of non-literal language that poses significant\nchallenges for speech synthesis due to its reliance on nuanced semantic,\ncontextual, and prosodic cues. While existing speech synthesis research has\nfocused primarily on broad emotional categories, sarcasm remains largely\nunexplored. In this paper, we propose a Large Language Model (LLM)-enhanced\nRetrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach\ncombines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture\npragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic\nexemplars retrieved via a Retrieval Augmented Generation (RAG) module, which\nprovide expressive reference patterns of sarcastic delivery. Integrated within\na VITS backbone, this dual conditioning enables more natural and contextually\nappropriate sarcastic speech. Experiments demonstrate that our method\noutperforms baselines in both objective measures and subjective evaluations,\nyielding improvements in speech naturalness, sarcastic expressivity, and\ndownstream sarcasm detection."
                },
                "authors": [
                    {
                        "name": "Zhu Li"
                    },
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Xiyuan Gao"
                    },
                    {
                        "name": "Shekhar Nayak"
                    },
                    {
                        "name": "Matt Coler"
                    }
                ],
                "author_detail": {
                    "name": "Matt Coler"
                },
                "author": "Matt Coler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11329v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11329v3",
                "updated": "2025-10-08T14:49:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    49,
                    25,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-16T14:53:50Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    53,
                    50,
                    4,
                    136,
                    0
                ],
                "title": "TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM\n  Inference"
                },
                "summary": "Distributed inference of large language models (LLMs) can introduce overheads\nof up to 20% even over GPUs connected via high-speed interconnects such as\nNVLink. Multiple techniques have been proposed to mitigate these overheads by\ndecomposing computations into finer-grained tasks and overlapping communication\nwith sub-tasks as they complete. However, fine-grained decomposition of a large\ncomputation into many smaller computations on GPUs results in overheads.\nFurthermore, the communication itself uses many streaming multiprocessors\n(SMs), adding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a\nToken-Splitting technique that divides the tokens in the inference batch into\ntwo approximately equal subsets in a wave-aware manner. The communication of\none subset is then overlapped with the computation of the other. In addition,\nTokenWeave optimizes the order of the layer normalization computation with\nrespect to communication operations and implements a novel fused\nAllReduce--RMSNorm kernel that carefully leverages Multimem instruction support\navailable on Hopper and Blackwell NVIDIA GPUs. These optimizations allow\nTokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover,\nour kernel enables the memory-bound RMSNorm to be overlapped with the other\nbatch's computation, providing additional gains.\n  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher\nthroughput across multiple models and workloads. In several settings,\nTokenWeave results in better performance compared to an equivalent model with\nall communication removed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed inference of large language models (LLMs) can introduce overheads\nof up to 20% even over GPUs connected via high-speed interconnects such as\nNVLink. Multiple techniques have been proposed to mitigate these overheads by\ndecomposing computations into finer-grained tasks and overlapping communication\nwith sub-tasks as they complete. However, fine-grained decomposition of a large\ncomputation into many smaller computations on GPUs results in overheads.\nFurthermore, the communication itself uses many streaming multiprocessors\n(SMs), adding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a\nToken-Splitting technique that divides the tokens in the inference batch into\ntwo approximately equal subsets in a wave-aware manner. The communication of\none subset is then overlapped with the computation of the other. In addition,\nTokenWeave optimizes the order of the layer normalization computation with\nrespect to communication operations and implements a novel fused\nAllReduce--RMSNorm kernel that carefully leverages Multimem instruction support\navailable on Hopper and Blackwell NVIDIA GPUs. These optimizations allow\nTokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover,\nour kernel enables the memory-bound RMSNorm to be overlapped with the other\nbatch's computation, providing additional gains.\n  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher\nthroughput across multiple models and workloads. In several settings,\nTokenWeave results in better performance compared to an equivalent model with\nall communication removed."
                },
                "authors": [
                    {
                        "name": "Raja Gond"
                    },
                    {
                        "name": "Nipun Kwatra"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    }
                ],
                "author_detail": {
                    "name": "Ramachandran Ramjee"
                },
                "author": "Ramachandran Ramjee",
                "arxiv_comment": "14 pages, 16 figures. For source code, see\n  https://github.com/microsoft/tokenweave. In version 2, Figure 6 shows All\n  Reduce bandwidth, not Reduce Scatter. The Multimem Reduce Scatter bandwidth\n  formula differs slightly from the Ring-based version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11329v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11329v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07091v1",
                "updated": "2025-10-08T14:47:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    47,
                    40,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:47:40Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    47,
                    40,
                    2,
                    281,
                    0
                ],
                "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from\n  Planning with Actions to Planning with Schemas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from\n  Planning with Actions to Planning with Schemas"
                },
                "summary": "Enabling LLMs to effectively operate long-horizon task which requires\nlong-term planning and multiple interactions is essential for open-world\nautonomy. Conventional methods adopt planning with actions where a executable\naction list would be provided as reference. However, this action representation\nchoice would be impractical when the environment action space is combinatorial\nexploded (e.g., open-ended real world). This naturally leads to a question: As\nenvironmental action space scales, what is the optimal action representation\nfor long-horizon agents? In this paper, we systematically study the\neffectiveness of two different action representations. The first one is\nconventional planning with actions (PwA) which is predominantly adopted for its\neffectiveness on existing benchmarks. The other one is planning with schemas\n(PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ]\nto [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable\nscalability. This alternative is motivated by its alignment with human\ncognition and its compliance with environment-imposed action format\nrestriction. We propose cognitive bandwidth perspective as a conceptual\nframework to qualitatively understand the differences between these two action\nrepresentations and empirically observe a representation-choice inflection\npoint between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve\nas evidence of the need for scalable representations. We further conduct\ncontrolled experiments to study how the location of this inflection point\ninteracts with different model capacities: stronger planning proficiency shifts\nthe inflection rightward, whereas better schema instantiation shifts it\nleftward. Finally, noting the suboptimal performance of PwS agents, we provide\nan actionable guide for building more capable PwS agents for better scalable\nautonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling LLMs to effectively operate long-horizon task which requires\nlong-term planning and multiple interactions is essential for open-world\nautonomy. Conventional methods adopt planning with actions where a executable\naction list would be provided as reference. However, this action representation\nchoice would be impractical when the environment action space is combinatorial\nexploded (e.g., open-ended real world). This naturally leads to a question: As\nenvironmental action space scales, what is the optimal action representation\nfor long-horizon agents? In this paper, we systematically study the\neffectiveness of two different action representations. The first one is\nconventional planning with actions (PwA) which is predominantly adopted for its\neffectiveness on existing benchmarks. The other one is planning with schemas\n(PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ]\nto [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable\nscalability. This alternative is motivated by its alignment with human\ncognition and its compliance with environment-imposed action format\nrestriction. We propose cognitive bandwidth perspective as a conceptual\nframework to qualitatively understand the differences between these two action\nrepresentations and empirically observe a representation-choice inflection\npoint between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve\nas evidence of the need for scalable representations. We further conduct\ncontrolled experiments to study how the location of this inflection point\ninteracts with different model capacities: stronger planning proficiency shifts\nthe inflection rightward, whereas better schema instantiation shifts it\nleftward. Finally, noting the suboptimal performance of PwS agents, we provide\nan actionable guide for building more capable PwS agents for better scalable\nautonomy."
                },
                "authors": [
                    {
                        "name": "Baixuan Xu"
                    },
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Zhaowei Wang"
                    },
                    {
                        "name": "Hong Ting Tsang"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07083v1",
                "updated": "2025-10-08T14:40:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    40,
                    33,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:40:33Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    40,
                    33,
                    2,
                    281,
                    0
                ],
                "title": "All Claims Are Equal, but Some Claims Are More Equal Than Others:\n  Importance-Sensitive Factuality Evaluation of LLM Generations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All Claims Are Equal, but Some Claims Are More Equal Than Others:\n  Importance-Sensitive Factuality Evaluation of LLM Generations"
                },
                "summary": "Existing methods for evaluating the factuality of large language model (LLM)\nresponses treat all claims as equally important. This results in misleading\nevaluations when vital information is missing or incorrect as it receives the\nsame weight as peripheral details, raising the question: how can we reliably\ndetect such differences when there are errors in key information? Current\napproaches that measure factuality tend to be insensitive to omitted or false\nkey information. To investigate this lack of sensitivity, we construct\nVITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses\ndesigned to omit or falsify key information. Using this dataset, we demonstrate\nthe insensitivities of existing evaluation metrics to key information errors.\nTo address this gap, we introduce VITAL, a set of metrics that provide greater\nsensitivity in measuring the factuality of responses by incorporating the\nrelevance and importance of claims with respect to the query. Our analysis\ndemonstrates that VITAL metrics more reliably detect errors in key information\nthan previous methods. Our dataset, metrics, and analysis provide a foundation\nfor more accurate and robust assessment of LLM factuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods for evaluating the factuality of large language model (LLM)\nresponses treat all claims as equally important. This results in misleading\nevaluations when vital information is missing or incorrect as it receives the\nsame weight as peripheral details, raising the question: how can we reliably\ndetect such differences when there are errors in key information? Current\napproaches that measure factuality tend to be insensitive to omitted or false\nkey information. To investigate this lack of sensitivity, we construct\nVITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses\ndesigned to omit or falsify key information. Using this dataset, we demonstrate\nthe insensitivities of existing evaluation metrics to key information errors.\nTo address this gap, we introduce VITAL, a set of metrics that provide greater\nsensitivity in measuring the factuality of responses by incorporating the\nrelevance and importance of claims with respect to the query. Our analysis\ndemonstrates that VITAL metrics more reliably detect errors in key information\nthan previous methods. Our dataset, metrics, and analysis provide a foundation\nfor more accurate and robust assessment of LLM factuality."
                },
                "authors": [
                    {
                        "name": "Miriam Wanner"
                    },
                    {
                        "name": "Leif Azzopardi"
                    },
                    {
                        "name": "Paul Thomas"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Nick Craswell"
                    }
                ],
                "author_detail": {
                    "name": "Nick Craswell"
                },
                "author": "Nick Craswell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11787v2",
                "updated": "2025-10-08T14:40:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    40,
                    12,
                    2,
                    281,
                    0
                ],
                "published": "2025-09-15T11:16:04Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    16,
                    4,
                    0,
                    258,
                    0
                ],
                "title": "CodeCureAgent: Automatic Classification and Repair of Static Analysis\n  Warnings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeCureAgent: Automatic Classification and Repair of Static Analysis\n  Warnings"
                },
                "summary": "Static analysis tools are widely used to detect bugs, vulnerabilities, and\ncode smells. Traditionally, developers must resolve these warnings manually.\nBecause this process is tedious, developers sometimes ignore warnings, leading\nto an accumulation of warnings and a degradation of code quality. This paper\npresents CodeCureAgent, an approach that harnesses LLM-based agents to\nautomatically analyze, classify, and repair static analysis warnings. Unlike\nprevious work, our method does not follow a predetermined algorithm. Instead,\nwe adopt an agentic framework that iteratively invokes tools to gather\nadditional information from the codebase (e.g., via code search) and edit the\ncodebase to resolve the warning. CodeCureAgent detects and suppresses false\npositives, while fixing true positives when identified. We equip CodeCureAgent\nwith a three-step heuristic to approve patches: (1) build the project, (2)\nverify that the warning disappears without introducing new warnings, and (3)\nrun the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube\nwarnings found in 106 Java projects and covering 291 distinct rules. Our\napproach produces plausible fixes for 96.8% of the warnings, outperforming\nstate-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,\nrespectively. Manual inspection of 291 cases reveals a correct-fix rate of\n86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.\nThe approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end\nprocessing time of about four minutes per warning. We envision CodeCureAgent\nhelping to clean existing codebases and being integrated into CI/CD pipelines\nto prevent the accumulation of static analysis warnings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static analysis tools are widely used to detect bugs, vulnerabilities, and\ncode smells. Traditionally, developers must resolve these warnings manually.\nBecause this process is tedious, developers sometimes ignore warnings, leading\nto an accumulation of warnings and a degradation of code quality. This paper\npresents CodeCureAgent, an approach that harnesses LLM-based agents to\nautomatically analyze, classify, and repair static analysis warnings. Unlike\nprevious work, our method does not follow a predetermined algorithm. Instead,\nwe adopt an agentic framework that iteratively invokes tools to gather\nadditional information from the codebase (e.g., via code search) and edit the\ncodebase to resolve the warning. CodeCureAgent detects and suppresses false\npositives, while fixing true positives when identified. We equip CodeCureAgent\nwith a three-step heuristic to approve patches: (1) build the project, (2)\nverify that the warning disappears without introducing new warnings, and (3)\nrun the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube\nwarnings found in 106 Java projects and covering 291 distinct rules. Our\napproach produces plausible fixes for 96.8% of the warnings, outperforming\nstate-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,\nrespectively. Manual inspection of 291 cases reveals a correct-fix rate of\n86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.\nThe approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end\nprocessing time of about four minutes per warning. We envision CodeCureAgent\nhelping to clean existing codebases and being integrated into CI/CD pipelines\nto prevent the accumulation of static analysis warnings."
                },
                "authors": [
                    {
                        "name": "Pascal Joos"
                    },
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05318v2",
                "updated": "2025-10-08T14:39:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    39,
                    59,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-06T19:31:47Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    19,
                    31,
                    47,
                    0,
                    279,
                    0
                ],
                "title": "BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language\n  Models via Lens of Dynamic Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language\n  Models via Lens of Dynamic Interactions"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nsingle-turn text-to-SQL tasks, but real-world database applications\npredominantly require multi-turn interactions to handle ambiguous queries,\nexecution errors, and evolving user requirements. Existing multi-turn\nbenchmarks fall short by treating conversation histories as static context or\nlimiting evaluation to read-only operations, failing to reflect\nproduction-grade database assistant challenges. We introduce BIRD-INTERACT, a\nbenchmark that restores this realism through: (1) a comprehensive interaction\nenvironment coupling each database with a hierarchical knowledge base, metadata\nfiles, and a function-driven user simulator, enabling models to solicit\nclarifications, retrieve knowledge, and recover from errors without human\nsupervision; (2) two evaluation settings consisting of a pre-defined\nconversational protocol (c-Interact) and an open-ended agentic setting\n(a-Interact) where models autonomously decide when to query the user simulator\nor explore the environment; (3) a challenging task suite covering the full CRUD\nspectrum for business-intelligence and operational use cases, guarded by\nexecutable test cases. Each task features ambiguous and follow-up sub-tasks\nrequiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600\ntasks, up to 11,796 interactions) for comprehensive performance assessment, and\nBIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed\nbehavioral analysis and rapid method development. Our empirical results\nhighlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in\nc-Interact and 17.00% in a-Interact. Analysis via memory grafting and\nInteraction Test-time Scaling validates the importance of effective interaction\nfor complex, dynamic text-to-SQL tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance on\nsingle-turn text-to-SQL tasks, but real-world database applications\npredominantly require multi-turn interactions to handle ambiguous queries,\nexecution errors, and evolving user requirements. Existing multi-turn\nbenchmarks fall short by treating conversation histories as static context or\nlimiting evaluation to read-only operations, failing to reflect\nproduction-grade database assistant challenges. We introduce BIRD-INTERACT, a\nbenchmark that restores this realism through: (1) a comprehensive interaction\nenvironment coupling each database with a hierarchical knowledge base, metadata\nfiles, and a function-driven user simulator, enabling models to solicit\nclarifications, retrieve knowledge, and recover from errors without human\nsupervision; (2) two evaluation settings consisting of a pre-defined\nconversational protocol (c-Interact) and an open-ended agentic setting\n(a-Interact) where models autonomously decide when to query the user simulator\nor explore the environment; (3) a challenging task suite covering the full CRUD\nspectrum for business-intelligence and operational use cases, guarded by\nexecutable test cases. Each task features ambiguous and follow-up sub-tasks\nrequiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600\ntasks, up to 11,796 interactions) for comprehensive performance assessment, and\nBIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed\nbehavioral analysis and rapid method development. Our empirical results\nhighlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in\nc-Interact and 17.00% in a-Interact. Analysis via memory grafting and\nInteraction Test-time Scaling validates the importance of effective interaction\nfor complex, dynamic text-to-SQL tasks."
                },
                "authors": [
                    {
                        "name": "Nan Huo"
                    },
                    {
                        "name": "Xiaohan Xu"
                    },
                    {
                        "name": "Jinyang Li"
                    },
                    {
                        "name": "Per Jacobsson"
                    },
                    {
                        "name": "Shipei Lin"
                    },
                    {
                        "name": "Bowen Qin"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Ge Qu"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Linheng Han"
                    },
                    {
                        "name": "Edward Alexander"
                    },
                    {
                        "name": "Xintong Zhu"
                    },
                    {
                        "name": "Rui Qin"
                    },
                    {
                        "name": "Ruihan Yu"
                    },
                    {
                        "name": "Yiyao Jin"
                    },
                    {
                        "name": "Feige Zhou"
                    },
                    {
                        "name": "Weihao Zhong"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Fatma Ozcan"
                    },
                    {
                        "name": "Yannis Papakonstantinou"
                    },
                    {
                        "name": "Reynold Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Reynold Cheng"
                },
                "author": "Reynold Cheng",
                "arxiv_comment": "47 pages, 26 figures, 11 tables. Submitted to arXiv; based on work\n  from The BIRD Team and Google Cloud. Dataset and code available at\n  https://bird-interact.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07081v1",
                "updated": "2025-10-08T14:39:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    39,
                    34,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:39:34Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    39,
                    34,
                    2,
                    281,
                    0
                ],
                "title": "Accelerating Diffusion LLM Inference via Local Determinism Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLM Inference via Local Determinism Propagation"
                },
                "summary": "Diffusion large language models (dLLMs) represent a significant advancement\nin text generation, offering parallel token decoding capabilities. However,\nexisting open-source implementations suffer from quality-speed trade-offs that\nimpede their practical deployment. Conservative sampling strategies typically\ndecode only the most confident token per step to ensure quality (i.e., greedy\ndecoding), at the cost of inference efficiency due to repeated redundant\nrefinement iterations--a phenomenon we term delayed decoding. Through\nsystematic analysis of dLLM decoding dynamics, we characterize this delayed\ndecoding behavior and propose a training-free adaptive parallel decoding\nstrategy, named LocalLeap, to address these inefficiencies. LocalLeap is built\non two fundamental empirical principles: local determinism propagation centered\non high-confidence anchors and progressive spatial consistency decay. By\napplying these principles, LocalLeap identifies anchors and performs localized\nrelaxed parallel decoding within bounded neighborhoods, achieving substantial\ninference step reduction through early commitment of already-determined tokens\nwithout compromising output quality. Comprehensive evaluation on various\nbenchmarks demonstrates that LocalLeap achieves 6.94$\\times$ throughput\nimprovements and reduces decoding steps to just 14.2\\% of the original\nrequirement, achieving these gains with negligible performance impact. The\nsource codes are available at: https://github.com/friedrichor/LocalLeap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) represent a significant advancement\nin text generation, offering parallel token decoding capabilities. However,\nexisting open-source implementations suffer from quality-speed trade-offs that\nimpede their practical deployment. Conservative sampling strategies typically\ndecode only the most confident token per step to ensure quality (i.e., greedy\ndecoding), at the cost of inference efficiency due to repeated redundant\nrefinement iterations--a phenomenon we term delayed decoding. Through\nsystematic analysis of dLLM decoding dynamics, we characterize this delayed\ndecoding behavior and propose a training-free adaptive parallel decoding\nstrategy, named LocalLeap, to address these inefficiencies. LocalLeap is built\non two fundamental empirical principles: local determinism propagation centered\non high-confidence anchors and progressive spatial consistency decay. By\napplying these principles, LocalLeap identifies anchors and performs localized\nrelaxed parallel decoding within bounded neighborhoods, achieving substantial\ninference step reduction through early commitment of already-determined tokens\nwithout compromising output quality. Comprehensive evaluation on various\nbenchmarks demonstrates that LocalLeap achieves 6.94$\\times$ throughput\nimprovements and reduces decoding steps to just 14.2\\% of the original\nrequirement, achieving these gains with negligible performance impact. The\nsource codes are available at: https://github.com/friedrichor/LocalLeap."
                },
                "authors": [
                    {
                        "name": "Fanheng Kong"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Yahui Liu"
                    },
                    {
                        "name": "Zirui Wu"
                    },
                    {
                        "name": "Yu Tian"
                    },
                    {
                        "name": "Victoria W."
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "21 pages, 4 figures. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07077v1",
                "updated": "2025-10-08T14:38:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    38,
                    25,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:38:25Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    38,
                    25,
                    2,
                    281,
                    0
                ],
                "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action Models for Robotics: A Review Towards Real-World\n  Applications"
                },
                "summary": "Amid growing efforts to leverage advances in large language models (LLMs) and\nvision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models\nhave recently gained significant attention. By unifying vision, language, and\naction data at scale, which have traditionally been studied separately, VLA\nmodels aim to learn policies that generalise across diverse tasks, objects,\nembodiments, and environments. This generalisation capability is expected to\nenable robots to solve novel downstream tasks with minimal or no additional\ntask-specific data, facilitating more flexible and scalable real-world\ndeployment. Unlike previous surveys that focus narrowly on action\nrepresentations or high-level model architectures, this work offers a\ncomprehensive, full-stack review, integrating both software and hardware\ncomponents of VLA systems. In particular, this paper provides a systematic\nreview of VLAs, covering their strategy and architectural transition,\narchitectures and building blocks, modality-specific processing techniques, and\nlearning paradigms. In addition, to support the deployment of VLAs in\nreal-world robotic applications, we also review commonly used robot platforms,\ndata collection strategies, publicly available datasets, data augmentation\nmethods, and evaluation benchmarks. Throughout this comprehensive survey, this\npaper aims to offer practical guidance for the robotics community in applying\nVLAs to real-world robotic systems. All references categorized by training\napproach, evaluation method, modality, and dataset are available in the table\non our project website: https://vla-survey.github.io .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amid growing efforts to leverage advances in large language models (LLMs) and\nvision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models\nhave recently gained significant attention. By unifying vision, language, and\naction data at scale, which have traditionally been studied separately, VLA\nmodels aim to learn policies that generalise across diverse tasks, objects,\nembodiments, and environments. This generalisation capability is expected to\nenable robots to solve novel downstream tasks with minimal or no additional\ntask-specific data, facilitating more flexible and scalable real-world\ndeployment. Unlike previous surveys that focus narrowly on action\nrepresentations or high-level model architectures, this work offers a\ncomprehensive, full-stack review, integrating both software and hardware\ncomponents of VLA systems. In particular, this paper provides a systematic\nreview of VLAs, covering their strategy and architectural transition,\narchitectures and building blocks, modality-specific processing techniques, and\nlearning paradigms. In addition, to support the deployment of VLAs in\nreal-world robotic applications, we also review commonly used robot platforms,\ndata collection strategies, publicly available datasets, data augmentation\nmethods, and evaluation benchmarks. Throughout this comprehensive survey, this\npaper aims to offer practical guidance for the robotics community in applying\nVLAs to real-world robotic systems. All references categorized by training\napproach, evaluation method, modality, and dataset are available in the table\non our project website: https://vla-survey.github.io ."
                },
                "authors": [
                    {
                        "name": "Kento Kawaharazuka"
                    },
                    {
                        "name": "Jihoon Oh"
                    },
                    {
                        "name": "Jun Yamada"
                    },
                    {
                        "name": "Ingmar Posner"
                    },
                    {
                        "name": "Yuke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Zhu"
                },
                "author": "Yuke Zhu",
                "arxiv_doi": "10.1109/ACCESS.2025.3609980",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3609980",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.07077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to IEEE Access, website: https://vla-survey.github.io",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07073v1",
                "updated": "2025-10-08T14:35:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    35,
                    9,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:35:09Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    35,
                    9,
                    2,
                    281,
                    0
                ],
                "title": "VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle\n  Routing Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle\n  Routing Problems"
                },
                "summary": "Designing high-performing heuristics for vehicle routing problems (VRPs) is a\ncomplex task that requires both intuition and deep domain knowledge. Large\nlanguage model (LLM)-based code generation has recently shown promise across\nmany domains, but it still falls short of producing heuristics that rival those\ncrafted by human experts. In this paper, we propose VRPAgent, a framework that\nintegrates LLM-generated components into a metaheuristic and refines them\nthrough a novel genetic search. By using the LLM to generate problem-specific\noperators, embedded within a generic metaheuristic framework, VRPAgent keeps\ntasks manageable, guarantees correctness, and still enables the discovery of\nnovel and powerful strategies. Across multiple problems, including the\ncapacitated VRP, the VRP with time windows, and the prize-collecting VRP, our\nmethod discovers heuristic operators that outperform handcrafted methods and\nrecent learning-based approaches while requiring only a single CPU core. To our\nknowledge, \\VRPAgent is the first LLM-based paradigm to advance the\nstate-of-the-art in VRPs, highlighting a promising future for automated\nheuristics discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing high-performing heuristics for vehicle routing problems (VRPs) is a\ncomplex task that requires both intuition and deep domain knowledge. Large\nlanguage model (LLM)-based code generation has recently shown promise across\nmany domains, but it still falls short of producing heuristics that rival those\ncrafted by human experts. In this paper, we propose VRPAgent, a framework that\nintegrates LLM-generated components into a metaheuristic and refines them\nthrough a novel genetic search. By using the LLM to generate problem-specific\noperators, embedded within a generic metaheuristic framework, VRPAgent keeps\ntasks manageable, guarantees correctness, and still enables the discovery of\nnovel and powerful strategies. Across multiple problems, including the\ncapacitated VRP, the VRP with time windows, and the prize-collecting VRP, our\nmethod discovers heuristic operators that outperform handcrafted methods and\nrecent learning-based approaches while requiring only a single CPU core. To our\nknowledge, \\VRPAgent is the first LLM-based paradigm to advance the\nstate-of-the-art in VRPs, highlighting a promising future for automated\nheuristics discovery."
                },
                "authors": [
                    {
                        "name": "André Hottung"
                    },
                    {
                        "name": "Federico Berto"
                    },
                    {
                        "name": "Chuanbo Hua"
                    },
                    {
                        "name": "Nayeli Gast Zepeda"
                    },
                    {
                        "name": "Daniel Wetzel"
                    },
                    {
                        "name": "Michael Römer"
                    },
                    {
                        "name": "Haoran Ye"
                    },
                    {
                        "name": "Davide Zago"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Jinkyoo Park"
                    },
                    {
                        "name": "Kevin Tierney"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Tierney"
                },
                "author": "Kevin Tierney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07067v1",
                "updated": "2025-10-08T14:31:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    31,
                    35,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:31:35Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    31,
                    35,
                    2,
                    281,
                    0
                ],
                "title": "Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied\n  AI Commands on VLA Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied\n  AI Commands on VLA Models"
                },
                "summary": "Vision Language Action (VLA) models are widely used in Embodied AI, enabling\nrobots to interpret and execute language instructions. However, their\nrobustness to natural language variability in real-world scenarios has not been\nthoroughly investigated. In this work, we present a novel systematic study of\nthe robustness of state-of-the-art VLA models under linguistic perturbations.\nSpecifically, we evaluate model performance under two types of instruction\nnoise: (1) human-generated paraphrasing and (2) the addition of irrelevant\ncontext. We further categorize irrelevant contexts into two groups according to\ntheir length and their semantic and lexical proximity to robot commands. In\nthis study, we observe consistent performance degradation as context size\nexpands. We also demonstrate that the model can exhibit relative robustness to\nrandom context, with a performance drop within 10%, while semantically and\nlexically similar context of the same length can trigger a quality decline of\naround 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To\nmitigate this, we propose an LLM-based filtering framework that extracts core\ncommands from noisy inputs. Incorporating our filtering step allows models to\nrecover up to 98.5% of their original performance under noisy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Action (VLA) models are widely used in Embodied AI, enabling\nrobots to interpret and execute language instructions. However, their\nrobustness to natural language variability in real-world scenarios has not been\nthoroughly investigated. In this work, we present a novel systematic study of\nthe robustness of state-of-the-art VLA models under linguistic perturbations.\nSpecifically, we evaluate model performance under two types of instruction\nnoise: (1) human-generated paraphrasing and (2) the addition of irrelevant\ncontext. We further categorize irrelevant contexts into two groups according to\ntheir length and their semantic and lexical proximity to robot commands. In\nthis study, we observe consistent performance degradation as context size\nexpands. We also demonstrate that the model can exhibit relative robustness to\nrandom context, with a performance drop within 10%, while semantically and\nlexically similar context of the same length can trigger a quality decline of\naround 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To\nmitigate this, we propose an LLM-based filtering framework that extracts core\ncommands from noisy inputs. Incorporating our filtering step allows models to\nrecover up to 98.5% of their original performance under noisy conditions."
                },
                "authors": [
                    {
                        "name": "Daria Pugacheva"
                    },
                    {
                        "name": "Andrey Moskalenko"
                    },
                    {
                        "name": "Denis Shepelev"
                    },
                    {
                        "name": "Andrey Kuznetsov"
                    },
                    {
                        "name": "Vlad Shakhuro"
                    },
                    {
                        "name": "Elena Tutubalina"
                    }
                ],
                "author_detail": {
                    "name": "Elena Tutubalina"
                },
                "author": "Elena Tutubalina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07064v1",
                "updated": "2025-10-08T14:28:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    28,
                    53,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:28:53Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    28,
                    53,
                    2,
                    281,
                    0
                ],
                "title": "Prompt Optimization Across Multiple Agents for Representing Diverse\n  Human Populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Optimization Across Multiple Agents for Representing Diverse\n  Human Populations"
                },
                "summary": "The difficulty and expense of obtaining large-scale human responses make\nLarge Language Models (LLMs) an attractive alternative and a promising proxy\nfor human behavior. However, prior work shows that LLMs often produce\nhomogeneous outputs that fail to capture the rich diversity of human\nperspectives and behaviors. Thus, rather than trying to capture this diversity\nwith a single LLM agent, we propose a novel framework to construct a set of\nagents that collectively capture the diversity of a given human population.\nEach agent is an LLM whose behavior is steered by conditioning on a small set\nof human demonstrations (task-response pairs) through in-context learning. The\ncentral challenge is therefore to select a representative set of LLM agents\nfrom the exponentially large space of possible agents. We tackle this selection\nproblem from the lens of submodular optimization. In particular, we develop\nmethods that offer different trade-offs regarding time complexity and\nperformance guarantees. Extensive experiments in crowdsourcing and educational\ndomains demonstrate that our approach constructs agents that more effectively\nrepresent human populations compared to baselines. Moreover, behavioral\nanalyses on new tasks show that these agents reproduce the behavior patterns\nand perspectives of the students and annotators they are designed to represent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The difficulty and expense of obtaining large-scale human responses make\nLarge Language Models (LLMs) an attractive alternative and a promising proxy\nfor human behavior. However, prior work shows that LLMs often produce\nhomogeneous outputs that fail to capture the rich diversity of human\nperspectives and behaviors. Thus, rather than trying to capture this diversity\nwith a single LLM agent, we propose a novel framework to construct a set of\nagents that collectively capture the diversity of a given human population.\nEach agent is an LLM whose behavior is steered by conditioning on a small set\nof human demonstrations (task-response pairs) through in-context learning. The\ncentral challenge is therefore to select a representative set of LLM agents\nfrom the exponentially large space of possible agents. We tackle this selection\nproblem from the lens of submodular optimization. In particular, we develop\nmethods that offer different trade-offs regarding time complexity and\nperformance guarantees. Extensive experiments in crowdsourcing and educational\ndomains demonstrate that our approach constructs agents that more effectively\nrepresent human populations compared to baselines. Moreover, behavioral\nanalyses on new tasks show that these agents reproduce the behavior patterns\nand perspectives of the students and annotators they are designed to represent."
                },
                "authors": [
                    {
                        "name": "Manh Hung Nguyen"
                    },
                    {
                        "name": "Sebastian Tschiatschek"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07061v1",
                "updated": "2025-10-08T14:27:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    27,
                    2,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:27:02Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    27,
                    2,
                    2,
                    281,
                    0
                ],
                "title": "Revisiting Metric Reliability for Fine-grained Evaluation of Machine\n  Translation and Summarization in Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Metric Reliability for Fine-grained Evaluation of Machine\n  Translation and Summarization in Indian Languages"
                },
                "summary": "While automatic metrics drive progress in Machine Translation (MT) and Text\nSummarization (TS), existing metrics have been developed and validated almost\nexclusively for English and other high-resource languages. This narrow focus\nleaves Indian languages, spoken by over 1.5 billion people, largely overlooked,\ncasting doubt on the universality of current evaluation practices. To address\nthis gap, we introduce ITEM, a large-scale benchmark that systematically\nevaluates the alignment of 26 automatic metrics with human judgments across six\nmajor Indian languages, enriched with fine-grained annotations. Our extensive\nevaluation, covering agreement with human judgments, sensitivity to outliers,\nlanguage-specific reliability, inter-metric correlations, and resilience to\ncontrolled perturbations, reveals four central findings: (1) LLM-based\nevaluators show the strongest alignment with human judgments at both segment\nand system levels; (2) outliers exert a significant impact on metric-human\nagreement; (3) in TS, metrics are more effective at capturing content fidelity,\nwhereas in MT, they better reflect fluency; and (4) metrics differ in their\nrobustness and sensitivity when subjected to diverse perturbations.\nCollectively, these findings offer critical guidance for advancing metric\ndesign and evaluation in Indian languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While automatic metrics drive progress in Machine Translation (MT) and Text\nSummarization (TS), existing metrics have been developed and validated almost\nexclusively for English and other high-resource languages. This narrow focus\nleaves Indian languages, spoken by over 1.5 billion people, largely overlooked,\ncasting doubt on the universality of current evaluation practices. To address\nthis gap, we introduce ITEM, a large-scale benchmark that systematically\nevaluates the alignment of 26 automatic metrics with human judgments across six\nmajor Indian languages, enriched with fine-grained annotations. Our extensive\nevaluation, covering agreement with human judgments, sensitivity to outliers,\nlanguage-specific reliability, inter-metric correlations, and resilience to\ncontrolled perturbations, reveals four central findings: (1) LLM-based\nevaluators show the strongest alignment with human judgments at both segment\nand system levels; (2) outliers exert a significant impact on metric-human\nagreement; (3) in TS, metrics are more effective at capturing content fidelity,\nwhereas in MT, they better reflect fluency; and (4) metrics differ in their\nrobustness and sensitivity when subjected to diverse perturbations.\nCollectively, these findings offer critical guidance for advancing metric\ndesign and evaluation in Indian languages."
                },
                "authors": [
                    {
                        "name": "Amir Hossein Yari"
                    },
                    {
                        "name": "Kalmit Kulkarni"
                    },
                    {
                        "name": "Ahmad Raza Khan"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "arxiv_comment": "18 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04023v2",
                "updated": "2025-10-08T14:20:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    20,
                    50,
                    2,
                    281,
                    0
                ],
                "published": "2025-07-05T12:31:17Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    12,
                    31,
                    17,
                    5,
                    186,
                    0
                ],
                "title": "Do LLMs Overthink Basic Math Reasoning? Benchmarking the\n  Accuracy-Efficiency Tradeoff in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Overthink Basic Math Reasoning? Benchmarking the\n  Accuracy-Efficiency Tradeoff in Language Models"
                },
                "summary": "Large language models (LLMs) achieve impressive performance on complex\nmathematical benchmarks yet sometimes fail on basic math reasoning while\ngenerating unnecessarily verbose responses. In this paper, we present a\nsystematic benchmark and comprehensive empirical study to evaluate the\nefficiency of reasoning in LLMs, focusing on the fundamental tradeoff between\naccuracy and overthinking. First, we formalize the accuracy-verbosity tradeoff.\nSecond, we introduce the Overthinking Score, a harmonic-mean metric combining\naccuracy and token-efficiency for holistic model evaluation. Third, we\nestablish an evaluation protocol with dynamically-generated data across 14\nbasic math tasks. Fourth, we conduct a large-scale empirical study evaluating\n53 LLMs, including reasoning and quantized variants across different reasoning\nbudgets. Our findings reveal: 1) model performance on complex benchmarks does\nnot translate directly to basic math reasoning; 2) reasoning models generate\n~18 more tokens while sometimes achieving lower accuracy and exhibit\ncatastrophic collapse when token is constrained, dropping by ~28; 3) the\naccuracy-verbosity relationship is non-monotonic with extended reasoning\nbudgets yielding diminishing returns (GPT-5/o-series models show zero accuracy\ngain from low -> medium -> high reasoning effort). Our findings challenge the\nassumption that longer reasoning in LLMs necessarily improves mathematical\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve impressive performance on complex\nmathematical benchmarks yet sometimes fail on basic math reasoning while\ngenerating unnecessarily verbose responses. In this paper, we present a\nsystematic benchmark and comprehensive empirical study to evaluate the\nefficiency of reasoning in LLMs, focusing on the fundamental tradeoff between\naccuracy and overthinking. First, we formalize the accuracy-verbosity tradeoff.\nSecond, we introduce the Overthinking Score, a harmonic-mean metric combining\naccuracy and token-efficiency for holistic model evaluation. Third, we\nestablish an evaluation protocol with dynamically-generated data across 14\nbasic math tasks. Fourth, we conduct a large-scale empirical study evaluating\n53 LLMs, including reasoning and quantized variants across different reasoning\nbudgets. Our findings reveal: 1) model performance on complex benchmarks does\nnot translate directly to basic math reasoning; 2) reasoning models generate\n~18 more tokens while sometimes achieving lower accuracy and exhibit\ncatastrophic collapse when token is constrained, dropping by ~28; 3) the\naccuracy-verbosity relationship is non-monotonic with extended reasoning\nbudgets yielding diminishing returns (GPT-5/o-series models show zero accuracy\ngain from low -> medium -> high reasoning effort). Our findings challenge the\nassumption that longer reasoning in LLMs necessarily improves mathematical\nreasoning."
                },
                "authors": [
                    {
                        "name": "Gaurav Srivastava"
                    },
                    {
                        "name": "Aafiya Hussain"
                    },
                    {
                        "name": "Sriram Srinivasan"
                    },
                    {
                        "name": "Xuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xuan Wang"
                },
                "author": "Xuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07048v1",
                "updated": "2025-10-08T14:16:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    16,
                    20,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:16:20Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    16,
                    20,
                    2,
                    281,
                    0
                ],
                "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language\n  Models"
                },
                "summary": "Despite their remarkable natural language understanding capabilities, Large\nLanguage Models (LLMs) have been underutilized for retrieval tasks. We present\nSearch-R3, a novel framework that addresses this limitation by adapting LLMs to\ngenerate search embeddings as a direct output of their reasoning process. Our\napproach exploits LLMs' chain-of-thought capabilities, allowing them to produce\nmore effective embeddings by reasoning step-by-step through complex semantic\nanalyses. We implement this through three complementary mechanisms. (1) a\nsupervised learning stage enables the model's ability to produce quality\nembeddings, (2) a reinforcement learning (RL) methodology that optimizes\nembedding generation alongside reasoning, and (3) a specialized RL environment\nthat efficiently handles evolving embedding representations without requiring\ncomplete corpus re-encoding at each training iteration. Our extensive\nevaluations on diverse benchmarks demonstrate that Search-R3 significantly\noutperforms prior methods by unifying the reasoning and embedding generation\nprocesses. This integrated post-training approach represents a substantial\nadvancement in handling complex knowledge-intensive tasks that require both\nsophisticated reasoning and effective information retrieval. Project page:\nhttps://github.com/ytgui/Search-R3",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable natural language understanding capabilities, Large\nLanguage Models (LLMs) have been underutilized for retrieval tasks. We present\nSearch-R3, a novel framework that addresses this limitation by adapting LLMs to\ngenerate search embeddings as a direct output of their reasoning process. Our\napproach exploits LLMs' chain-of-thought capabilities, allowing them to produce\nmore effective embeddings by reasoning step-by-step through complex semantic\nanalyses. We implement this through three complementary mechanisms. (1) a\nsupervised learning stage enables the model's ability to produce quality\nembeddings, (2) a reinforcement learning (RL) methodology that optimizes\nembedding generation alongside reasoning, and (3) a specialized RL environment\nthat efficiently handles evolving embedding representations without requiring\ncomplete corpus re-encoding at each training iteration. Our extensive\nevaluations on diverse benchmarks demonstrate that Search-R3 significantly\noutperforms prior methods by unifying the reasoning and embedding generation\nprocesses. This integrated post-training approach represents a substantial\nadvancement in handling complex knowledge-intensive tasks that require both\nsophisticated reasoning and effective information retrieval. Project page:\nhttps://github.com/ytgui/Search-R3"
                },
                "authors": [
                    {
                        "name": "Yuntao Gui"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07043v1",
                "updated": "2025-10-08T14:09:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    9,
                    46,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:09:46Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    9,
                    46,
                    2,
                    281,
                    0
                ],
                "title": "COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference\n  Optimization"
                },
                "summary": "Real-world large language model (LLM) agents must master strategic tool use\nand user preference optimization through multi-turn interactions to assist\nusers with complex planning tasks. We introduce COMPASS (Constrained\nOptimization through Multi-turn Planning and Strategic Solutions), a benchmark\nthat evaluates agents on realistic travel-planning scenarios. We cast travel\nplanning as a constrained preference optimization problem, where agents must\nsatisfy hard constraints while simultaneously optimizing soft user preferences.\nTo support this, we build a realistic travel database covering transportation,\naccommodation, and ticketing for 20 U.S. National Parks, along with a\ncomprehensive tool ecosystem that mirrors commercial booking platforms.\nEvaluating state-of-the-art models, we uncover two critical gaps: (i) an\nacceptable-optimal gap, where agents reliably meet constraints but fail to\noptimize preferences, and (ii) a plan-coordination gap, where performance\ncollapses on multi-service (flight and hotel) coordination tasks, especially\nfor open-source models. By grounding reasoning and planning in a practical,\nuser-facing domain, COMPASS provides a benchmark that directly measures an\nagent's ability to optimize user preferences in realistic tasks, bridging\ntheoretical advances with real-world impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world large language model (LLM) agents must master strategic tool use\nand user preference optimization through multi-turn interactions to assist\nusers with complex planning tasks. We introduce COMPASS (Constrained\nOptimization through Multi-turn Planning and Strategic Solutions), a benchmark\nthat evaluates agents on realistic travel-planning scenarios. We cast travel\nplanning as a constrained preference optimization problem, where agents must\nsatisfy hard constraints while simultaneously optimizing soft user preferences.\nTo support this, we build a realistic travel database covering transportation,\naccommodation, and ticketing for 20 U.S. National Parks, along with a\ncomprehensive tool ecosystem that mirrors commercial booking platforms.\nEvaluating state-of-the-art models, we uncover two critical gaps: (i) an\nacceptable-optimal gap, where agents reliably meet constraints but fail to\noptimize preferences, and (ii) a plan-coordination gap, where performance\ncollapses on multi-service (flight and hotel) coordination tasks, especially\nfor open-source models. By grounding reasoning and planning in a practical,\nuser-facing domain, COMPASS provides a benchmark that directly measures an\nagent's ability to optimize user preferences in realistic tasks, bridging\ntheoretical advances with real-world impact."
                },
                "authors": [
                    {
                        "name": "Tian Qin"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Ting-Yao Hu"
                    },
                    {
                        "name": "Raviteja Vemulapalli"
                    },
                    {
                        "name": "Hema Swetha Koppula"
                    },
                    {
                        "name": "Zhiyang Xu"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Mert Cemri"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Meng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Cao"
                },
                "author": "Meng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19366v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19366v3",
                "updated": "2025-10-08T14:06:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    6,
                    26,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-26T18:54:52Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    18,
                    54,
                    52,
                    1,
                    238,
                    0
                ],
                "title": "Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying\n  Hallucinations in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying\n  Hallucinations in Multimodal LLMs"
                },
                "summary": "Hallucinations in LLMs--especially in multimodal settings--undermine\nreliability. We present a rigorous, information-geometric framework in\ndiffusion dynamics that quantifies hallucination in MLLMs: model outputs are\nembedded spectrally on multimodal graph Laplacians, and gaps to a truth\nmanifold define a semantic-distortion metric. We derive Courant--Fischer bounds\non a temperature-dependent hallucination energy and use RKHS eigenmodes to\nobtain modality-aware, interpretable measures that track evolution over prompts\nand time. This reframes hallucination as measurable and bounded, providing a\nprincipled basis for evaluation and mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in LLMs--especially in multimodal settings--undermine\nreliability. We present a rigorous, information-geometric framework in\ndiffusion dynamics that quantifies hallucination in MLLMs: model outputs are\nembedded spectrally on multimodal graph Laplacians, and gaps to a truth\nmanifold define a semantic-distortion metric. We derive Courant--Fischer bounds\non a temperature-dependent hallucination energy and use RKHS eigenmodes to\nobtain modality-aware, interpretable measures that track evolution over prompts\nand time. This reframes hallucination as measurable and bounded, providing a\nprincipled basis for evaluation and mitigation."
                },
                "authors": [
                    {
                        "name": "Supratik Sarkar"
                    },
                    {
                        "name": "Swagatam Das"
                    }
                ],
                "author_detail": {
                    "name": "Swagatam Das"
                },
                "author": "Swagatam Das",
                "arxiv_comment": "29 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19366v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19366v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53B21, 46E22 (Primary), 68R10 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07041v1",
                "updated": "2025-10-08T14:06:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    6,
                    17,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:06:17Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    6,
                    17,
                    2,
                    281,
                    0
                ],
                "title": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-Bench: A Comprehensive Understanding of U-Net through 100-Variant\n  Benchmarking"
                },
                "summary": "Over the past decade, U-Net has been the dominant architecture in medical\nimage segmentation, leading to the development of thousands of U-shaped\nvariants. Despite its widespread adoption, there is still no comprehensive\nbenchmark to systematically evaluate their performance and utility, largely\nbecause of insufficient statistical validation and limited consideration of\nefficiency and generalization across diverse datasets. To bridge this gap, we\npresent U-Bench, the first large-scale, statistically rigorous benchmark that\nevaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our\ncontributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates\nmodels along three key dimensions: statistical robustness, zero-shot\ngeneralization, and computational efficiency. We introduce a novel metric,\nU-Score, which jointly captures the performance-efficiency trade-off, offering\na deployment-oriented perspective on model progress. (2) Systematic Analysis\nand Model Selection Guidance: We summarize key findings from the large-scale\nevaluation and systematically analyze the impact of dataset characteristics and\narchitectural paradigms on model performance. Based on these insights, we\npropose a model advisor agent to guide researchers in selecting the most\nsuitable models for specific datasets and tasks. (3) Public Availability: We\nprovide all code, models, protocols, and weights, enabling the community to\nreproduce our results and extend the benchmark with future methods. In summary,\nU-Bench not only exposes gaps in previous evaluations but also establishes a\nfoundation for fair, reproducible, and practically relevant benchmarking in the\nnext decade of U-Net-based segmentation models. The project can be accessed at:\nhttps://fenghetan9.github.io/ubench. Code is available at:\nhttps://github.com/FengheTan9/U-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decade, U-Net has been the dominant architecture in medical\nimage segmentation, leading to the development of thousands of U-shaped\nvariants. Despite its widespread adoption, there is still no comprehensive\nbenchmark to systematically evaluate their performance and utility, largely\nbecause of insufficient statistical validation and limited consideration of\nefficiency and generalization across diverse datasets. To bridge this gap, we\npresent U-Bench, the first large-scale, statistically rigorous benchmark that\nevaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our\ncontributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates\nmodels along three key dimensions: statistical robustness, zero-shot\ngeneralization, and computational efficiency. We introduce a novel metric,\nU-Score, which jointly captures the performance-efficiency trade-off, offering\na deployment-oriented perspective on model progress. (2) Systematic Analysis\nand Model Selection Guidance: We summarize key findings from the large-scale\nevaluation and systematically analyze the impact of dataset characteristics and\narchitectural paradigms on model performance. Based on these insights, we\npropose a model advisor agent to guide researchers in selecting the most\nsuitable models for specific datasets and tasks. (3) Public Availability: We\nprovide all code, models, protocols, and weights, enabling the community to\nreproduce our results and extend the benchmark with future methods. In summary,\nU-Bench not only exposes gaps in previous evaluations but also establishes a\nfoundation for fair, reproducible, and practically relevant benchmarking in the\nnext decade of U-Net-based segmentation models. The project can be accessed at:\nhttps://fenghetan9.github.io/ubench. Code is available at:\nhttps://github.com/FengheTan9/U-Bench."
                },
                "authors": [
                    {
                        "name": "Fenghe Tang"
                    },
                    {
                        "name": "Chengqi Dong"
                    },
                    {
                        "name": "Wenxin Ma"
                    },
                    {
                        "name": "Zikang Xu"
                    },
                    {
                        "name": "Heqin Zhu"
                    },
                    {
                        "name": "Zihang Jiang"
                    },
                    {
                        "name": "Rongsheng Wang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Chenxu Wu"
                    },
                    {
                        "name": "Shaohua Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shaohua Kevin Zhou"
                },
                "author": "Shaohua Kevin Zhou",
                "arxiv_comment": "54 pages. The project can be accessed at:\n  https://fenghetan9.github.io/ubench. Code is available at:\n  https://github.com/FengheTan9/U-Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07038v1",
                "updated": "2025-10-08T14:04:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    4,
                    27,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T14:04:27Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    4,
                    27,
                    2,
                    281,
                    0
                ],
                "title": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive\n  Tool Use with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive\n  Tool Use with Reinforcement Learning"
                },
                "summary": "Recent advances in large language models (LLMs) have popularized test-time\nscaling, where models generate additional reasoning tokens before producing\nfinal answers. These approaches have demonstrated significant performance\nimprovements on benchmarks involving mathematical reasoning. However, language\nmodels relying solely on direct inference still struggle with tasks demanding\nup-to-date knowledge or computational tools such as calculators and code\ninterpreters for complex arithmetic operations. To overcome these limitations,\nwe propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement\nlearning framework that systematically integrates multi-hop reasoning with\nadaptive tool-calling capabilities. Our approach employs a modified version of\nDynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,\nwhich we adapt specifically for tool invocation scenarios, enabling models to\ndynamically interleave complex reasoning with on-demand tool usage (including\nsearch APIs and Python interpreters).\n  To support this research, we introduce two new datasets: TAPO-easy-60K and\nTAPO-hard-18K, specifically designed to train and evaluate both fact-based\nreasoning and mathematical calculation capabilities. Our experiments on\nQwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,\nwith both models achieving state-of-the-art performance on tasks requiring\nexternal knowledge and mathematical computation among methods with comparable\nparameters. Notably, TAPO achieves more efficient tool utilization than\nbaseline methods while preventing excessive calls caused by reward hacking.\nThese results highlight the significant potential of combining advanced\nreasoning with tool usage to enhance model performance in knowledge-intensive\nand computationally demanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have popularized test-time\nscaling, where models generate additional reasoning tokens before producing\nfinal answers. These approaches have demonstrated significant performance\nimprovements on benchmarks involving mathematical reasoning. However, language\nmodels relying solely on direct inference still struggle with tasks demanding\nup-to-date knowledge or computational tools such as calculators and code\ninterpreters for complex arithmetic operations. To overcome these limitations,\nwe propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement\nlearning framework that systematically integrates multi-hop reasoning with\nadaptive tool-calling capabilities. Our approach employs a modified version of\nDynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,\nwhich we adapt specifically for tool invocation scenarios, enabling models to\ndynamically interleave complex reasoning with on-demand tool usage (including\nsearch APIs and Python interpreters).\n  To support this research, we introduce two new datasets: TAPO-easy-60K and\nTAPO-hard-18K, specifically designed to train and evaluate both fact-based\nreasoning and mathematical calculation capabilities. Our experiments on\nQwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,\nwith both models achieving state-of-the-art performance on tasks requiring\nexternal knowledge and mathematical computation among methods with comparable\nparameters. Notably, TAPO achieves more efficient tool utilization than\nbaseline methods while preventing excessive calls caused by reward hacking.\nThese results highlight the significant potential of combining advanced\nreasoning with tool usage to enhance model performance in knowledge-intensive\nand computationally demanding tasks."
                },
                "authors": [
                    {
                        "name": "Wenxun Wu"
                    },
                    {
                        "name": "Yuanyang Li"
                    },
                    {
                        "name": "Guhan Chen"
                    },
                    {
                        "name": "Linyue Wang"
                    },
                    {
                        "name": "Hongyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Chen"
                },
                "author": "Hongyang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07037v2",
                "updated": "2025-10-09T04:45:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    4,
                    45,
                    15,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-08T14:04:14Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    4,
                    14,
                    2,
                    281,
                    0
                ],
                "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era\n  of Large Language Models"
                },
                "summary": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multiling ual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing 308\nstudies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+\nlanguages. We classify recent advances by architecture, training strategy, and\nevaluation methodology, outlining how LLMs have reshaped CSW modeling and what\nchallenges persist. The paper concludes with a roadmap emphasizing the need for\ninclusive datasets, fair evaluation, and linguistically grounded models to\nachieve truly multilingual intelligence. A curated collection of all resources\nis maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multiling ual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing 308\nstudies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+\nlanguages. We classify recent advances by architecture, training strategy, and\nevaluation methodology, outlining how LLMs have reshaped CSW modeling and what\nchallenges persist. The paper concludes with a roadmap emphasizing the need for\ninclusive datasets, fair evaluation, and linguistically grounded models to\nachieve truly multilingual intelligence. A curated collection of all resources\nis maintained at https://github.com/lingo-iitgn/awesome-code-mixing/."
                },
                "authors": [
                    {
                        "name": "Rajvee Sheth"
                    },
                    {
                        "name": "Samridhi Raj Sinha"
                    },
                    {
                        "name": "Mahavir Patil"
                    },
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Mayank Singh"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Singh"
                },
                "author": "Mayank Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07024v2",
                "updated": "2025-10-09T07:23:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    7,
                    23,
                    3,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-08T13:48:38Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    48,
                    38,
                    2,
                    281,
                    0
                ],
                "title": "Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge"
                },
                "summary": "LLMs are remarkable artifacts that have revolutionized a range of NLP and AI\ntasks. A significant contributor is their factual knowledge, which, to date,\nremains poorly understood, and is usually analyzed from biased samples. In this\npaper, we take a deep tour into the factual knowledge (or beliefs) of a\nfrontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited\nset of 100 million beliefs of one of the strongest currently available frontier\nLLMs, GPT-4.1. We find that the models' factual knowledge differs quite\nsignificantly from established knowledge bases, and that its accuracy is\nsignificantly lower than indicated by previous benchmarks. We also find that\ninconsistency, ambiguity and hallucinations are major issues, shedding light on\nfuture research opportunities concerning factual LLM knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are remarkable artifacts that have revolutionized a range of NLP and AI\ntasks. A significant contributor is their factual knowledge, which, to date,\nremains poorly understood, and is usually analyzed from biased samples. In this\npaper, we take a deep tour into the factual knowledge (or beliefs) of a\nfrontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited\nset of 100 million beliefs of one of the strongest currently available frontier\nLLMs, GPT-4.1. We find that the models' factual knowledge differs quite\nsignificantly from established knowledge bases, and that its accuracy is\nsignificantly lower than indicated by previous benchmarks. We also find that\ninconsistency, ambiguity and hallucinations are major issues, shedding light on\nfuture research opportunities concerning factual LLM knowledge."
                },
                "authors": [
                    {
                        "name": "Shrestha Ghosh"
                    },
                    {
                        "name": "Luca Giordano"
                    },
                    {
                        "name": "Yujia Hu"
                    },
                    {
                        "name": "Tuan-Phong Nguyen"
                    },
                    {
                        "name": "Simon Razniewski"
                    }
                ],
                "author_detail": {
                    "name": "Simon Razniewski"
                },
                "author": "Simon Razniewski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07019v1",
                "updated": "2025-10-08T13:44:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    44,
                    57,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T13:44:57Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    44,
                    57,
                    2,
                    281,
                    0
                ],
                "title": "Native Hybrid Attention for Efficient Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Native Hybrid Attention for Efficient Sequence Modeling"
                },
                "summary": "Transformers excel at sequence modeling but face quadratic complexity, while\nlinear attention offers improved efficiency but often compromises recall\naccuracy over long contexts. In this work, we introduce Native Hybrid Attention\n(NHA), a novel hybrid architecture of linear and full attention that integrates\nboth intra \\& inter-layer hybridization into a unified layer design. NHA\nmaintains long-term context in key-value slots updated by a linear RNN, and\naugments them with short-term tokens from a sliding window. A single\n\\texttt{softmax attention} operation is then applied over all keys and values,\nenabling per-token and per-head context-dependent weighting without requiring\nadditional fusion parameters. The inter-layer behavior is controlled through a\nsingle hyperparameter, the sliding window size, which allows smooth adjustment\nbetween purely linear and full attention while keeping all layers structurally\nuniform. Experimental results show that NHA surpasses Transformers and other\nhybrid baselines on recall-intensive and commonsense reasoning tasks.\nFurthermore, pretrained LLMs can be structurally hybridized with NHA, achieving\ncompetitive accuracy while delivering significant efficiency gains. Code is\navailable at https://github.com/JusenD/NHA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers excel at sequence modeling but face quadratic complexity, while\nlinear attention offers improved efficiency but often compromises recall\naccuracy over long contexts. In this work, we introduce Native Hybrid Attention\n(NHA), a novel hybrid architecture of linear and full attention that integrates\nboth intra \\& inter-layer hybridization into a unified layer design. NHA\nmaintains long-term context in key-value slots updated by a linear RNN, and\naugments them with short-term tokens from a sliding window. A single\n\\texttt{softmax attention} operation is then applied over all keys and values,\nenabling per-token and per-head context-dependent weighting without requiring\nadditional fusion parameters. The inter-layer behavior is controlled through a\nsingle hyperparameter, the sliding window size, which allows smooth adjustment\nbetween purely linear and full attention while keeping all layers structurally\nuniform. Experimental results show that NHA surpasses Transformers and other\nhybrid baselines on recall-intensive and commonsense reasoning tasks.\nFurthermore, pretrained LLMs can be structurally hybridized with NHA, achieving\ncompetitive accuracy while delivering significant efficiency gains. Code is\navailable at https://github.com/JusenD/NHA."
                },
                "authors": [
                    {
                        "name": "Jusen Du"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "Technical report, 16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05516v2",
                "updated": "2025-10-08T13:33:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    33,
                    28,
                    2,
                    281,
                    0
                ],
                "published": "2025-06-05T18:58:12Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    18,
                    58,
                    12,
                    3,
                    156,
                    0
                ],
                "title": "Learning to Recover: Dynamic Reward Shaping with Wheel-Leg Coordination\n  for Fallen Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Recover: Dynamic Reward Shaping with Wheel-Leg Coordination\n  for Fallen Robots"
                },
                "summary": "Adaptive recovery from fall incidents are essential skills for the practical\ndeployment of wheeled-legged robots, which uniquely combine the agility of legs\nwith the speed of wheels for rapid recovery. However, traditional methods\nrelying on preplanned recovery motions, simplified dynamics or sparse rewards\noften fail to produce robust recovery policies. This paper presents a\nlearning-based framework integrating Episode-based Dynamic Reward Shaping and\ncurriculum learning, which dynamically balances exploration of diverse recovery\nmaneuvers with precise posture refinement. An asymmetric actor-critic\narchitecture accelerates training by leveraging privileged information in\nsimulation, while noise-injected observations enhance robustness against\nuncertainties. We further demonstrate that synergistic wheel-leg coordination\nreduces joint torque consumption by 15.8% and 26.2% and improves stabilization\nthrough energy transfer mechanisms. Extensive evaluations on two distinct\nquadruped platforms achieve recovery success rates up to 99.1% and 97.8%\nwithout platform-specific tuning. The supplementary material is available at\nhttps://boyuandeng.github.io/L2R-WheelLegCoordination/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive recovery from fall incidents are essential skills for the practical\ndeployment of wheeled-legged robots, which uniquely combine the agility of legs\nwith the speed of wheels for rapid recovery. However, traditional methods\nrelying on preplanned recovery motions, simplified dynamics or sparse rewards\noften fail to produce robust recovery policies. This paper presents a\nlearning-based framework integrating Episode-based Dynamic Reward Shaping and\ncurriculum learning, which dynamically balances exploration of diverse recovery\nmaneuvers with precise posture refinement. An asymmetric actor-critic\narchitecture accelerates training by leveraging privileged information in\nsimulation, while noise-injected observations enhance robustness against\nuncertainties. We further demonstrate that synergistic wheel-leg coordination\nreduces joint torque consumption by 15.8% and 26.2% and improves stabilization\nthrough energy transfer mechanisms. Extensive evaluations on two distinct\nquadruped platforms achieve recovery success rates up to 99.1% and 97.8%\nwithout platform-specific tuning. The supplementary material is available at\nhttps://boyuandeng.github.io/L2R-WheelLegCoordination/"
                },
                "authors": [
                    {
                        "name": "Boyuan Deng"
                    },
                    {
                        "name": "Luca Rossini"
                    },
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Dimitrios Kanoulas"
                    },
                    {
                        "name": "Nikolaos Tsagarakis"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Tsagarakis"
                },
                "author": "Nikolaos Tsagarakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09621v2",
                "updated": "2025-10-08T13:28:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    28,
                    52,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-13T08:53:13Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    8,
                    53,
                    13,
                    2,
                    225,
                    0
                ],
                "title": "Interpretable Robot Control via Structured Behavior Trees and Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Robot Control via Structured Behavior Trees and Large\n  Language Models"
                },
                "summary": "As intelligent robots become more integrated into human environments, there\nis a growing need for intuitive and reliable Human-Robot Interaction (HRI)\ninterfaces that are adaptable and more natural to interact with. Traditional\nrobot control methods often require users to adapt to interfaces or memorize\npredefined commands, limiting usability in dynamic, unstructured environments.\nThis paper presents a novel framework that bridges natural language\nunderstanding and robotic execution by combining Large Language Models (LLMs)\nwith Behavior Trees. This integration enables robots to interpret natural\nlanguage instructions given by users and translate them into executable actions\nby activating domain-specific plugins. The system supports scalable and modular\nintegration, with a primary focus on perception-based functionalities, such as\nperson tracking and hand gesture recognition. To evaluate the system, a series\nof real-world experiments was conducted across diverse environments.\nExperimental results demonstrate that the proposed approach is practical in\nreal-world scenarios, with an average cognition-to-execution accuracy of\napproximately 94%, making a significant contribution to HRI systems and robots.\nThe complete source code of the framework is publicly available at\nhttps://github.com/snt-arg/robot_suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As intelligent robots become more integrated into human environments, there\nis a growing need for intuitive and reliable Human-Robot Interaction (HRI)\ninterfaces that are adaptable and more natural to interact with. Traditional\nrobot control methods often require users to adapt to interfaces or memorize\npredefined commands, limiting usability in dynamic, unstructured environments.\nThis paper presents a novel framework that bridges natural language\nunderstanding and robotic execution by combining Large Language Models (LLMs)\nwith Behavior Trees. This integration enables robots to interpret natural\nlanguage instructions given by users and translate them into executable actions\nby activating domain-specific plugins. The system supports scalable and modular\nintegration, with a primary focus on perception-based functionalities, such as\nperson tracking and hand gesture recognition. To evaluate the system, a series\nof real-world experiments was conducted across diverse environments.\nExperimental results demonstrate that the proposed approach is practical in\nreal-world scenarios, with an average cognition-to-execution accuracy of\napproximately 94%, making a significant contribution to HRI systems and robots.\nThe complete source code of the framework is publicly available at\nhttps://github.com/snt-arg/robot_suite."
                },
                "authors": [
                    {
                        "name": "Ingrid Maéva Chekam"
                    },
                    {
                        "name": "Ines Pastor-Martinez"
                    },
                    {
                        "name": "Ali Tourani"
                    },
                    {
                        "name": "Jose Andres Millan-Romera"
                    },
                    {
                        "name": "Laura Ribeiro"
                    },
                    {
                        "name": "Pedro Miguel Bastos Soares"
                    },
                    {
                        "name": "Holger Voos"
                    },
                    {
                        "name": "Jose Luis Sanchez-Lopez"
                    }
                ],
                "author_detail": {
                    "name": "Jose Luis Sanchez-Lopez"
                },
                "author": "Jose Luis Sanchez-Lopez",
                "arxiv_comment": "15 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14161v2",
                "updated": "2025-10-08T13:24:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    24,
                    5,
                    2,
                    281,
                    0
                ],
                "published": "2025-06-17T03:50:57Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    3,
                    50,
                    57,
                    1,
                    168,
                    0
                ],
                "title": "MIST: Towards Multi-dimensional Implicit BiaS Evaluation of LLMs via\n  Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIST: Towards Multi-dimensional Implicit BiaS Evaluation of LLMs via\n  Theory of Mind"
                },
                "summary": "Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity\nfor reasoning about mental states, yet failures in this capacity often manifest\nas systematic implicit bias. Evaluating this bias is challenging, as\nconventional direct-query methods are susceptible to social desirability\neffects and fail to capture its subtle, multi-dimensional nature. To this end,\nwe propose an evaluation framework that leverages the Stereotype Content Model\n(SCM) to reconceptualize bias as a multi-dimensional failure in ToM across\nCompetence, Sociability, and Morality. The framework introduces two indirect\ntasks: the Word Association Bias Test (WABT) to assess implicit lexical\nassociations and the Affective Attribution Test (AAT) to measure covert\naffective leanings, both designed to probe latent stereotypes without\ntriggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs\ndemonstrate our framework's capacity to reveal complex bias structures,\nincluding pervasive sociability bias, multi-dimensional divergence, and\nasymmetric stereotype amplification, thereby providing a more robust\nmethodology for identifying the structural nature of implicit bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity\nfor reasoning about mental states, yet failures in this capacity often manifest\nas systematic implicit bias. Evaluating this bias is challenging, as\nconventional direct-query methods are susceptible to social desirability\neffects and fail to capture its subtle, multi-dimensional nature. To this end,\nwe propose an evaluation framework that leverages the Stereotype Content Model\n(SCM) to reconceptualize bias as a multi-dimensional failure in ToM across\nCompetence, Sociability, and Morality. The framework introduces two indirect\ntasks: the Word Association Bias Test (WABT) to assess implicit lexical\nassociations and the Affective Attribution Test (AAT) to measure covert\naffective leanings, both designed to probe latent stereotypes without\ntriggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs\ndemonstrate our framework's capacity to reveal complex bias structures,\nincluding pervasive sociability bias, multi-dimensional divergence, and\nasymmetric stereotype amplification, thereby providing a more robust\nmethodology for identifying the structural nature of implicit bias."
                },
                "authors": [
                    {
                        "name": "Yanlin Li"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Huimin Liu"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Yupeng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yupeng Hu"
                },
                "author": "Yupeng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07000v1",
                "updated": "2025-10-08T13:23:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    23,
                    45,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T13:23:45Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    23,
                    45,
                    2,
                    281,
                    0
                ],
                "title": "Pragyaan: Designing and Curating High-Quality Cultural Post-Training\n  Datasets for Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pragyaan: Designing and Curating High-Quality Cultural Post-Training\n  Datasets for Indian Languages"
                },
                "summary": "The effectiveness of Large Language Models (LLMs) depends heavily on the\navailability of high-quality post-training data, particularly\ninstruction-tuning and preference-based examples. Existing open-source\ndatasets, however, often lack multilingual coverage, cultural grounding, and\nsuffer from task diversity gaps that are especially pronounced for Indian\nlanguages. We introduce a human-in-the-loop pipeline that combines translations\nwith synthetic expansion to produce reliable and diverse Indic post-training\ndata. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and\nPragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56\nsub-categories, leveraging 57 diverse datasets. Our dataset protocol\nincorporates several often-overlooked dimensions and emphasize task diversity,\nmulti-turn dialogue, instruction fidelity, safety alignment, and preservation\nof cultural nuance, providing a foundation for more inclusive and effective\nmultilingual LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of Large Language Models (LLMs) depends heavily on the\navailability of high-quality post-training data, particularly\ninstruction-tuning and preference-based examples. Existing open-source\ndatasets, however, often lack multilingual coverage, cultural grounding, and\nsuffer from task diversity gaps that are especially pronounced for Indian\nlanguages. We introduce a human-in-the-loop pipeline that combines translations\nwith synthetic expansion to produce reliable and diverse Indic post-training\ndata. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and\nPragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56\nsub-categories, leveraging 57 diverse datasets. Our dataset protocol\nincorporates several often-overlooked dimensions and emphasize task diversity,\nmulti-turn dialogue, instruction fidelity, safety alignment, and preservation\nof cultural nuance, providing a foundation for more inclusive and effective\nmultilingual LLMs."
                },
                "authors": [
                    {
                        "name": "Neel Prabhanjan Rachamalla"
                    },
                    {
                        "name": "Aravind Konakalla"
                    },
                    {
                        "name": "Gautam Rajeev"
                    },
                    {
                        "name": "Ashish Kulkarni"
                    },
                    {
                        "name": "Chandra Khatri"
                    },
                    {
                        "name": "Shubham Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Shubham Agarwal"
                },
                "author": "Shubham Agarwal",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06999v1",
                "updated": "2025-10-08T13:22:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    22,
                    20,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T13:22:20Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    22,
                    20,
                    2,
                    281,
                    0
                ],
                "title": "Towards Reliable Retrieval in RAG Systems for Large Legal Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Retrieval in RAG Systems for Large Legal Datasets"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is a promising approach to mitigate\nhallucinations in Large Language Models (LLMs) for legal applications, but its\nreliability is critically dependent on the accuracy of the retrieval step. This\nis particularly challenging in the legal domain, where large databases of\nstructurally similar documents often cause retrieval systems to fail. In this\npaper, we address this challenge by first identifying and quantifying a\ncritical failure mode we term Document-Level Retrieval Mismatch (DRM), where\nthe retriever selects information from entirely incorrect source documents. To\nmitigate DRM, we investigate a simple and computationally efficient technique\nwhich we refer to as Summary-Augmented Chunking (SAC). This method enhances\neach text chunk with a document-level synthetic summary, thereby injecting\ncrucial global context that would otherwise be lost during a standard chunking\nprocess. Our experiments on a diverse set of legal information retrieval tasks\nshow that SAC greatly reduces DRM and, consequently, also improves text-level\nretrieval precision and recall. Interestingly, we find that a generic\nsummarization strategy outperforms an approach that incorporates legal expert\ndomain knowledge to target specific legal elements. Our work provides evidence\nthat this practical, scalable, and easily integrable technique enhances the\nreliability of RAG systems when applied to large-scale legal document datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a promising approach to mitigate\nhallucinations in Large Language Models (LLMs) for legal applications, but its\nreliability is critically dependent on the accuracy of the retrieval step. This\nis particularly challenging in the legal domain, where large databases of\nstructurally similar documents often cause retrieval systems to fail. In this\npaper, we address this challenge by first identifying and quantifying a\ncritical failure mode we term Document-Level Retrieval Mismatch (DRM), where\nthe retriever selects information from entirely incorrect source documents. To\nmitigate DRM, we investigate a simple and computationally efficient technique\nwhich we refer to as Summary-Augmented Chunking (SAC). This method enhances\neach text chunk with a document-level synthetic summary, thereby injecting\ncrucial global context that would otherwise be lost during a standard chunking\nprocess. Our experiments on a diverse set of legal information retrieval tasks\nshow that SAC greatly reduces DRM and, consequently, also improves text-level\nretrieval precision and recall. Interestingly, we find that a generic\nsummarization strategy outperforms an approach that incorporates legal expert\ndomain knowledge to target specific legal elements. Our work provides evidence\nthat this practical, scalable, and easily integrable technique enhances the\nreliability of RAG systems when applied to large-scale legal document datasets."
                },
                "authors": [
                    {
                        "name": "Markus Reuter"
                    },
                    {
                        "name": "Tobias Lingenberg"
                    },
                    {
                        "name": "Rūta Liepiņa"
                    },
                    {
                        "name": "Francesca Lagioia"
                    },
                    {
                        "name": "Marco Lippi"
                    },
                    {
                        "name": "Giovanni Sartor"
                    },
                    {
                        "name": "Andrea Passerini"
                    },
                    {
                        "name": "Burcu Sayin"
                    }
                ],
                "author_detail": {
                    "name": "Burcu Sayin"
                },
                "author": "Burcu Sayin",
                "arxiv_comment": "Accepted for the 7th Natural Legal Language Processing Workshop (NLLP\n  2025), co-located with EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; K.5.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06997v1",
                "updated": "2025-10-08T13:20:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    20,
                    40,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T13:20:40Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    20,
                    40,
                    2,
                    281,
                    0
                ],
                "title": "The Limits of Goal-Setting Theory in LLM-Driven Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Limits of Goal-Setting Theory in LLM-Driven Assessment"
                },
                "summary": "Many users interact with AI tools like ChatGPT using a mental model that\ntreats the system as human-like, which we call Model H. According to\ngoal-setting theory, increased specificity in goals should reduce performance\nvariance. If Model H holds, then prompting a chatbot with more detailed\ninstructions should lead to more consistent evaluation behavior.\n  This paper tests that assumption through a controlled experiment in which\nChatGPT evaluated 29 student submissions using four prompts with increasing\nspecificity. We measured consistency using intra-rater reliability (Cohen's\nKappa) across repeated runs.\n  Contrary to expectations, performance did not improve consistently with\nincreased prompt specificity, and performance variance remained largely\nunchanged. These findings challenge the assumption that LLMs behave like human\nevaluators and highlight the need for greater robustness and improved input\nintegration in future model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many users interact with AI tools like ChatGPT using a mental model that\ntreats the system as human-like, which we call Model H. According to\ngoal-setting theory, increased specificity in goals should reduce performance\nvariance. If Model H holds, then prompting a chatbot with more detailed\ninstructions should lead to more consistent evaluation behavior.\n  This paper tests that assumption through a controlled experiment in which\nChatGPT evaluated 29 student submissions using four prompts with increasing\nspecificity. We measured consistency using intra-rater reliability (Cohen's\nKappa) across repeated runs.\n  Contrary to expectations, performance did not improve consistently with\nincreased prompt specificity, and performance variance remained largely\nunchanged. These findings challenge the assumption that LLMs behave like human\nevaluators and highlight the need for greater robustness and improved input\nintegration in future model development."
                },
                "authors": [
                    {
                        "name": "Mrityunjay Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Mrityunjay Kumar"
                },
                "author": "Mrityunjay Kumar",
                "arxiv_comment": "Accepted at T4E 2025 for poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14146v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14146v4",
                "updated": "2025-10-08T13:20:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    20,
                    32,
                    2,
                    281,
                    0
                ],
                "published": "2025-08-19T16:37:19Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    37,
                    19,
                    1,
                    231,
                    0
                ],
                "title": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based\n  Peer Review Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based\n  Peer Review Automation"
                },
                "summary": "With the rapid growth of academic publications, peer review has become an\nessential yet time-consuming responsibility within the research community.\nLarge Language Models (LLMs) have increasingly been adopted to assist in the\ngeneration of review comments; however, current LLM-based review tasks lack a\nunified evaluation benchmark to rigorously assess the models' ability to\nproduce comprehensive, accurate, and human-aligned assessments, particularly in\nscenarios involving multimodal content such as figures and tables. To address\nthis gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans\nmultiple disciplines and modalities. MMReview includes multimodal content and\nexpert-written review comments for 240 papers across 17 research domains within\nfour major academic disciplines: Artificial Intelligence, Natural Sciences,\nEngineering Sciences, and Social Sciences. We design a total of 13 tasks\ngrouped into four core categories, aimed at evaluating the performance of LLMs\nand Multimodal LLMs (MLLMs) in step-wise review generation, outcome\nformulation, alignment with human preferences, and robustness to adversarial\ninput manipulation. Extensive experiments conducted on 16 open-source models\nand 5 advanced closed-source models demonstrate the thoroughness of the\nbenchmark. We envision MMReview as a critical step toward establishing a\nstandardized foundation for the development of automated peer review systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of academic publications, peer review has become an\nessential yet time-consuming responsibility within the research community.\nLarge Language Models (LLMs) have increasingly been adopted to assist in the\ngeneration of review comments; however, current LLM-based review tasks lack a\nunified evaluation benchmark to rigorously assess the models' ability to\nproduce comprehensive, accurate, and human-aligned assessments, particularly in\nscenarios involving multimodal content such as figures and tables. To address\nthis gap, we propose \\textbf{MMReview}, a comprehensive benchmark that spans\nmultiple disciplines and modalities. MMReview includes multimodal content and\nexpert-written review comments for 240 papers across 17 research domains within\nfour major academic disciplines: Artificial Intelligence, Natural Sciences,\nEngineering Sciences, and Social Sciences. We design a total of 13 tasks\ngrouped into four core categories, aimed at evaluating the performance of LLMs\nand Multimodal LLMs (MLLMs) in step-wise review generation, outcome\nformulation, alignment with human preferences, and robustness to adversarial\ninput manipulation. Extensive experiments conducted on 16 open-source models\nand 5 advanced closed-source models demonstrate the thoroughness of the\nbenchmark. We envision MMReview as a critical step toward establishing a\nstandardized foundation for the development of automated peer review systems."
                },
                "authors": [
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Jiacheng Ruan"
                    },
                    {
                        "name": "Zongyun Zhang"
                    },
                    {
                        "name": "Jingsheng Gao"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14146v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14146v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06994v1",
                "updated": "2025-10-08T13:18:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    18,
                    42,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T13:18:42Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    18,
                    42,
                    2,
                    281,
                    0
                ],
                "title": "RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning"
                },
                "summary": "This paper presents the vision, scientific contributions, and technical\ndetails of RedTWIZ: an adaptive and diverse multi-turn red teaming framework,\nto audit the robustness of Large Language Models (LLMs) in AI-assisted software\ndevelopment. Our work is driven by three major research streams: (1) robust and\nsystematic assessment of LLM conversational jailbreaks; (2) a diverse\ngenerative multi-turn attack suite, supporting compositional, realistic and\ngoal-oriented jailbreak conversational strategies; and (3) a hierarchical\nattack planner, which adaptively plans, serializes, and triggers attacks\ntailored to specific LLM's vulnerabilities. Together, these contributions form\na unified framework -- combining assessment, attack generation, and strategic\nplanning -- to comprehensively evaluate and expose weaknesses in LLMs'\nrobustness. Extensive evaluation is conducted to systematically assess and\nanalyze the performance of the overall system and each component. Experimental\nresults demonstrate that our multi-turn adversarial attack strategies can\nsuccessfully lead state-of-the-art LLMs to produce unsafe generations,\nhighlighting the pressing need for more research into enhancing LLM's\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the vision, scientific contributions, and technical\ndetails of RedTWIZ: an adaptive and diverse multi-turn red teaming framework,\nto audit the robustness of Large Language Models (LLMs) in AI-assisted software\ndevelopment. Our work is driven by three major research streams: (1) robust and\nsystematic assessment of LLM conversational jailbreaks; (2) a diverse\ngenerative multi-turn attack suite, supporting compositional, realistic and\ngoal-oriented jailbreak conversational strategies; and (3) a hierarchical\nattack planner, which adaptively plans, serializes, and triggers attacks\ntailored to specific LLM's vulnerabilities. Together, these contributions form\na unified framework -- combining assessment, attack generation, and strategic\nplanning -- to comprehensively evaluate and expose weaknesses in LLMs'\nrobustness. Extensive evaluation is conducted to systematically assess and\nanalyze the performance of the overall system and each component. Experimental\nresults demonstrate that our multi-turn adversarial attack strategies can\nsuccessfully lead state-of-the-art LLMs to produce unsafe generations,\nhighlighting the pressing need for more research into enhancing LLM's\nrobustness."
                },
                "authors": [
                    {
                        "name": "Artur Horal"
                    },
                    {
                        "name": "Daniel Pina"
                    },
                    {
                        "name": "Henrique Paz"
                    },
                    {
                        "name": "Iago Paulo"
                    },
                    {
                        "name": "João Soares"
                    },
                    {
                        "name": "Rafael Ferreira"
                    },
                    {
                        "name": "Diogo Tavares"
                    },
                    {
                        "name": "Diogo Glória-Silva"
                    },
                    {
                        "name": "João Magalhães"
                    },
                    {
                        "name": "David Semedo"
                    }
                ],
                "author_detail": {
                    "name": "David Semedo"
                },
                "author": "David Semedo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13430v3",
                "updated": "2025-10-08T13:18:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    18,
                    30,
                    2,
                    281,
                    0
                ],
                "published": "2025-06-16T12:47:37Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    12,
                    47,
                    37,
                    0,
                    167,
                    0
                ],
                "title": "Uncertainty-Aware Remaining Lifespan Prediction from Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Remaining Lifespan Prediction from Images"
                },
                "summary": "Predicting mortality-related outcomes from images offers the prospect of\naccessible, noninvasive, and scalable health screening. We present a method\nthat leverages pretrained vision transformer foundation models to estimate\nremaining lifespan from facial and whole-body images, alongside robust\nuncertainty quantification. We show that predictive uncertainty varies\nsystematically with the true remaining lifespan, and that this uncertainty can\nbe effectively modeled by learning a Gaussian distribution for each sample. Our\napproach achieves state-of-the-art mean absolute error (MAE) of 7.41 years on\nan established dataset, and further achieves 4.91 and 4.99 years MAE on two\nnew, higher-quality datasets curated and published in this work. Importantly,\nour models provide calibrated uncertainty estimates, as demonstrated by a\nbucketed expected calibration error of 0.82 years on the Faces Dataset. While\nnot intended for clinical deployment, these results highlight the potential of\nextracting medically relevant signals from images. We make all code and\ndatasets available to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting mortality-related outcomes from images offers the prospect of\naccessible, noninvasive, and scalable health screening. We present a method\nthat leverages pretrained vision transformer foundation models to estimate\nremaining lifespan from facial and whole-body images, alongside robust\nuncertainty quantification. We show that predictive uncertainty varies\nsystematically with the true remaining lifespan, and that this uncertainty can\nbe effectively modeled by learning a Gaussian distribution for each sample. Our\napproach achieves state-of-the-art mean absolute error (MAE) of 7.41 years on\nan established dataset, and further achieves 4.91 and 4.99 years MAE on two\nnew, higher-quality datasets curated and published in this work. Importantly,\nour models provide calibrated uncertainty estimates, as demonstrated by a\nbucketed expected calibration error of 0.82 years on the Faces Dataset. While\nnot intended for clinical deployment, these results highlight the potential of\nextracting medically relevant signals from images. We make all code and\ndatasets available to facilitate further research."
                },
                "authors": [
                    {
                        "name": "Tristan Kenneweg"
                    },
                    {
                        "name": "Philip Kenneweg"
                    },
                    {
                        "name": "Barbara Hammer"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Hammer"
                },
                "author": "Barbara Hammer",
                "arxiv_comment": "Submitted to ISVC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06989v1",
                "updated": "2025-10-08T13:13:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    13,
                    18,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T13:13:18Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    13,
                    18,
                    2,
                    281,
                    0
                ],
                "title": "Human-aligned AI Model Cards with Weighted Hierarchy Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-aligned AI Model Cards with Weighted Hierarchy Architecture"
                },
                "summary": "The proliferation of Large Language Models (LLMs) has led to a burgeoning\necosystem of specialized, domain-specific models. While this rapid growth\naccelerates innovation, it has simultaneously created significant challenges in\nmodel discovery and adoption. Users struggle to navigate this landscape due to\ninconsistent, incomplete, and imbalanced documentation across platforms.\nExisting documentation frameworks, such as Model Cards and FactSheets, attempt\nto standardize reporting but are often static, predominantly qualitative, and\nlack the quantitative mechanisms needed for rigorous cross-model comparison.\nThis gap exacerbates model underutilization and hinders responsible adoption.\nTo address these shortcomings, we introduce the Comprehensive Responsible AI\nModel Card Framework (CRAI-MCF), a novel approach that transitions from static\ndisclosures to actionable, human-aligned documentation. Grounded in Value\nSensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240\nopen-source projects, distilling 217 parameters into an eight-module,\nvalue-aligned architecture. Our framework introduces a quantitative sufficiency\ncriterion to operationalize evaluation and enables rigorous cross-model\ncomparison under a unified scheme. By balancing technical, ethical, and\noperational dimensions, CRAI-MCF empowers practitioners to efficiently assess,\nselect, and adopt LLMs with greater confidence and operational integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Large Language Models (LLMs) has led to a burgeoning\necosystem of specialized, domain-specific models. While this rapid growth\naccelerates innovation, it has simultaneously created significant challenges in\nmodel discovery and adoption. Users struggle to navigate this landscape due to\ninconsistent, incomplete, and imbalanced documentation across platforms.\nExisting documentation frameworks, such as Model Cards and FactSheets, attempt\nto standardize reporting but are often static, predominantly qualitative, and\nlack the quantitative mechanisms needed for rigorous cross-model comparison.\nThis gap exacerbates model underutilization and hinders responsible adoption.\nTo address these shortcomings, we introduce the Comprehensive Responsible AI\nModel Card Framework (CRAI-MCF), a novel approach that transitions from static\ndisclosures to actionable, human-aligned documentation. Grounded in Value\nSensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240\nopen-source projects, distilling 217 parameters into an eight-module,\nvalue-aligned architecture. Our framework introduces a quantitative sufficiency\ncriterion to operationalize evaluation and enables rigorous cross-model\ncomparison under a unified scheme. By balancing technical, ethical, and\noperational dimensions, CRAI-MCF empowers practitioners to efficiently assess,\nselect, and adopt LLMs with greater confidence and operational integrity."
                },
                "authors": [
                    {
                        "name": "Pengyue Yang"
                    },
                    {
                        "name": "Haolin Jin"
                    },
                    {
                        "name": "Qingwen Zeng"
                    },
                    {
                        "name": "Jiawen Wen"
                    },
                    {
                        "name": "Harry Rao"
                    },
                    {
                        "name": "Huaming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huaming Chen"
                },
                "author": "Huaming Chen",
                "arxiv_comment": "10 pages, 5 figures. Submitted to ICSE SEIP 2026 (Software\n  Engineering in Practice)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06984v1",
                "updated": "2025-10-08T13:09:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    9,
                    51,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T13:09:51Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    9,
                    51,
                    2,
                    281,
                    0
                ],
                "title": "An empirical study on declined proposals: why are these proposals\n  declined?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An empirical study on declined proposals: why are these proposals\n  declined?"
                },
                "summary": "Design-level decisions in open-source software (OSS) projects are often made\nthrough structured mechanisms such as proposals, which require substantial\ncommunity discussion and review. Despite their importance, the proposal process\nis resource-intensive and often leads to contributor frustration, especially\nwhen proposals are declined without clear feedback. Yet, the reasons behind\nproposal rejection remain poorly understood, limiting opportunities to\nstreamline the process or guide contributors effectively. This study\ninvestigates the characteristics and outcomes of proposals in the Go\nprogramming language to understand why proposals are declined and how such\noutcomes might be anticipated. We conduct a mixed-method empirical study on\n1,091 proposals submitted to the Go project. We quantify proposal outcomes,\nbuild a taxonomy of decline reasons, and evaluate large language models (LLMs)\nfor predicting these outcomes. We find that proposals are more often declined\nthan accepted, and resolution typically takes over a month. Only 14.7% of\ndeclined proposals are ever resubmitted. Through qualitative coding, we\nidentify nine key reasons for proposal decline, such as duplication, limited\nuse cases, or violations of project principles. This taxonomy can help\ncontributors address issues in advance, e.g., checking for existing\nalternatives can reduce redundancy. We also demonstrate that GPT-based models\ncan predict decline decisions early in the discussion (F1 score = 0.71 with\npartial comments), offering a practical tool for prioritizing review effort.\nOur findings reveal inefficiencies in the proposal process and highlight\nactionable opportunities for improving both contributor experience and reviewer\nworkload by enabling early triage and guiding contributors to strengthen their\nproposals using a structured understanding of past decline reasons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design-level decisions in open-source software (OSS) projects are often made\nthrough structured mechanisms such as proposals, which require substantial\ncommunity discussion and review. Despite their importance, the proposal process\nis resource-intensive and often leads to contributor frustration, especially\nwhen proposals are declined without clear feedback. Yet, the reasons behind\nproposal rejection remain poorly understood, limiting opportunities to\nstreamline the process or guide contributors effectively. This study\ninvestigates the characteristics and outcomes of proposals in the Go\nprogramming language to understand why proposals are declined and how such\noutcomes might be anticipated. We conduct a mixed-method empirical study on\n1,091 proposals submitted to the Go project. We quantify proposal outcomes,\nbuild a taxonomy of decline reasons, and evaluate large language models (LLMs)\nfor predicting these outcomes. We find that proposals are more often declined\nthan accepted, and resolution typically takes over a month. Only 14.7% of\ndeclined proposals are ever resubmitted. Through qualitative coding, we\nidentify nine key reasons for proposal decline, such as duplication, limited\nuse cases, or violations of project principles. This taxonomy can help\ncontributors address issues in advance, e.g., checking for existing\nalternatives can reduce redundancy. We also demonstrate that GPT-based models\ncan predict decline decisions early in the discussion (F1 score = 0.71 with\npartial comments), offering a practical tool for prioritizing review effort.\nOur findings reveal inefficiencies in the proposal process and highlight\nactionable opportunities for improving both contributor experience and reviewer\nworkload by enabling early triage and guiding contributors to strengthen their\nproposals using a structured understanding of past decline reasons."
                },
                "authors": [
                    {
                        "name": "Masanari Kondo"
                    },
                    {
                        "name": "Mahmoud Alfadel"
                    },
                    {
                        "name": "Shane McIntosh"
                    },
                    {
                        "name": "Yasutaka Kamei"
                    },
                    {
                        "name": "Naoyasu Ubayashi"
                    }
                ],
                "author_detail": {
                    "name": "Naoyasu Ubayashi"
                },
                "author": "Naoyasu Ubayashi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06975v1",
                "updated": "2025-10-08T13:00:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    0,
                    23,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T13:00:23Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    0,
                    23,
                    2,
                    281,
                    0
                ],
                "title": "VelLMes: A high-interaction AI-based deception framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VelLMes: A high-interaction AI-based deception framework"
                },
                "summary": "There are very few SotA deception systems based on Large Language Models. The\nexisting ones are limited only to simulating one type of service, mainly SSH\nshells. These systems - but also the deception technologies not based on LLMs -\nlack an extensive evaluation that includes human attackers. Generative AI has\nrecently become a valuable asset for cybersecurity researchers and\npractitioners, and the field of cyber-deception is no exception. Researchers\nhave demonstrated how LLMs can be leveraged to create realistic-looking\nhoneytokens, fake users, and even simulated systems that can be used as\nhoneypots. This paper presents an AI-based deception framework called VelLMes,\nwhich can simulate multiple protocols and services such as SSH Linux shell,\nMySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus\nVelLMes offers a variety of choices for deception design based on the users'\nneeds. VelLMes is designed to be attacked by humans, so interactivity and\nrealism are key for its performance. We evaluate the generative capabilities\nand the deception capabilities. Generative capabilities were evaluated using\nunit tests for LLMs. The results of the unit tests show that, with careful\nprompting, LLMs can produce realistic-looking responses, with some LLMs having\na 100% passing rate. In the case of the SSH Linux shell, we evaluated deception\ncapabilities with 89 human attackers. The results showed that about 30% of the\nattackers thought that they were interacting with a real system when they were\nassigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH\nLinux shell honeypot on the Internet to capture real-life attacks. Analysis of\nthese attacks showed us that LLM honeypots simulating Linux shells can perform\nwell against unstructured and unexpected attacks on the Internet, responding\ncorrectly to most of the issued commands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There are very few SotA deception systems based on Large Language Models. The\nexisting ones are limited only to simulating one type of service, mainly SSH\nshells. These systems - but also the deception technologies not based on LLMs -\nlack an extensive evaluation that includes human attackers. Generative AI has\nrecently become a valuable asset for cybersecurity researchers and\npractitioners, and the field of cyber-deception is no exception. Researchers\nhave demonstrated how LLMs can be leveraged to create realistic-looking\nhoneytokens, fake users, and even simulated systems that can be used as\nhoneypots. This paper presents an AI-based deception framework called VelLMes,\nwhich can simulate multiple protocols and services such as SSH Linux shell,\nMySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus\nVelLMes offers a variety of choices for deception design based on the users'\nneeds. VelLMes is designed to be attacked by humans, so interactivity and\nrealism are key for its performance. We evaluate the generative capabilities\nand the deception capabilities. Generative capabilities were evaluated using\nunit tests for LLMs. The results of the unit tests show that, with careful\nprompting, LLMs can produce realistic-looking responses, with some LLMs having\na 100% passing rate. In the case of the SSH Linux shell, we evaluated deception\ncapabilities with 89 human attackers. The results showed that about 30% of the\nattackers thought that they were interacting with a real system when they were\nassigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH\nLinux shell honeypot on the Internet to capture real-life attacks. Analysis of\nthese attacks showed us that LLM honeypots simulating Linux shells can perform\nwell against unstructured and unexpected attacks on the Internet, responding\ncorrectly to most of the issued commands."
                },
                "authors": [
                    {
                        "name": "Muris Sladić"
                    },
                    {
                        "name": "Veronica Valeros"
                    },
                    {
                        "name": "Carlos Catania"
                    },
                    {
                        "name": "Sebastian Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Garcia"
                },
                "arxiv_affiliation": "Czech Technical University in Prague",
                "author": "Sebastian Garcia",
                "arxiv_doi": "10.1109/EuroSPW67616.2025.00082",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/EuroSPW67616.2025.00082",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.06975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages. 9 figures. 1 table. This is a preprint of a paper that was\n  presented at the Active Defense and Deception Workshop colocated with IEEE\n  EuroS&P 2025 conference",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06974v1",
                "updated": "2025-10-08T13:00:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    0,
                    12,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T13:00:12Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    13,
                    0,
                    12,
                    2,
                    281,
                    0
                ],
                "title": "Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and\n  Social Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and\n  Social Groups"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in user-facing\napplications, raising concerns about their potential to reflect and amplify\nsocial biases. We investigate social identity framing in Chinese LLMs using\nMandarin-specific prompts across ten representative Chinese LLMs, evaluating\nresponses to ingroup (\"We\") and outgroup (\"They\") framings, and extending the\nsetting to 240 social groups salient in the Chinese context. To complement\ncontrolled experiments, we further analyze Chinese-language conversations from\na corpus of real interactions between users and chatbots. Across models, we\nobserve systematic ingroup-positive and outgroup-negative tendencies, which are\nnot confined to synthetic prompts but also appear in naturalistic dialogue,\nindicating that bias dynamics might strengthen in real interactions. Our study\nprovides a language-aware evaluation framework for Chinese LLMs, demonstrating\nthat social identity biases documented in English generalize\ncross-linguistically and intensify in user-facing contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in user-facing\napplications, raising concerns about their potential to reflect and amplify\nsocial biases. We investigate social identity framing in Chinese LLMs using\nMandarin-specific prompts across ten representative Chinese LLMs, evaluating\nresponses to ingroup (\"We\") and outgroup (\"They\") framings, and extending the\nsetting to 240 social groups salient in the Chinese context. To complement\ncontrolled experiments, we further analyze Chinese-language conversations from\na corpus of real interactions between users and chatbots. Across models, we\nobserve systematic ingroup-positive and outgroup-negative tendencies, which are\nnot confined to synthetic prompts but also appear in naturalistic dialogue,\nindicating that bias dynamics might strengthen in real interactions. Our study\nprovides a language-aware evaluation framework for Chinese LLMs, demonstrating\nthat social identity biases documented in English generalize\ncross-linguistically and intensify in user-facing contexts."
                },
                "authors": [
                    {
                        "name": "Geng Liu"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Junjie Mu"
                    },
                    {
                        "name": "Mengxiao Zhu"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06965v1",
                "updated": "2025-10-08T12:53:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    53,
                    6,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T12:53:06Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    53,
                    6,
                    2,
                    281,
                    0
                ],
                "title": "EDUMATH: Generating Standards-aligned Educational Math Word Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDUMATH: Generating Standards-aligned Educational Math Word Problems"
                },
                "summary": "Math word problems (MWPs) are critical K-12 educational tools, and\ncustomizing them to students' interests and ability levels can increase\nlearning outcomes. However, teachers struggle to find time to customize MWPs\nfor each student given large class sizes and increasing burnout. We propose\nthat LLMs can support math education by generating MWPs customized to student\ninterests and math education standards. To this end, we use a joint human\nexpert-LLM judge approach to evaluate over 11,000 MWPs generated by open and\nclosed LLMs and develop the first teacher-annotated dataset for\nstandards-aligned educational MWP generation. We show the value of our data by\nusing it to train a 12B open model that matches the performance of larger and\nmore capable open models. We also use our teacher-annotated data to train a\ntext classifier that enables a 30B open LLM to outperform existing closed\nbaselines without any training. Next, we show our models' MWPs are more similar\nto human-written MWPs than those from existing models. We conclude by\nconducting the first study of customized LLM-generated MWPs with grade school\nstudents, finding they perform similarly on our models' MWPs relative to\nhuman-written MWPs but consistently prefer our customized MWPs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Math word problems (MWPs) are critical K-12 educational tools, and\ncustomizing them to students' interests and ability levels can increase\nlearning outcomes. However, teachers struggle to find time to customize MWPs\nfor each student given large class sizes and increasing burnout. We propose\nthat LLMs can support math education by generating MWPs customized to student\ninterests and math education standards. To this end, we use a joint human\nexpert-LLM judge approach to evaluate over 11,000 MWPs generated by open and\nclosed LLMs and develop the first teacher-annotated dataset for\nstandards-aligned educational MWP generation. We show the value of our data by\nusing it to train a 12B open model that matches the performance of larger and\nmore capable open models. We also use our teacher-annotated data to train a\ntext classifier that enables a 30B open LLM to outperform existing closed\nbaselines without any training. Next, we show our models' MWPs are more similar\nto human-written MWPs than those from existing models. We conclude by\nconducting the first study of customized LLM-generated MWPs with grade school\nstudents, finding they perform similarly on our models' MWPs relative to\nhuman-written MWPs but consistently prefer our customized MWPs."
                },
                "authors": [
                    {
                        "name": "Bryan R. Christ"
                    },
                    {
                        "name": "Penelope Molitz"
                    },
                    {
                        "name": "Jonathan Kropko"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hartvigsen"
                },
                "author": "Thomas Hartvigsen",
                "arxiv_comment": "32 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06961v2",
                "updated": "2025-10-09T07:39:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    7,
                    39,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-08T12:44:51Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    44,
                    51,
                    2,
                    281,
                    0
                ],
                "title": "Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual\n  and Long-Form Speech Recognition Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual\n  and Long-Form Speech Recognition Evaluation"
                },
                "summary": "Despite rapid progress, ASR evaluation remains saturated with short-form\nEnglish, and efficiency is rarely reported. We present the Open ASR\nLeaderboard, a fully reproducible benchmark and interactive leaderboard\ncomparing 60+ open-source and proprietary systems across 11 datasets, including\ndedicated multilingual and long-form tracks. We standardize text normalization\nand report both word error rate (WER) and inverse real-time factor (RTFx),\nenabling fair accuracy-efficiency comparisons. For English transcription,\nConformer encoders paired with LLM decoders achieve the best average WER but\nare slower, while CTC and TDT decoders deliver much better RTFx, making them\nattractive for long-form and offline use. Whisper-derived encoders fine-tuned\nfor English improve accuracy but often trade off multilingual coverage. All\ncode and dataset loaders are open-sourced to support transparent, extensible\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid progress, ASR evaluation remains saturated with short-form\nEnglish, and efficiency is rarely reported. We present the Open ASR\nLeaderboard, a fully reproducible benchmark and interactive leaderboard\ncomparing 60+ open-source and proprietary systems across 11 datasets, including\ndedicated multilingual and long-form tracks. We standardize text normalization\nand report both word error rate (WER) and inverse real-time factor (RTFx),\nenabling fair accuracy-efficiency comparisons. For English transcription,\nConformer encoders paired with LLM decoders achieve the best average WER but\nare slower, while CTC and TDT decoders deliver much better RTFx, making them\nattractive for long-form and offline use. Whisper-derived encoders fine-tuned\nfor English improve accuracy but often trade off multilingual coverage. All\ncode and dataset loaders are open-sourced to support transparent, extensible\nevaluation."
                },
                "authors": [
                    {
                        "name": "Vaibhav Srivastav"
                    },
                    {
                        "name": "Steven Zheng"
                    },
                    {
                        "name": "Eric Bezzam"
                    },
                    {
                        "name": "Eustache Le Bihan"
                    },
                    {
                        "name": "Nithin Koluguri"
                    },
                    {
                        "name": "Piotr Żelasko"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Adel Moumen"
                    },
                    {
                        "name": "Sanchit Gandhi"
                    }
                ],
                "author_detail": {
                    "name": "Sanchit Gandhi"
                },
                "author": "Sanchit Gandhi",
                "arxiv_comment": "Submitted to ICASSP 2026; Leaderboard:\n  https://huggingface.co/spaces/hf-audio/open_asr_leaderboard ; Code:\n  https://github.com/huggingface/open_asr_leaderboard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06957v1",
                "updated": "2025-10-08T12:42:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    42,
                    7,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T12:42:07Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    42,
                    7,
                    2,
                    281,
                    0
                ],
                "title": "Accelerating Sparse Ternary GEMM for Quantized LLM inference on Apple\n  Silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Sparse Ternary GEMM for Quantized LLM inference on Apple\n  Silicon"
                },
                "summary": "Sparse Ternary General Matrix-Matrix Multiplication (GEMM) remains\nunder-optimized in existing libraries for Apple Silicon CPUs. We present a\nSparse Ternary GEMM kernel optimized specifically for Apple's M-series\nprocessors. We propose a set of architecture-aware optimizations, including a\nnovel blocked and interleaved sparse data format to improve memory locality,\nstrategies to increase Instruction-Level Parallelism (ILP), and NEON-based\nSingle Instruction Multiple Data (SIMD) vectorization to exploit data-level\nparallelism. Our scalar implementation achieves up to a 5.98x performance\nincrease over a traditional Ternary Compressed Sparse Column (TCSC) baseline\nfor large matrices with 50% ternary nonzero values (sparsity), reaching up to a\n50.2% of the processor's theoretical peak performance, and remains stable\nacross varying sparsity levels. Our vectorized implementation delivers up to a\n5.59x performance increase for large matrices with 25% sparsity, and remains\nstable across varying sparsity levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Ternary General Matrix-Matrix Multiplication (GEMM) remains\nunder-optimized in existing libraries for Apple Silicon CPUs. We present a\nSparse Ternary GEMM kernel optimized specifically for Apple's M-series\nprocessors. We propose a set of architecture-aware optimizations, including a\nnovel blocked and interleaved sparse data format to improve memory locality,\nstrategies to increase Instruction-Level Parallelism (ILP), and NEON-based\nSingle Instruction Multiple Data (SIMD) vectorization to exploit data-level\nparallelism. Our scalar implementation achieves up to a 5.98x performance\nincrease over a traditional Ternary Compressed Sparse Column (TCSC) baseline\nfor large matrices with 50% ternary nonzero values (sparsity), reaching up to a\n50.2% of the processor's theoretical peak performance, and remains stable\nacross varying sparsity levels. Our vectorized implementation delivers up to a\n5.59x performance increase for large matrices with 25% sparsity, and remains\nstable across varying sparsity levels."
                },
                "authors": [
                    {
                        "name": "Baraq Lipshitz"
                    },
                    {
                        "name": "Alessio Melone"
                    },
                    {
                        "name": "Charalampos Maraziaris"
                    },
                    {
                        "name": "Muhammed Bilal"
                    }
                ],
                "author_detail": {
                    "name": "Muhammed Bilal"
                },
                "arxiv_affiliation": "ETH Zurich",
                "author": "Muhammed Bilal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06953v1",
                "updated": "2025-10-08T12:37:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    37,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T12:37:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    37,
                    4,
                    2,
                    281,
                    0
                ],
                "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning\n  Traces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning\n  Traces"
                },
                "summary": "The Uniform Information Density (UID) hypothesis suggests that effective\ncommunication maintains a stable flow of information. In this work, we revisit\nthis principle in the context of large language model (LLM) reasoning traces,\nasking whether step-level uniformity reflects reasoning quality. To this end,\nwe propose an entropy-based stepwise information density metric and introduce\ntwo complementary measures of uniformity, local and global uniformity scores.\nAcross the experiments on six different reasoning benchmarks, we find that\nstep-level uniformity not only provides a strong theoretical lens but also\nyields practical performance benefits; for example, selecting reasoning traces\nwith more uniform information density at the step-level improves accuracy by\n10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals\nthat correct reasoning traces tend to avoid sharp information density spikes,\nwhile incorrect traces exhibit irregular information bursts. These results\ndemonstrate that UID-inspired information density measures outperform\nalternative internal signals as predictors of reasoning quality. Results\nhighlight the uniformity of the information density as a robust diagnostic and\nselection criterion for building more reliable and accurate reasoning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Uniform Information Density (UID) hypothesis suggests that effective\ncommunication maintains a stable flow of information. In this work, we revisit\nthis principle in the context of large language model (LLM) reasoning traces,\nasking whether step-level uniformity reflects reasoning quality. To this end,\nwe propose an entropy-based stepwise information density metric and introduce\ntwo complementary measures of uniformity, local and global uniformity scores.\nAcross the experiments on six different reasoning benchmarks, we find that\nstep-level uniformity not only provides a strong theoretical lens but also\nyields practical performance benefits; for example, selecting reasoning traces\nwith more uniform information density at the step-level improves accuracy by\n10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals\nthat correct reasoning traces tend to avoid sharp information density spikes,\nwhile incorrect traces exhibit irregular information bursts. These results\ndemonstrate that UID-inspired information density measures outperform\nalternative internal signals as predictors of reasoning quality. Results\nhighlight the uniformity of the information density as a robust diagnostic and\nselection criterion for building more reliable and accurate reasoning systems."
                },
                "authors": [
                    {
                        "name": "Minju Gwak"
                    },
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Jaehyung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyung Kim"
                },
                "author": "Jaehyung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06952v1",
                "updated": "2025-10-08T12:35:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    35,
                    35,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T12:35:35Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    35,
                    35,
                    2,
                    281,
                    0
                ],
                "title": "OBJVanish: Physically Realizable Text-to-3D Adv. Generation of\n  LiDAR-Invisible Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OBJVanish: Physically Realizable Text-to-3D Adv. Generation of\n  LiDAR-Invisible Objects"
                },
                "summary": "LiDAR-based 3D object detectors are fundamental to autonomous driving, where\nfailing to detect objects poses severe safety risks. Developing effective 3D\nadversarial attacks is essential for thoroughly testing these detection systems\nand exposing their vulnerabilities before real-world deployment. However,\nexisting adversarial attacks that add optimized perturbations to 3D points have\ntwo critical limitations: they rarely cause complete object disappearance and\nprove difficult to implement in physical environments. We introduce the\ntext-to-3D adversarial generation method, a novel approach enabling physically\nrealizable attacks that can generate 3D models of objects truly invisible to\nLiDAR detectors and be easily realized in the real world. Specifically, we\npresent the first empirical study that systematically investigates the factors\ninfluencing detection vulnerability by manipulating the topology, connectivity,\nand intensity of individual pedestrian 3D models and combining pedestrians with\nmultiple objects within the CARLA simulation environment. Building on the\ninsights, we propose the physically-informed text-to-3D adversarial generation\n(Phy3DAdvGen) that systematically optimizes text prompts by iteratively\nrefining verbs, objects, and poses to produce LiDAR-invisible pedestrians. To\nensure physical realizability, we construct a comprehensive object pool\ncontaining 13 3D models of real objects and constrain Phy3DAdvGen to generate\n3D objects based on combinations of objects in this set. Extensive experiments\ndemonstrate that our approach can generate 3D pedestrians that evade six\nstate-of-the-art (SOTA) LiDAR 3D detectors in both CARLA simulation and\nphysical environments, thereby highlighting vulnerabilities in safety-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR-based 3D object detectors are fundamental to autonomous driving, where\nfailing to detect objects poses severe safety risks. Developing effective 3D\nadversarial attacks is essential for thoroughly testing these detection systems\nand exposing their vulnerabilities before real-world deployment. However,\nexisting adversarial attacks that add optimized perturbations to 3D points have\ntwo critical limitations: they rarely cause complete object disappearance and\nprove difficult to implement in physical environments. We introduce the\ntext-to-3D adversarial generation method, a novel approach enabling physically\nrealizable attacks that can generate 3D models of objects truly invisible to\nLiDAR detectors and be easily realized in the real world. Specifically, we\npresent the first empirical study that systematically investigates the factors\ninfluencing detection vulnerability by manipulating the topology, connectivity,\nand intensity of individual pedestrian 3D models and combining pedestrians with\nmultiple objects within the CARLA simulation environment. Building on the\ninsights, we propose the physically-informed text-to-3D adversarial generation\n(Phy3DAdvGen) that systematically optimizes text prompts by iteratively\nrefining verbs, objects, and poses to produce LiDAR-invisible pedestrians. To\nensure physical realizability, we construct a comprehensive object pool\ncontaining 13 3D models of real objects and constrain Phy3DAdvGen to generate\n3D objects based on combinations of objects in this set. Extensive experiments\ndemonstrate that our approach can generate 3D pedestrians that evade six\nstate-of-the-art (SOTA) LiDAR 3D detectors in both CARLA simulation and\nphysical environments, thereby highlighting vulnerabilities in safety-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Wuqi Wang"
                    },
                    {
                        "name": "Yanan Zhang"
                    },
                    {
                        "name": "Jingzheng Li"
                    },
                    {
                        "name": "Haigen Min"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Xingyu Zhao"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Qing Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qing Guo"
                },
                "author": "Qing Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03654v2",
                "updated": "2025-10-08T12:30:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    30,
                    55,
                    2,
                    281,
                    0
                ],
                "published": "2025-03-05T16:32:47Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    32,
                    47,
                    2,
                    64,
                    0
                ],
                "title": "Improving Neutral Point-of-View Generation with Data- and\n  Parameter-Efficient RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Neutral Point-of-View Generation with Data- and\n  Parameter-Efficient RL"
                },
                "summary": "The paper shows that parameter-efficient reinforcement learning (PE-RL) is a\nhighly effective training regime to improve large language models' (LLMs)\nability to answer queries on sensitive topics with a Neutral Point of View\n(NPOV), i.e. to provide significantly more informative, diverse and impartial\nanswers. This is shown by evaluating PE-RL and multiple strong\nbaselines-including LoRA finetuning (strongest baseline), SFT and RLHF. PE-RL\nnot only improves on overall NPOV quality compared to the strongest baseline\n($97.06\\%\\rightarrow 99.08\\%$), but also scores much higher on features\nlinguists identify as key to separating sufficient answers from \"great''\nanswers ($60.25\\%\\rightarrow 85.21\\%$ for presence of supportive details,\n$68.74\\%\\rightarrow 91.43\\%$ for absence of oversimplification). A qualitative\nanalysis corroborates this. Moreover, our evaluation also finds a key property\nof PE-RL for this task: unlike methods that update all parameters, it\ngeneralises out of topic. Finally, to enable further studies we also release\nthe dataset, SHQ-NPOV, and provide a methodology to create such datasets\nthrough iterative rounds of human peer-critique and annotator training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper shows that parameter-efficient reinforcement learning (PE-RL) is a\nhighly effective training regime to improve large language models' (LLMs)\nability to answer queries on sensitive topics with a Neutral Point of View\n(NPOV), i.e. to provide significantly more informative, diverse and impartial\nanswers. This is shown by evaluating PE-RL and multiple strong\nbaselines-including LoRA finetuning (strongest baseline), SFT and RLHF. PE-RL\nnot only improves on overall NPOV quality compared to the strongest baseline\n($97.06\\%\\rightarrow 99.08\\%$), but also scores much higher on features\nlinguists identify as key to separating sufficient answers from \"great''\nanswers ($60.25\\%\\rightarrow 85.21\\%$ for presence of supportive details,\n$68.74\\%\\rightarrow 91.43\\%$ for absence of oversimplification). A qualitative\nanalysis corroborates this. Moreover, our evaluation also finds a key property\nof PE-RL for this task: unlike methods that update all parameters, it\ngeneralises out of topic. Finally, to enable further studies we also release\nthe dataset, SHQ-NPOV, and provide a methodology to create such datasets\nthrough iterative rounds of human peer-critique and annotator training."
                },
                "authors": [
                    {
                        "name": "Jessica Hoffmann"
                    },
                    {
                        "name": "Christiane Ahlheim"
                    },
                    {
                        "name": "Zac Yu"
                    },
                    {
                        "name": "Aria Walfrand"
                    },
                    {
                        "name": "Jarvis Jin"
                    },
                    {
                        "name": "Marie Tano"
                    },
                    {
                        "name": "Ahmad Beirami"
                    },
                    {
                        "name": "Erin van Liemt"
                    },
                    {
                        "name": "Nithum Thain"
                    },
                    {
                        "name": "Hakim Sidahmed"
                    },
                    {
                        "name": "Lucas Dixon"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Dixon"
                },
                "author": "Lucas Dixon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04601v2",
                "updated": "2025-10-08T12:26:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    26,
                    35,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-06T09:06:38Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    6,
                    38,
                    0,
                    279,
                    0
                ],
                "title": "FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient\n  Federated Large Language Models Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient\n  Federated Large Language Models Fine-Tuning"
                },
                "summary": "The current paradigm of training large language models (LLMs) on publicly\navailable Web data is becoming unsustainable, with high-quality data sources in\nspecialized domains nearing exhaustion. Federated Learning (FL) emerges as a\npractical solution for the next generation of AI on a decentralized Web,\nenabling privacy-preserving collaborative fine-tuning by leveraging private\ndata distributed across a global client base. While Low-Rank Adaptation (LoRA)\nis the standard for efficient fine-tuning, its application in federated\nsettings presents a critical challenge: communication overhead remains a\nsignificant bottleneck across the Web's heterogeneous network conditions. The\nstructural redundancy within LoRA parameters not only incurs a heavy\ncommunication burden but also introduces conflicts when aggregating client\nupdates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose\nframework designed for communication-efficient federated LLMs fine-tuning. We\nfirst introduce an importance-aware sparsification method that preserves the\nstructural integrity of LoRA updates to reduce the uploaded parameter count.\nThe server then reconstructs and aggregates these updates in a full-rank space\nto mitigate conflicts. Finally, it decomposes the global update into a sparse\nlow-rank format for broadcast, ensuring a symmetrically efficient cycle. We\nalso propose an efficient variant, FedSRD-e, to reduce computational overhead.\nExperimental results on 10 benchmarks demonstrate that our framework\nsignificantly reduces communication costs by up to 90\\% while even improving\nmodel performance on heterogeneous client data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current paradigm of training large language models (LLMs) on publicly\navailable Web data is becoming unsustainable, with high-quality data sources in\nspecialized domains nearing exhaustion. Federated Learning (FL) emerges as a\npractical solution for the next generation of AI on a decentralized Web,\nenabling privacy-preserving collaborative fine-tuning by leveraging private\ndata distributed across a global client base. While Low-Rank Adaptation (LoRA)\nis the standard for efficient fine-tuning, its application in federated\nsettings presents a critical challenge: communication overhead remains a\nsignificant bottleneck across the Web's heterogeneous network conditions. The\nstructural redundancy within LoRA parameters not only incurs a heavy\ncommunication burden but also introduces conflicts when aggregating client\nupdates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose\nframework designed for communication-efficient federated LLMs fine-tuning. We\nfirst introduce an importance-aware sparsification method that preserves the\nstructural integrity of LoRA updates to reduce the uploaded parameter count.\nThe server then reconstructs and aggregates these updates in a full-rank space\nto mitigate conflicts. Finally, it decomposes the global update into a sparse\nlow-rank format for broadcast, ensuring a symmetrically efficient cycle. We\nalso propose an efficient variant, FedSRD-e, to reduce computational overhead.\nExperimental results on 10 benchmarks demonstrate that our framework\nsignificantly reduces communication costs by up to 90\\% while even improving\nmodel performance on heterogeneous client data."
                },
                "authors": [
                    {
                        "name": "Guochen Yan"
                    },
                    {
                        "name": "Luyuan Xie"
                    },
                    {
                        "name": "Qingni Shen"
                    },
                    {
                        "name": "Yuejian Fang"
                    },
                    {
                        "name": "Zhonghai Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhonghai Wu"
                },
                "author": "Zhonghai Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06931v1",
                "updated": "2025-10-08T12:12:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    12,
                    46,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T12:12:46Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    12,
                    46,
                    2,
                    281,
                    0
                ],
                "title": "Textual interpretation of transient image classifications from large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual interpretation of transient image classifications from large\n  language models"
                },
                "summary": "Modern astronomical surveys deliver immense volumes of transient detections,\nyet distinguishing real astrophysical signals (for example, explosive events)\nfrom bogus imaging artefacts remains a challenge. Convolutional neural networks\nare effectively used for real versus bogus classification; however, their\nreliance on opaque latent representations hinders interpretability. Here we\nshow that large language models (LLMs) can approach the performance level of a\nconvolutional neural network on three optical transient survey datasets\n(Pan-STARRS, MeerLICHT and ATLAS) while simultaneously producing direct,\nhuman-readable descriptions for every candidate. Using only 15 examples and\nconcise instructions, Google's LLM, Gemini, achieves a 93% average accuracy\nacross datasets that span a range of resolution and pixel scales. We also show\nthat a second LLM can assess the coherence of the output of the first model,\nenabling iterative refinement by identifying problematic cases. This framework\nallows users to define the desired classification behaviour through natural\nlanguage and examples, bypassing traditional training pipelines. Furthermore,\nby generating textual descriptions of observed features, LLMs enable users to\nquery classifications as if navigating an annotated catalogue, rather than\ndeciphering abstract latent spaces. As next-generation telescopes and surveys\nfurther increase the amount of data available, LLM-based classification could\nhelp bridge the gap between automated detection and transparent, human-level\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern astronomical surveys deliver immense volumes of transient detections,\nyet distinguishing real astrophysical signals (for example, explosive events)\nfrom bogus imaging artefacts remains a challenge. Convolutional neural networks\nare effectively used for real versus bogus classification; however, their\nreliance on opaque latent representations hinders interpretability. Here we\nshow that large language models (LLMs) can approach the performance level of a\nconvolutional neural network on three optical transient survey datasets\n(Pan-STARRS, MeerLICHT and ATLAS) while simultaneously producing direct,\nhuman-readable descriptions for every candidate. Using only 15 examples and\nconcise instructions, Google's LLM, Gemini, achieves a 93% average accuracy\nacross datasets that span a range of resolution and pixel scales. We also show\nthat a second LLM can assess the coherence of the output of the first model,\nenabling iterative refinement by identifying problematic cases. This framework\nallows users to define the desired classification behaviour through natural\nlanguage and examples, bypassing traditional training pipelines. Furthermore,\nby generating textual descriptions of observed features, LLMs enable users to\nquery classifications as if navigating an annotated catalogue, rather than\ndeciphering abstract latent spaces. As next-generation telescopes and surveys\nfurther increase the amount of data available, LLM-based classification could\nhelp bridge the gap between automated detection and transparent, human-level\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Fiorenzo Stoppa"
                    },
                    {
                        "name": "Turan Bulmus"
                    },
                    {
                        "name": "Steven Bloemen"
                    },
                    {
                        "name": "Stephen J. Smartt"
                    },
                    {
                        "name": "Paul J. Groot"
                    },
                    {
                        "name": "Paul Vreeswijk"
                    },
                    {
                        "name": "Ken W. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Ken W. Smith"
                },
                "author": "Ken W. Smith",
                "arxiv_doi": "10.1038/s41550-025-02670-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41550-025-02670-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.06931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Nature Astronomy (2025). Publisher's Version of Record\n  (CC BY 4.0). DOI: 10.1038/s41550-025-02670-z",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05732v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05732v3",
                "updated": "2025-10-08T12:07:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    7,
                    29,
                    2,
                    281,
                    0
                ],
                "published": "2024-11-08T17:44:40Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    17,
                    44,
                    40,
                    4,
                    313,
                    0
                ],
                "title": "A risk model and analysis method for the psychological safety of human\n  and autonomous vehicles interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A risk model and analysis method for the psychological safety of human\n  and autonomous vehicles interaction"
                },
                "summary": "The rapid advancement of artificial intelligence and autonomous driving\ntechnologies has significantly propelled the development of autonomous vehicles\n(AVs). However, psychological barriers continue to impede widespread AV\nadoption, despite technological progress. This paper addresses the critical yet\noften overlooked aspect of psychological safety in AV design and operation.\nWhile traditional safety standards focus primarily on physical safety, this\npaper emphasizes the psychological implications that arise from human\ninteractions with autonomous vehicles, highlighting the importance of trust and\nperceived risk as significant factors influencing user acceptance. The paper\nmakes a methodological proposal, a framework for addressing AVs psychological\nsafety consisting of three key contributions. First, it introduces a definition\nof psychological safety in AVs context. Secondly, it proposes a risk model for\nidentifying and assessing AVs psychological hazards and risks. PsySIL\n(Psychological Safety Integrity Level), a classification of AV psychological\nrisk levels is developed. Thirdly, an adapted system-theoretic analysis method\nfor AVs psychological safety is proposed. The paper illustrates the application\nof the framework for assessing potential psychological hazards using a scenario\ninvolving a family's experience with an autonomous vehicle, pioneering a\nsystems approach towards evaluating situations that could lead to psychological\nharm. By establishing a framework that incorporates psychological safety\nalongside physical safety, the paper contributes to the broader discourse on\nthe safe deployment of autonomous vehicle, aiming to guide future developments\nin user-centred design and regulatory practices, while acknowledging the\nlimitations brought by the application of the proposals on a rather simple but\npedagogical illustrative example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of artificial intelligence and autonomous driving\ntechnologies has significantly propelled the development of autonomous vehicles\n(AVs). However, psychological barriers continue to impede widespread AV\nadoption, despite technological progress. This paper addresses the critical yet\noften overlooked aspect of psychological safety in AV design and operation.\nWhile traditional safety standards focus primarily on physical safety, this\npaper emphasizes the psychological implications that arise from human\ninteractions with autonomous vehicles, highlighting the importance of trust and\nperceived risk as significant factors influencing user acceptance. The paper\nmakes a methodological proposal, a framework for addressing AVs psychological\nsafety consisting of three key contributions. First, it introduces a definition\nof psychological safety in AVs context. Secondly, it proposes a risk model for\nidentifying and assessing AVs psychological hazards and risks. PsySIL\n(Psychological Safety Integrity Level), a classification of AV psychological\nrisk levels is developed. Thirdly, an adapted system-theoretic analysis method\nfor AVs psychological safety is proposed. The paper illustrates the application\nof the framework for assessing potential psychological hazards using a scenario\ninvolving a family's experience with an autonomous vehicle, pioneering a\nsystems approach towards evaluating situations that could lead to psychological\nharm. By establishing a framework that incorporates psychological safety\nalongside physical safety, the paper contributes to the broader discourse on\nthe safe deployment of autonomous vehicle, aiming to guide future developments\nin user-centred design and regulatory practices, while acknowledging the\nlimitations brought by the application of the proposals on a rather simple but\npedagogical illustrative example."
                },
                "authors": [
                    {
                        "name": "Yandika Sirgabsou"
                    },
                    {
                        "name": "Benjamin Hardin"
                    },
                    {
                        "name": "François Leblanc"
                    },
                    {
                        "name": "Efi Raili"
                    },
                    {
                        "name": "Pericle Salvini"
                    },
                    {
                        "name": "David Jackson"
                    },
                    {
                        "name": "Marina Jirotka"
                    },
                    {
                        "name": "Lars Kunze"
                    }
                ],
                "author_detail": {
                    "name": "Lars Kunze"
                },
                "author": "Lars Kunze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05732v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05732v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06924v1",
                "updated": "2025-10-08T12:03:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    3,
                    21,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T12:03:21Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    12,
                    3,
                    21,
                    2,
                    281,
                    0
                ],
                "title": "Ethical AI prompt recommendations in large language models using\n  collaborative filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ethical AI prompt recommendations in large language models using\n  collaborative filtering"
                },
                "summary": "As large language models (LLMs) shape AI development, ensuring ethical prompt\nrecommendations is crucial. LLMs offer innovation but risk bias, fairness\nissues, and accountability concerns. Traditional oversight methods struggle\nwith scalability, necessitating dynamic solutions. This paper proposes using\ncollaborative filtering, a technique from recommendation systems, to enhance\nethical prompt selection. By leveraging user interactions, it promotes ethical\nguidelines while reducing bias. Contributions include a synthetic dataset for\nprompt recommendations and the application of collaborative filtering. The work\nalso tackles challenges in ethical AI, such as bias mitigation, transparency,\nand preventing unethical prompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) shape AI development, ensuring ethical prompt\nrecommendations is crucial. LLMs offer innovation but risk bias, fairness\nissues, and accountability concerns. Traditional oversight methods struggle\nwith scalability, necessitating dynamic solutions. This paper proposes using\ncollaborative filtering, a technique from recommendation systems, to enhance\nethical prompt selection. By leveraging user interactions, it promotes ethical\nguidelines while reducing bias. Contributions include a synthetic dataset for\nprompt recommendations and the application of collaborative filtering. The work\nalso tackles challenges in ethical AI, such as bias mitigation, transparency,\nand preventing unethical prompt engineering."
                },
                "authors": [
                    {
                        "name": "Jordan Nelson"
                    },
                    {
                        "name": "Almas Baimagambetov"
                    },
                    {
                        "name": "Konstantinos Avgerinakis"
                    },
                    {
                        "name": "Nikolaos Polatidis"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Polatidis"
                },
                "author": "Nikolaos Polatidis",
                "arxiv_comment": "This paper has been accepted to by the International Journal of\n  Parallel, Emergent & Distributed Systems (Taylor and Francis) and has an\n  assigned DOI. We have already chose to make this open access using CC BY. The\n  article is not yet available online on the publisher's website. The DOI is:\n  doi.org/10.1080/17445760.2025.2573086",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]