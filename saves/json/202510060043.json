[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.02312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02312v1",
                "updated": "2025-10-02T17:59:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation"
                },
                "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference."
                },
                "authors": [
                    {
                        "name": "Anna Kuzina"
                    },
                    {
                        "name": "Maciej Pioro"
                    },
                    {
                        "name": "Paul N. Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02084v2",
                "updated": "2025-10-03T05:10:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    5,
                    10,
                    2,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-02T14:50:50Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    50,
                    50,
                    3,
                    275,
                    0
                ],
                "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting"
                },
                "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries."
                },
                "authors": [
                    {
                        "name": "Kuiye Ding"
                    },
                    {
                        "name": "Fanda Fan"
                    },
                    {
                        "name": "Zheya Wang"
                    },
                    {
                        "name": "Hongxiao Li"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Chunjie Luo"
                    },
                    {
                        "name": "Jianfeng Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Zhan"
                },
                "author": "Jianfeng Zhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v2",
                "updated": "2025-10-02T14:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    42,
                    41,
                    3,
                    275,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine caching timings and adopting handcrafted rules for\nmulti-step cache utilization. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail to cope\nwith diverse samples. In this paper, a strong sample-specific correlation is\nrevealed between the variation patterns of the shallow-layer feature\ndifferences in the diffusion model and those of deep-layer features. Moreover,\nwe have observed that the features from different model layers form similar\ntrajectories. Based on these observations, we present DiCache, a novel\ntraining-free adaptive caching strategy for accelerating diffusion models at\nruntime, answering both when and how to cache within a unified framework.\nSpecifically, DiCache is composed of two principal components: (1) Online Probe\nProfiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly\nindicator for the caching error in real time, enabling the model to dynamically\ncustomize the caching schedule for each sample. (2) Dynamic Cache Trajectory\nAlignment adaptively approximates the deep-layer feature output from multi-step\nhistorical caches based on the shallow-layer feature trajectory, facilitating\nhigher visual quality. Extensive experiments validate DiCache's capability in\nachieving higher efficiency and improved fidelity over state-of-the-art\napproaches on various leading diffusion models including WAN 2.1, HunyuanVideo\nand Flux.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine caching timings and adopting handcrafted rules for\nmulti-step cache utilization. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail to cope\nwith diverse samples. In this paper, a strong sample-specific correlation is\nrevealed between the variation patterns of the shallow-layer feature\ndifferences in the diffusion model and those of deep-layer features. Moreover,\nwe have observed that the features from different model layers form similar\ntrajectories. Based on these observations, we present DiCache, a novel\ntraining-free adaptive caching strategy for accelerating diffusion models at\nruntime, answering both when and how to cache within a unified framework.\nSpecifically, DiCache is composed of two principal components: (1) Online Probe\nProfiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly\nindicator for the caching error in real time, enabling the model to dynamically\ncustomize the caching schedule for each sample. (2) Dynamic Cache Trajectory\nAlignment adaptively approximates the deep-layer feature output from multi-step\nhistorical caches based on the shallow-layer feature trajectory, facilitating\nhigher visual quality. Extensive experiments validate DiCache's capability in\nachieving higher efficiency and improved fidelity over state-of-the-art\napproaches on various leading diffusion models including WAN 2.1, HunyuanVideo\nand Flux."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "Project Page: https://bujiazi.github.io/dicache.github.io/ Code:\n  https://github.com/Bujiazi/DiCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v3",
                "updated": "2025-10-02T14:09:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    9,
                    3,
                    3,
                    275,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "arxiv_journal_ref": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01884v1",
                "updated": "2025-10-02T10:49:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    49,
                    54,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T10:49:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    49,
                    54,
                    3,
                    275,
                    0
                ],
                "title": "Study of the $^{20}$Ne($p,γ$)$^{21}$Na reaction at LUNA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the $^{20}$Ne($p,γ$)$^{21}$Na reaction at LUNA"
                },
                "summary": "The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al\nisotopes. The $^{20}$Ne($p,\\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the\nfirst and slowest reaction of the NeNa cycle and it controls the speed at which\nthe entire cycle proceeds. At the state of the art, the uncertainty on the\n20Ne(p,{\\gamma})21Na reaction rate affects the production of the elements in\nthe NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK,\nthe rate is dominated by the 366 keV resonance corresponding to the excited\nstate of EX = 2797.5 keV and by the direct capture component. The present study\nfocus on the study of the 366 keV resonance and the direct capture below 400\nkeV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction has been measured using the intense\nproton beam delivered by the LUNA 400 kV accelerator and a windowless\ndifferential-pumping gas target. The products of the reaction are detected with\ntwo high-purity germanium detectors. The experimental details and preliminary\nresults on the 366 keV resonance and on the direct capture component at very\nlow energies will be shown, together with their possible impact on the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al\nisotopes. The $^{20}$Ne($p,\\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the\nfirst and slowest reaction of the NeNa cycle and it controls the speed at which\nthe entire cycle proceeds. At the state of the art, the uncertainty on the\n20Ne(p,{\\gamma})21Na reaction rate affects the production of the elements in\nthe NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK,\nthe rate is dominated by the 366 keV resonance corresponding to the excited\nstate of EX = 2797.5 keV and by the direct capture component. The present study\nfocus on the study of the 366 keV resonance and the direct capture below 400\nkeV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction has been measured using the intense\nproton beam delivered by the LUNA 400 kV accelerator and a windowless\ndifferential-pumping gas target. The products of the reaction are detected with\ntwo high-purity germanium detectors. The experimental details and preliminary\nresults on the 366 keV resonance and on the direct capture component at very\nlow energies will be shown, together with their possible impact on the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction rate."
                },
                "authors": [
                    {
                        "name": "A. Caciolli"
                    }
                ],
                "author_detail": {
                    "name": "A. Caciolli"
                },
                "arxiv_affiliation": "on behalf of the LUNA collaboration",
                "author": "A. Caciolli",
                "arxiv_doi": "10.1051/epjconf/202429207005",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/epjconf/202429207005",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.01884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "EPJ Web Conf., 292 (2024) 07005",
                "arxiv_primary_category": {
                    "term": "nucl-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20211v2",
                "updated": "2025-10-02T04:11:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    4,
                    11,
                    7,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-26T16:52:40Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    52,
                    40,
                    0,
                    146,
                    0
                ],
                "title": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection"
                },
                "summary": "Fine-tuning large foundation models is essential for building expert models\ntailored to specialized tasks and domains, but fully updating billions of\nparameters is computationally prohibitive. Reducing the number of trainable\nparameters using parameter-efficient fine-tuning is therefore crucial not only\nto reduce training costs but also to mitigate storage, caching, and serving\noverheads during deployment. Prior works, such as Singular Vectors-guided\nFine-Tuning, have shown that exploiting the geometry of pre-trained weights can\nsignificantly improve parameter-efficiency, but they lack a solid theoretical\nfoundation. In this paper, we introduce Parameter-efficient Fine-tuning with\nColumn Space Projection (PiCa), a novel theoretically grounded PEFT method. We\nprove that projecting gradients onto the principal column space of pre-trained\nweights provides an effective inductive bias for adaptation and further enhance\nparameter efficiency through a novel weight-sharing strategy. Across diverse\nNLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines\nunder comparable or smaller parameter budgets, demonstrating both theoretical\nrigor and practical effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large foundation models is essential for building expert models\ntailored to specialized tasks and domains, but fully updating billions of\nparameters is computationally prohibitive. Reducing the number of trainable\nparameters using parameter-efficient fine-tuning is therefore crucial not only\nto reduce training costs but also to mitigate storage, caching, and serving\noverheads during deployment. Prior works, such as Singular Vectors-guided\nFine-Tuning, have shown that exploiting the geometry of pre-trained weights can\nsignificantly improve parameter-efficiency, but they lack a solid theoretical\nfoundation. In this paper, we introduce Parameter-efficient Fine-tuning with\nColumn Space Projection (PiCa), a novel theoretically grounded PEFT method. We\nprove that projecting gradients onto the principal column space of pre-trained\nweights provides an effective inductive bias for adaptation and further enhance\nparameter efficiency through a novel weight-sharing strategy. Across diverse\nNLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines\nunder comparable or smaller parameter budgets, demonstrating both theoretical\nrigor and practical effectiveness."
                },
                "authors": [
                    {
                        "name": "Junseo Hwang"
                    },
                    {
                        "name": "Wonguk Cho"
                    },
                    {
                        "name": "Taesup Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taesup Kim"
                },
                "author": "Taesup Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07447v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07447v4",
                "updated": "2025-10-01T20:30:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    20,
                    30,
                    18,
                    2,
                    274,
                    0
                ],
                "published": "2024-11-12T00:10:34Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    0,
                    10,
                    34,
                    1,
                    317,
                    0
                ],
                "title": "Faster LLM Inference using DBMS-Inspired Preemption and Cache\n  Replacement Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster LLM Inference using DBMS-Inspired Preemption and Cache\n  Replacement Policies"
                },
                "summary": "LLMs are increasingly used world-wide from daily tasks to agentic systems and\ndata analytics, requiring significant GPU resources. LLM inference systems,\nhowever, are slow compared to database systems, and inference performance and\nmechanism have been often regarded as a black box, limiting the expansion of\nthe use of LLMs inside databases and other performance-critical applications.\nThis paper first analyzes the LLM inference performance and focuses on a data\nmanagement issue inside LLM inference. We find that inference systems lack an\nadequate resource cost model and optimization strategy to schedule requests\nwith their intermediate results in a cache reside in GPU memory when executing\nmultiple concurrent inference requests. We adapt classic database techniques by\nbuilding cost models for concurrent inference requests and a new cache\nreplacement policy tailored for LLM inference, which can substantially save GPU\ncosts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used world-wide from daily tasks to agentic systems and\ndata analytics, requiring significant GPU resources. LLM inference systems,\nhowever, are slow compared to database systems, and inference performance and\nmechanism have been often regarded as a black box, limiting the expansion of\nthe use of LLMs inside databases and other performance-critical applications.\nThis paper first analyzes the LLM inference performance and focuses on a data\nmanagement issue inside LLM inference. We find that inference systems lack an\nadequate resource cost model and optimization strategy to schedule requests\nwith their intermediate results in a cache reside in GPU memory when executing\nmultiple concurrent inference requests. We adapt classic database techniques by\nbuilding cost models for concurrent inference requests and a new cache\nreplacement policy tailored for LLM inference, which can substantially save GPU\ncosts."
                },
                "authors": [
                    {
                        "name": "Kyoungmin Kim"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Kijae Hong"
                    },
                    {
                        "name": "Anastasia Ailamaki"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Ailamaki"
                },
                "author": "Anastasia Ailamaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07447v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07447v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v2",
                "updated": "2025-10-01T19:06:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    19,
                    6,
                    10,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v2",
                "updated": "2025-10-01T18:55:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    55,
                    20,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01336v1",
                "updated": "2025-10-01T18:04:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    4,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T18:04:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    4,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiSpec: Hierarchical Speculative Decoding for LLMs"
                },
                "summary": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Sujay Sanghavi"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00948v1",
                "updated": "2025-10-01T14:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    21,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T14:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    21,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution"
                },
                "summary": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR."
                },
                "authors": [
                    {
                        "name": "Ziqing Zhang"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Xi Li"
                    },
                    {
                        "name": "Yucong Chen"
                    },
                    {
                        "name": "Bingnan Duan"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhang"
                },
                "author": "Yulun Zhang",
                "arxiv_comment": "Code will be available at https://github.com/Kai-Liu001/InfVSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26432v2",
                "updated": "2025-10-01T11:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    26,
                    36,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size"
                },
                "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs."
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Yuto Karashima"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00636v1",
                "updated": "2025-10-01T08:12:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    12,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T08:12:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    12,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution"
                },
                "summary": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Maximilian Jeblick"
                    },
                    {
                        "name": "Simon Jégou"
                    }
                ],
                "author_detail": {
                    "name": "Simon Jégou"
                },
                "author": "Simon Jégou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v1",
                "updated": "2025-10-01T06:38:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Panagiotis Karras"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Jignesh M. Patel"
                },
                "author": "Jignesh M. Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00536v1",
                "updated": "2025-10-01T05:37:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    37,
                    54,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T05:37:54Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    37,
                    54,
                    2,
                    274,
                    0
                ],
                "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness"
                },
                "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance."
                },
                "authors": [
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Haoyi Qiu"
                    },
                    {
                        "name": "Yutong Dai"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25454v2",
                "updated": "2025-10-01T05:09:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    9,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T20:00:29Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    0,
                    29,
                    0,
                    272,
                    0
                ],
                "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search"
                },
                "summary": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation."
                },
                "authors": [
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Aaron Tu"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01290v1",
                "updated": "2025-10-01T04:09:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    9,
                    2,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T04:09:02Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    9,
                    2,
                    2,
                    274,
                    0
                ],
                "title": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning\n  Models"
                },
                "summary": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Akshat Ramachandran"
                    },
                    {
                        "name": "Marina Neseem"
                    },
                    {
                        "name": "Charbel Sakr"
                    },
                    {
                        "name": "Rangharajan Venkatesan"
                    },
                    {
                        "name": "Brucek Khailany"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01289v1",
                "updated": "2025-10-01T02:56:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    56,
                    59,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T02:56:59Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    56,
                    59,
                    2,
                    274,
                    0
                ],
                "title": "Detailed Derivation of the Scalar Explicit Expressions Governing the\n  Electric Field, Current Density, and Volumetric Power Density in the Four\n  Types of Linear Divergent MHD Channels Under a Unidirectional Applied\n  Magnetic Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detailed Derivation of the Scalar Explicit Expressions Governing the\n  Electric Field, Current Density, and Volumetric Power Density in the Four\n  Types of Linear Divergent MHD Channels Under a Unidirectional Applied\n  Magnetic Field"
                },
                "summary": "The current study belongs to the field of applied mathematics in plasma\nphysics and electric power, where mathematical analysis of the algebraic\nequations governing the electric field vector, and the electric-current density\nfield vector within a Magnetohydrodynamic (MHD) linear two-dimensional\ndivergent supersonic channel is utilized to derive analytical expressions for\nthese important fields, as well as closed-form equations for the volumetric\npower density (output electric power per unit volume of the plasma channel).\nThe expressions presented here describe analytically the operation of the MHD\nchannel as an electric power source within an Open-Cycle Magnetohydrodynamic\n(OCMHD) generator. The four common types of the MHD linear channels are covered\nhere: namely, (1) continuous-electrode Faraday channel, (2) linear Hall\nchannel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode\nchannel. The mathematical results, their detailed derivation, and the companion\ngraphical illustrations aid in making a proper decision regarding which channel\ntype is the most suitable for a given application.Under typical operational\nconditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000\nm/s plasma speed, as well as an optimized load factor of 0.5, we estimate the\nfollowing numerical values (unsigned magnitudes) for the continuous-electrode\nFaraday channel (with a Hall parameter of 1): useful electric field (across the\nexternal electric load): 5 kV/m, useful electric current-density (between the\nterminal electrodes within the channel): 12.5 kA/m2 , volumetric power density\n(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric\nefficiency (for the electric field or voltage): 50%. For the Halllinear channel\n(with a Hall parameter of 5), these quantitative performance values become25\nkV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current study belongs to the field of applied mathematics in plasma\nphysics and electric power, where mathematical analysis of the algebraic\nequations governing the electric field vector, and the electric-current density\nfield vector within a Magnetohydrodynamic (MHD) linear two-dimensional\ndivergent supersonic channel is utilized to derive analytical expressions for\nthese important fields, as well as closed-form equations for the volumetric\npower density (output electric power per unit volume of the plasma channel).\nThe expressions presented here describe analytically the operation of the MHD\nchannel as an electric power source within an Open-Cycle Magnetohydrodynamic\n(OCMHD) generator. The four common types of the MHD linear channels are covered\nhere: namely, (1) continuous-electrode Faraday channel, (2) linear Hall\nchannel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode\nchannel. The mathematical results, their detailed derivation, and the companion\ngraphical illustrations aid in making a proper decision regarding which channel\ntype is the most suitable for a given application.Under typical operational\nconditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000\nm/s plasma speed, as well as an optimized load factor of 0.5, we estimate the\nfollowing numerical values (unsigned magnitudes) for the continuous-electrode\nFaraday channel (with a Hall parameter of 1): useful electric field (across the\nexternal electric load): 5 kV/m, useful electric current-density (between the\nterminal electrodes within the channel): 12.5 kA/m2 , volumetric power density\n(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric\nefficiency (for the electric field or voltage): 50%. For the Halllinear channel\n(with a Hall parameter of 5), these quantitative performance values become25\nkV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%."
                },
                "authors": [
                    {
                        "name": "Osama A. Marzouk"
                    }
                ],
                "author_detail": {
                    "name": "Osama A. Marzouk"
                },
                "author": "Osama A. Marzouk",
                "arxiv_doi": "10.37256/cm.6420256918",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.37256/cm.6420256918",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.01289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "41 pages, 8 figures, 4 tables, published journal article,\n  peer-reviewed, open access",
                "arxiv_journal_ref": "Contemporary Mathematics. volume 6, issue 4, pages 4060-4100,\n  https://ojs.wiserpub.com/index.php/CM/article/view/6918 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00A79, 03H10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00294v1",
                "updated": "2025-09-30T21:28:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    28,
                    4,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T21:28:04Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    28,
                    4,
                    1,
                    273,
                    0
                ],
                "title": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for\n  Diffusion Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for\n  Diffusion Large Language Models"
                },
                "summary": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of\nlanguage modeling beyond autoregressive next-token prediction. Thanks to their\nbidirectional attention mechanism, DLLMs are more capable of capturing the\nconnection of context, and thus show unique advantages in challenges like the\nfamous \"reversal curse\" or learning under data-constrained scenarios. However,\nthis bidirectional nature also brings an obstacle that DLLMs are not inherently\ncompatible with KV Cache, and consequently, the inference efficiency is not\ncompetitive compared with autoregressive models. Taking advantage of their\ninherent capability of multi-token prediction, existing parallel decoding\nalgorithms can speed up the DLLM inference, but at the cost of non-negligible\nperformance degradation. To overcome this challenge, we introduce Free\nDraft-and-Verification (Freedave), a novel fast sampling algorithm tailored for\nDLLMs that achieves lossless parallel decoding. Specifically, we propose a\npipeline of parallel-decoded candidate generation and verification, which is\nguaranteed to reproduce the same sequence generated by static sampling, without\nintroducing extra model forward calls. By applying Freedave, the throughput of\nDLLMs can be boosted up to $2.8\\times$ without performance degradation on math\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of\nlanguage modeling beyond autoregressive next-token prediction. Thanks to their\nbidirectional attention mechanism, DLLMs are more capable of capturing the\nconnection of context, and thus show unique advantages in challenges like the\nfamous \"reversal curse\" or learning under data-constrained scenarios. However,\nthis bidirectional nature also brings an obstacle that DLLMs are not inherently\ncompatible with KV Cache, and consequently, the inference efficiency is not\ncompetitive compared with autoregressive models. Taking advantage of their\ninherent capability of multi-token prediction, existing parallel decoding\nalgorithms can speed up the DLLM inference, but at the cost of non-negligible\nperformance degradation. To overcome this challenge, we introduce Free\nDraft-and-Verification (Freedave), a novel fast sampling algorithm tailored for\nDLLMs that achieves lossless parallel decoding. Specifically, we propose a\npipeline of parallel-decoded candidate generation and verification, which is\nguaranteed to reproduce the same sequence generated by static sampling, without\nintroducing extra model forward calls. By applying Freedave, the throughput of\nDLLMs can be boosted up to $2.8\\times$ without performance degradation on math\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Shutong Wu"
                    },
                    {
                        "name": "Jiawei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhang"
                },
                "author": "Jiawei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00231v1",
                "updated": "2025-09-30T19:55:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    55,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T19:55:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    55,
                    26,
                    1,
                    273,
                    0
                ],
                "title": "The Pitfalls of KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pitfalls of KV Cache Compression"
                },
                "summary": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks."
                },
                "authors": [
                    {
                        "name": "Alex Chen"
                    },
                    {
                        "name": "Renato Geh"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Daniel Israel"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Israel"
                },
                "author": "Daniel Israel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00184v1",
                "updated": "2025-09-30T19:03:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    3,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T19:03:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    3,
                    26,
                    1,
                    273,
                    0
                ],
                "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls"
                },
                "summary": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Bai"
                    },
                    {
                        "name": "Itamar Pres"
                    },
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Chenhao Tan"
                    },
                    {
                        "name": "Stuart Shieber"
                    },
                    {
                        "name": "Fernanda Viégas"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Andrew Lee"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lee"
                },
                "author": "Andrew Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v1",
                "updated": "2025-09-30T17:15:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v2",
                "updated": "2025-09-30T16:42:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    42,
                    50,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v3",
                "updated": "2025-09-30T15:44:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    44,
                    29,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_doi": "10.1109/TED.2025.3617043",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TED.2025.3617043",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to IEEE Trans. Elec. Dev. Work enabled in part by NanoIC\n  pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26328v1",
                "updated": "2025-09-30T14:40:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:40:18Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM v2: Efficient Block-Diffusion LLM"
                },
                "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v4",
                "updated": "2025-09-30T14:13:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    13,
                    20,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by NeurIPS 2025. Code at https://github.com/NVlabs/Long-RL\n  and model at https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v2",
                "updated": "2025-09-30T09:10:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    9,
                    10,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Lin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shi"
                },
                "author": "Lin Shi",
                "arxiv_comment": "Accepted by ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v2",
                "updated": "2025-09-30T02:51:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    51,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "NeurIPS 2025 Oral. Code: https://github.com/snu-mllab/KVzip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25681v1",
                "updated": "2025-09-30T02:36:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T02:36:11Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought"
                },
                "summary": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Yi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Xu"
                },
                "author": "Yi Xu",
                "arxiv_comment": "technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25401v1",
                "updated": "2025-09-29T18:57:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T18:57:14Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers"
                },
                "summary": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality."
                },
                "authors": [
                    {
                        "name": "Liang Qiao"
                    },
                    {
                        "name": "Yue Dai"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Hongyu Kan"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25188v2",
                "updated": "2025-10-03T00:40:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    0,
                    40,
                    49,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-29T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    59,
                    54,
                    0,
                    272,
                    0
                ],
                "title": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Learnable Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Learnable Parallel Decoding"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache."
                },
                "authors": [
                    {
                        "name": "Wenrui Bao"
                    },
                    {
                        "name": "Zhiben Chen"
                    },
                    {
                        "name": "Dan Xu"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25155v1",
                "updated": "2025-09-29T17:55:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:55:43Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "title": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units"
                },
                "summary": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts."
                },
                "authors": [
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rakshith Jayanth"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IEEE HiPC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v2",
                "updated": "2025-09-29T15:20:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    20,
                    29,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "EMNLP 2025; 15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v2",
                "updated": "2025-09-29T15:15:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    49,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24832v1",
                "updated": "2025-09-29T14:16:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching"
                },
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "arxiv_comment": "11 figures, 14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24791v1",
                "updated": "2025-09-29T13:45:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:45:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "title": "Vision Function Layer in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Function Layer in Multimodal LLMs"
                },
                "summary": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models."
                },
                "authors": [
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang",
                "arxiv_comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v3",
                "updated": "2025-09-29T12:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    34,
                    50,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v1",
                "updated": "2025-09-29T12:28:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24626v1",
                "updated": "2025-09-29T11:35:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T11:35:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving"
                },
                "summary": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "14 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24407v1",
                "updated": "2025-09-29T07:54:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T07:54:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network"
                },
                "summary": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter."
                },
                "authors": [
                    {
                        "name": "Karl C. Linne"
                    },
                    {
                        "name": "Yuanyuan Li"
                    },
                    {
                        "name": "Debashri Roy"
                    },
                    {
                        "name": "Kaushik Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Chowdhury"
                },
                "arxiv_affiliation": "Kai Li",
                "author": "Kaushik Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v2",
                "updated": "2025-09-29T05:12:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    5,
                    12,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v2",
                "updated": "2025-09-29T02:46:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    2,
                    46,
                    45,
                    0,
                    272,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24178v1",
                "updated": "2025-09-29T01:52:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T01:52:10Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "title": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring"
                },
                "summary": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhou"
                    },
                    {
                        "name": "Steve Majerus"
                    },
                    {
                        "name": "Gourav Datta"
                    }
                ],
                "author_detail": {
                    "name": "Gourav Datta"
                },
                "author": "Gourav Datta",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24088v1",
                "updated": "2025-09-28T21:47:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T21:47:20Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Moyan Li"
                    },
                    {
                        "name": "Shaoyuan Xu"
                    },
                    {
                        "name": "Jinmiao Fu"
                    },
                    {
                        "name": "Xinhai Hou"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Bryan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wang"
                },
                "author": "Bryan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24007v1",
                "updated": "2025-09-28T17:59:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T17:59:15Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "title": "Sequential Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Diffusion Language Models"
                },
                "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM"
                },
                "authors": [
                    {
                        "name": "Yangzhou Liu"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Xiaobo Liang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Yanting Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "14 pages, 5 figures, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23928v1",
                "updated": "2025-09-28T15:05:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T15:05:21Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models"
                },
                "summary": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference."
                },
                "authors": [
                    {
                        "name": "Zhinan Xie"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Jian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jian Cheng"
                },
                "author": "Jian Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v2",
                "updated": "2025-09-28T08:32:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    8,
                    32,
                    26,
                    6,
                    271,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23601v1",
                "updated": "2025-09-28T03:12:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T03:12:43Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration"
                },
                "summary": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba."
                },
                "authors": [
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Chen Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lyu"
                },
                "author": "Chen Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v2",
                "updated": "2025-09-27T20:13:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    20,
                    13,
                    25,
                    5,
                    270,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Fei Yi"
                    },
                    {
                        "name": "Weidi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Zeng"
                },
                "author": "Weidi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23179v1",
                "updated": "2025-09-27T08:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T08:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "title": "A Near-Cache Architectural Framework for Cryptographic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Near-Cache Architectural Framework for Cryptographic Computing"
                },
                "summary": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath."
                },
                "authors": [
                    {
                        "name": "Jingyao Zhang"
                    },
                    {
                        "name": "Elaheh Sadredini"
                    }
                ],
                "author_detail": {
                    "name": "Elaheh Sadredini"
                },
                "author": "Elaheh Sadredini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v4",
                "updated": "2025-09-27T07:41:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    7,
                    41,
                    38,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "Runtime Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runtime Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23094v1",
                "updated": "2025-09-27T04:07:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T04:07:23Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching"
                },
                "summary": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache."
                },
                "authors": [
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Yue Cai"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Chonghan Liu"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v3",
                "updated": "2025-09-27T03:37:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    3,
                    37,
                    40,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v4",
                "updated": "2025-09-26T21:40:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    21,
                    40,
                    58,
                    4,
                    269,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22875v1",
                "updated": "2025-09-26T19:40:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T19:40:33Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "title": "On KV-Poisson Structure and related invariants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On KV-Poisson Structure and related invariants"
                },
                "summary": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures."
                },
                "authors": [
                    {
                        "name": "Prosper Rosaire Mama Assandje"
                    },
                    {
                        "name": "Herguey Mopeng"
                    },
                    {
                        "name": "Joseph Dongho"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Dongho"
                },
                "author": "Joseph Dongho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v2",
                "updated": "2025-09-26T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Controlling Frozen LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Controlling Frozen LLMs"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "James R. Glass"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v1",
                "updated": "2025-09-26T17:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22548v1",
                "updated": "2025-09-26T16:29:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:29:37Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation"
                },
                "summary": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/."
                },
                "authors": [
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Dekang Qi"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Feng Xiong"
                    },
                    {
                        "name": "Shichao Xie"
                    },
                    {
                        "name": "Xiaolong Wu"
                    },
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "arxiv_comment": "Project page: https://miv-xjtu.github.io/JanusVLN.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22516v1",
                "updated": "2025-09-26T16:00:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:00:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments"
                },
                "summary": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment."
                },
                "authors": [
                    {
                        "name": "Rakesh Thakur"
                    },
                    {
                        "name": "Shivaansh Kaushik"
                    },
                    {
                        "name": "Gauri Chopra"
                    },
                    {
                        "name": "Harsh Rohilla"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Rohilla"
                },
                "author": "Harsh Rohilla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22512v1",
                "updated": "2025-09-26T15:54:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:54:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability"
                },
                "summary": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware."
                },
                "authors": [
                    {
                        "name": "Soroush Ahadi"
                    },
                    {
                        "name": "Mehdi Modarressi"
                    },
                    {
                        "name": "Masoud Daneshtalab"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Daneshtalab"
                },
                "author": "Masoud Daneshtalab",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "n/a",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22488v1",
                "updated": "2025-09-26T15:35:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:35:05Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "title": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT"
                },
                "summary": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches."
                },
                "authors": [
                    {
                        "name": "Pierre-Antoine Rodesch"
                    },
                    {
                        "name": "Anaïs Viry"
                    },
                    {
                        "name": "Mouad Khorsi"
                    },
                    {
                        "name": "Fabio Becce"
                    },
                    {
                        "name": "Jérôme Damet"
                    },
                    {
                        "name": "Lucía Gallego Manzano"
                    }
                ],
                "author_detail": {
                    "name": "Lucía Gallego Manzano"
                },
                "author": "Lucía Gallego Manzano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v3",
                "updated": "2025-09-26T14:35:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    35,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning"
                },
                "summary": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22323v1",
                "updated": "2025-09-26T13:20:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:20:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality."
                },
                "authors": [
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Zhiwei Tang"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Pengfei Zhou"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v8",
                "updated": "2025-09-26T10:00:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    10,
                    0,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22756v1",
                "updated": "2025-09-26T09:33:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T09:33:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous\n  Driving"
                },
                "summary": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences."
                },
                "authors": [
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Changjie Wu"
                    },
                    {
                        "name": "Huiyuan Yan"
                    },
                    {
                        "name": "Yifan Bai"
                    },
                    {
                        "name": "Xinran Liu"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Yujian Yuan"
                    },
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v2",
                "updated": "2025-09-26T07:14:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    7,
                    14,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21917v1",
                "updated": "2025-09-26T05:57:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    57,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T05:57:04Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    57,
                    4,
                    4,
                    269,
                    0
                ],
                "title": "Taming Flow-based I2V Models for Creative Video Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Flow-based I2V Models for Creative Video Editing"
                },
                "summary": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity."
                },
                "authors": [
                    {
                        "name": "Xianghao Kong"
                    },
                    {
                        "name": "Hansheng Chen"
                    },
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Lvmin Zhang"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Anyi Rao"
                    }
                ],
                "author_detail": {
                    "name": "Anyi Rao"
                },
                "author": "Anyi Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21857v1",
                "updated": "2025-09-26T04:32:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    32,
                    56,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T04:32:56Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    32,
                    56,
                    4,
                    269,
                    0
                ],
                "title": "2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with\n  sub-micron fin width",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with\n  sub-micron fin width"
                },
                "summary": "In this letter, we present a kilovolt-class \\b{eta}-Ga2O3 vertical trench\nSchottky barrier diode with a field plate incorporating narrow fin width (Wfin)\nstructures of sub-micron dimensions. We used a nanolaminate dielectric\ncomprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric\nand for field plate edge termination. Both Wfin of 200 nm and 500 nm\ndemonstrate excellent on-state performance with specific on-resistance (Ron,sp)\nof 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist\nplanarization and etch-back process was employed to expose the top of the fins\nfor Schottky contact formation, eliminating critical lithographic alignment\nchallenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV\nwith very low leakage currents before catastrophic breakdown. The measured\nbreakdown voltage is limited by dielectric breakdown at the trench bottom\ncorner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD\nsimulation shows a reduced electric field at the surface of the\nmetal-semiconductor junction due to the RESURF effect, resulting in very low\nreverse leakage before breakdown. The parallel plane electric field in the\n\\b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using\naccurately extracted drift layer doping profile from high voltage CV\nmeasurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current\nspreading) was calculated. Enhanced RESURF by integration of high-k dielectrics\nwith self-aligned photoresist planarization, offers a promising pathway towards\nhigh figure of merit, low leakage high-performance vertical devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we present a kilovolt-class \\b{eta}-Ga2O3 vertical trench\nSchottky barrier diode with a field plate incorporating narrow fin width (Wfin)\nstructures of sub-micron dimensions. We used a nanolaminate dielectric\ncomprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric\nand for field plate edge termination. Both Wfin of 200 nm and 500 nm\ndemonstrate excellent on-state performance with specific on-resistance (Ron,sp)\nof 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist\nplanarization and etch-back process was employed to expose the top of the fins\nfor Schottky contact formation, eliminating critical lithographic alignment\nchallenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV\nwith very low leakage currents before catastrophic breakdown. The measured\nbreakdown voltage is limited by dielectric breakdown at the trench bottom\ncorner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD\nsimulation shows a reduced electric field at the surface of the\nmetal-semiconductor junction due to the RESURF effect, resulting in very low\nreverse leakage before breakdown. The parallel plane electric field in the\n\\b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using\naccurately extracted drift layer doping profile from high voltage CV\nmeasurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current\nspreading) was calculated. Enhanced RESURF by integration of high-k dielectrics\nwith self-aligned photoresist planarization, offers a promising pathway towards\nhigh figure of merit, low leakage high-performance vertical devices."
                },
                "authors": [
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21842v1",
                "updated": "2025-09-26T04:03:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    3,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T04:03:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    3,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents"
                },
                "summary": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks."
                },
                "authors": [
                    {
                        "name": "Yansong Ning"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Jun Fang"
                    },
                    {
                        "name": "Kan Zheng"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v3",
                "updated": "2025-09-26T03:24:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    24,
                    20,
                    4,
                    269,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v3",
                "updated": "2025-09-26T03:17:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    17,
                    15,
                    4,
                    269,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21623v1",
                "updated": "2025-09-25T21:42:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    42,
                    27,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T21:42:27Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    42,
                    27,
                    3,
                    268,
                    0
                ],
                "title": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's\n  Rule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's\n  Rule"
                },
                "summary": "The expanding long-context capabilities of large language models are\nconstrained by a significant memory bottleneck: the key-value (KV) cache\nrequired for autoregressive generation. This bottleneck is substantial; for\ninstance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of\n4 requires approximately 16GB for its KV cache, a size exceeding the model's\nweights. While KV-cache compression via low-rank projection is a promising\ndirection, existing methods rely on a static, offline-learned subspace that\nperforms poorly under data distribution shifts. To overcome these limitations,\nwe introduce OjaKV, a novel framework that integrates a strategic hybrid\nstorage policy with online subspace adaptation. First, OjaKV recognizes that\nnot all tokens are equally important for compression; it preserves the crucial\nfirst and most recent tokens in full-rank, maintaining high-fidelity anchors\nfor attention. Second, for the vast majority of intermediate tokens, it applies\nlow-rank compression by incrementally adapting the projection basis using Oja's\nalgorithm for online principal component analysis. This adaptation involves a\ncomprehensive update during prompt prefilling and lightweight periodic updates\nduring decoding, ensuring the subspace remains aligned with the evolving\ncontext. Crucially, our framework is fully compatible with modern attention\nmodules like FlashAttention. Experiments demonstrate that OjaKV maintains or\neven improves zero-shot accuracy at high compression ratios. In particular,\nOjaKV achieves its strongest gains on very long-context benchmarks that require\ncomplex reasoning, highlighting the importance of online subspace adaptation in\ndynamically tracking context shifts. These results establish our hybrid\nframework as a practical, plug-and-play solution for memory-efficient\nlong-context inference without requiring model fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding long-context capabilities of large language models are\nconstrained by a significant memory bottleneck: the key-value (KV) cache\nrequired for autoregressive generation. This bottleneck is substantial; for\ninstance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of\n4 requires approximately 16GB for its KV cache, a size exceeding the model's\nweights. While KV-cache compression via low-rank projection is a promising\ndirection, existing methods rely on a static, offline-learned subspace that\nperforms poorly under data distribution shifts. To overcome these limitations,\nwe introduce OjaKV, a novel framework that integrates a strategic hybrid\nstorage policy with online subspace adaptation. First, OjaKV recognizes that\nnot all tokens are equally important for compression; it preserves the crucial\nfirst and most recent tokens in full-rank, maintaining high-fidelity anchors\nfor attention. Second, for the vast majority of intermediate tokens, it applies\nlow-rank compression by incrementally adapting the projection basis using Oja's\nalgorithm for online principal component analysis. This adaptation involves a\ncomprehensive update during prompt prefilling and lightweight periodic updates\nduring decoding, ensuring the subspace remains aligned with the evolving\ncontext. Crucially, our framework is fully compatible with modern attention\nmodules like FlashAttention. Experiments demonstrate that OjaKV maintains or\neven improves zero-shot accuracy at high compression ratios. In particular,\nOjaKV achieves its strongest gains on very long-context benchmarks that require\ncomplex reasoning, highlighting the importance of online subspace adaptation in\ndynamically tracking context shifts. These results establish our hybrid\nframework as a practical, plug-and-play solution for memory-efficient\nlong-context inference without requiring model fine-tuning."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pin-Yu Chen"
                },
                "author": "Pin-Yu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21463v1",
                "updated": "2025-09-25T19:29:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    29,
                    25,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T19:29:25Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    29,
                    25,
                    3,
                    268,
                    0
                ],
                "title": "Enhanced Generative Machine Listener",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Generative Machine Listener"
                },
                "summary": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies."
                },
                "authors": [
                    {
                        "name": "Vishnu Raj"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Shiv Gehlot"
                    },
                    {
                        "name": "Lars Villemoes"
                    },
                    {
                        "name": "Arijit Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Arijit Biswas"
                },
                "author": "Arijit Biswas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v2",
                "updated": "2025-09-25T13:55:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    55,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21153v1",
                "updated": "2025-09-25T13:39:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:39:16Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP"
                },
                "summary": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings."
                },
                "authors": [
                    {
                        "name": "Moshe Kimhi"
                    },
                    {
                        "name": "Erez Koifman"
                    },
                    {
                        "name": "Ehud Rivlin"
                    },
                    {
                        "name": "Eli Schwartz"
                    },
                    {
                        "name": "Chaim Baskin"
                    }
                ],
                "author_detail": {
                    "name": "Chaim Baskin"
                },
                "author": "Chaim Baskin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v2",
                "updated": "2025-09-25T13:15:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    45,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "18 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v2",
                "updated": "2025-09-25T10:24:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    24,
                    14,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20979v1",
                "updated": "2025-09-25T10:23:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T10:23:50Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "title": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference"
                },
                "summary": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Yirong Zhang"
                    },
                    {
                        "name": "Jiahong Yu"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jianping Zou"
                    },
                    {
                        "name": "Gang Xiong"
                    },
                    {
                        "name": "Kingsum Chow"
                    },
                    {
                        "name": "Shuibing He"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v5",
                "updated": "2025-09-25T09:49:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    9,
                    49,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v2",
                "updated": "2025-09-25T03:30:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    30,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v2",
                "updated": "2025-09-25T03:00:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    0,
                    22,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20617v1",
                "updated": "2025-09-24T23:47:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T23:47:55Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "title": "DELM: a Python toolkit for Data Extraction with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELM: a Python toolkit for Data Extraction with Language Models"
                },
                "summary": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}."
                },
                "authors": [
                    {
                        "name": "Eric Fithian"
                    },
                    {
                        "name": "Kirill Skobelev"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Skobelev"
                },
                "author": "Kirill Skobelev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v2",
                "updated": "2025-09-24T16:56:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    56,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective"
                },
                "summary": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19729v1",
                "updated": "2025-09-24T03:15:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T03:15:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference"
                },
                "summary": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Haoyu Chen"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Kun Qian"
                    },
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Jin Zhao"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "12 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v2",
                "updated": "2025-09-24T01:32:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    32,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_doi": "10.1145/3731599.3767498",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731599.3767498",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01239v1",
                "updated": "2025-09-24T01:20:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    20,
                    47,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T01:20:47Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    20,
                    47,
                    2,
                    267,
                    0
                ],
                "title": "CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn\n  Interactions with a Single On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn\n  Interactions with a Single On-Device LLM"
                },
                "summary": "We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which\nis a novel execution system for efficient sub-task handling in multi-turn\ninteractions with a single on-device large language model (LLM). As LLMs become\nincreasingly capable, a single model is expected to handle diverse sub-tasks\nthat more effectively and comprehensively support answering user requests.\nNaive approach reprocesses the entire conversation context when switching\nbetween main and sub-tasks (e.g., query rewriting, summarization), incurring\nsignificant computational overhead. CIFLEX mitigates this overhead by reusing\nthe key-value (KV) cache from the main task and injecting only task-specific\ninstructions into isolated side paths. After sub-task execution, the model\nrolls back to the main path via cached context, thereby avoiding redundant\nprefill computation. To support sub-task selection, we also develop a\nhierarchical classification strategy tailored for small-scale models,\ndecomposing multi-choice decisions into binary ones. Experiments show that\nCIFLEX significantly reduces computational costs without degrading task\nperformance, enabling scalable and efficient multi-task dialogue on-device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which\nis a novel execution system for efficient sub-task handling in multi-turn\ninteractions with a single on-device large language model (LLM). As LLMs become\nincreasingly capable, a single model is expected to handle diverse sub-tasks\nthat more effectively and comprehensively support answering user requests.\nNaive approach reprocesses the entire conversation context when switching\nbetween main and sub-tasks (e.g., query rewriting, summarization), incurring\nsignificant computational overhead. CIFLEX mitigates this overhead by reusing\nthe key-value (KV) cache from the main task and injecting only task-specific\ninstructions into isolated side paths. After sub-task execution, the model\nrolls back to the main path via cached context, thereby avoiding redundant\nprefill computation. To support sub-task selection, we also develop a\nhierarchical classification strategy tailored for small-scale models,\ndecomposing multi-choice decisions into binary ones. Experiments show that\nCIFLEX significantly reduces computational costs without degrading task\nperformance, enabling scalable and efficient multi-task dialogue on-device."
                },
                "authors": [
                    {
                        "name": "Juntae Lee"
                    },
                    {
                        "name": "Jihwan Bang"
                    },
                    {
                        "name": "Seunghan Yang"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "accepted at EMNLP 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19599v1",
                "updated": "2025-09-23T21:46:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T21:46:38Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "title": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing."
                },
                "authors": [
                    {
                        "name": "Danilo Trombino"
                    },
                    {
                        "name": "Vincenzo Pecorella"
                    },
                    {
                        "name": "Alessandro de Giulii"
                    },
                    {
                        "name": "Davide Tresoldi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Tresoldi"
                },
                "author": "Davide Tresoldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v4",
                "updated": "2025-09-23T21:08:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    8,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "CVPR 2025. Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v3",
                "updated": "2025-09-23T20:25:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    20,
                    25,
                    15,
                    1,
                    266,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19459v1",
                "updated": "2025-09-23T18:14:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T18:14:21Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "title": "Automated Insertion of Flushes and Fences for Persistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Insertion of Flushes and Fences for Persistency"
                },
                "summary": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations."
                },
                "authors": [
                    {
                        "name": "Yutong Guo"
                    },
                    {
                        "name": "Weiyu Luo"
                    },
                    {
                        "name": "Brian Demsky"
                    }
                ],
                "author_detail": {
                    "name": "Brian Demsky"
                },
                "author": "Brian Demsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19228v1",
                "updated": "2025-09-23T16:49:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:49:43Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "title": "CompLLM: Compression for Long Context Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompLLM: Compression for Long Context Q&A"
                },
                "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility."
                },
                "authors": [
                    {
                        "name": "Gabriele Berton"
                    },
                    {
                        "name": "Jayakrishnan Unnikrishnan"
                    },
                    {
                        "name": "Son Tran"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19061v1",
                "updated": "2025-09-23T14:25:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:25:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics"
                },
                "summary": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators."
                },
                "authors": [
                    {
                        "name": "Marcel Ferrari"
                    },
                    {
                        "name": "Cyrill Püntener"
                    },
                    {
                        "name": "Alexander Sotoudeh"
                    },
                    {
                        "name": "Niklas Viebig"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Viebig"
                },
                "author": "Niklas Viebig",
                "arxiv_comment": "15 pages, 5 figures, appendix has 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F08, 65N55, 65N22, 76M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.8; F.2.1; D.1.3; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18909v1",
                "updated": "2025-09-23T12:32:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:32:51Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "title": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation"
                },
                "summary": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge."
                },
                "authors": [
                    {
                        "name": "Jan Wichelmann"
                    },
                    {
                        "name": "Anja Rabich"
                    },
                    {
                        "name": "Anna P\"atschke"
                    },
                    {
                        "name": "Thomas Eisenbarth"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Eisenbarth"
                },
                "author": "Thomas Eisenbarth",
                "arxiv_doi": "10.1109/SP54263.2024.00261",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP54263.2024.00261",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.18909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE Symposium on Security and Privacy (SP), San Francisco,\n  CA, USA, 2024, pp. 4182-4199",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v3",
                "updated": "2025-09-23T08:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    31,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v2",
                "updated": "2025-09-23T08:24:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    24,
                    7,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18684v1",
                "updated": "2025-09-23T06:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T06:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "title": "Static Estimation of Reuse Profiles for Arrays in Nested Loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Estimation of Reuse Profiles for Arrays in Nested Loops"
                },
                "summary": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "This paper is accepted at the MEMSYS 2025 conference, 11th\n  International Symposium on Memory Systems, Washington D.C., October 7 -\n  October 8, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18670v1",
                "updated": "2025-09-23T05:39:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T05:39:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases"
                },
                "summary": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Kyuri Park"
                    },
                    {
                        "name": "Youngjae Kim"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "arxiv_comment": "11 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18592v1",
                "updated": "2025-09-23T03:23:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T03:23:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation"
                },
                "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/."
                },
                "authors": [
                    {
                        "name": "Neel P. Bhatt"
                    },
                    {
                        "name": "Yunhao Yang"
                    },
                    {
                        "name": "Rohan Siva"
                    },
                    {
                        "name": "Pranay Samineni"
                    },
                    {
                        "name": "Daniel Milan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "arxiv_comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00329v2",
                "updated": "2025-09-22T19:20:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    20,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-31T00:52:17Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation"
                },
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}."
                },
                "authors": [
                    {
                        "name": "Muhammad Adnan"
                    },
                    {
                        "name": "Nithesh Kurella"
                    },
                    {
                        "name": "Akhil Arunkumar"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "arxiv_comment": "Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v1",
                "updated": "2025-09-22T19:08:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.02312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02312v1",
                "updated": "2025-10-02T17:59:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation"
                },
                "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference."
                },
                "authors": [
                    {
                        "name": "Anna Kuzina"
                    },
                    {
                        "name": "Maciej Pioro"
                    },
                    {
                        "name": "Paul N. Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02311v1",
                "updated": "2025-10-02T17:59:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    50,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:50Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    50,
                    3,
                    275,
                    0
                ],
                "title": "Inferring Dynamic Physical Properties from Video Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Dynamic Physical Properties from Video Foundation Models"
                },
                "summary": "We study the task of predicting dynamic physical properties from videos. More\nspecifically, we consider physical properties that require temporal information\nto be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,\nand dynamic friction of an object sliding on a surface. To this end, we make\nthe following contributions: (i) We collect a new video dataset for each\nphysical property, consisting of synthetic training and testing splits, as well\nas a real split for real world evaluation. (ii) We explore three ways to infer\nthe physical property from videos: (a) an oracle method where we supply the\nvisual cues that intrinsically reflect the property using classical computer\nvision techniques; (b) a simple read out mechanism using a visual prompt and\ntrainable prompt vector for cross-attention on pre-trained video generative and\nself-supervised models; and (c) prompt strategies for Multi-modal Large\nLanguage Models (MLLMs). (iii) We show that video foundation models trained in\na generative or self-supervised manner achieve a similar performance, though\nbehind that of the oracle, and MLLMs are currently inferior to the other\nmodels, though their performance can be improved through suitable prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the task of predicting dynamic physical properties from videos. More\nspecifically, we consider physical properties that require temporal information\nto be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,\nand dynamic friction of an object sliding on a surface. To this end, we make\nthe following contributions: (i) We collect a new video dataset for each\nphysical property, consisting of synthetic training and testing splits, as well\nas a real split for real world evaluation. (ii) We explore three ways to infer\nthe physical property from videos: (a) an oracle method where we supply the\nvisual cues that intrinsically reflect the property using classical computer\nvision techniques; (b) a simple read out mechanism using a visual prompt and\ntrainable prompt vector for cross-attention on pre-trained video generative and\nself-supervised models; and (c) prompt strategies for Multi-modal Large\nLanguage Models (MLLMs). (iii) We show that video foundation models trained in\na generative or self-supervised manner achieve a similar performance, though\nbehind that of the oracle, and MLLMs are currently inferior to the other\nmodels, though their performance can be improved through suitable prompting."
                },
                "authors": [
                    {
                        "name": "Guanqi Zhan"
                    },
                    {
                        "name": "Xianzheng Ma"
                    },
                    {
                        "name": "Weidi Xie"
                    },
                    {
                        "name": "Andrew Zisserman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zisserman"
                },
                "author": "Andrew Zisserman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02306v1",
                "updated": "2025-10-02T17:59:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    41,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:41Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    41,
                    3,
                    275,
                    0
                ],
                "title": "Drawing Conclusions from Draws: Rethinking Preference Semantics in\n  Arena-Style LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drawing Conclusions from Draws: Rethinking Preference Semantics in\n  Arena-Style LLM Evaluation"
                },
                "summary": "In arena-style evaluation of large language models (LLMs), two LLMs respond\nto a user query, and the user chooses the winning response or deems the\n\"battle\" a draw, resulting in an adjustment to the ratings of both models. The\nprevailing approach for modeling these rating dynamics is to view battles as\ntwo-player game matches, as in chess, and apply the Elo rating system and its\nderivatives. In this paper, we critically examine this paradigm. Specifically,\nwe question whether a draw genuinely means that the two models are equal and\nhence whether their ratings should be equalized. Instead, we conjecture that\ndraws are more indicative of query difficulty: if the query is too easy, then\nboth models are more likely to succeed equally. On three real-world arena\ndatasets, we show that ignoring rating updates for draws yields a 1-3% relative\nincrease in battle outcome prediction accuracy (which includes draws) for all\nfour rating systems studied. Further analyses suggest that draws occur more for\nqueries rated as very easy and those as highly objective, with risk ratios of\n1.37 and 1.35, respectively. We recommend future rating systems to reconsider\nexisting draw semantics and to account for query properties in rating updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In arena-style evaluation of large language models (LLMs), two LLMs respond\nto a user query, and the user chooses the winning response or deems the\n\"battle\" a draw, resulting in an adjustment to the ratings of both models. The\nprevailing approach for modeling these rating dynamics is to view battles as\ntwo-player game matches, as in chess, and apply the Elo rating system and its\nderivatives. In this paper, we critically examine this paradigm. Specifically,\nwe question whether a draw genuinely means that the two models are equal and\nhence whether their ratings should be equalized. Instead, we conjecture that\ndraws are more indicative of query difficulty: if the query is too easy, then\nboth models are more likely to succeed equally. On three real-world arena\ndatasets, we show that ignoring rating updates for draws yields a 1-3% relative\nincrease in battle outcome prediction accuracy (which includes draws) for all\nfour rating systems studied. Further analyses suggest that draws occur more for\nqueries rated as very easy and those as highly objective, with risk ratios of\n1.37 and 1.35, respectively. We recommend future rating systems to reconsider\nexisting draw semantics and to account for query properties in rating updates."
                },
                "authors": [
                    {
                        "name": "Raphael Tang"
                    },
                    {
                        "name": "Crystina Zhang"
                    },
                    {
                        "name": "Wenyan Li"
                    },
                    {
                        "name": "Carmen Lai"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02300v1",
                "updated": "2025-10-02T17:59:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    6,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:06Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    6,
                    3,
                    275,
                    0
                ],
                "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models"
                },
                "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference."
                },
                "authors": [
                    {
                        "name": "Runqian Wang"
                    },
                    {
                        "name": "Yilun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Du"
                },
                "author": "Yilun Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02291v1",
                "updated": "2025-10-02T17:58:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    58,
                    37,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:58:37Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    58,
                    37,
                    3,
                    275,
                    0
                ],
                "title": "Test-Time Anchoring for Discrete Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Anchoring for Discrete Diffusion Posterior Sampling"
                },
                "summary": "We study the problem of posterior sampling using pretrained discrete\ndiffusion foundation models, aiming to recover images from noisy measurements\nwithout retraining task-specific models. While diffusion models have achieved\nremarkable success in generative modeling, most advances rely on continuous\nGaussian diffusion. In contrast, discrete diffusion offers a unified framework\nfor jointly modeling categorical data such as text and images. Beyond\nunification, discrete diffusion provides faster inference, finer control, and\nprincipled training-free Bayesian inference, making it particularly well-suited\nfor posterior sampling. However, existing approaches to discrete diffusion\nposterior sampling face severe challenges: derivative-free guidance yields\nsparse signals, continuous relaxations limit applicability, and split Gibbs\nsamplers suffer from the curse of dimensionality. To overcome these\nlimitations, we introduce Anchored Posterior Sampling (APS) for masked\ndiffusion foundation models, built on two key innovations -- quantized\nexpectation for gradient-like guidance in discrete embedding space, and\nanchored remasking for adaptive decoding. Our approach achieves\nstate-of-the-art performance among discrete diffusion samplers across linear\nand nonlinear inverse problems on the standard benchmarks. We further\ndemonstrate the benefits of our approach in training-free stylization and\ntext-guided editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of posterior sampling using pretrained discrete\ndiffusion foundation models, aiming to recover images from noisy measurements\nwithout retraining task-specific models. While diffusion models have achieved\nremarkable success in generative modeling, most advances rely on continuous\nGaussian diffusion. In contrast, discrete diffusion offers a unified framework\nfor jointly modeling categorical data such as text and images. Beyond\nunification, discrete diffusion provides faster inference, finer control, and\nprincipled training-free Bayesian inference, making it particularly well-suited\nfor posterior sampling. However, existing approaches to discrete diffusion\nposterior sampling face severe challenges: derivative-free guidance yields\nsparse signals, continuous relaxations limit applicability, and split Gibbs\nsamplers suffer from the curse of dimensionality. To overcome these\nlimitations, we introduce Anchored Posterior Sampling (APS) for masked\ndiffusion foundation models, built on two key innovations -- quantized\nexpectation for gradient-like guidance in discrete embedding space, and\nanchored remasking for adaptive decoding. Our approach achieves\nstate-of-the-art performance among discrete diffusion samplers across linear\nand nonlinear inverse problems on the standard benchmarks. We further\ndemonstrate the benefits of our approach in training-free stylization and\ntext-guided editing."
                },
                "authors": [
                    {
                        "name": "Litu Rout"
                    },
                    {
                        "name": "Andreas Lugmayr"
                    },
                    {
                        "name": "Yasamin Jafarian"
                    },
                    {
                        "name": "Srivatsan Varadharajan"
                    },
                    {
                        "name": "Constantine Caramanis"
                    },
                    {
                        "name": "Sanjay Shakkottai"
                    },
                    {
                        "name": "Ira Kemelmacher-Shlizerman"
                    }
                ],
                "author_detail": {
                    "name": "Ira Kemelmacher-Shlizerman"
                },
                "author": "Ira Kemelmacher-Shlizerman",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12021v2",
                "updated": "2025-10-02T17:57:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    57,
                    32,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-15T15:01:03Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    1,
                    3,
                    0,
                    258,
                    0
                ],
                "title": "LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code\n  Analysis"
                },
                "summary": "Large language models (LLMs) have become an essential tool to support\ndevelopers using traditional text-based programming languages, but the\ngraphical notation of the block-based Scratch programming environment inhibits\nthe use of LLMs. To overcome this limitation, we propose the LitterBox+\nframework that extends the Scratch static code analysis tool LitterBox with the\ngenerative abilities of LLMs. By converting block-based code to a textual\nrepresentation suitable for LLMs, LitterBox+ allows users to query LLMs about\ntheir programs, about quality issues reported by LitterBox, and it allows\ngenerating code fixes. Besides offering a programmatic API for these\nfunctionalities, LitterBox+ also extends the Scratch user interface to make\nthese functionalities available directly in the environment familiar to\nlearners. The framework is designed to be easily extensible with other prompts,\nLLM providers, and new features combining the program analysis capabilities of\nLitterBox with the generative features of LLMs. We provide a screencast\ndemonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become an essential tool to support\ndevelopers using traditional text-based programming languages, but the\ngraphical notation of the block-based Scratch programming environment inhibits\nthe use of LLMs. To overcome this limitation, we propose the LitterBox+\nframework that extends the Scratch static code analysis tool LitterBox with the\ngenerative abilities of LLMs. By converting block-based code to a textual\nrepresentation suitable for LLMs, LitterBox+ allows users to query LLMs about\ntheir programs, about quality issues reported by LitterBox, and it allows\ngenerating code fixes. Besides offering a programmatic API for these\nfunctionalities, LitterBox+ also extends the Scratch user interface to make\nthese functionalities available directly in the environment familiar to\nlearners. The framework is designed to be easily extensible with other prompts,\nLLM providers, and new features combining the program analysis capabilities of\nLitterBox with the generative features of LLMs. We provide a screencast\ndemonstrating the tool at https://youtu.be/RZ6E0xgrIgQ."
                },
                "authors": [
                    {
                        "name": "Benedikt Fein"
                    },
                    {
                        "name": "Florian Obermüller"
                    },
                    {
                        "name": "Gordon Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Fraser"
                },
                "author": "Gordon Fraser",
                "arxiv_comment": "ASE 2025 Tool Demonstration Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02286v1",
                "updated": "2025-10-02T17:57:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    57,
                    5,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:57:05Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    57,
                    5,
                    3,
                    275,
                    0
                ],
                "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks"
                },
                "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns."
                },
                "authors": [
                    {
                        "name": "Ruohao Guo"
                    },
                    {
                        "name": "Afshin Oroojlooy"
                    },
                    {
                        "name": "Roshan Sridhar"
                    },
                    {
                        "name": "Miguel Ballesteros"
                    },
                    {
                        "name": "Alan Ritter"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02284v1",
                "updated": "2025-10-02T17:56:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    56,
                    46,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:56:46Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    56,
                    46,
                    3,
                    275,
                    0
                ],
                "title": "Learning to Generate Object Interactions with Physics-Guided Video\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Generate Object Interactions with Physics-Guided Video\n  Diffusion"
                },
                "summary": "Recent models for video generation have achieved remarkable progress and are\nnow deployed in film, social media production, and advertising. Beyond their\ncreative potential, such models also hold promise as world simulators for\nrobotics and embodied decision making. Despite strong advances, however,\ncurrent approaches still struggle to generate physically plausible object\ninteractions and lack physics-grounded control mechanisms. To address this\nlimitation, we introduce KineMask, an approach for physics-guided video\ngeneration that enables realistic rigid body control, interactions, and\neffects. Given a single image and a specified object velocity, our method\ngenerates videos with inferred motions and future object interactions. We\npropose a two-stage training strategy that gradually removes future motion\nsupervision via object masks. Using this strategy we train video diffusion\nmodels (VDMs) on synthetic scenes of simple interactions and demonstrate\nsignificant improvements of object interactions in real scenes. Furthermore,\nKineMask integrates low-level motion control with high-level textual\nconditioning via predictive scene descriptions, leading to effective support\nfor synthesis of complex dynamical phenomena. Extensive experiments show that\nKineMask achieves strong improvements over recent models of comparable size.\nAblation studies further highlight the complementary roles of low- and\nhigh-level conditioning in VDMs. Our code, model, and data will be made\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent models for video generation have achieved remarkable progress and are\nnow deployed in film, social media production, and advertising. Beyond their\ncreative potential, such models also hold promise as world simulators for\nrobotics and embodied decision making. Despite strong advances, however,\ncurrent approaches still struggle to generate physically plausible object\ninteractions and lack physics-grounded control mechanisms. To address this\nlimitation, we introduce KineMask, an approach for physics-guided video\ngeneration that enables realistic rigid body control, interactions, and\neffects. Given a single image and a specified object velocity, our method\ngenerates videos with inferred motions and future object interactions. We\npropose a two-stage training strategy that gradually removes future motion\nsupervision via object masks. Using this strategy we train video diffusion\nmodels (VDMs) on synthetic scenes of simple interactions and demonstrate\nsignificant improvements of object interactions in real scenes. Furthermore,\nKineMask integrates low-level motion control with high-level textual\nconditioning via predictive scene descriptions, leading to effective support\nfor synthesis of complex dynamical phenomena. Extensive experiments show that\nKineMask achieves strong improvements over recent models of comparable size.\nAblation studies further highlight the complementary roles of low- and\nhigh-level conditioning in VDMs. Our code, model, and data will be made\npublicly available."
                },
                "authors": [
                    {
                        "name": "David Romero"
                    },
                    {
                        "name": "Ariana Bermudez"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Fabio Pizzati"
                    },
                    {
                        "name": "Ivan Laptev"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Laptev"
                },
                "author": "Ivan Laptev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02279v1",
                "updated": "2025-10-02T17:54:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    54,
                    9,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:54:09Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    54,
                    9,
                    3,
                    275,
                    0
                ],
                "title": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods\n  for Natural Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods\n  for Natural Language Generation"
                },
                "summary": "Hallucinations are a common issue that undermine the reliability of large\nlanguage models (LLMs). Recent studies have identified a specific subset of\nhallucinations, known as confabulations, which arise due to predictive\nuncertainty of LLMs. To detect confabulations, various methods for estimating\npredictive uncertainty in natural language generation (NLG) have been\ndeveloped. These methods are typically evaluated by correlating uncertainty\nestimates with the correctness of generated text, with question-answering (QA)\ndatasets serving as the standard benchmark. However, commonly used approximate\ncorrectness functions have substantial disagreement between each other and,\nconsequently, in the ranking of the uncertainty estimation methods. This allows\none to inflate the apparent performance of uncertainty estimation methods. We\npropose using several alternative risk indicators for risk correlation\nexperiments that improve robustness of empirical assessment of UE algorithms\nfor NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge\nvariants leads to reducing the evaluation biases. Furthermore, we explore\nstructured tasks as well as out of distribution and perturbation detection\ntasks which provide robust and controllable risk indicators. Finally, we\npropose to use an Elo rating of uncertainty estimation methods to give an\nobjective summarization over extensive evaluation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations are a common issue that undermine the reliability of large\nlanguage models (LLMs). Recent studies have identified a specific subset of\nhallucinations, known as confabulations, which arise due to predictive\nuncertainty of LLMs. To detect confabulations, various methods for estimating\npredictive uncertainty in natural language generation (NLG) have been\ndeveloped. These methods are typically evaluated by correlating uncertainty\nestimates with the correctness of generated text, with question-answering (QA)\ndatasets serving as the standard benchmark. However, commonly used approximate\ncorrectness functions have substantial disagreement between each other and,\nconsequently, in the ranking of the uncertainty estimation methods. This allows\none to inflate the apparent performance of uncertainty estimation methods. We\npropose using several alternative risk indicators for risk correlation\nexperiments that improve robustness of empirical assessment of UE algorithms\nfor NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge\nvariants leads to reducing the evaluation biases. Furthermore, we explore\nstructured tasks as well as out of distribution and perturbation detection\ntasks which provide robust and controllable risk indicators. Finally, we\npropose to use an Elo rating of uncertainty estimation methods to give an\nobjective summarization over extensive evaluation settings."
                },
                "authors": [
                    {
                        "name": "Mykyta Ielanskyi"
                    },
                    {
                        "name": "Kajetan Schweighofer"
                    },
                    {
                        "name": "Lukas Aichberger"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02271v1",
                "updated": "2025-10-02T17:48:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    48,
                    3,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:48:03Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    48,
                    3,
                    3,
                    275,
                    0
                ],
                "title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in\n  Tool-Augmented Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in\n  Tool-Augmented Agents"
                },
                "summary": "Information seeking is a fundamental requirement for humans. However,\nexisting LLM agents rely heavily on open-web search, which exposes two\nfundamental weaknesses: online content is noisy and unreliable, and many\nreal-world tasks require precise, domain-specific knowledge unavailable from\nthe web. The emergence of the Model Context Protocol (MCP) now allows agents to\ninterface with thousands of specialized tools, seemingly resolving this\nlimitation. Yet it remains unclear whether agents can effectively leverage such\ntools -- and more importantly, whether they can integrate them with\ngeneral-purpose search to solve complex tasks. Therefore, we introduce\nInfoMosaic-Bench, the first benchmark dedicated to multi-source information\nseeking in tool-augmented agents. Covering six representative domains\n(medicine, finance, maps, video, web, and multi-domain integration),\nInfoMosaic-Bench requires agents to combine general-purpose search with\ndomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable\npipeline that grounds task conditions in verified tool outputs, enforces\ncross-source dependencies, and filters out shortcut cases solvable by trivial\nlookup. This design guarantees both reliability and non-triviality. Experiments\nwith 14 state-of-the-art LLM agents reveal three findings: (i) web information\nalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass\nrate; (ii) domain tools provide selective but inconsistent benefits, improving\nsome domains while degrading others; and (iii) 22.4% of failures arise from\nincorrect tool usage or selection, highlighting that current LLMs still\nstruggle with even basic tool handling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information seeking is a fundamental requirement for humans. However,\nexisting LLM agents rely heavily on open-web search, which exposes two\nfundamental weaknesses: online content is noisy and unreliable, and many\nreal-world tasks require precise, domain-specific knowledge unavailable from\nthe web. The emergence of the Model Context Protocol (MCP) now allows agents to\ninterface with thousands of specialized tools, seemingly resolving this\nlimitation. Yet it remains unclear whether agents can effectively leverage such\ntools -- and more importantly, whether they can integrate them with\ngeneral-purpose search to solve complex tasks. Therefore, we introduce\nInfoMosaic-Bench, the first benchmark dedicated to multi-source information\nseeking in tool-augmented agents. Covering six representative domains\n(medicine, finance, maps, video, web, and multi-domain integration),\nInfoMosaic-Bench requires agents to combine general-purpose search with\ndomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable\npipeline that grounds task conditions in verified tool outputs, enforces\ncross-source dependencies, and filters out shortcut cases solvable by trivial\nlookup. This design guarantees both reliability and non-triviality. Experiments\nwith 14 state-of-the-art LLM agents reveal three findings: (i) web information\nalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass\nrate; (ii) domain tools provide selective but inconsistent benefits, improving\nsome domains while degrading others; and (iii) 22.4% of failures arise from\nincorrect tool usage or selection, highlighting that current LLMs still\nstruggle with even basic tool handling."
                },
                "authors": [
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Yuanshuo Zhang"
                    },
                    {
                        "name": "Xiyuan Yang"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Gongyi Zou"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Menglan Chen"
                    },
                    {
                        "name": "Shuo Tang"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02270v1",
                "updated": "2025-10-02T17:47:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    47,
                    39,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:47:39Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    47,
                    39,
                    3,
                    275,
                    0
                ],
                "title": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for\n  Fine-Grained Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for\n  Fine-Grained Image Classification"
                },
                "summary": "Unsupervised adaptation of CLIP-based vision-language models (VLMs) for\nfine-grained image classification requires sensitivity to microscopic local\ncues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse\nglobal features restricts its performance on fine-grained classification tasks.\nPrior efforts inject fine-grained knowledge by aligning large language model\n(LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach\noverlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training\nframework that jointly refines CLIP's visual and textual representations using\nfine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)\nwithin a lightweight TokenFusion module, which builds a saliency-guided\n$\\texttt{[FG]}$ token from patch embeddings and fuses it with the global\n$\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we\nintroduce a two-headed LLM-derived classifier: a frozen classifier that, via\nmulti-view alignment, provides a stable text-based prior for pseudo-labeling,\nand a learnable classifier initialized from LLM descriptions and fine-tuned\nwith TokenFusion. We further develop Dynamic Knowledge Aggregation, which\nconvexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to\niteratively refine pseudo-labels. Together, these components uncover latent\nfine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy\ngain across 13 fine-grained benchmarks while requiring only light adaptation.\nOur code is available at https://github.com/sathiiii/microCLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised adaptation of CLIP-based vision-language models (VLMs) for\nfine-grained image classification requires sensitivity to microscopic local\ncues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse\nglobal features restricts its performance on fine-grained classification tasks.\nPrior efforts inject fine-grained knowledge by aligning large language model\n(LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach\noverlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training\nframework that jointly refines CLIP's visual and textual representations using\nfine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)\nwithin a lightweight TokenFusion module, which builds a saliency-guided\n$\\texttt{[FG]}$ token from patch embeddings and fuses it with the global\n$\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we\nintroduce a two-headed LLM-derived classifier: a frozen classifier that, via\nmulti-view alignment, provides a stable text-based prior for pseudo-labeling,\nand a learnable classifier initialized from LLM descriptions and fine-tuned\nwith TokenFusion. We further develop Dynamic Knowledge Aggregation, which\nconvexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to\niteratively refine pseudo-labels. Together, these components uncover latent\nfine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy\ngain across 13 fine-grained benchmarks while requiring only light adaptation.\nOur code is available at https://github.com/sathiiii/microCLIP."
                },
                "authors": [
                    {
                        "name": "Sathira Silva"
                    },
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02268v1",
                "updated": "2025-10-02T17:47:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    47,
                    6,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:47:06Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    47,
                    6,
                    3,
                    275,
                    0
                ],
                "title": "Do You Know Where Your Camera Is? View-Invariant Policy Learning with\n  Camera Conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do You Know Where Your Camera Is? View-Invariant Policy Learning with\n  Camera Conditioning"
                },
                "summary": "We study view-invariant imitation learning by explicitly conditioning\npolicies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we\nshow that conditioning on extrinsics significantly improves generalization\nacross viewpoints for standard behavior cloning policies, including ACT,\nDiffusion Policy, and SmolVLA. To evaluate policy robustness under realistic\nviewpoint shifts, we introduce six manipulation tasks in RoboSuite and\nManiSkill that pair \"fixed\" and \"randomized\" scene variants, decoupling\nbackground cues from camera pose. Our analysis reveals that policies without\nextrinsics often infer camera pose using visual cues from static backgrounds in\nfixed scenes; this shortcut collapses when workspace geometry or camera\nplacement shifts. Conditioning on extrinsics restores performance and yields\nrobust RGB-only control without depth. We release the tasks, demonstrations,\nand code at https://ripl.github.io/know_your_camera/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study view-invariant imitation learning by explicitly conditioning\npolicies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we\nshow that conditioning on extrinsics significantly improves generalization\nacross viewpoints for standard behavior cloning policies, including ACT,\nDiffusion Policy, and SmolVLA. To evaluate policy robustness under realistic\nviewpoint shifts, we introduce six manipulation tasks in RoboSuite and\nManiSkill that pair \"fixed\" and \"randomized\" scene variants, decoupling\nbackground cues from camera pose. Our analysis reveals that policies without\nextrinsics often infer camera pose using visual cues from static backgrounds in\nfixed scenes; this shortcut collapses when workspace geometry or camera\nplacement shifts. Conditioning on extrinsics restores performance and yields\nrobust RGB-only control without depth. We release the tasks, demonstrations,\nand code at https://ripl.github.io/know_your_camera/ ."
                },
                "authors": [
                    {
                        "name": "Tianchong Jiang"
                    },
                    {
                        "name": "Jingtian Ji"
                    },
                    {
                        "name": "Xiangshan Tan"
                    },
                    {
                        "name": "Jiading Fang"
                    },
                    {
                        "name": "Anand Bhattad"
                    },
                    {
                        "name": "Vitor Guizilini"
                    },
                    {
                        "name": "Matthew R. Walter"
                    }
                ],
                "author_detail": {
                    "name": "Matthew R. Walter"
                },
                "author": "Matthew R. Walter",
                "arxiv_comment": "Code and project materials are available at\n  ripl.github.io/know_your_camera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02263v1",
                "updated": "2025-10-02T17:44:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    44,
                    23,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:44:23Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    44,
                    23,
                    3,
                    275,
                    0
                ],
                "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems"
                },
                "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration."
                },
                "authors": [
                    {
                        "name": "Yuxiao Qu"
                    },
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Yoonho Lee"
                    },
                    {
                        "name": "Amrith Setlur"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "Chelsea Finn"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20444v2",
                "updated": "2025-10-02T17:43:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    43,
                    5,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-24T18:00:45Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    18,
                    0,
                    45,
                    2,
                    267,
                    0
                ],
                "title": "SAGAbg III: Environmental Stellar Mass Functions, Self-Quenching, and\n  the Stellar-to-Halo Mass Relation in the Dwarf Galaxy Regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGAbg III: Environmental Stellar Mass Functions, Self-Quenching, and\n  the Stellar-to-Halo Mass Relation in the Dwarf Galaxy Regime"
                },
                "summary": "Recent efforts have extended our view of the number and properties of\nsatellite galaxies beyond the Local Group firmly down to $\\rm M_\\star\\sim 10^6\nM_\\odot$. A similarly complete view of the field dwarf population has lagged\nbehind. Using the background galaxies sample from the Satellites Around\nGalactic Analogs (SAGA) Survey at $z<0.05$, we take inventory of the dwarf\npopulation down to $\\rm M_\\star \\sim 5\\times10^6 M_\\odot$ using three metrics:\nthe stellar mass function (SMF) as function of environment, the stellar-to-halo\nmass relation (SHMR) of dwarf galaxies inferred via abundance matching, and the\nquenched fraction of highly isolated dwarfs. We find that the low-mass SMF\nshape shows minimal environmental dependence, with the field dwarf SMF\ndescribed by a low-mass power-law index of $\\alpha_1=-1.44\\pm0.09$ down to $\\rm\nM_\\star \\sim 5\\times10^6 M_\\odot$, and that the quenched fraction of isolated\ndwarfs drops monotonically to $f_{q} \\sim 10^{-3}$ at $\\rm M_\\star \\sim \\rm\n10^{8.5} M_\\odot$. Though slightly steeper than estimates from \\HI{} kinematic\nmeasures, our inferred SHMR agrees with literature measurements of satellite\nsystems, consistent with minimal environmental dependence of the SHMR in the\nprobed mass range. Finally, although most contemporary cosmological simulations\nagainst which we compare accurately predict the \\sagalocal{} SHMR, we find that\nbig-box cosmological simulations largely over-predict isolated galaxy quenched\nfractions via a turnaround in $f_q(\\rm M_\\star)$ at $\\rm 10^8\\lesssim\nM_\\star/M_\\odot\\lesssim 10^9$, underscoring the complexities in disentangling\nthe drivers of galaxy formation and the need for systematic multidimensional\nobservations of the dwarf population across environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent efforts have extended our view of the number and properties of\nsatellite galaxies beyond the Local Group firmly down to $\\rm M_\\star\\sim 10^6\nM_\\odot$. A similarly complete view of the field dwarf population has lagged\nbehind. Using the background galaxies sample from the Satellites Around\nGalactic Analogs (SAGA) Survey at $z<0.05$, we take inventory of the dwarf\npopulation down to $\\rm M_\\star \\sim 5\\times10^6 M_\\odot$ using three metrics:\nthe stellar mass function (SMF) as function of environment, the stellar-to-halo\nmass relation (SHMR) of dwarf galaxies inferred via abundance matching, and the\nquenched fraction of highly isolated dwarfs. We find that the low-mass SMF\nshape shows minimal environmental dependence, with the field dwarf SMF\ndescribed by a low-mass power-law index of $\\alpha_1=-1.44\\pm0.09$ down to $\\rm\nM_\\star \\sim 5\\times10^6 M_\\odot$, and that the quenched fraction of isolated\ndwarfs drops monotonically to $f_{q} \\sim 10^{-3}$ at $\\rm M_\\star \\sim \\rm\n10^{8.5} M_\\odot$. Though slightly steeper than estimates from \\HI{} kinematic\nmeasures, our inferred SHMR agrees with literature measurements of satellite\nsystems, consistent with minimal environmental dependence of the SHMR in the\nprobed mass range. Finally, although most contemporary cosmological simulations\nagainst which we compare accurately predict the \\sagalocal{} SHMR, we find that\nbig-box cosmological simulations largely over-predict isolated galaxy quenched\nfractions via a turnaround in $f_q(\\rm M_\\star)$ at $\\rm 10^8\\lesssim\nM_\\star/M_\\odot\\lesssim 10^9$, underscoring the complexities in disentangling\nthe drivers of galaxy formation and the need for systematic multidimensional\nobservations of the dwarf population across environments."
                },
                "authors": [
                    {
                        "name": "Erin Kado-Fong"
                    },
                    {
                        "name": "Yao-Yuan Mao"
                    },
                    {
                        "name": "Yasmeen Asali"
                    },
                    {
                        "name": "Marla Geha"
                    },
                    {
                        "name": "Risa H. Wechsler"
                    },
                    {
                        "name": "Mithi A. C. de los Reyes"
                    },
                    {
                        "name": "Yunchong Wang"
                    },
                    {
                        "name": "Ethan O. Nadler"
                    },
                    {
                        "name": "Nitya Kallivayalil"
                    },
                    {
                        "name": "Erik J. Tollerud"
                    },
                    {
                        "name": "Benjamin Weiner"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Weiner"
                },
                "author": "Benjamin Weiner",
                "arxiv_comment": "34 pages, 12 figures, 6 pages; submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02262v1",
                "updated": "2025-10-02T17:43:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    43,
                    1,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:43:01Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    43,
                    1,
                    3,
                    275,
                    0
                ],
                "title": "From Frames to Clips: Efficient Key Clip Selection for Long-Form Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Frames to Clips: Efficient Key Clip Selection for Long-Form Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VLMs) have achieved remarkable results on a\nvariety of vision language tasks, yet their practical use is limited by the\n\"needle in a haystack\" problem: the massive number of visual tokens produced\nfrom raw video frames exhausts the model's context window. Existing solutions\nalleviate this issue by selecting a sparse set of frames, thereby reducing\ntoken count, but such frame-wise selection discards essential temporal\ndynamics, leading to suboptimal reasoning about motion and event continuity. In\nthis work we systematically explore the impact of temporal information and\ndemonstrate that extending selection from isolated key frames to key clips,\nwhich are short, temporally coherent segments, improves video understanding. To\nmaintain a fixed computational budget while accommodating the larger token\nfootprint of clips, we propose an adaptive resolution strategy that dynamically\nbalances spatial resolution and clip length, ensuring a constant token count\nper video. Experiments on three long-form video benchmarks demonstrate that our\ntraining-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and\n10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These\nresults highlight the importance of preserving temporal coherence in frame\nselection and provide a practical pathway for scaling Video LLMs to real world\nvideo understanding applications. Project webpage is available at\nhttps://guangyusun.com/f2c .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VLMs) have achieved remarkable results on a\nvariety of vision language tasks, yet their practical use is limited by the\n\"needle in a haystack\" problem: the massive number of visual tokens produced\nfrom raw video frames exhausts the model's context window. Existing solutions\nalleviate this issue by selecting a sparse set of frames, thereby reducing\ntoken count, but such frame-wise selection discards essential temporal\ndynamics, leading to suboptimal reasoning about motion and event continuity. In\nthis work we systematically explore the impact of temporal information and\ndemonstrate that extending selection from isolated key frames to key clips,\nwhich are short, temporally coherent segments, improves video understanding. To\nmaintain a fixed computational budget while accommodating the larger token\nfootprint of clips, we propose an adaptive resolution strategy that dynamically\nbalances spatial resolution and clip length, ensuring a constant token count\nper video. Experiments on three long-form video benchmarks demonstrate that our\ntraining-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and\n10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These\nresults highlight the importance of preserving temporal coherence in frame\nselection and provide a practical pathway for scaling Video LLMs to real world\nvideo understanding applications. Project webpage is available at\nhttps://guangyusun.com/f2c ."
                },
                "authors": [
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Archit Singhal"
                    },
                    {
                        "name": "Burak Uzkent"
                    },
                    {
                        "name": "Mubarak Shah"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Garin Kessler"
                    }
                ],
                "author_detail": {
                    "name": "Garin Kessler"
                },
                "author": "Garin Kessler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13231v2",
                "updated": "2025-10-02T17:42:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    42,
                    30,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-17T15:41:57Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    41,
                    57,
                    3,
                    198,
                    0
                ],
                "title": "VITA: Vision-to-Action Flow Matching Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA: Vision-to-Action Flow Matching Policy"
                },
                "summary": "Conventional flow matching and diffusion-based policies sample through\niterative denoising from standard noise distributions (e.g., Gaussian), and\nrequire conditioning mechanisms to incorporate visual information during the\ngenerative process, incurring substantial time and memory overhead. To reduce\nthe complexity, we develop VITA(VIsion-To-Action policy), a noise-free and\nconditioning-free policy learning framework that directly maps visual\nrepresentations to latent actions using flow matching. VITA treats latent\nvisual representations as the source of the flow, thus eliminating the need of\nconditioning. As expected, bridging vision and action is challenging, because\nactions are lower-dimensional, less structured, and sparser than visual\nrepresentations; moreover, flow matching requires the source and target to have\nthe same dimensionality. To overcome this, we introduce an action autoencoder\nthat maps raw actions into a structured latent space aligned with visual\nlatents, trained jointly with flow matching. To further prevent latent space\ncollapse, we propose flow latent decoding, which anchors the latent generation\nprocess by backpropagating the action reconstruction loss through the flow\nmatching ODE (ordinary differential equations) solving steps. We evaluate VITA\non 8 simulation and 2 real-world tasks from ALOHA and Robomimic. VITA\noutperforms or matches state-of-the-art generative policies, while achieving\n1.5-2.3x faster inference compared to conventional methods with conditioning.\nProject page: https://ucd-dare.github.io/VITA/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional flow matching and diffusion-based policies sample through\niterative denoising from standard noise distributions (e.g., Gaussian), and\nrequire conditioning mechanisms to incorporate visual information during the\ngenerative process, incurring substantial time and memory overhead. To reduce\nthe complexity, we develop VITA(VIsion-To-Action policy), a noise-free and\nconditioning-free policy learning framework that directly maps visual\nrepresentations to latent actions using flow matching. VITA treats latent\nvisual representations as the source of the flow, thus eliminating the need of\nconditioning. As expected, bridging vision and action is challenging, because\nactions are lower-dimensional, less structured, and sparser than visual\nrepresentations; moreover, flow matching requires the source and target to have\nthe same dimensionality. To overcome this, we introduce an action autoencoder\nthat maps raw actions into a structured latent space aligned with visual\nlatents, trained jointly with flow matching. To further prevent latent space\ncollapse, we propose flow latent decoding, which anchors the latent generation\nprocess by backpropagating the action reconstruction loss through the flow\nmatching ODE (ordinary differential equations) solving steps. We evaluate VITA\non 8 simulation and 2 real-world tasks from ALOHA and Robomimic. VITA\noutperforms or matches state-of-the-art generative policies, while achieving\n1.5-2.3x faster inference compared to conventional methods with conditioning.\nProject page: https://ucd-dare.github.io/VITA/"
                },
                "authors": [
                    {
                        "name": "Dechen Gao"
                    },
                    {
                        "name": "Boqi Zhao"
                    },
                    {
                        "name": "Andrew Lee"
                    },
                    {
                        "name": "Ian Chuang"
                    },
                    {
                        "name": "Hanchu Zhou"
                    },
                    {
                        "name": "Hang Wang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Junshan Zhang"
                    },
                    {
                        "name": "Iman Soltani"
                    }
                ],
                "author_detail": {
                    "name": "Iman Soltani"
                },
                "author": "Iman Soltani",
                "arxiv_comment": "Project page: https://ucd-dare.github.io/VITA/ Code:\n  https://github.com/ucd-dare/VITA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02260v1",
                "updated": "2025-10-02T17:42:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    42,
                    26,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:42:26Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    42,
                    26,
                    3,
                    275,
                    0
                ],
                "title": "Mapping the Cloud-Driven Atmospheric Dynamics & Chemistry of an Isolated\n  Exoplanet Analog with Harmonic Signatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping the Cloud-Driven Atmospheric Dynamics & Chemistry of an Isolated\n  Exoplanet Analog with Harmonic Signatures"
                },
                "summary": "Young planetary-mass objects and brown dwarfs near the L/T spectral\ntransition exhibit enhanced spectrophotometric variability over field brown\ndwarfs. Patchy clouds, auroral processes, stratospheric hot spots, and complex\ncarbon chemistry have all been proposed as potential sources of this\nvariability. Using time-resolved, low-to-mid-resolution spectroscopy collected\nwith the JWST/NIRISS and NIRSpec instruments, we apply harmonic analysis to\nSIMP J0136, a highly variable, young, isolated planetary-mass object. Odd\nharmonics (k=3) at pressure levels (> 1 bar) corresponding to iron and\nforsterite cloud formation suggest North/South hemispheric asymmetry in the\ncloudy, and likely equatorial, regions. We use the inferred harmonics, along\nwith 1-D substellar atmospheric models, to map the flux variability by\natmospheric pressure level. These vertical maps demonstrate robust interaction\nbetween deep convective weather layers and the overlying stratified and\nradiative atmosphere. We identify distinct time-varying structures in the\nnear-infrared that we interpret as planetary-scale wave (e.g., Rossby or\nKelvin)-associated cloud modulation. We detect variability in water (S/N =\n14.0), carbon monoxide (S/N = 13.0), and methane (S/N = 14.9) molecular\nsignatures. Forsterite cloud modulation is anti-correlated with overlying\ncarbon monoxide and water abundances and correlated with deep methane\nabsorption, suggesting complex interaction between cloud formation, atmospheric\nchemistry, and temperature structure. Furthermore, we identify distinct\nharmonic behavior between methane and carbon monoxide absorption bands,\nproviding evidence for time-resolved disequilibrium carbon chemistry. At the\nlowest pressures (< 100 mbar), we find that the mapped methane lines transition\nfrom absorption to emission, supporting evidence of high-altitude auroral\nheating via electron precipitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Young planetary-mass objects and brown dwarfs near the L/T spectral\ntransition exhibit enhanced spectrophotometric variability over field brown\ndwarfs. Patchy clouds, auroral processes, stratospheric hot spots, and complex\ncarbon chemistry have all been proposed as potential sources of this\nvariability. Using time-resolved, low-to-mid-resolution spectroscopy collected\nwith the JWST/NIRISS and NIRSpec instruments, we apply harmonic analysis to\nSIMP J0136, a highly variable, young, isolated planetary-mass object. Odd\nharmonics (k=3) at pressure levels (> 1 bar) corresponding to iron and\nforsterite cloud formation suggest North/South hemispheric asymmetry in the\ncloudy, and likely equatorial, regions. We use the inferred harmonics, along\nwith 1-D substellar atmospheric models, to map the flux variability by\natmospheric pressure level. These vertical maps demonstrate robust interaction\nbetween deep convective weather layers and the overlying stratified and\nradiative atmosphere. We identify distinct time-varying structures in the\nnear-infrared that we interpret as planetary-scale wave (e.g., Rossby or\nKelvin)-associated cloud modulation. We detect variability in water (S/N =\n14.0), carbon monoxide (S/N = 13.0), and methane (S/N = 14.9) molecular\nsignatures. Forsterite cloud modulation is anti-correlated with overlying\ncarbon monoxide and water abundances and correlated with deep methane\nabsorption, suggesting complex interaction between cloud formation, atmospheric\nchemistry, and temperature structure. Furthermore, we identify distinct\nharmonic behavior between methane and carbon monoxide absorption bands,\nproviding evidence for time-resolved disequilibrium carbon chemistry. At the\nlowest pressures (< 100 mbar), we find that the mapped methane lines transition\nfrom absorption to emission, supporting evidence of high-altitude auroral\nheating via electron precipitation."
                },
                "authors": [
                    {
                        "name": "Michael K. Plummer"
                    },
                    {
                        "name": "Francis P. Cocchini"
                    },
                    {
                        "name": "Peter A. Kearns"
                    },
                    {
                        "name": "Allison McCarthy"
                    },
                    {
                        "name": "Étienne Artigau"
                    },
                    {
                        "name": "Nicolas B. Cowan"
                    },
                    {
                        "name": "Roman Akhmetshyn"
                    },
                    {
                        "name": "Johanna Vos"
                    },
                    {
                        "name": "Evert Nasedkin"
                    },
                    {
                        "name": "Channon Visscher"
                    },
                    {
                        "name": "Björn Benneke"
                    },
                    {
                        "name": "René Doyon"
                    },
                    {
                        "name": "Stanimir A. Metchev"
                    },
                    {
                        "name": "Jason F. Rowe"
                    },
                    {
                        "name": "Genaro Suárez"
                    }
                ],
                "author_detail": {
                    "name": "Genaro Suárez"
                },
                "author": "Genaro Suárez",
                "arxiv_comment": "16 pages, 6 figures, submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02259v1",
                "updated": "2025-10-02T17:42:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    42,
                    10,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:42:10Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    42,
                    10,
                    3,
                    275,
                    0
                ],
                "title": "Transformers Discover Molecular Structure Without Graph Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers Discover Molecular Structure Without Graph Priors"
                },
                "summary": "Graph Neural Networks (GNNs) are the dominant architecture for molecular\nmachine learning, particularly for molecular property prediction and machine\nlearning interatomic potentials (MLIPs). GNNs perform message passing on\npredefined graphs often induced by a fixed radius cutoff or k-nearest neighbor\nscheme. While this design aligns with the locality present in many molecular\ntasks, a hard-coded graph can limit expressivity due to the fixed receptive\nfield and slows down inference with sparse graph operations. In this work, we\ninvestigate whether pure, unmodified Transformers trained directly on Cartesian\ncoordinates$\\unicode{x2013}$without predefined graphs or physical\npriors$\\unicode{x2013}$can approximate molecular energies and forces. As a\nstarting point for our analysis, we demonstrate how to train a Transformer to\ncompetitive energy and force mean absolute errors under a matched training\ncompute budget, relative to a state-of-the-art equivariant GNN on the OMol25\ndataset. We discover that the Transformer learns physically consistent\npatterns$\\unicode{x2013}$such as attention weights that decay inversely with\ninteratomic distance$\\unicode{x2013}$and flexibly adapts them across different\nmolecular environments due to the absence of hard-coded biases. The use of a\nstandard Transformer also unlocks predictable improvements with respect to\nscaling training resources, consistent with empirical scaling laws observed in\nother domains. Our results demonstrate that many favorable properties of GNNs\ncan emerge adaptively in Transformers, challenging the necessity of hard-coded\ngraph inductive biases and pointing toward standardized, scalable architectures\nfor molecular modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are the dominant architecture for molecular\nmachine learning, particularly for molecular property prediction and machine\nlearning interatomic potentials (MLIPs). GNNs perform message passing on\npredefined graphs often induced by a fixed radius cutoff or k-nearest neighbor\nscheme. While this design aligns with the locality present in many molecular\ntasks, a hard-coded graph can limit expressivity due to the fixed receptive\nfield and slows down inference with sparse graph operations. In this work, we\ninvestigate whether pure, unmodified Transformers trained directly on Cartesian\ncoordinates$\\unicode{x2013}$without predefined graphs or physical\npriors$\\unicode{x2013}$can approximate molecular energies and forces. As a\nstarting point for our analysis, we demonstrate how to train a Transformer to\ncompetitive energy and force mean absolute errors under a matched training\ncompute budget, relative to a state-of-the-art equivariant GNN on the OMol25\ndataset. We discover that the Transformer learns physically consistent\npatterns$\\unicode{x2013}$such as attention weights that decay inversely with\ninteratomic distance$\\unicode{x2013}$and flexibly adapts them across different\nmolecular environments due to the absence of hard-coded biases. The use of a\nstandard Transformer also unlocks predictable improvements with respect to\nscaling training resources, consistent with empirical scaling laws observed in\nother domains. Our results demonstrate that many favorable properties of GNNs\ncan emerge adaptively in Transformers, challenging the necessity of hard-coded\ngraph inductive biases and pointing toward standardized, scalable architectures\nfor molecular modeling."
                },
                "authors": [
                    {
                        "name": "Tobias Kreiman"
                    },
                    {
                        "name": "Yutong Bai"
                    },
                    {
                        "name": "Fadi Atieh"
                    },
                    {
                        "name": "Elizabeth Weaver"
                    },
                    {
                        "name": "Eric Qu"
                    },
                    {
                        "name": "Aditi S. Krishnapriyan"
                    }
                ],
                "author_detail": {
                    "name": "Aditi S. Krishnapriyan"
                },
                "author": "Aditi S. Krishnapriyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18436v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18436v4",
                "updated": "2025-10-02T17:40:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    40,
                    50,
                    3,
                    275,
                    0
                ],
                "published": "2024-10-24T05:14:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    14,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case\n  Study on English-Korean Code-Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case\n  Study on English-Korean Code-Switching"
                },
                "summary": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can activate, or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can activate, or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks."
                },
                "authors": [
                    {
                        "name": "Seoyeon Kim"
                    },
                    {
                        "name": "Huiseo Kim"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18436v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18436v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02249v1",
                "updated": "2025-10-02T17:36:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    36,
                    50,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:36:50Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    36,
                    50,
                    3,
                    275,
                    0
                ],
                "title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative\n  Entropy Regulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative\n  Entropy Regulation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\non complex problems using long Chain-of-Thought (CoT) reasoning. However, they\noften suffer from overthinking, meaning generating unnecessarily lengthy\nreasoning steps for simpler problems. This issue may degrade the efficiency of\nthe models and make them difficult to adapt the reasoning depth to the\ncomplexity of problems. To address this, we introduce a novel metric Token\nEntropy Cumulative Average (TECA), which measures the extent of exploration\nthroughout the reasoning process. We further propose a novel reasoning paradigm\n-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy\nRegulation (CER) mechanism. This paradigm leverages TECA to help the model\ndynamically determine the optimal point to conclude its thought process and\nprovide a final answer, thus achieving efficient reasoning. Experimental\nresults across diverse mathematical benchmarks show that our approach\nsubstantially mitigates overthinking without sacrificing problem-solving\nability. With our thinking paradigm, the average response length decreases by\nup to 71% on simpler datasets, demonstrating the effectiveness of our method in\ncreating a more efficient and adaptive reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\non complex problems using long Chain-of-Thought (CoT) reasoning. However, they\noften suffer from overthinking, meaning generating unnecessarily lengthy\nreasoning steps for simpler problems. This issue may degrade the efficiency of\nthe models and make them difficult to adapt the reasoning depth to the\ncomplexity of problems. To address this, we introduce a novel metric Token\nEntropy Cumulative Average (TECA), which measures the extent of exploration\nthroughout the reasoning process. We further propose a novel reasoning paradigm\n-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy\nRegulation (CER) mechanism. This paradigm leverages TECA to help the model\ndynamically determine the optimal point to conclude its thought process and\nprovide a final answer, thus achieving efficient reasoning. Experimental\nresults across diverse mathematical benchmarks show that our approach\nsubstantially mitigates overthinking without sacrificing problem-solving\nability. With our thinking paradigm, the average response length decreases by\nup to 71% on simpler datasets, demonstrating the effectiveness of our method in\ncreating a more efficient and adaptive reasoning process."
                },
                "authors": [
                    {
                        "name": "Tianyi Jiang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Yujuan Ding"
                    },
                    {
                        "name": "Kainian Zhu"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09674v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09674v4",
                "updated": "2025-10-02T17:36:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    36,
                    23,
                    3,
                    275,
                    0
                ],
                "published": "2025-03-12T17:41:25Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    41,
                    25,
                    2,
                    71,
                    0
                ],
                "title": "Probabilistic Reasoning with LLMs for k-anonymity Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Reasoning with LLMs for k-anonymity Estimation"
                },
                "summary": "Probabilistic reasoning is a key aspect of both human and artificial\nintelligence that allows for handling uncertainty and ambiguity in\ndecision-making. In this paper, we introduce a new numerical reasoning task\nunder uncertainty for large language models, focusing on estimating the privacy\nrisk of user-generated documents containing privacy-sensitive information. We\npropose BRANCH, a new LLM methodology that estimates the k-privacy value of a\ntext-the size of the population matching the given information. BRANCH\nfactorizes a joint probability distribution of personal information as random\nvariables. The probability of each factor in a population is estimated\nseparately using a Bayesian network and combined to compute the final k-value.\nOur experiments show that this method successfully estimates the k-value 73% of\nthe time, a 13% increase compared to o3-mini with chain-of-thought reasoning.\nWe also find that LLM uncertainty is a good indicator for accuracy, as\nhigh-variance predictions are 37.47% less accurate on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic reasoning is a key aspect of both human and artificial\nintelligence that allows for handling uncertainty and ambiguity in\ndecision-making. In this paper, we introduce a new numerical reasoning task\nunder uncertainty for large language models, focusing on estimating the privacy\nrisk of user-generated documents containing privacy-sensitive information. We\npropose BRANCH, a new LLM methodology that estimates the k-privacy value of a\ntext-the size of the population matching the given information. BRANCH\nfactorizes a joint probability distribution of personal information as random\nvariables. The probability of each factor in a population is estimated\nseparately using a Bayesian network and combined to compute the final k-value.\nOur experiments show that this method successfully estimates the k-value 73% of\nthe time, a 13% increase compared to o3-mini with chain-of-thought reasoning.\nWe also find that LLM uncertainty is a good indicator for accuracy, as\nhigh-variance predictions are 37.47% less accurate on average."
                },
                "authors": [
                    {
                        "name": "Jonathan Zheng"
                    },
                    {
                        "name": "Sauvik Das"
                    },
                    {
                        "name": "Alan Ritter"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "10 pages, Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09674v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09674v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02243v1",
                "updated": "2025-10-02T17:30:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    30,
                    8,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:30:08Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    30,
                    8,
                    3,
                    275,
                    0
                ],
                "title": "AccurateRAG: A Framework for Building Accurate Retrieval-Augmented\n  Question-Answering Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccurateRAG: A Framework for Building Accurate Retrieval-Augmented\n  Question-Answering Applications"
                },
                "summary": "We introduce AccurateRAG -- a novel framework for constructing\nhigh-performance question-answering applications based on retrieval-augmented\ngeneration (RAG). Our framework offers a pipeline for development efficiency\nwith tools for raw dataset processing, fine-tuning data generation, text\nembedding & LLM fine-tuning, output evaluation, and building RAG systems\nlocally. Experimental results show that our framework outperforms previous\nstrong baselines and obtains new state-of-the-art question-answering\nperformance on benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AccurateRAG -- a novel framework for constructing\nhigh-performance question-answering applications based on retrieval-augmented\ngeneration (RAG). Our framework offers a pipeline for development efficiency\nwith tools for raw dataset processing, fine-tuning data generation, text\nembedding & LLM fine-tuning, output evaluation, and building RAG systems\nlocally. Experimental results show that our framework outperforms previous\nstrong baselines and obtains new state-of-the-art question-answering\nperformance on benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Linh The Nguyen"
                    },
                    {
                        "name": "Chi Tran"
                    },
                    {
                        "name": "Dung Ngoc Nguyen"
                    },
                    {
                        "name": "Van-Cuong Pham"
                    },
                    {
                        "name": "Hoang Ngo"
                    },
                    {
                        "name": "Dat Quoc Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dat Quoc Nguyen"
                },
                "author": "Dat Quoc Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02242v1",
                "updated": "2025-10-02T17:29:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    29,
                    53,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:29:53Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    29,
                    53,
                    3,
                    275,
                    0
                ],
                "title": "Transfer of Stability from the Classical to the Fractional Anisotropic\n  Calderón Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer of Stability from the Classical to the Fractional Anisotropic\n  Calderón Problem"
                },
                "summary": "We discuss two spectral fractional anisotropic Calder\\'on problems with\nsource-to-solution measurements and their quantitative relation to the\nclassical Calder\\'on problem. Firstly, we consider the anistropic fractional\nCalder\\'on problem from [FGKU25]. In this setting, we quantify the relation\nbetween the local and nonlocal Calder\\'on problems which had been deduced in\n[R25] and provide an associated stability estimate. As a consequence, any\nstability result which holds on the level of the local problem with\nsource-to-solution data has a direct nonlocal analogue (up to a logarithmic\nloss). Secondly, we introduce and discuss the fractional Calder\\'on problem\nwith source-to-solution measurements for the spectral fractional Dirichlet\nLaplacian on open, bounded, connected, Lipschitz sets on $\\mathbb{R}^n$. Also\nin this context, we provide a qualitative and quantitative transfer of\nuniqueness from the local to the nonlocal setting. As a consequence, we infer\nthe first stability results for the principal part for a fractional Calder\\'on\ntype problem for which no reduction of Liouville type is known. Our arguments\nrely on quantitative unique continuation arguments. As a result of independent\ninterest, we also prove a quantitative relation between source-to-solution and\nDirichlet-to-Neumann measurements for the classical Calder\\'on problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We discuss two spectral fractional anisotropic Calder\\'on problems with\nsource-to-solution measurements and their quantitative relation to the\nclassical Calder\\'on problem. Firstly, we consider the anistropic fractional\nCalder\\'on problem from [FGKU25]. In this setting, we quantify the relation\nbetween the local and nonlocal Calder\\'on problems which had been deduced in\n[R25] and provide an associated stability estimate. As a consequence, any\nstability result which holds on the level of the local problem with\nsource-to-solution data has a direct nonlocal analogue (up to a logarithmic\nloss). Secondly, we introduce and discuss the fractional Calder\\'on problem\nwith source-to-solution measurements for the spectral fractional Dirichlet\nLaplacian on open, bounded, connected, Lipschitz sets on $\\mathbb{R}^n$. Also\nin this context, we provide a qualitative and quantitative transfer of\nuniqueness from the local to the nonlocal setting. As a consequence, we infer\nthe first stability results for the principal part for a fractional Calder\\'on\ntype problem for which no reduction of Liouville type is known. Our arguments\nrely on quantitative unique continuation arguments. As a result of independent\ninterest, we also prove a quantitative relation between source-to-solution and\nDirichlet-to-Neumann measurements for the classical Calder\\'on problem."
                },
                "authors": [
                    {
                        "name": "Hendrik Baers"
                    },
                    {
                        "name": "Angkana Rüland"
                    }
                ],
                "author_detail": {
                    "name": "Angkana Rüland"
                },
                "author": "Angkana Rüland",
                "arxiv_comment": "54 pages, 4 figures, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02241v1",
                "updated": "2025-10-02T17:29:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    29,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:29:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    29,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "Study on LLMs for Promptagator-Style Dense Retriever Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study on LLMs for Promptagator-Style Dense Retriever Training"
                },
                "summary": "Promptagator demonstrated that Large Language Models (LLMs) with few-shot\nprompts can be used as task-specific query generators for fine-tuning\ndomain-specialized dense retrieval models. However, the original Promptagator\napproach relied on proprietary and large-scale LLMs which users may not have\naccess to or may be prohibited from using with sensitive data. In this work, we\nstudy the impact of open-source LLMs at accessible scales ($\\leq$14B\nparameters) as an alternative. Our results demonstrate that open-source LLMs as\nsmall as 3B parameters can serve as effective Promptagator-style query\ngenerators. We hope our work will inform practitioners with reliable\nalternatives for synthetic data generation and give insights to maximize\nfine-tuning results for domain-specific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Promptagator demonstrated that Large Language Models (LLMs) with few-shot\nprompts can be used as task-specific query generators for fine-tuning\ndomain-specialized dense retrieval models. However, the original Promptagator\napproach relied on proprietary and large-scale LLMs which users may not have\naccess to or may be prohibited from using with sensitive data. In this work, we\nstudy the impact of open-source LLMs at accessible scales ($\\leq$14B\nparameters) as an alternative. Our results demonstrate that open-source LLMs as\nsmall as 3B parameters can serve as effective Promptagator-style query\ngenerators. We hope our work will inform practitioners with reliable\nalternatives for synthetic data generation and give insights to maximize\nfine-tuning results for domain-specific applications."
                },
                "authors": [
                    {
                        "name": "Daniel Gwon"
                    },
                    {
                        "name": "Nour Jedidi"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "CIKM 2025 short research paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00404v2",
                "updated": "2025-10-02T17:28:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    28,
                    55,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-01T01:29:31Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    29,
                    31,
                    2,
                    274,
                    0
                ],
                "title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features"
                },
                "summary": "Sparse autoencoders (SAEs) have emerged as powerful techniques for\ninterpretability of large language models (LLMs), aiming to decompose hidden\nstates into meaningful semantic features. While several SAE variants have been\nproposed, there remains no principled framework to derive SAEs from the\noriginal dictionary learning formulation. In this work, we introduce such a\nframework by unrolling the proximal gradient method for sparse coding. We show\nthat a single-step update naturally recovers common SAE variants, including\nReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation\nof existing SAEs: their sparsity-inducing regularizers enforce non-negativity,\npreventing a single feature from representing bidirectional concepts (e.g.,\nmale vs. female). This structural constraint fragments semantic axes into\nseparate, redundant features, limiting representational completeness. To\naddress this issue, we propose AbsTopK SAE, a new variant derived from the\n$\\ell_0$ sparsity constraint that applies hard thresholding over the\nlargest-magnitude activations. By preserving both positive and negative\nactivations, AbsTopK uncovers richer, bidirectional conceptual representations.\nComprehensive experiments across four LLMs and seven probing and steering tasks\nshow that AbsTopK improves reconstruction fidelity, enhances interpretability,\nand enables single features to encode contrasting concepts. Remarkably, AbsTopK\nmatches or even surpasses the Difference-in-Mean method, a supervised approach\nthat requires labeled data for each concept and has been shown in prior work to\noutperform SAEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) have emerged as powerful techniques for\ninterpretability of large language models (LLMs), aiming to decompose hidden\nstates into meaningful semantic features. While several SAE variants have been\nproposed, there remains no principled framework to derive SAEs from the\noriginal dictionary learning formulation. In this work, we introduce such a\nframework by unrolling the proximal gradient method for sparse coding. We show\nthat a single-step update naturally recovers common SAE variants, including\nReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation\nof existing SAEs: their sparsity-inducing regularizers enforce non-negativity,\npreventing a single feature from representing bidirectional concepts (e.g.,\nmale vs. female). This structural constraint fragments semantic axes into\nseparate, redundant features, limiting representational completeness. To\naddress this issue, we propose AbsTopK SAE, a new variant derived from the\n$\\ell_0$ sparsity constraint that applies hard thresholding over the\nlargest-magnitude activations. By preserving both positive and negative\nactivations, AbsTopK uncovers richer, bidirectional conceptual representations.\nComprehensive experiments across four LLMs and seven probing and steering tasks\nshow that AbsTopK improves reconstruction fidelity, enhances interpretability,\nand enables single features to encode contrasting concepts. Remarkably, AbsTopK\nmatches or even surpasses the Difference-in-Mean method, a supervised approach\nthat requires labeled data for each concept and has been shown in prior work to\noutperform SAEs."
                },
                "authors": [
                    {
                        "name": "Xudong Zhu"
                    },
                    {
                        "name": "Mohammad Mahdi Khalili"
                    },
                    {
                        "name": "Zhihui Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zhihui Zhu"
                },
                "author": "Zhihui Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10582v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10582v3",
                "updated": "2025-10-02T17:20:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    20,
                    32,
                    3,
                    275,
                    0
                ],
                "published": "2025-01-17T22:20:55Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    22,
                    20,
                    55,
                    4,
                    17,
                    0
                ],
                "title": "Adapting Large Language Models for Character-based Augmentative and\n  Alternative Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models for Character-based Augmentative and\n  Alternative Communication"
                },
                "summary": "Users of Augmentative and Alternative Communication (AAC) may write\nletter-by-letter via an interface that uses a character language model.\nHowever, most state-of-the-art large pretrained language models predict subword\ntokens of variable length. We investigate how to practically use such models to\nmake accurate and efficient character predictions. Our algorithm for producing\ncharacter predictions from a subword large language model (LLM) provides more\naccurate predictions than using a classification layer, a byte-level LLM, or an\nn-gram model. Additionally, we investigate a domain adaptation procedure based\non a large dataset of sentences we curated based on scoring how useful each\nsentence might be for spoken or written AAC communication. We find our\nprocedure further improves model performance on simple, conversational text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users of Augmentative and Alternative Communication (AAC) may write\nletter-by-letter via an interface that uses a character language model.\nHowever, most state-of-the-art large pretrained language models predict subword\ntokens of variable length. We investigate how to practically use such models to\nmake accurate and efficient character predictions. Our algorithm for producing\ncharacter predictions from a subword large language model (LLM) provides more\naccurate predictions than using a classification layer, a byte-level LLM, or an\nn-gram model. Additionally, we investigate a domain adaptation procedure based\non a large dataset of sentences we curated based on scoring how useful each\nsentence might be for spoken or written AAC communication. We find our\nprocedure further improves model performance on simple, conversational text."
                },
                "authors": [
                    {
                        "name": "Dylan Gaines"
                    },
                    {
                        "name": "Keith Vertanen"
                    }
                ],
                "author_detail": {
                    "name": "Keith Vertanen"
                },
                "author": "Keith Vertanen",
                "arxiv_comment": "To appear in Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10582v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10582v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02230v1",
                "updated": "2025-10-02T17:17:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    17,
                    27,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:17:27Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    17,
                    27,
                    3,
                    275,
                    0
                ],
                "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains\n  Language Models"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference."
                },
                "authors": [
                    {
                        "name": "Phuc Minh Nguyen"
                    },
                    {
                        "name": "Chinh D. La"
                    },
                    {
                        "name": "Duy M. H. Nguyen"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    },
                    {
                        "name": "Binh T. Nguyen"
                    },
                    {
                        "name": "Khoa D. Doan"
                    }
                ],
                "author_detail": {
                    "name": "Khoa D. Doan"
                },
                "author": "Khoa D. Doan",
                "arxiv_comment": "23 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02228v1",
                "updated": "2025-10-02T17:14:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    14,
                    34,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:14:34Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    14,
                    34,
                    3,
                    275,
                    0
                ],
                "title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity"
                },
                "summary": "Scaling laws play a central role in the success of Large Language Models\n(LLMs), enabling the prediction of model performance relative to compute\nbudgets prior to training. While Transformers have been the dominant\narchitecture, recent alternatives such as xLSTM offer linear complexity with\nrespect to context length while remaining competitive in the billion-parameter\nregime. We conduct a comparative investigation on the scaling behavior of\nTransformers and xLSTM along the following lines, providing insights to guide\nfuture model design and deployment. First, we study the scaling behavior for\nxLSTM in compute-optimal and over-training regimes using both IsoFLOP and\nparametric fit approaches on a wide range of model sizes (80M-7B) and number of\ntraining tokens (2B-2T). Second, we examine the dependence of optimal model\nsizes on context length, a pivotal aspect that was largely ignored in previous\nwork. Finally, we analyze inference-time scaling characteristics. Our findings\nreveal that in typical LLM training and inference scenarios, xLSTM scales\nfavorably compared to Transformers. Importantly, xLSTM's advantage widens as\ntraining and inference contexts grow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws play a central role in the success of Large Language Models\n(LLMs), enabling the prediction of model performance relative to compute\nbudgets prior to training. While Transformers have been the dominant\narchitecture, recent alternatives such as xLSTM offer linear complexity with\nrespect to context length while remaining competitive in the billion-parameter\nregime. We conduct a comparative investigation on the scaling behavior of\nTransformers and xLSTM along the following lines, providing insights to guide\nfuture model design and deployment. First, we study the scaling behavior for\nxLSTM in compute-optimal and over-training regimes using both IsoFLOP and\nparametric fit approaches on a wide range of model sizes (80M-7B) and number of\ntraining tokens (2B-2T). Second, we examine the dependence of optimal model\nsizes on context length, a pivotal aspect that was largely ignored in previous\nwork. Finally, we analyze inference-time scaling characteristics. Our findings\nreveal that in typical LLM training and inference scenarios, xLSTM scales\nfavorably compared to Transformers. Importantly, xLSTM's advantage widens as\ntraining and inference contexts grow."
                },
                "authors": [
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Kajetan Schweighofer"
                    },
                    {
                        "name": "Sebastian Böck"
                    },
                    {
                        "name": "Sebastian Lehner"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "arxiv_comment": "Code and data available at\n  https://github.com/NX-AI/xlstm_scaling_laws",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02227v1",
                "updated": "2025-10-02T17:14:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    14,
                    0,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:14:00Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    14,
                    0,
                    3,
                    275,
                    0
                ],
                "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Yuan"
                    },
                    {
                        "name": "Yujuan Ding"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Jinyu Cai"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Hengtao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hengtao Shen"
                },
                "author": "Hengtao Shen",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02226v1",
                "updated": "2025-10-02T17:13:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    13,
                    35,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:13:35Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    13,
                    35,
                    3,
                    275,
                    0
                ],
                "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TempoControl: Temporal Attention Guidance for Text-to-Video Models"
                },
                "summary": "Recent advances in generative video models have enabled the creation of\nhigh-quality videos based on natural language prompts. However, these models\nfrequently lack fine-grained temporal control, meaning they do not allow users\nto specify when particular visual elements should appear within a generated\nsequence. In this work, we introduce TempoControl, a method that allows for\ntemporal alignment of visual concepts during inference, without requiring\nretraining or additional supervision. TempoControl utilizes cross-attention\nmaps, a key component of text-to-video diffusion models, to guide the timing of\nconcepts through a novel optimization approach. Our method steers attention\nusing three complementary principles: aligning its temporal shape with a\ncontrol signal (via correlation), amplifying it where visibility is needed (via\nenergy), and maintaining spatial focus (via entropy). TempoControl allows\nprecise control over timing while ensuring high video quality and diversity. We\ndemonstrate its effectiveness across various video generation applications,\nincluding temporal reordering for single and multiple objects, as well as\naction and audio-aligned generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative video models have enabled the creation of\nhigh-quality videos based on natural language prompts. However, these models\nfrequently lack fine-grained temporal control, meaning they do not allow users\nto specify when particular visual elements should appear within a generated\nsequence. In this work, we introduce TempoControl, a method that allows for\ntemporal alignment of visual concepts during inference, without requiring\nretraining or additional supervision. TempoControl utilizes cross-attention\nmaps, a key component of text-to-video diffusion models, to guide the timing of\nconcepts through a novel optimization approach. Our method steers attention\nusing three complementary principles: aligning its temporal shape with a\ncontrol signal (via correlation), amplifying it where visibility is needed (via\nenergy), and maintaining spatial focus (via entropy). TempoControl allows\nprecise control over timing while ensuring high video quality and diversity. We\ndemonstrate its effectiveness across various video generation applications,\nincluding temporal reordering for single and multiple objects, as well as\naction and audio-aligned generation."
                },
                "authors": [
                    {
                        "name": "Shira Schiber"
                    },
                    {
                        "name": "Ofir Lindenbaum"
                    },
                    {
                        "name": "Idan Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "Idan Schwartz"
                },
                "author": "Idan Schwartz",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02222v1",
                "updated": "2025-10-02T17:06:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    6,
                    13,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:06:13Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    6,
                    13,
                    3,
                    275,
                    0
                ],
                "title": "Collaborative Edge Inference via Semantic Grouping under Wireless\n  Channel Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Edge Inference via Semantic Grouping under Wireless\n  Channel Constraints"
                },
                "summary": "In this paper, we study the framework of collaborative inference, or edge\nensembles. This framework enables multiple edge devices to improve\nclassification accuracy by exchanging intermediate features rather than raw\nobservations. However, efficient communication strategies are essential to\nbalance accuracy and bandwidth limitations. Building upon a key-query mechanism\nfor selective information exchange, this work extends collaborative inference\nby studying the impact of channel noise in feature communication, the choice of\nintermediate collaboration points, and the communication-accuracy trade-off\nacross tasks. By analyzing how different collaboration points affect\nperformance and exploring communication pruning, we show that it is possible to\noptimize accuracy while minimizing resource usage. We show that the\nintermediate collaboration approach is robust to channel errors and that the\nquery transmission needs a higher degree of reliability than the data\ntransmission itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the framework of collaborative inference, or edge\nensembles. This framework enables multiple edge devices to improve\nclassification accuracy by exchanging intermediate features rather than raw\nobservations. However, efficient communication strategies are essential to\nbalance accuracy and bandwidth limitations. Building upon a key-query mechanism\nfor selective information exchange, this work extends collaborative inference\nby studying the impact of channel noise in feature communication, the choice of\nintermediate collaboration points, and the communication-accuracy trade-off\nacross tasks. By analyzing how different collaboration points affect\nperformance and exploring communication pruning, we show that it is possible to\noptimize accuracy while minimizing resource usage. We show that the\nintermediate collaboration approach is robust to channel errors and that the\nquery transmission needs a higher degree of reliability than the data\ntransmission itself."
                },
                "authors": [
                    {
                        "name": "Mateus P. Mota"
                    },
                    {
                        "name": "Mattia Merluzzi"
                    },
                    {
                        "name": "Emilio Calvanese Strinati"
                    }
                ],
                "author_detail": {
                    "name": "Emilio Calvanese Strinati"
                },
                "author": "Emilio Calvanese Strinati",
                "arxiv_comment": "5 pages, 5 figures. Accepted at 33rd European Signal Processing\n  Conference (EUSIPCO 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02219v1",
                "updated": "2025-10-02T17:03:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    3,
                    9,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:03:09Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    3,
                    9,
                    3,
                    275,
                    0
                ],
                "title": "Contrastive Retrieval Heads Improve Attention-Based Re-Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Retrieval Heads Improve Attention-Based Re-Ranking"
                },
                "summary": "The strong zero-shot and long-context capabilities of recent Large Language\nModels (LLMs) have paved the way for highly effective re-ranking systems.\nAttention-based re-rankers leverage attention weights from transformer heads to\nproduce relevance scores, but not all heads are created equally: many\ncontribute noise and redundancy, thus limiting performance. To address this, we\nintroduce CoRe heads, a small set of retrieval heads identified via a\ncontrastive scoring metric that explicitly rewards high attention heads that\ncorrelate with relevant documents, while downplaying nodes with higher\nattention that correlate with irrelevant documents. This relative ranking\ncriterion isolates the most discriminative heads for re-ranking and yields a\nstate-of-the-art list-wise re-ranker. Extensive experiments with three LLMs\nshow that aggregated signals from CoRe heads, constituting less than 1% of all\nheads, substantially improve re-ranking accuracy over strong baselines. We\nfurther find that CoRe heads are concentrated in middle layers, and pruning the\ncomputation of final 50% of model layers preserves accuracy while significantly\nreducing inference time and memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong zero-shot and long-context capabilities of recent Large Language\nModels (LLMs) have paved the way for highly effective re-ranking systems.\nAttention-based re-rankers leverage attention weights from transformer heads to\nproduce relevance scores, but not all heads are created equally: many\ncontribute noise and redundancy, thus limiting performance. To address this, we\nintroduce CoRe heads, a small set of retrieval heads identified via a\ncontrastive scoring metric that explicitly rewards high attention heads that\ncorrelate with relevant documents, while downplaying nodes with higher\nattention that correlate with irrelevant documents. This relative ranking\ncriterion isolates the most discriminative heads for re-ranking and yields a\nstate-of-the-art list-wise re-ranker. Extensive experiments with three LLMs\nshow that aggregated signals from CoRe heads, constituting less than 1% of all\nheads, substantially improve re-ranking accuracy over strong baselines. We\nfurther find that CoRe heads are concentrated in middle layers, and pruning the\ncomputation of final 50% of model layers preserves accuracy while significantly\nreducing inference time and memory usage."
                },
                "authors": [
                    {
                        "name": "Linh Tran"
                    },
                    {
                        "name": "Yulong Li"
                    },
                    {
                        "name": "Radu Florian"
                    },
                    {
                        "name": "Wei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Wei Sun"
                },
                "author": "Wei Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09119v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09119v6",
                "updated": "2025-10-02T17:02:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    2,
                    26,
                    3,
                    275,
                    0
                ],
                "published": "2025-03-12T07:12:02Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    12,
                    2,
                    2,
                    71,
                    0
                ],
                "title": "Training Hybrid Deep Quantum Neural Network for Efficient Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Hybrid Deep Quantum Neural Network for Efficient Reinforcement\n  Learning"
                },
                "summary": "Quantum circuits embed data in a Hilbert space whose dimensionality grows\nexponentially with the number of qubits, allowing even shallow parameterised\nquantum circuits (PQCs) to represent highly-correlated probability\ndistributions that are costly for classical networks to capture.\nReinforcement-learning (RL) agents, which must reason over long-horizon,\ncontinuous-control tasks, stand to benefit from this expressive quantum feature\nspace, but only if the quantum layers can be trained jointly with the\nsurrounding deep-neural components. Current gradient-estimation techniques\n(e.g., parameter-shift rule) make such hybrid training impractical for\nrealistic RL workloads, because every gradient step requires a prohibitive\nnumber of circuit evaluations and thus erodes the potential quantum advantage.\nWe introduce qtDNN, a tangential surrogate that locally approximates a PQC with\na small differentiable network trained on-the-fly from the same minibatch.\nEmbedding qtDNN inside the computation graph yields scalable batch gradients\nwhile keeping the original quantum layer for inference. Building on qtDNN we\ndesign hDQNN-TD3, a hybrid deep quantum neural network for continuous-control\nreinforcement learning based on the TD3 architecture, which matches or exceeds\nstate-of-the-art classical performance on popular benchmarks. The method opens\na path toward applying hybrid quantum models to large-scale RL and other\ngradient-intensive machine-learning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum circuits embed data in a Hilbert space whose dimensionality grows\nexponentially with the number of qubits, allowing even shallow parameterised\nquantum circuits (PQCs) to represent highly-correlated probability\ndistributions that are costly for classical networks to capture.\nReinforcement-learning (RL) agents, which must reason over long-horizon,\ncontinuous-control tasks, stand to benefit from this expressive quantum feature\nspace, but only if the quantum layers can be trained jointly with the\nsurrounding deep-neural components. Current gradient-estimation techniques\n(e.g., parameter-shift rule) make such hybrid training impractical for\nrealistic RL workloads, because every gradient step requires a prohibitive\nnumber of circuit evaluations and thus erodes the potential quantum advantage.\nWe introduce qtDNN, a tangential surrogate that locally approximates a PQC with\na small differentiable network trained on-the-fly from the same minibatch.\nEmbedding qtDNN inside the computation graph yields scalable batch gradients\nwhile keeping the original quantum layer for inference. Building on qtDNN we\ndesign hDQNN-TD3, a hybrid deep quantum neural network for continuous-control\nreinforcement learning based on the TD3 architecture, which matches or exceeds\nstate-of-the-art classical performance on popular benchmarks. The method opens\na path toward applying hybrid quantum models to large-scale RL and other\ngradient-intensive machine-learning tasks."
                },
                "authors": [
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jeremy Kulcsar"
                    },
                    {
                        "name": "Xueyin Chen"
                    },
                    {
                        "name": "Giulio Giaconi"
                    },
                    {
                        "name": "Georgios Korpas"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Korpas"
                },
                "author": "Georgios Korpas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09119v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09119v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02212v1",
                "updated": "2025-10-02T16:57:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    57,
                    24,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:57:24Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    57,
                    24,
                    3,
                    275,
                    0
                ],
                "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via\n  Reinforcement Learning"
                },
                "summary": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified\nframework for training masked diffusion large language models (dLLMs) to reason\nnot only better (furious), but also faster via reinforcement learning (RL). We\nfirst unify the existing baseline approach such as d1 by proposing to train\nsurrogate policies via off-policy RL, whose likelihood is much more tractable\nas an approximation to the true dLLM policy. This naturally motivates a more\naccurate and informative two-stage likelihood approximation combined with\nimportance sampling correction, which leads to generalized RL algorithms with\nbetter sample efficiency and superior task performance. Second, we propose a\nnew direction of joint training efficient samplers/controllers of dLLMs policy.\nVia RL, we incentivize dLLMs' natural multi-token prediction capabilities by\nletting the model learn to adaptively allocate an inference threshold for each\nprompt. By jointly training the sampler, we yield better accuracies with lower\nnumber of function evaluations (NFEs) compared to training the model only,\nobtaining the best performance in improving the Pareto frontier of the\ninference-time compute of dLLMs. We showcase the effectiveness of our pipeline\nby training open source large diffusion language models over benchmark math and\nplanning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified\nframework for training masked diffusion large language models (dLLMs) to reason\nnot only better (furious), but also faster via reinforcement learning (RL). We\nfirst unify the existing baseline approach such as d1 by proposing to train\nsurrogate policies via off-policy RL, whose likelihood is much more tractable\nas an approximation to the true dLLM policy. This naturally motivates a more\naccurate and informative two-stage likelihood approximation combined with\nimportance sampling correction, which leads to generalized RL algorithms with\nbetter sample efficiency and superior task performance. Second, we propose a\nnew direction of joint training efficient samplers/controllers of dLLMs policy.\nVia RL, we incentivize dLLMs' natural multi-token prediction capabilities by\nletting the model learn to adaptively allocate an inference threshold for each\nprompt. By jointly training the sampler, we yield better accuracies with lower\nnumber of function evaluations (NFEs) compared to training the model only,\nobtaining the best performance in improving the Pareto frontier of the\ninference-time compute of dLLMs. We showcase the effectiveness of our pipeline\nby training open source large diffusion language models over benchmark math and\nplanning tasks."
                },
                "authors": [
                    {
                        "name": "Hanyang Zhao"
                    },
                    {
                        "name": "Dawen Liang"
                    },
                    {
                        "name": "Wenpin Tang"
                    },
                    {
                        "name": "David Yao"
                    },
                    {
                        "name": "Nathan Kallus"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Kallus"
                },
                "author": "Nathan Kallus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00708v2",
                "updated": "2025-10-02T16:56:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    56,
                    36,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-31T20:56:54Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    20,
                    56,
                    54,
                    5,
                    151,
                    0
                ],
                "title": "DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph\n  Completion across General and Biomedical Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph\n  Completion across General and Biomedical Domains"
                },
                "summary": "Knowledge graph completion (KGC) aims to predict missing triples in knowledge\ngraphs (KGs) by leveraging existing triples and textual information. Recently,\ngenerative large language models (LLMs) have been increasingly employed for\ngraph tasks. However, current approaches typically encode graph context in\ntextual form, which fails to fully exploit the potential of LLMs for perceiving\nand reasoning about graph structures. To address this limitation, we propose\nDrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph\nCompletion). DrKGC employs a flexible lightweight model training strategy to\nlearn structural embeddings and logical rules within the KG. It then leverages\na novel bottom-up graph retrieval method to extract a subgraph for each query\nguided by the learned rules. Finally, a graph convolutional network (GCN)\nadapter uses the retrieved subgraph to enhance the structural embeddings, which\nare then integrated into the prompt for effective LLM fine-tuning. Experimental\nresults on two general domain benchmark datasets and two biomedical datasets\ndemonstrate the superior performance of DrKGC. Furthermore, a realistic case\nstudy in the biomedical domain highlights its interpretability and practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graph completion (KGC) aims to predict missing triples in knowledge\ngraphs (KGs) by leveraging existing triples and textual information. Recently,\ngenerative large language models (LLMs) have been increasingly employed for\ngraph tasks. However, current approaches typically encode graph context in\ntextual form, which fails to fully exploit the potential of LLMs for perceiving\nand reasoning about graph structures. To address this limitation, we propose\nDrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph\nCompletion). DrKGC employs a flexible lightweight model training strategy to\nlearn structural embeddings and logical rules within the KG. It then leverages\na novel bottom-up graph retrieval method to extract a subgraph for each query\nguided by the learned rules. Finally, a graph convolutional network (GCN)\nadapter uses the retrieved subgraph to enhance the structural embeddings, which\nare then integrated into the prompt for effective LLM fine-tuning. Experimental\nresults on two general domain benchmark datasets and two biomedical datasets\ndemonstrate the superior performance of DrKGC. Furthermore, a realistic case\nstudy in the biomedical domain highlights its interpretability and practical\nutility."
                },
                "authors": [
                    {
                        "name": "Yongkang Xiao"
                    },
                    {
                        "name": "Sinian Zhang"
                    },
                    {
                        "name": "Yi Dai"
                    },
                    {
                        "name": "Huixue Zhou"
                    },
                    {
                        "name": "Jue Hou"
                    },
                    {
                        "name": "Jie Ding"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_comment": "Accepted at EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02209v1",
                "updated": "2025-10-02T16:54:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    54,
                    57,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:54:57Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    54,
                    57,
                    3,
                    275,
                    0
                ],
                "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?"
                },
                "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain."
                },
                "authors": [
                    {
                        "name": "Yanxu Chen"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Yantao Liu"
                    },
                    {
                        "name": "Jin Ye"
                    },
                    {
                        "name": "Jianing Yu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02206v1",
                "updated": "2025-10-02T16:52:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    52,
                    45,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:52:45Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    52,
                    45,
                    3,
                    275,
                    0
                ],
                "title": "Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling"
                },
                "summary": "Sequence-to-sequence models have become central in Artificial Intelligence,\nparticularly following the introduction of the transformer architecture. While\ninitially developed for Natural Language Processing, these models have\ndemonstrated utility across domains, including Computer Vision. Such models\nrequire mechanisms to exchange information along the time dimension, typically\nusing recurrent or self-attention layers. However, self-attention scales\nquadratically with sequence length, limiting its practicality for very long\nsequences.\n  We introduce Poolformer, a sequence-to-sequence model that replaces\nself-attention with recurrent layers and incorporates pooling operations to\nreduce sequence length. Poolformer is defined recursively using SkipBlocks,\nwhich contain residual blocks, a down-pooling layer, a nested SkipBlock, an\nup-pooling layer, and additional residual blocks. We conduct extensive\nexperiments to support our architectural choices.\n  Our results show that pooling greatly accelerates training, improves\nperceptual metrics (FID and IS), and prevents overfitting. Our experiments also\nsuggest that long-range dependencies are handled by deep layers, while shallow\nlayers take care of short-term features.\n  Evaluated on raw audio, which naturally features long sequence lengths,\nPoolformer outperforms state-of-the-art models such as SaShiMi and Mamba.\nFuture directions include applications to text and vision, as well as\nmulti-modal scenarios, where a Poolformer-based LLM could effectively process\ndense representations of images and videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence models have become central in Artificial Intelligence,\nparticularly following the introduction of the transformer architecture. While\ninitially developed for Natural Language Processing, these models have\ndemonstrated utility across domains, including Computer Vision. Such models\nrequire mechanisms to exchange information along the time dimension, typically\nusing recurrent or self-attention layers. However, self-attention scales\nquadratically with sequence length, limiting its practicality for very long\nsequences.\n  We introduce Poolformer, a sequence-to-sequence model that replaces\nself-attention with recurrent layers and incorporates pooling operations to\nreduce sequence length. Poolformer is defined recursively using SkipBlocks,\nwhich contain residual blocks, a down-pooling layer, a nested SkipBlock, an\nup-pooling layer, and additional residual blocks. We conduct extensive\nexperiments to support our architectural choices.\n  Our results show that pooling greatly accelerates training, improves\nperceptual metrics (FID and IS), and prevents overfitting. Our experiments also\nsuggest that long-range dependencies are handled by deep layers, while shallow\nlayers take care of short-term features.\n  Evaluated on raw audio, which naturally features long sequence lengths,\nPoolformer outperforms state-of-the-art models such as SaShiMi and Mamba.\nFuture directions include applications to text and vision, as well as\nmulti-modal scenarios, where a Poolformer-based LLM could effectively process\ndense representations of images and videos."
                },
                "authors": [
                    {
                        "name": "Daniel Gallo Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Gallo Fernández"
                },
                "author": "Daniel Gallo Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02201v1",
                "updated": "2025-10-02T16:50:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    50,
                    16,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:50:16Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    50,
                    16,
                    3,
                    275,
                    0
                ],
                "title": "Understanding the Origins of Super-Puff Planets: A New Mass-Loss Regime\n  Coupled to Planetary Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Origins of Super-Puff Planets: A New Mass-Loss Regime\n  Coupled to Planetary Evolution"
                },
                "summary": "Super-puffs are a class of low-mass, large-radius planets that have\nchallenged planet formation and evolution models. Their high inferred H/He mass\nfractions, required to explain their physical sizes, would lead to rapid\natmospheric escape, raising questions about their long-term retention. Recent\nmodeling work indicates that low-mass planets typically require 50\\% less H/He\nmass to match their observed radius, due to significant roles of the radiative\natmosphere and interior heating from the rock/iron core. Here, through a new\nquantitative analysis of XUV-driven escape in sub-Neptunes, we find that\nprevious studies overestimated mass loss, as scaling laws in low-gravity\nregimes deviate greatly from the widely used energy-limited regime. We define a\nnew regime, thermal-energy-mediated photoevaporation (TEMP), in which thermal\nenergy conversion critically sets the mass-loss rate. These effects make\nsuper-puffs more resilient to mass loss than previously thought. We develop a\ncoupled evolution model integrating this updated thermal evolution framework\nwith a 1D hydrodynamic photoevaporation model. Applying this novel, joint model\nto observed super-puffs and young low-density planets, we find that their\nmasses, radii and transit pressures align with predictions assuming either a\nclear or hazy atmosphere. This indicates that super-puffs have undergone a\ncombination of boil-off and photoevaporative mass loss, with boil-off\ndominating the process. Our results indicate that low-density planets typically\npossess both a thick convective envelope and substantial radiative atmosphere,\nwhich contribute to their large radii. For this to occur, these planets must\nhave intermediate masses of 5-10$M_\\oplus$ and receive stellar insolation\n$\\lesssim 30F_\\oplus$, favoring FG-type stars over M-dwarfs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super-puffs are a class of low-mass, large-radius planets that have\nchallenged planet formation and evolution models. Their high inferred H/He mass\nfractions, required to explain their physical sizes, would lead to rapid\natmospheric escape, raising questions about their long-term retention. Recent\nmodeling work indicates that low-mass planets typically require 50\\% less H/He\nmass to match their observed radius, due to significant roles of the radiative\natmosphere and interior heating from the rock/iron core. Here, through a new\nquantitative analysis of XUV-driven escape in sub-Neptunes, we find that\nprevious studies overestimated mass loss, as scaling laws in low-gravity\nregimes deviate greatly from the widely used energy-limited regime. We define a\nnew regime, thermal-energy-mediated photoevaporation (TEMP), in which thermal\nenergy conversion critically sets the mass-loss rate. These effects make\nsuper-puffs more resilient to mass loss than previously thought. We develop a\ncoupled evolution model integrating this updated thermal evolution framework\nwith a 1D hydrodynamic photoevaporation model. Applying this novel, joint model\nto observed super-puffs and young low-density planets, we find that their\nmasses, radii and transit pressures align with predictions assuming either a\nclear or hazy atmosphere. This indicates that super-puffs have undergone a\ncombination of boil-off and photoevaporative mass loss, with boil-off\ndominating the process. Our results indicate that low-density planets typically\npossess both a thick convective envelope and substantial radiative atmosphere,\nwhich contribute to their large radii. For this to occur, these planets must\nhave intermediate masses of 5-10$M_\\oplus$ and receive stellar insolation\n$\\lesssim 30F_\\oplus$, favoring FG-type stars over M-dwarfs."
                },
                "authors": [
                    {
                        "name": "Yao Tang"
                    },
                    {
                        "name": "Jonathan J. Fortney"
                    },
                    {
                        "name": "Ruth Murray-Clay"
                    },
                    {
                        "name": "Madelyne Broome"
                    }
                ],
                "author_detail": {
                    "name": "Madelyne Broome"
                },
                "author": "Madelyne Broome",
                "arxiv_comment": "22 pages, 12 figures; second revision with minor comments under\n  review at ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02200v1",
                "updated": "2025-10-02T16:49:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    49,
                    27,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:49:27Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    49,
                    27,
                    3,
                    275,
                    0
                ],
                "title": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge\n  Graph Exploration Utilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge\n  Graph Exploration Utilities"
                },
                "summary": "Interacting with knowledge graphs can be a daunting task for people without a\nbackground in computer science since the query language that is used (SPARQL)\nhas a high barrier of entry. Large language models (LLMs) can lower that\nbarrier by providing support in the form of Text2SPARQL translation. In this\npaper we introduce a generalized method based on SPINACH, an LLM backed agent\nthat translates natural language questions to SPARQL queries not in a single\nshot, but as an iterative process of exploration and execution. We describe the\noverall architecture and reasoning behind our design decisions, and also\nconduct a thorough analysis of the agent behavior to gain insights into future\nareas for targeted improvements. This work was motivated by the Text2SPARQL\nchallenge, a challenge that was held to facilitate improvements in the\nText2SPARQL domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interacting with knowledge graphs can be a daunting task for people without a\nbackground in computer science since the query language that is used (SPARQL)\nhas a high barrier of entry. Large language models (LLMs) can lower that\nbarrier by providing support in the form of Text2SPARQL translation. In this\npaper we introduce a generalized method based on SPINACH, an LLM backed agent\nthat translates natural language questions to SPARQL queries not in a single\nshot, but as an iterative process of exploration and execution. We describe the\noverall architecture and reasoning behind our design decisions, and also\nconduct a thorough analysis of the agent behavior to gain insights into future\nareas for targeted improvements. This work was motivated by the Text2SPARQL\nchallenge, a challenge that was held to facilitate improvements in the\nText2SPARQL domain."
                },
                "authors": [
                    {
                        "name": "Felix Brei"
                    },
                    {
                        "name": "Lorenz Bühmann"
                    },
                    {
                        "name": "Johannes Frey"
                    },
                    {
                        "name": "Daniel Gerber"
                    },
                    {
                        "name": "Lars-Peter Meyer"
                    },
                    {
                        "name": "Claus Stadler"
                    },
                    {
                        "name": "Kirill Bulert"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Bulert"
                },
                "author": "Kirill Bulert",
                "arxiv_comment": "peer reviewed publication at Text2SPARQL Workshop @ ESWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21499v2",
                "updated": "2025-10-02T16:45:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    45,
                    24,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-25T19:57:36Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    57,
                    36,
                    3,
                    268,
                    0
                ],
                "title": "On Code-Induced Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Code-Induced Reasoning in LLMs"
                },
                "summary": "Code data has been shown to enhance the reasoning capabilities of large\nlanguage models (LLMs), but it remains unclear which aspects of code are most\nresponsible. We investigate this question with a systematic, data-centric\nframework. We construct parallel instruction datasets in ten programming\nlanguages and apply controlled perturbations that selectively disrupt\nstructural or semantic properties of code. We then finetune LLMs from five\nmodel families and eight scales on each variant and evaluate their performance\non natural language, math, and code tasks. Across 3,331 experiments, our\nresults show that LLMs are more vulnerable to structural perturbations than\nsemantic ones, particularly on math and code tasks. Appropriate abstractions\nlike pseudocode and flowcharts can be as effective as code, while encoding the\nsame information with fewer tokens without adhering to original syntax can\noften retain or even improve performance. Remarkably, even corrupted code with\nmisleading signals remains competitive when surface-level regularities persist.\nFinally, syntactic styles also shape task-specific gains with Python favoring\nnatural language reasoning and lower-level languages such as Java and Rust\nfavoring math. Through our systematic framework, we aim to provide insight into\nhow different properties of code influence reasoning and inform the design of\ntraining data for enhancing LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code data has been shown to enhance the reasoning capabilities of large\nlanguage models (LLMs), but it remains unclear which aspects of code are most\nresponsible. We investigate this question with a systematic, data-centric\nframework. We construct parallel instruction datasets in ten programming\nlanguages and apply controlled perturbations that selectively disrupt\nstructural or semantic properties of code. We then finetune LLMs from five\nmodel families and eight scales on each variant and evaluate their performance\non natural language, math, and code tasks. Across 3,331 experiments, our\nresults show that LLMs are more vulnerable to structural perturbations than\nsemantic ones, particularly on math and code tasks. Appropriate abstractions\nlike pseudocode and flowcharts can be as effective as code, while encoding the\nsame information with fewer tokens without adhering to original syntax can\noften retain or even improve performance. Remarkably, even corrupted code with\nmisleading signals remains competitive when surface-level regularities persist.\nFinally, syntactic styles also shape task-specific gains with Python favoring\nnatural language reasoning and lower-level languages such as Java and Rust\nfavoring math. Through our systematic framework, we aim to provide insight into\nhow different properties of code influence reasoning and inform the design of\ntraining data for enhancing LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Abdul Waheed"
                    },
                    {
                        "name": "Zhen Wu"
                    },
                    {
                        "name": "Carolyn Rosé"
                    },
                    {
                        "name": "Daphne Ippolito"
                    }
                ],
                "author_detail": {
                    "name": "Daphne Ippolito"
                },
                "author": "Daphne Ippolito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02194v1",
                "updated": "2025-10-02T16:43:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    43,
                    33,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:43:33Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    43,
                    33,
                    3,
                    275,
                    0
                ],
                "title": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress across a wide\nrange of tasks, but remain vulnerable to safety risks such as harmful content\ngeneration and jailbreak attacks. Existing safety techniques -- including\nexternal guardrails, inference-time guidance, and post-training alignment --\neach face limitations in balancing safety, utility, and controllability. In\nthis work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM\nsafety through safety-aware upcycling. Our approach first identifies\nsafety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)\nstructure, where the router acts as a soft guardrail that selectively activates\noriginal MLPs and added safety experts. We further introduce a two-stage SFT\nstrategy to strengthen safety discrimination while preserving general\ncapabilities. To enable flexible control at inference time, we introduce a\nsafety temperature mechanism, allowing dynamic adjustment of the trade-off\nbetween safety and utility. Experiments across multiple benchmarks, base model,\nand model scales demonstrate that UpSafe$^\\circ$C achieves robust safety\nimprovements against harmful and jailbreak inputs, while maintaining\ncompetitive performance on general tasks. Moreover, analysis shows that safety\ntemperature provides fine-grained inference-time control that achieves the\nPareto-optimal frontier between utility and safety. Our results highlight a new\ndirection for LLM safety: moving from static alignment toward dynamic, modular,\nand inference-aware control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress across a wide\nrange of tasks, but remain vulnerable to safety risks such as harmful content\ngeneration and jailbreak attacks. Existing safety techniques -- including\nexternal guardrails, inference-time guidance, and post-training alignment --\neach face limitations in balancing safety, utility, and controllability. In\nthis work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM\nsafety through safety-aware upcycling. Our approach first identifies\nsafety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)\nstructure, where the router acts as a soft guardrail that selectively activates\noriginal MLPs and added safety experts. We further introduce a two-stage SFT\nstrategy to strengthen safety discrimination while preserving general\ncapabilities. To enable flexible control at inference time, we introduce a\nsafety temperature mechanism, allowing dynamic adjustment of the trade-off\nbetween safety and utility. Experiments across multiple benchmarks, base model,\nand model scales demonstrate that UpSafe$^\\circ$C achieves robust safety\nimprovements against harmful and jailbreak inputs, while maintaining\ncompetitive performance on general tasks. Moreover, analysis shows that safety\ntemperature provides fine-grained inference-time control that achieves the\nPareto-optimal frontier between utility and safety. Our results highlight a new\ndirection for LLM safety: moving from static alignment toward dynamic, modular,\nand inference-aware control."
                },
                "authors": [
                    {
                        "name": "Yuhao Sun"
                    },
                    {
                        "name": "Zhuoer Xu"
                    },
                    {
                        "name": "Shiwen Cui"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Lingyun Yu"
                    },
                    {
                        "name": "Yongdong Zhang"
                    },
                    {
                        "name": "Hongtao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Xie"
                },
                "author": "Hongtao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02191v1",
                "updated": "2025-10-02T16:40:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    40,
                    21,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:40:21Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    40,
                    21,
                    3,
                    275,
                    0
                ],
                "title": "Joint Channel and Semantic-aware Grouping for Effective Collaborative\n  Edge Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Channel and Semantic-aware Grouping for Effective Collaborative\n  Edge Inference"
                },
                "summary": "We focus on collaborative edge inference over wireless, which enables\nmultiple devices to cooperate to improve inference performance in the presence\nof corrupted data. Exploiting a key-query mechanism for selective information\nexchange (or, group formation for collaboration), we recall the effect of\nwireless channel impairments in feature communication. We argue and show that a\ndisjoint approach, which only considers either the semantic relevance or\nchannel state between devices, performs poorly, especially in harsh propagation\nconditions. Based on these findings, we propose a joint approach that takes\ninto account semantic information relevance and channel states when grouping\ndevices for collaboration, by making the general attention weights dependent of\nthe channel information. Numerical simulations show the superiority of the\njoint approach against local inference on corrupted data, as well as compared\nto collaborative inference with disjoint decisions that either consider\napplication or physical layer parameters when forming groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We focus on collaborative edge inference over wireless, which enables\nmultiple devices to cooperate to improve inference performance in the presence\nof corrupted data. Exploiting a key-query mechanism for selective information\nexchange (or, group formation for collaboration), we recall the effect of\nwireless channel impairments in feature communication. We argue and show that a\ndisjoint approach, which only considers either the semantic relevance or\nchannel state between devices, performs poorly, especially in harsh propagation\nconditions. Based on these findings, we propose a joint approach that takes\ninto account semantic information relevance and channel states when grouping\ndevices for collaboration, by making the general attention weights dependent of\nthe channel information. Numerical simulations show the superiority of the\njoint approach against local inference on corrupted data, as well as compared\nto collaborative inference with disjoint decisions that either consider\napplication or physical layer parameters when forming groups."
                },
                "authors": [
                    {
                        "name": "Mateus P. Mota"
                    },
                    {
                        "name": "Mattia Merluzzi"
                    },
                    {
                        "name": "Emilio Calvanese Strinati"
                    }
                ],
                "author_detail": {
                    "name": "Emilio Calvanese Strinati"
                },
                "author": "Emilio Calvanese Strinati",
                "arxiv_doi": "10.1109/SPAWC66079.2025.11143310",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SPAWC66079.2025.11143310",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.02191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in IEEE SPAWC 2025",
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02186v1",
                "updated": "2025-10-02T16:37:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    37,
                    56,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:37:56Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    37,
                    56,
                    3,
                    275,
                    0
                ],
                "title": "GeoPurify: A Data-Efficient Geometric Distillation Framework for\n  Open-Vocabulary 3D Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoPurify: A Data-Efficient Geometric Distillation Framework for\n  Open-Vocabulary 3D Segmentation"
                },
                "summary": "Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to\n3D semantic segmentation expose a persistent trade-off. Directly projecting 2D\nfeatures into 3D yields noisy and fragmented predictions, whereas enforcing\ngeometric coherence necessitates costly training pipelines and large-scale\nannotated 3D data. We argue that this limitation stems from the dominant\nsegmentation-and-matching paradigm, which fails to reconcile 2D semantics with\n3D geometric structure. The geometric cues are not eliminated during the\n2D-to-3D transfer but remain latent within the noisy and view-aggregated\nfeatures. To exploit this property, we propose GeoPurify that applies a small\nStudent Affinity Network to purify 2D VLM-generated 3D point features using\ngeometric priors distilled from a 3D self-supervised teacher model. During\ninference, we devise a Geometry-Guided Pooling module to further denoise the\npoint cloud and ensure the semantic and structural consistency. Benefiting from\nlatent geometric information and the learned affinity network, GeoPurify\neffectively mitigates the trade-off and achieves superior data efficiency.\nExtensive experiments on major 3D benchmarks demonstrate that GeoPurify\nachieves or surpasses state-of-the-art performance while utilizing only about\n1.5% of the training data. Our codes and checkpoints are available at\n[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to\n3D semantic segmentation expose a persistent trade-off. Directly projecting 2D\nfeatures into 3D yields noisy and fragmented predictions, whereas enforcing\ngeometric coherence necessitates costly training pipelines and large-scale\nannotated 3D data. We argue that this limitation stems from the dominant\nsegmentation-and-matching paradigm, which fails to reconcile 2D semantics with\n3D geometric structure. The geometric cues are not eliminated during the\n2D-to-3D transfer but remain latent within the noisy and view-aggregated\nfeatures. To exploit this property, we propose GeoPurify that applies a small\nStudent Affinity Network to purify 2D VLM-generated 3D point features using\ngeometric priors distilled from a 3D self-supervised teacher model. During\ninference, we devise a Geometry-Guided Pooling module to further denoise the\npoint cloud and ensure the semantic and structural consistency. Benefiting from\nlatent geometric information and the learned affinity network, GeoPurify\neffectively mitigates the trade-off and achieves superior data efficiency.\nExtensive experiments on major 3D benchmarks demonstrate that GeoPurify\nachieves or surpasses state-of-the-art performance while utilizing only about\n1.5% of the training data. Our codes and checkpoints are available at\n[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify)."
                },
                "authors": [
                    {
                        "name": "Weijia Dou"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Bo Peng"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02185v1",
                "updated": "2025-10-02T16:36:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    36,
                    56,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:36:56Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    36,
                    56,
                    3,
                    275,
                    0
                ],
                "title": "FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen\n  Using Agentic AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen\n  Using Agentic AI"
                },
                "summary": "Fuzz testing has become a cornerstone technique for identifying software bugs\nand security vulnerabilities, with broad adoption in both industry and\nopen-source communities. Directly fuzzing a function requires fuzz drivers,\nwhich translate random fuzzer inputs into valid arguments for the target\nfunction. Given the cost and expertise required to manually develop fuzz\ndrivers, methods exist that leverage program analysis and Large Language Models\nto automatically generate these drivers. However, the generated fuzz drivers\nfrequently lead to false positive crashes, especially in functions highly\nstructured input and complex state requirements. This problem is especially\ncrucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as\nreporting false positive crashes to maintainers impede trust in both the system\nand the team.\n  This paper presents two AI-driven strategies to reduce false positives in\nOSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,\nconstraint-based fuzz driver generation proactively enforces constraints on a\nfunction's inputs and state to guide driver creation. Second, context-based\ncrash validation reactively analyzes function callers to determine whether\nreported crashes are feasible from program entry points. Using 1,500 benchmark\nfunctions from OSS-Fuzz, we show that these strategies reduce spurious crashes\nby up to 8%, cut reported crashes by more than half, and demonstrate that\nfrontier LLMs can serve as reliable program analysis agents. Our results\nhighlight the promise and challenges of integrating AI into large-scale fuzzing\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzz testing has become a cornerstone technique for identifying software bugs\nand security vulnerabilities, with broad adoption in both industry and\nopen-source communities. Directly fuzzing a function requires fuzz drivers,\nwhich translate random fuzzer inputs into valid arguments for the target\nfunction. Given the cost and expertise required to manually develop fuzz\ndrivers, methods exist that leverage program analysis and Large Language Models\nto automatically generate these drivers. However, the generated fuzz drivers\nfrequently lead to false positive crashes, especially in functions highly\nstructured input and complex state requirements. This problem is especially\ncrucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as\nreporting false positive crashes to maintainers impede trust in both the system\nand the team.\n  This paper presents two AI-driven strategies to reduce false positives in\nOSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,\nconstraint-based fuzz driver generation proactively enforces constraints on a\nfunction's inputs and state to guide driver creation. Second, context-based\ncrash validation reactively analyzes function callers to determine whether\nreported crashes are feasible from program entry points. Using 1,500 benchmark\nfunctions from OSS-Fuzz, we show that these strategies reduce spurious crashes\nby up to 8%, cut reported crashes by more than half, and demonstrate that\nfrontier LLMs can serve as reliable program analysis agents. Our results\nhighlight the promise and challenges of integrating AI into large-scale fuzzing\npipelines."
                },
                "authors": [
                    {
                        "name": "Paschal C. Amusuo"
                    },
                    {
                        "name": "Dongge Liu"
                    },
                    {
                        "name": "Ricardo Andres Calvo Mendez"
                    },
                    {
                        "name": "Jonathan Metzman"
                    },
                    {
                        "name": "Oliver Chang"
                    },
                    {
                        "name": "James C. Davis"
                    }
                ],
                "author_detail": {
                    "name": "James C. Davis"
                },
                "author": "James C. Davis",
                "arxiv_comment": "12 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; F.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03206v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03206v3",
                "updated": "2025-10-02T16:36:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    36,
                    8,
                    3,
                    275,
                    0
                ],
                "published": "2025-04-04T06:35:02Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    35,
                    2,
                    4,
                    94,
                    0
                ],
                "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward"
                },
                "summary": "Effective conversational agents like large language models (LLMs) must\npersonalize their interactions to adapt to user preferences, personalities, and\nattributes across diverse domains like education and healthcare. Current\nmethods like Reinforcement Learning from Human Feedback (RLHF), often\nprioritize helpfulness and safety but fall short in fostering truly empathetic,\nadaptive, and personalized dialogues. Existing personalization approaches\ntypically rely on extensive user history, limiting their effectiveness for new\nor context-limited users. To address these limitations, we propose leveraging a\nuser model to incorporate a curiosity-based intrinsic reward into multi-turn\nRLHF. This novel reward mechanism encourages the LLM agent to actively infer\nuser traits by optimizing conversations to improve its user model's accuracy.\nConsequently, the agent delivers more personalized interactions by learning\nmore about the user. We demonstrate our method's effectiveness in two distinct\ndomains: significantly improving personalization performance in a\nconversational recommendation task, and personalizing conversations for\ndifferent learning styles in an educational setting. We show improved\ngeneralization capabilities compared to traditional multi-turn RLHF, all while\nmaintaining conversation quality. Our method offers a promising solution for\ncreating more personalized, adaptive, and engaging conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective conversational agents like large language models (LLMs) must\npersonalize their interactions to adapt to user preferences, personalities, and\nattributes across diverse domains like education and healthcare. Current\nmethods like Reinforcement Learning from Human Feedback (RLHF), often\nprioritize helpfulness and safety but fall short in fostering truly empathetic,\nadaptive, and personalized dialogues. Existing personalization approaches\ntypically rely on extensive user history, limiting their effectiveness for new\nor context-limited users. To address these limitations, we propose leveraging a\nuser model to incorporate a curiosity-based intrinsic reward into multi-turn\nRLHF. This novel reward mechanism encourages the LLM agent to actively infer\nuser traits by optimizing conversations to improve its user model's accuracy.\nConsequently, the agent delivers more personalized interactions by learning\nmore about the user. We demonstrate our method's effectiveness in two distinct\ndomains: significantly improving personalization performance in a\nconversational recommendation task, and personalizing conversations for\ndifferent learning styles in an educational setting. We show improved\ngeneralization capabilities compared to traditional multi-turn RLHF, all while\nmaintaining conversation quality. Our method offers a promising solution for\ncreating more personalized, adaptive, and engaging conversational agents."
                },
                "authors": [
                    {
                        "name": "Yanming Wan"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Marwa Abdulhai"
                    },
                    {
                        "name": "Lior Shani"
                    },
                    {
                        "name": "Natasha Jaques"
                    }
                ],
                "author_detail": {
                    "name": "Natasha Jaques"
                },
                "author": "Natasha Jaques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03206v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03206v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02182v1",
                "updated": "2025-10-02T16:33:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    33,
                    40,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:33:40Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    33,
                    40,
                    3,
                    275,
                    0
                ],
                "title": "Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex\n  with Mutual Information-Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex\n  with Mutual Information-Guided Diffusion"
                },
                "summary": "Understanding how neural populations in higher visual areas encode\nobject-centered visual information remains a central challenge in computational\nneuroscience. Prior works have investigated representational alignment between\nartificial neural networks and the visual cortex. Nevertheless, these findings\nare indirect and offer limited insights to the structure of neural populations\nthemselves. Similarly, decoding-based methods have quantified semantic features\nfrom neural populations but have not uncovered their underlying organizations.\nThis leaves open a scientific question: \"how feature-specific visual\ninformation is distributed across neural populations in higher visual areas,\nand whether it is organized into structured, semantically meaningful\nsubspaces.\" To tackle this problem, we present MIG-Vis, a method that leverages\nthe generative power of diffusion models to visualize and validate the\nvisual-semantic attributes encoded in neural latent subspaces. Our method first\nuses a variational autoencoder to infer a group-wise disentangled neural latent\nsubspace from neural populations. Subsequently, we propose a mutual information\n(MI)-guided diffusion synthesis procedure to visualize the specific\nvisual-semantic features encoded by each latent group. We validate MIG-Vis on\nmulti-session neural spiking datasets from the inferior temporal (IT) cortex of\ntwo macaques. The synthesized results demonstrate that our method identifies\nneural latent groups with clear semantic selectivity to diverse visual\nfeatures, including object pose, inter-category transformations, and\nintra-class content. These findings provide direct, interpretable evidence of\nstructured semantic representation in the higher visual cortex and advance our\nunderstanding of its encoding principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how neural populations in higher visual areas encode\nobject-centered visual information remains a central challenge in computational\nneuroscience. Prior works have investigated representational alignment between\nartificial neural networks and the visual cortex. Nevertheless, these findings\nare indirect and offer limited insights to the structure of neural populations\nthemselves. Similarly, decoding-based methods have quantified semantic features\nfrom neural populations but have not uncovered their underlying organizations.\nThis leaves open a scientific question: \"how feature-specific visual\ninformation is distributed across neural populations in higher visual areas,\nand whether it is organized into structured, semantically meaningful\nsubspaces.\" To tackle this problem, we present MIG-Vis, a method that leverages\nthe generative power of diffusion models to visualize and validate the\nvisual-semantic attributes encoded in neural latent subspaces. Our method first\nuses a variational autoencoder to infer a group-wise disentangled neural latent\nsubspace from neural populations. Subsequently, we propose a mutual information\n(MI)-guided diffusion synthesis procedure to visualize the specific\nvisual-semantic features encoded by each latent group. We validate MIG-Vis on\nmulti-session neural spiking datasets from the inferior temporal (IT) cortex of\ntwo macaques. The synthesized results demonstrate that our method identifies\nneural latent groups with clear semantic selectivity to diverse visual\nfeatures, including object pose, inter-category transformations, and\nintra-class content. These findings provide direct, interpretable evidence of\nstructured semantic representation in the higher visual cortex and advance our\nunderstanding of its encoding principles."
                },
                "authors": [
                    {
                        "name": "Yule Wang"
                    },
                    {
                        "name": "Joseph Yu"
                    },
                    {
                        "name": "Chengrui Li"
                    },
                    {
                        "name": "Weihan Li"
                    },
                    {
                        "name": "Anqi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Anqi Wu"
                },
                "author": "Anqi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02178v1",
                "updated": "2025-10-02T16:30:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    30,
                    37,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:30:37Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    30,
                    37,
                    3,
                    275,
                    0
                ],
                "title": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical\n  Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical\n  Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis"
                },
                "summary": "3D indoor layout synthesis is crucial for creating virtual environments.\nTraditional methods struggle with generalization due to fixed datasets. While\nrecent LLM and VLM-based approaches offer improved semantic richness, they\noften lack robust and flexible refinement, resulting in suboptimal layouts. We\ndevelop DisCo-Layout, a novel framework that disentangles and coordinates\nphysical and semantic refinement. For independent refinement, our Semantic\nRefinement Tool (SRT) corrects abstract object relationships, while the\nPhysical Refinement Tool (PRT) resolves concrete spatial issues via a\ngrid-matching algorithm. For collaborative refinement, a multi-agent framework\nintelligently orchestrates these tools, featuring a planner for placement\nrules, a designer for initial layouts, and an evaluator for assessment.\nExperiments demonstrate DisCo-Layout's state-of-the-art performance, generating\nrealistic, coherent, and generalizable 3D indoor layouts. Our code will be\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D indoor layout synthesis is crucial for creating virtual environments.\nTraditional methods struggle with generalization due to fixed datasets. While\nrecent LLM and VLM-based approaches offer improved semantic richness, they\noften lack robust and flexible refinement, resulting in suboptimal layouts. We\ndevelop DisCo-Layout, a novel framework that disentangles and coordinates\nphysical and semantic refinement. For independent refinement, our Semantic\nRefinement Tool (SRT) corrects abstract object relationships, while the\nPhysical Refinement Tool (PRT) resolves concrete spatial issues via a\ngrid-matching algorithm. For collaborative refinement, a multi-agent framework\nintelligently orchestrates these tools, featuring a planner for placement\nrules, a designer for initial layouts, and an evaluator for assessment.\nExperiments demonstrate DisCo-Layout's state-of-the-art performance, generating\nrealistic, coherent, and generalizable 3D indoor layouts. Our code will be\npublicly available."
                },
                "authors": [
                    {
                        "name": "Jialin Gao"
                    },
                    {
                        "name": "Donghao Zhou"
                    },
                    {
                        "name": "Mingjian Liang"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Chi-Wing Fu"
                    },
                    {
                        "name": "Xiaowei Hu"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng-Ann Heng"
                },
                "author": "Pheng-Ann Heng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02173v1",
                "updated": "2025-10-02T16:24:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    24,
                    28,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:24:28Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    24,
                    28,
                    3,
                    275,
                    0
                ],
                "title": "Learning to Reason for Hallucination Span Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason for Hallucination Span Detection"
                },
                "summary": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans."
                },
                "authors": [
                    {
                        "name": "Hsuan Su"
                    },
                    {
                        "name": "Ting-Yao Hu"
                    },
                    {
                        "name": "Hema Swetha Koppula"
                    },
                    {
                        "name": "Kundan Krishna"
                    },
                    {
                        "name": "Hadi Pouransari"
                    },
                    {
                        "name": "Cheng-Yu Hsieh"
                    },
                    {
                        "name": "Cem Koc"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Oncel Tuzel"
                    },
                    {
                        "name": "Raviteja Vemulapalli"
                    }
                ],
                "author_detail": {
                    "name": "Raviteja Vemulapalli"
                },
                "author": "Raviteja Vemulapalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10862v2",
                "updated": "2025-10-02T16:15:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    15,
                    20,
                    3,
                    275,
                    0
                ],
                "published": "2024-10-07T19:53:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    19,
                    53,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "Superficial Safety Alignment Hypothesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superficial Safety Alignment Hypothesis"
                },
                "summary": "As large language models (LLMs) are overwhelmingly more and more integrated\ninto various applications, ensuring they generate safe responses is a pressing\nneed. Previous studies on alignment have largely focused on general\ninstruction-following but have often overlooked the distinct properties of\nsafety alignment, such as the brittleness of safety mechanisms. To bridge the\ngap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which\nposits that safety alignment teaches an otherwise unsafe model to choose the\ncorrect reasoning direction - fulfill or refuse users' requests - interpreted\nas an implicit binary classification task. Through SSAH, we hypothesize that\nonly a few essential components can establish safety guardrails in LLMs. We\nsuccessfully identify four types of attribute-critical components: Safety\nCritical Unit (SCU), Utility Critical Unit (UCU), Complex Unit (CU), and\nRedundant Unit (RU). Our findings show that freezing certain safety-critical\ncomponents during fine-tuning allows the model to retain its safety attributes\nwhile adapting to new tasks. Similarly, we show that leveraging redundant units\nin the pre-trained model as an \"alignment budget\" can effectively minimize the\nalignment tax while achieving the alignment goal. All considered, this paper\nconcludes that the atomic functional unit for safety in LLMs is at the neuron\nlevel and underscores that safety alignment should not be complicated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are overwhelmingly more and more integrated\ninto various applications, ensuring they generate safe responses is a pressing\nneed. Previous studies on alignment have largely focused on general\ninstruction-following but have often overlooked the distinct properties of\nsafety alignment, such as the brittleness of safety mechanisms. To bridge the\ngap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which\nposits that safety alignment teaches an otherwise unsafe model to choose the\ncorrect reasoning direction - fulfill or refuse users' requests - interpreted\nas an implicit binary classification task. Through SSAH, we hypothesize that\nonly a few essential components can establish safety guardrails in LLMs. We\nsuccessfully identify four types of attribute-critical components: Safety\nCritical Unit (SCU), Utility Critical Unit (UCU), Complex Unit (CU), and\nRedundant Unit (RU). Our findings show that freezing certain safety-critical\ncomponents during fine-tuning allows the model to retain its safety attributes\nwhile adapting to new tasks. Similarly, we show that leveraging redundant units\nin the pre-trained model as an \"alignment budget\" can effectively minimize the\nalignment tax while achieving the alignment goal. All considered, this paper\nconcludes that the atomic functional unit for safety in LLMs is at the neuron\nlevel and underscores that safety alignment should not be complicated."
                },
                "authors": [
                    {
                        "name": "Jianwei Li"
                    },
                    {
                        "name": "Jung-Eun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jung-Eun Kim"
                },
                "author": "Jung-Eun Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00750v2",
                "updated": "2025-10-02T16:10:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    10,
                    36,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-31T23:32:01Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    32,
                    1,
                    5,
                    151,
                    0
                ],
                "title": "CodeSense: a Real-World Benchmark and Dataset for Code Semantic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSense: a Real-World Benchmark and Dataset for Code Semantic\n  Reasoning"
                },
                "summary": "Understanding and reasoning about code semantics is essential for enhancing\ncode LLMs' abilities to solve real-world software engineering (SE) tasks.\nAlthough several code reasoning benchmarks exist, most rely on synthetic\ndatasets or educational coding problems and focus on coarse-grained reasoning\ntasks such as input/output prediction, limiting their effectiveness in\nevaluating LLMs in practical SE contexts. To bridge this gap, we propose\nCodeSense, the first benchmark that makes available a spectrum of fine-grained\ncode reasoning tasks concerned with the software engineering of real-world\ncode. We collected Python, C and Java software projects from real-world\nrepositories. We executed tests from these repositories, collected their\nexecution traces, and constructed a ground truth dataset for fine-grained\nsemantic reasoning tasks. We then performed comprehensive evaluations on\nstate-of-the-art LLMs. Our results show a clear performance gap for the models\nto handle fine-grained reasoning tasks. Although prompting techniques such as\nchain-of-thought and in-context learning helped, the lack of code semantics in\nLLMs fundamentally limit models' capabilities of code reasoning. Besides\ndataset, benchmark and evaluation, our work produced an execution tracing\nframework and tool set that make it easy to collect ground truth for\nfine-grained SE reasoning tasks, offering a strong basis for future benchmark\nconstruction and model post training. Our code and data are located at\nhttps://codesense-bench.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and reasoning about code semantics is essential for enhancing\ncode LLMs' abilities to solve real-world software engineering (SE) tasks.\nAlthough several code reasoning benchmarks exist, most rely on synthetic\ndatasets or educational coding problems and focus on coarse-grained reasoning\ntasks such as input/output prediction, limiting their effectiveness in\nevaluating LLMs in practical SE contexts. To bridge this gap, we propose\nCodeSense, the first benchmark that makes available a spectrum of fine-grained\ncode reasoning tasks concerned with the software engineering of real-world\ncode. We collected Python, C and Java software projects from real-world\nrepositories. We executed tests from these repositories, collected their\nexecution traces, and constructed a ground truth dataset for fine-grained\nsemantic reasoning tasks. We then performed comprehensive evaluations on\nstate-of-the-art LLMs. Our results show a clear performance gap for the models\nto handle fine-grained reasoning tasks. Although prompting techniques such as\nchain-of-thought and in-context learning helped, the lack of code semantics in\nLLMs fundamentally limit models' capabilities of code reasoning. Besides\ndataset, benchmark and evaluation, our work produced an execution tracing\nframework and tool set that make it easy to collect ground truth for\nfine-grained SE reasoning tasks, offering a strong basis for future benchmark\nconstruction and model post training. Our code and data are located at\nhttps://codesense-bench.github.io/."
                },
                "authors": [
                    {
                        "name": "Monoshi Kumar Roy"
                    },
                    {
                        "name": "Simin Chen"
                    },
                    {
                        "name": "Benjamin Steenhoek"
                    },
                    {
                        "name": "Jinjun Peng"
                    },
                    {
                        "name": "Gail Kaiser"
                    },
                    {
                        "name": "Baishakhi Ray"
                    },
                    {
                        "name": "Wei Le"
                    }
                ],
                "author_detail": {
                    "name": "Wei Le"
                },
                "author": "Wei Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02157v1",
                "updated": "2025-10-02T16:08:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    8,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:08:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    8,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "Agentic Reasoning and Refinement through Semantic Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Reasoning and Refinement through Semantic Interaction"
                },
                "summary": "Sensemaking report writing often requires multiple refinements in the\niterative process. While Large Language Models (LLMs) have shown promise in\ngenerating initial reports based on human visual workspace representations,\nthey struggle to precisely incorporate sequential semantic interactions during\nthe refinement process. We introduce VIS-ReAct, a framework that reasons about\nnewly-added semantic interactions in visual workspaces to steer the LLM for\nreport refinement.\n  VIS-ReAct is a two-agent framework: a primary LLM analysis agent interprets\nnew semantic interactions to infer user intentions and generate refinement\nplanning, followed by an LLM refinement agent that updates reports accordingly.\nThrough case study, VIS-ReAct outperforms baseline and VIS-ReAct (without LLM\nanalysis) on targeted refinement, semantic fidelity, and transparent inference.\nResults demonstrate that VIS-ReAct better handles various interaction types and\ngranularities while enhancing the transparency of human-LLM collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensemaking report writing often requires multiple refinements in the\niterative process. While Large Language Models (LLMs) have shown promise in\ngenerating initial reports based on human visual workspace representations,\nthey struggle to precisely incorporate sequential semantic interactions during\nthe refinement process. We introduce VIS-ReAct, a framework that reasons about\nnewly-added semantic interactions in visual workspaces to steer the LLM for\nreport refinement.\n  VIS-ReAct is a two-agent framework: a primary LLM analysis agent interprets\nnew semantic interactions to infer user intentions and generate refinement\nplanning, followed by an LLM refinement agent that updates reports accordingly.\nThrough case study, VIS-ReAct outperforms baseline and VIS-ReAct (without LLM\nanalysis) on targeted refinement, semantic fidelity, and transparent inference.\nResults demonstrate that VIS-ReAct better handles various interaction types and\ngranularities while enhancing the transparency of human-LLM collaboration."
                },
                "authors": [
                    {
                        "name": "Xuxin Tang"
                    },
                    {
                        "name": "Rehema Abulikemu"
                    },
                    {
                        "name": "Eric Krokos"
                    },
                    {
                        "name": "Kirsten Whitley"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Chris North"
                    }
                ],
                "author_detail": {
                    "name": "Chris North"
                },
                "author": "Chris North",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22863v2",
                "updated": "2025-10-02T16:05:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    5,
                    48,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-30T17:38:49Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    17,
                    38,
                    49,
                    2,
                    211,
                    0
                ],
                "title": "Formation of over-massive black holes in high-redshift disk galaxies via\n  globular cluster accretion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formation of over-massive black holes in high-redshift disk galaxies via\n  globular cluster accretion"
                },
                "summary": "Recent observations with the James Webb Space Telescope (JWST) have suggested\nthe existence of over-massive black holes (OMBHs) in high-redshift galaxies. In\nthis paper, we propose a new mechanism for the formation of OMBHs, based on the\naccretion of globular clusters (GCs) in compact disk galaxies. We derive the\nconditions under which OMBHs can form, focusing on key parameters such as halo\nmass, redshift, and halo spin parameter. Our results show that at redshift $z =\n10$, a halo with mass $10^{11}~M_{\\odot}$ and a spin parameter of $\\sim 0.02$\ncan form a black hole of $2.3 \\times 10^{8}~M_{\\odot}$ through GC migration and\naccretion via tidal disruption events (TDEs). The resulting black\nhole-to-stellar mass ratio can reach $\\sim 0.1$, corresponding to the fraction\nof GC mass accreted onto the black hole. This mechanism thus provides a\nplausible explanation for the OMBHs observed by JWST. Furthermore, by combining\nour model with the halo mass function and the spin-parameter distribution, we\nconstruct black hole mass functions that reproduce the number densities of the\nmassive BH candidates UHZ1 and GHZ9 at $z \\approx 10$, as well as the\nabundances of BHs with masses $\\gtrsim 10^{8}~\\rm{M_\\odot}$ at $z \\approx 5$\ninferred from JWST observations. However, our model overpredicts the abundance\nof BHs with masses $ < 10^{8}~\\rm{M_\\odot}$, suggesting that moderately\nmassive, inactive BHs are more frequent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent observations with the James Webb Space Telescope (JWST) have suggested\nthe existence of over-massive black holes (OMBHs) in high-redshift galaxies. In\nthis paper, we propose a new mechanism for the formation of OMBHs, based on the\naccretion of globular clusters (GCs) in compact disk galaxies. We derive the\nconditions under which OMBHs can form, focusing on key parameters such as halo\nmass, redshift, and halo spin parameter. Our results show that at redshift $z =\n10$, a halo with mass $10^{11}~M_{\\odot}$ and a spin parameter of $\\sim 0.02$\ncan form a black hole of $2.3 \\times 10^{8}~M_{\\odot}$ through GC migration and\naccretion via tidal disruption events (TDEs). The resulting black\nhole-to-stellar mass ratio can reach $\\sim 0.1$, corresponding to the fraction\nof GC mass accreted onto the black hole. This mechanism thus provides a\nplausible explanation for the OMBHs observed by JWST. Furthermore, by combining\nour model with the halo mass function and the spin-parameter distribution, we\nconstruct black hole mass functions that reproduce the number densities of the\nmassive BH candidates UHZ1 and GHZ9 at $z \\approx 10$, as well as the\nabundances of BHs with masses $\\gtrsim 10^{8}~\\rm{M_\\odot}$ at $z \\approx 5$\ninferred from JWST observations. However, our model overpredicts the abundance\nof BHs with masses $ < 10^{8}~\\rm{M_\\odot}$, suggesting that moderately\nmassive, inactive BHs are more frequent."
                },
                "authors": [
                    {
                        "name": "Hidenobu Yajima"
                    }
                ],
                "author_detail": {
                    "name": "Hidenobu Yajima"
                },
                "author": "Hidenobu Yajima",
                "arxiv_comment": "16 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02152v1",
                "updated": "2025-10-02T16:04:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    4,
                    28,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:04:28Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    4,
                    28,
                    3,
                    275,
                    0
                ],
                "title": "Multivariate distributional modeling of low, moderate, and large\n  intensities without threshold selection steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate distributional modeling of low, moderate, and large\n  intensities without threshold selection steps"
                },
                "summary": "In fields such as hydrology and climatology, modelling the entire\ndistribution of positive data is essential, as stakeholders require insights\ninto the full range of values, from low to extreme. Traditional approaches\noften segment the distribution into separate regions, which introduces\nsubjectivity and limits coherence. This is especially true when dealing with\nmultivariate data.\n  In line with multivariate extreme value theory, this paper presents a\nunified, threshold-free framework for modelling marginal behaviours and\ndependence structures based on an extended generalized Pareto distribution\n(EGPD). We propose decomposing multivariate data into radial and angular\ncomponents. The radial component is modelled using a semi-parametric EGPD and\nthe angular distribution is permitted to vary conditionally. This approach\nallows for sufficiently flexible dependence modelling.\n  The hierarchical structure of the model facilitates the inference process.\nFirst, we combine classical maximum likelihood estimation (MLE) methods with\nsemi-parametric approaches based on Bernstein polynomials to estimate the\ndistribution of the radial component. Then, we use multivariate regression\ntechniques to estimate the angular component's parameters.\n  The model is evaluated through synthetic simulations and applied to\nhydrological datasets to exemplify its capacity to capture heavy-tailed\nmarginals and complex multivariate dependencies without threshold\nspecification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In fields such as hydrology and climatology, modelling the entire\ndistribution of positive data is essential, as stakeholders require insights\ninto the full range of values, from low to extreme. Traditional approaches\noften segment the distribution into separate regions, which introduces\nsubjectivity and limits coherence. This is especially true when dealing with\nmultivariate data.\n  In line with multivariate extreme value theory, this paper presents a\nunified, threshold-free framework for modelling marginal behaviours and\ndependence structures based on an extended generalized Pareto distribution\n(EGPD). We propose decomposing multivariate data into radial and angular\ncomponents. The radial component is modelled using a semi-parametric EGPD and\nthe angular distribution is permitted to vary conditionally. This approach\nallows for sufficiently flexible dependence modelling.\n  The hierarchical structure of the model facilitates the inference process.\nFirst, we combine classical maximum likelihood estimation (MLE) methods with\nsemi-parametric approaches based on Bernstein polynomials to estimate the\ndistribution of the radial component. Then, we use multivariate regression\ntechniques to estimate the angular component's parameters.\n  The model is evaluated through synthetic simulations and applied to\nhydrological datasets to exemplify its capacity to capture heavy-tailed\nmarginals and complex multivariate dependencies without threshold\nspecification."
                },
                "authors": [
                    {
                        "name": "Carlo Gaetan"
                    },
                    {
                        "name": "Philippe Naveau"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Naveau"
                },
                "author": "Philippe Naveau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15153v2",
                "updated": "2025-10-02T15:55:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    55,
                    21,
                    3,
                    275,
                    0
                ],
                "published": "2025-02-21T02:24:43Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    2,
                    24,
                    43,
                    4,
                    52,
                    0
                ],
                "title": "When Disagreements Elicit Robustness: Investigating Self-Repair\n  Capabilities under LLM Multi-Agent Disagreements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Disagreements Elicit Robustness: Investigating Self-Repair\n  Capabilities under LLM Multi-Agent Disagreements"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have upgraded them from\nsophisticated text generators to autonomous agents capable of cooperation and\ntool use in multi-agent systems (MAS). However, it remains unclear how\ndisagreements shape collective decision-making. In this paper, we revisit the\nrole of disagreement and argue that general, partially overlapping\ndisagreements prevent premature consensus and expand the explored solution\nspace, while disagreements on task-critical steps can derail collaboration\ndepending on the topology of solution paths. We investigate two collaborative\nsettings with distinct path structures: collaborative reasoning (CounterFact,\nMQuAKE-cf), which typically follows a single evidential chain, whereas\ncollaborative programming (HumanEval, GAIA) often adopts multiple valid\nimplementations. Disagreements are instantiated as general heterogeneity among\nagents and as task-critical counterfactual knowledge edits injected into\ncontext or parameters. Experiments reveal that general disagreements\nconsistently improve success by encouraging complementary exploration. By\ncontrast, task-critical disagreements substantially reduce success on\nsingle-path reasoning, yet have a limited impact on programming, where agents\ncan choose alternative solutions. Trace analyses show that MAS frequently\nbypasses the edited facts in programming but rarely does so in reasoning,\nrevealing an emergent self-repair capability that depends on solution-path\nrather than scale alone. Our code is available at\nhttps://github.com/wbw625/MultiAgentRobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have upgraded them from\nsophisticated text generators to autonomous agents capable of cooperation and\ntool use in multi-agent systems (MAS). However, it remains unclear how\ndisagreements shape collective decision-making. In this paper, we revisit the\nrole of disagreement and argue that general, partially overlapping\ndisagreements prevent premature consensus and expand the explored solution\nspace, while disagreements on task-critical steps can derail collaboration\ndepending on the topology of solution paths. We investigate two collaborative\nsettings with distinct path structures: collaborative reasoning (CounterFact,\nMQuAKE-cf), which typically follows a single evidential chain, whereas\ncollaborative programming (HumanEval, GAIA) often adopts multiple valid\nimplementations. Disagreements are instantiated as general heterogeneity among\nagents and as task-critical counterfactual knowledge edits injected into\ncontext or parameters. Experiments reveal that general disagreements\nconsistently improve success by encouraging complementary exploration. By\ncontrast, task-critical disagreements substantially reduce success on\nsingle-path reasoning, yet have a limited impact on programming, where agents\ncan choose alternative solutions. Trace analyses show that MAS frequently\nbypasses the edited facts in programming but rarely does so in reasoning,\nrevealing an emergent self-repair capability that depends on solution-path\nrather than scale alone. Our code is available at\nhttps://github.com/wbw625/MultiAgentRobustness."
                },
                "authors": [
                    {
                        "name": "Tianjie Ju"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Mong-Li Lee"
                    },
                    {
                        "name": "Wynne Hsu"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Qianren Wang"
                    },
                    {
                        "name": "Pengzhou Cheng"
                    },
                    {
                        "name": "Zongru Wu"
                    },
                    {
                        "name": "Haodong Zhao"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Gongshen Liu"
                },
                "author": "Gongshen Liu",
                "arxiv_comment": "Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14746v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14746v3",
                "updated": "2025-10-02T15:55:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    55,
                    20,
                    3,
                    275,
                    0
                ],
                "published": "2025-08-20T14:43:04Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    43,
                    4,
                    2,
                    232,
                    0
                ],
                "title": "MissionHD: Hyperdimensional Refinement of Distribution-Deficient\n  Reasoning Graphs for Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MissionHD: Hyperdimensional Refinement of Distribution-Deficient\n  Reasoning Graphs for Video Anomaly Detection"
                },
                "summary": "LLM-generated reasoning graphs, referred to as mission-specific graphs\n(MSGs), are increasingly used for video anomaly detection (VAD) and recognition\n(VAR). These MSGs are novel artifacts: they often exhibit skewed connectivity\nand lack large-scale datasets for pre-training, which makes existing graph\nstructure refinement (GSR) methods ineffective. To address this challenge, we\npropose HDC-constrained Graph Structure Refinement (HDC-GSR), a paradigm that\nleverages hyperdimensional computing (HDC) to optimize decodable graph\nrepresentations without relying on structural-distribution learning. Building\non this paradigm, we introduce MissionHD, an HDC framework that encodes graphs\nwith constrained graph-neural operations, aligns them directly with downstream\ntask loss, and decodes refined structures. Experiments on VAD/VAR benchmarks\ndemonstrate that MissionHD-refined graphs consistently improve performance,\nestablishing HDC-GSR as an effective pre-processing step for structured\nreasoning in video anomaly tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-generated reasoning graphs, referred to as mission-specific graphs\n(MSGs), are increasingly used for video anomaly detection (VAD) and recognition\n(VAR). These MSGs are novel artifacts: they often exhibit skewed connectivity\nand lack large-scale datasets for pre-training, which makes existing graph\nstructure refinement (GSR) methods ineffective. To address this challenge, we\npropose HDC-constrained Graph Structure Refinement (HDC-GSR), a paradigm that\nleverages hyperdimensional computing (HDC) to optimize decodable graph\nrepresentations without relying on structural-distribution learning. Building\non this paradigm, we introduce MissionHD, an HDC framework that encodes graphs\nwith constrained graph-neural operations, aligns them directly with downstream\ntask loss, and decodes refined structures. Experiments on VAD/VAR benchmarks\ndemonstrate that MissionHD-refined graphs consistently improve performance,\nestablishing HDC-GSR as an effective pre-processing step for structured\nreasoning in video anomaly tasks."
                },
                "authors": [
                    {
                        "name": "Sanggeon Yun"
                    },
                    {
                        "name": "Raheeb Hassan"
                    },
                    {
                        "name": "Ryozo Masukawa"
                    },
                    {
                        "name": "Nathaniel D. Bastian"
                    },
                    {
                        "name": "Mohsen Imani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Imani"
                },
                "author": "Mohsen Imani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14746v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14746v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02128v1",
                "updated": "2025-10-02T15:38:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    38,
                    57,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T15:38:57Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    38,
                    57,
                    3,
                    275,
                    0
                ],
                "title": "The Disparate Impacts of Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Disparate Impacts of Speculative Decoding"
                },
                "summary": "The practice of speculative decoding, whereby inference is probabilistically\nsupported by a smaller, cheaper, ``drafter'' model, has become a standard\ntechnique for systematically reducing the decoding time of large language\nmodels. This paper conducts an analysis of speculative decoding through the\nlens of its potential disparate speed-up rates across tasks. Crucially, the\npaper shows that speed-up gained from speculative decoding is not uniformly\ndistributed across tasks, consistently diminishing for under-fit, and often\nunderrepresented tasks. To better understand this phenomenon, we derive an\nanalysis to quantify this observed ``unfairness'' and draw attention to the\nfactors that motivate such disparate speed-ups to emerge. Further, guided by\nthese insights, the paper proposes a mitigation strategy designed to reduce\nspeed-up disparities and validates the approach across several model pairs,\nrevealing on average a 12% improvement in our fairness metric.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The practice of speculative decoding, whereby inference is probabilistically\nsupported by a smaller, cheaper, ``drafter'' model, has become a standard\ntechnique for systematically reducing the decoding time of large language\nmodels. This paper conducts an analysis of speculative decoding through the\nlens of its potential disparate speed-up rates across tasks. Crucially, the\npaper shows that speed-up gained from speculative decoding is not uniformly\ndistributed across tasks, consistently diminishing for under-fit, and often\nunderrepresented tasks. To better understand this phenomenon, we derive an\nanalysis to quantify this observed ``unfairness'' and draw attention to the\nfactors that motivate such disparate speed-ups to emerge. Further, guided by\nthese insights, the paper proposes a mitigation strategy designed to reduce\nspeed-up disparities and validates the approach across several model pairs,\nrevealing on average a 12% improvement in our fairness metric."
                },
                "authors": [
                    {
                        "name": "Jameson Sandler"
                    },
                    {
                        "name": "Ahmet Üstün"
                    },
                    {
                        "name": "Marco Romanelli"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Ferdinando Fioretto"
                    }
                ],
                "author_detail": {
                    "name": "Ferdinando Fioretto"
                },
                "author": "Ferdinando Fioretto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02123v1",
                "updated": "2025-10-02T15:34:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    34,
                    41,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T15:34:41Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    34,
                    41,
                    3,
                    275,
                    0
                ],
                "title": "Identifying Subgroup and Context Effects in Conjoint Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Subgroup and Context Effects in Conjoint Experiments"
                },
                "summary": "Conjoint experiments have become central to survey research in political\nscience and related fields because they allow researchers to study preferences\nacross multiple attributes simultaneously. Beyond estimating main effects,\nscholars increasingly analyze heterogeneity through subgroup analysis and\ncontextual variables, raising methodological challenges in detecting and\ninterpreting interaction effects. Statistical power constraints, common in\nsurvey experiments, further complicate this task. This paper addresses the\nquestion: how can both main and interaction effects be reliably inferred in\nconjoint studies? We contribute in two ways. First, we conduct a systematic\nevaluation of leading approaches, including post-hoc corrections, sparse\nregression methods, and Bayesian models, across simulation regimes that vary\nsparsity, noise, and data availability. Second, we propose a novel black-box\ninference framework that leverages machine learning to recover main and\ninteraction effects in conjoint experiments. Our approach balances\ncomputational efficiency with accuracy, providing a practical tool for\nresearchers studying heterogeneous effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conjoint experiments have become central to survey research in political\nscience and related fields because they allow researchers to study preferences\nacross multiple attributes simultaneously. Beyond estimating main effects,\nscholars increasingly analyze heterogeneity through subgroup analysis and\ncontextual variables, raising methodological challenges in detecting and\ninterpreting interaction effects. Statistical power constraints, common in\nsurvey experiments, further complicate this task. This paper addresses the\nquestion: how can both main and interaction effects be reliably inferred in\nconjoint studies? We contribute in two ways. First, we conduct a systematic\nevaluation of leading approaches, including post-hoc corrections, sparse\nregression methods, and Bayesian models, across simulation regimes that vary\nsparsity, noise, and data availability. Second, we propose a novel black-box\ninference framework that leverages machine learning to recover main and\ninteraction effects in conjoint experiments. Our approach balances\ncomputational efficiency with accuracy, providing a practical tool for\nresearchers studying heterogeneous effects."
                },
                "authors": [
                    {
                        "name": "Steven Wang"
                    },
                    {
                        "name": "Isys Johnson"
                    },
                    {
                        "name": "Jessica Grogan"
                    },
                    {
                        "name": "Lalit Jain"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Kyle Hunt"
                    },
                    {
                        "name": "Kenneth Joseph"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Joseph"
                },
                "author": "Kenneth Joseph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00106v2",
                "updated": "2025-10-02T15:29:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    29,
                    23,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-30T18:00:02Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    18,
                    0,
                    2,
                    1,
                    273,
                    0
                ],
                "title": "Eccentric binary black holes: A new framework for numerical relativity\n  waveform surrogates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eccentric binary black holes: A new framework for numerical relativity\n  waveform surrogates"
                },
                "summary": "Mounting evidence indicates that some of the gravitational wave signals\nobserved by the LIGO/Virgo/KAGRA observatories might arise from eccentric\ncompact object binaries, increasing the urgency for accurate waveform models\nfor such systems. While for non-eccentric binaries, surrogate models are\nefficient and accurate, the additional features due to eccentricity have posed\na challenge. In this letter, we present a novel method for decomposing\neccentric numerical relativity waveforms which makes them amenable to surrogate\nmodelling techniques. We parameterize the inspiral in the radial phase domain,\nfactoring out eccentricity-induced dephasing and thus enhancing compressibility\nand accuracy. This is combined with a second surrogate for the merger-ringdown\nin the time-domain and a novel technique to take advantage of the approximate\nperiodicity with radial oscillations during the inspiral. We apply this\nprocedure to the $(2,2)$ mode for non-spinning black hole binaries, and\ndemonstrate that the resulting surrogate, NRSurE_q4NoSpin_22, is able to\nfaithfully reproduce the underlying numerical relativity waveforms, with\nmaximum mismatches of $5\\times10^{-4}$ and median mismatches of\n$2\\times10^{-5}$. This technique paves the way for high-accuracy parameter\nestimation with eccentric models, a key ingredient for astrophysical inference\nand tests of general relativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mounting evidence indicates that some of the gravitational wave signals\nobserved by the LIGO/Virgo/KAGRA observatories might arise from eccentric\ncompact object binaries, increasing the urgency for accurate waveform models\nfor such systems. While for non-eccentric binaries, surrogate models are\nefficient and accurate, the additional features due to eccentricity have posed\na challenge. In this letter, we present a novel method for decomposing\neccentric numerical relativity waveforms which makes them amenable to surrogate\nmodelling techniques. We parameterize the inspiral in the radial phase domain,\nfactoring out eccentricity-induced dephasing and thus enhancing compressibility\nand accuracy. This is combined with a second surrogate for the merger-ringdown\nin the time-domain and a novel technique to take advantage of the approximate\nperiodicity with radial oscillations during the inspiral. We apply this\nprocedure to the $(2,2)$ mode for non-spinning black hole binaries, and\ndemonstrate that the resulting surrogate, NRSurE_q4NoSpin_22, is able to\nfaithfully reproduce the underlying numerical relativity waveforms, with\nmaximum mismatches of $5\\times10^{-4}$ and median mismatches of\n$2\\times10^{-5}$. This technique paves the way for high-accuracy parameter\nestimation with eccentric models, a key ingredient for astrophysical inference\nand tests of general relativity."
                },
                "authors": [
                    {
                        "name": "Peter James Nee"
                    },
                    {
                        "name": "Adhrit Ravichandran"
                    },
                    {
                        "name": "Scott E. Field"
                    },
                    {
                        "name": "Tousif Islam"
                    },
                    {
                        "name": "Harald P. Pfeiffer"
                    },
                    {
                        "name": "Vijay Varma"
                    },
                    {
                        "name": "Michael Boyle"
                    },
                    {
                        "name": "Andrea Ceja"
                    },
                    {
                        "name": "Noora Ghadiri"
                    },
                    {
                        "name": "Lawrence E. Kidder"
                    },
                    {
                        "name": "Prayush Kumar"
                    },
                    {
                        "name": "Akash Maurya"
                    },
                    {
                        "name": "Marlo Morales"
                    },
                    {
                        "name": "Antoni Ramos-Buades"
                    },
                    {
                        "name": "Abhishek Ravishankar"
                    },
                    {
                        "name": "Katie Rink"
                    },
                    {
                        "name": "Hannes R. Rüter"
                    },
                    {
                        "name": "Mark A. Scheel"
                    },
                    {
                        "name": "Md Arif Shaikh"
                    },
                    {
                        "name": "Daniel Tellez"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Tellez"
                },
                "author": "Daniel Tellez",
                "arxiv_comment": "11 pages, 5 figures; updated references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18379v2",
                "updated": "2025-10-02T15:20:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    20,
                    2,
                    3,
                    275,
                    0
                ],
                "published": "2025-08-25T18:13:50Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    18,
                    13,
                    50,
                    0,
                    237,
                    0
                ],
                "title": "REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking"
                },
                "summary": "Large Language Models (LLMs) have shown strong capabilities in document\nre-ranking, a key component in modern Information Retrieval (IR) systems.\nHowever, existing LLM-based approaches face notable limitations, including\nranking uncertainty, unstable top-k recovery, and high token cost due to\ntoken-intensive prompting. To effectively address these limitations, we propose\nREALM, an uncertainty-aware re-ranking framework that models LLM-derived\nrelevance as Gaussian distributions and refines them through recursive Bayesian\nupdates. By explicitly capturing uncertainty and minimizing redundant queries,\nREALM achieves better rankings more efficiently. Experimental results\ndemonstrate that our REALM surpasses state-of-the-art re-rankers while\nsignificantly reducing token usage and latency, improving NDCG@10 by 0.7-11.9\nand simultaneously reducing the number of LLM inferences by 23.4-84.4%,\npromoting it as the next-generation re-ranker for modern IR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong capabilities in document\nre-ranking, a key component in modern Information Retrieval (IR) systems.\nHowever, existing LLM-based approaches face notable limitations, including\nranking uncertainty, unstable top-k recovery, and high token cost due to\ntoken-intensive prompting. To effectively address these limitations, we propose\nREALM, an uncertainty-aware re-ranking framework that models LLM-derived\nrelevance as Gaussian distributions and refines them through recursive Bayesian\nupdates. By explicitly capturing uncertainty and minimizing redundant queries,\nREALM achieves better rankings more efficiently. Experimental results\ndemonstrate that our REALM surpasses state-of-the-art re-rankers while\nsignificantly reducing token usage and latency, improving NDCG@10 by 0.7-11.9\nand simultaneously reducing the number of LLM inferences by 23.4-84.4%,\npromoting it as the next-generation re-ranker for modern IR systems."
                },
                "authors": [
                    {
                        "name": "Pinhuan Wang"
                    },
                    {
                        "name": "Zhiqiu Xia"
                    },
                    {
                        "name": "Chunhua Liao"
                    },
                    {
                        "name": "Feiyi Wang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_comment": "EMNLP 2025 (Main Conference, Oral). 15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09404v2",
                "updated": "2025-10-02T15:18:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    18,
                    2,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-12T21:16:08Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    21,
                    16,
                    8,
                    5,
                    193,
                    0
                ],
                "title": "Scaling Laws for Optimal Data Mixtures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Optimal Data Mixtures"
                },
                "summary": "Large foundation models are typically trained on data from multiple domains,\nwith the data mixture--the proportion of each domain used--playing a critical\nrole in model performance. The standard approach to selecting this mixture\nrelies on trial and error, which becomes impractical for large-scale\npretraining. We propose a systematic method to determine the optimal data\nmixture for any target domain using scaling laws. Our approach accurately\npredicts the loss of a model of size $N$ trained with $D$ tokens and a specific\ndomain weight vector $h$. We validate the universality of these scaling laws by\ndemonstrating their predictive power in three distinct and large-scale\nsettings: large language model (LLM), native multimodal model (NMM), and large\nvision models (LVM) pretraining. We further show that these scaling laws can\nextrapolate to new data mixtures and across scales: their parameters can be\naccurately estimated using a few small-scale training runs, and used to\nestimate the performance at larger scales and unseen domain weights. The\nscaling laws allow to derive the optimal domain weights for any target domain\nunder a given training budget ($N$,$D$), providing a principled alternative to\ncostly trial-and-error methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large foundation models are typically trained on data from multiple domains,\nwith the data mixture--the proportion of each domain used--playing a critical\nrole in model performance. The standard approach to selecting this mixture\nrelies on trial and error, which becomes impractical for large-scale\npretraining. We propose a systematic method to determine the optimal data\nmixture for any target domain using scaling laws. Our approach accurately\npredicts the loss of a model of size $N$ trained with $D$ tokens and a specific\ndomain weight vector $h$. We validate the universality of these scaling laws by\ndemonstrating their predictive power in three distinct and large-scale\nsettings: large language model (LLM), native multimodal model (NMM), and large\nvision models (LVM) pretraining. We further show that these scaling laws can\nextrapolate to new data mixtures and across scales: their parameters can be\naccurately estimated using a few small-scale training runs, and used to\nestimate the performance at larger scales and unseen domain weights. The\nscaling laws allow to derive the optimal domain weights for any target domain\nunder a given training budget ($N$,$D$), providing a principled alternative to\ncostly trial-and-error methods."
                },
                "authors": [
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Louis Bethune"
                    },
                    {
                        "name": "Dan Busbridge"
                    },
                    {
                        "name": "David Grangier"
                    },
                    {
                        "name": "Enrico Fini"
                    },
                    {
                        "name": "Alaaeldin El-Nouby"
                    },
                    {
                        "name": "Pierre Ablin"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Ablin"
                },
                "author": "Pierre Ablin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00553v2",
                "updated": "2025-10-02T15:16:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    16,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-01T06:13:50Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    13,
                    50,
                    2,
                    274,
                    0
                ],
                "title": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models"
                },
                "summary": "Recent advances in reasoning capabilities of large language models (LLMs) are\nlargely driven by reinforcement learning (RL), yet the underlying parameter\ndynamics during RL training remain poorly understood. This work identifies two\nfundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1\nDominance, where the top singular subspace of the parameter update matrix\nnearly fully determines reasoning improvements, recovering over 99\\% of\nperformance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace\nevolves linearly throughout training, enabling accurate prediction from early\ncheckpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the\ngeneralizability of these properties. More importantly, based on these\nfindings, we propose AlphaRL, a plug-in acceleration framework that\nextrapolates the final parameter update using a short early training window,\nachieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning\nperformance without extra modules or hyperparameter tuning. This positions our\nfinding as a versatile and practical tool for large-scale RL, opening a path\ntoward principled, interpretable, and efficient training paradigm for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning capabilities of large language models (LLMs) are\nlargely driven by reinforcement learning (RL), yet the underlying parameter\ndynamics during RL training remain poorly understood. This work identifies two\nfundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1\nDominance, where the top singular subspace of the parameter update matrix\nnearly fully determines reasoning improvements, recovering over 99\\% of\nperformance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace\nevolves linearly throughout training, enabling accurate prediction from early\ncheckpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the\ngeneralizability of these properties. More importantly, based on these\nfindings, we propose AlphaRL, a plug-in acceleration framework that\nextrapolates the final parameter update using a short early training window,\nachieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning\nperformance without extra modules or hyperparameter tuning. This positions our\nfinding as a versatile and practical tool for large-scale RL, opening a path\ntoward principled, interpretable, and efficient training paradigm for LLMs."
                },
                "authors": [
                    {
                        "name": "Yuchen Cai"
                    },
                    {
                        "name": "Ding Cao"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Yuqing Huang"
                    },
                    {
                        "name": "Zhenyu Tan"
                    },
                    {
                        "name": "Benyi Zhang"
                    },
                    {
                        "name": "Guiquan Liu"
                    },
                    {
                        "name": "Junfeng Fang"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Fang"
                },
                "author": "Junfeng Fang",
                "arxiv_comment": "43 pages, 28 figures; 43",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02108v1",
                "updated": "2025-10-02T15:15:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    15,
                    50,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T15:15:50Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    15,
                    50,
                    3,
                    275,
                    0
                ],
                "title": "Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant\n  Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant\n  Neural Network"
                },
                "summary": "Although symbol-level precoding (SLP) based on constructive interference (CI)\nexploitation offers performance gains, its high complexity remains a\nbottleneck. This paper addresses this challenge with an end-to-end deep\nlearning (DL) framework with low inference complexity that leverages the\nstructure of the optimal SLP solution in the closed-form and its inherent\ntensor equivariance (TE), where TE denotes that a permutation of the input\ninduces the corresponding permutation of the output. Building upon the\ncomputationally efficient model-based formulations, as well as their known\nclosed-form solutions, we analyze their relationship with linear precoding (LP)\nand investigate the corresponding optimality condition. We then construct a\nmapping from the problem formulation to the solution and prove its TE, based on\nwhich the designed networks reveal a specific parameter-sharing pattern that\ndelivers low computational complexity and strong generalization. Leveraging\nthese, we propose the backbone of the framework with an attention-based TE\nmodule, achieving linear computational complexity. Furthermore, we demonstrate\nthat such a framework is also applicable to imperfect CSI scenarios, where we\ndesign a TE-based network to map the CSI, statistics, and symbols to auxiliary\nvariables. Simulation results show that the proposed framework captures\nsubstantial performance gains of optimal SLP, while achieving an approximately\n80-times speedup over conventional methods and maintaining strong\ngeneralization across user numbers and symbol block lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although symbol-level precoding (SLP) based on constructive interference (CI)\nexploitation offers performance gains, its high complexity remains a\nbottleneck. This paper addresses this challenge with an end-to-end deep\nlearning (DL) framework with low inference complexity that leverages the\nstructure of the optimal SLP solution in the closed-form and its inherent\ntensor equivariance (TE), where TE denotes that a permutation of the input\ninduces the corresponding permutation of the output. Building upon the\ncomputationally efficient model-based formulations, as well as their known\nclosed-form solutions, we analyze their relationship with linear precoding (LP)\nand investigate the corresponding optimality condition. We then construct a\nmapping from the problem formulation to the solution and prove its TE, based on\nwhich the designed networks reveal a specific parameter-sharing pattern that\ndelivers low computational complexity and strong generalization. Leveraging\nthese, we propose the backbone of the framework with an attention-based TE\nmodule, achieving linear computational complexity. Furthermore, we demonstrate\nthat such a framework is also applicable to imperfect CSI scenarios, where we\ndesign a TE-based network to map the CSI, statistics, and symbols to auxiliary\nvariables. Simulation results show that the proposed framework captures\nsubstantial performance gains of optimal SLP, while achieving an approximately\n80-times speedup over conventional methods and maintaining strong\ngeneralization across user numbers and symbol block lengths."
                },
                "authors": [
                    {
                        "name": "Jinshuo Zhang"
                    },
                    {
                        "name": "Yafei Wang"
                    },
                    {
                        "name": "Xinping Yi"
                    },
                    {
                        "name": "Wenjin Wang"
                    },
                    {
                        "name": "Shi Jin"
                    },
                    {
                        "name": "Symeon Chatzinotas"
                    },
                    {
                        "name": "Björn Ottersten"
                    }
                ],
                "author_detail": {
                    "name": "Björn Ottersten"
                },
                "author": "Björn Ottersten",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02104v1",
                "updated": "2025-10-02T15:10:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    10,
                    26,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T15:10:26Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    10,
                    26,
                    3,
                    275,
                    0
                ],
                "title": "LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot\n  Grasping with Ambiguous Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot\n  Grasping with Ambiguous Instructions"
                },
                "summary": "The existing language-driven grasping methods struggle to fully handle\nambiguous instructions containing implicit intents. To tackle this challenge,\nwe propose LangGrasp, a novel language-interactive robotic grasping framework.\nThe framework integrates fine-tuned large language models (LLMs) to leverage\ntheir robust commonsense understanding and environmental perception\ncapabilities, thereby deducing implicit intents from linguistic instructions\nand clarifying task requirements along with target manipulation objects.\nFurthermore, our designed point cloud localization module, guided by 2D part\nsegmentation, enables partial point cloud localization in scenes, thereby\nextending grasping operations from coarse-grained object-level to fine-grained\npart-level manipulation. Experimental results show that the LangGrasp framework\naccurately resolves implicit intents in ambiguous instructions, identifying\ncritical operations and target information that are unstated yet essential for\ntask completion. Additionally, it dynamically selects optimal grasping poses by\nintegrating environmental information. This enables high-precision grasping\nfrom object-level to part-level manipulation, significantly enhancing the\nadaptability and task execution efficiency of robots in unstructured\nenvironments. More information and code are available here:\nhttps://github.com/wu467/LangGrasp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The existing language-driven grasping methods struggle to fully handle\nambiguous instructions containing implicit intents. To tackle this challenge,\nwe propose LangGrasp, a novel language-interactive robotic grasping framework.\nThe framework integrates fine-tuned large language models (LLMs) to leverage\ntheir robust commonsense understanding and environmental perception\ncapabilities, thereby deducing implicit intents from linguistic instructions\nand clarifying task requirements along with target manipulation objects.\nFurthermore, our designed point cloud localization module, guided by 2D part\nsegmentation, enables partial point cloud localization in scenes, thereby\nextending grasping operations from coarse-grained object-level to fine-grained\npart-level manipulation. Experimental results show that the LangGrasp framework\naccurately resolves implicit intents in ambiguous instructions, identifying\ncritical operations and target information that are unstated yet essential for\ntask completion. Additionally, it dynamically selects optimal grasping poses by\nintegrating environmental information. This enables high-precision grasping\nfrom object-level to part-level manipulation, significantly enhancing the\nadaptability and task execution efficiency of robots in unstructured\nenvironments. More information and code are available here:\nhttps://github.com/wu467/LangGrasp."
                },
                "authors": [
                    {
                        "name": "Yunhan Lin"
                    },
                    {
                        "name": "Wenqi Wu"
                    },
                    {
                        "name": "Zhijie Zhang"
                    },
                    {
                        "name": "Huasong Min"
                    }
                ],
                "author_detail": {
                    "name": "Huasong Min"
                },
                "author": "Huasong Min",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14073v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14073v4",
                "updated": "2025-10-02T15:06:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    6,
                    38,
                    3,
                    275,
                    0
                ],
                "published": "2023-09-25T12:07:00Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    12,
                    7,
                    0,
                    0,
                    268,
                    0
                ],
                "title": "Neural Network Parameter-optimization of Gaussian pmDAGs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Network Parameter-optimization of Gaussian pmDAGs"
                },
                "summary": "Finding the parameters of a latent variable causal model is central to causal\ninference and causal identification. In this article, we show that existing\ngraphical structures that are used in causal inference are not stable under\nmarginalization of Gaussian Bayesian networks, and present a graphical\nstructure that faithfully represent margins of Gaussian Bayesian networks. We\npresent the first duality between parameter optimization of a latent variable\nmodel and training a feed-forward neural network in the parameter space of the\nassumed family of distributions. Based on this observation, we develop an\nalgorithm for parameter optimization of these graphical structures based on a\ngiven observational distribution. Then, we provide conditions for causal effect\nidentifiability in the Gaussian setting. We propose an meta-algorithm that\nchecks whether a causal effect is identifiable or not. Moreover, we lay a\ngrounding for generalizing the duality between a neural network and a causal\nmodel from the Gaussian to other distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding the parameters of a latent variable causal model is central to causal\ninference and causal identification. In this article, we show that existing\ngraphical structures that are used in causal inference are not stable under\nmarginalization of Gaussian Bayesian networks, and present a graphical\nstructure that faithfully represent margins of Gaussian Bayesian networks. We\npresent the first duality between parameter optimization of a latent variable\nmodel and training a feed-forward neural network in the parameter space of the\nassumed family of distributions. Based on this observation, we develop an\nalgorithm for parameter optimization of these graphical structures based on a\ngiven observational distribution. Then, we provide conditions for causal effect\nidentifiability in the Gaussian setting. We propose an meta-algorithm that\nchecks whether a causal effect is identifiable or not. Moreover, we lay a\ngrounding for generalizing the duality between a neural network and a causal\nmodel from the Gaussian to other distributions."
                },
                "authors": [
                    {
                        "name": "Mehrzad Saremi"
                    }
                ],
                "author_detail": {
                    "name": "Mehrzad Saremi"
                },
                "author": "Mehrzad Saremi",
                "arxiv_comment": "52 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14073v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14073v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02099v1",
                "updated": "2025-10-02T15:06:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    6,
                    0,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T15:06:00Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    6,
                    0,
                    3,
                    275,
                    0
                ],
                "title": "Multiplier-free In-Memory Vector-Matrix Multiplication Using Distributed\n  Arithmetic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiplier-free In-Memory Vector-Matrix Multiplication Using Distributed\n  Arithmetic"
                },
                "summary": "Vector-Matrix Multiplication (VMM) is the fundamental and frequently required\ncomputation in inference of Neural Networks (NN). Due to the large data\nmovement required during inference, VMM can benefit greatly from in-memory\ncomputing. However, ADC/DACs required for in-memory VMM consume significant\npower and area. `Distributed Arithmetic (DA)', a technique in computer\narchitecture prevalent in 1980s was used to achieve inner product or dot\nproduct of two vectors without using a hard-wired multiplier when one of the\nvectors is a constant. In this work, we extend the DA technique to multiply an\ninput vector with a constant matrix. By storing the sum of the weights in\nmemory, DA achieves VMM using shift-and-add circuits in the periphery of ReRAM\nmemory. We verify functional and also estimate non-functional properties\n(latency, energy, area) by performing transistor-level simulations. Using\nenergy-efficient sensing and fine grained pipelining, our approach achieves 4.5\nx less latency and 12 x less energy than VMM performed in memory conventionally\nby bit slicing. Furthermore, DA completely eliminated the need for power-hungry\nADCs which are the main source of area and energy consumption in the current\nVMM implementations in memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector-Matrix Multiplication (VMM) is the fundamental and frequently required\ncomputation in inference of Neural Networks (NN). Due to the large data\nmovement required during inference, VMM can benefit greatly from in-memory\ncomputing. However, ADC/DACs required for in-memory VMM consume significant\npower and area. `Distributed Arithmetic (DA)', a technique in computer\narchitecture prevalent in 1980s was used to achieve inner product or dot\nproduct of two vectors without using a hard-wired multiplier when one of the\nvectors is a constant. In this work, we extend the DA technique to multiply an\ninput vector with a constant matrix. By storing the sum of the weights in\nmemory, DA achieves VMM using shift-and-add circuits in the periphery of ReRAM\nmemory. We verify functional and also estimate non-functional properties\n(latency, energy, area) by performing transistor-level simulations. Using\nenergy-efficient sensing and fine grained pipelining, our approach achieves 4.5\nx less latency and 12 x less energy than VMM performed in memory conventionally\nby bit slicing. Furthermore, DA completely eliminated the need for power-hungry\nADCs which are the main source of area and energy consumption in the current\nVMM implementations in memory."
                },
                "authors": [
                    {
                        "name": "Felix Zeller"
                    },
                    {
                        "name": "John Reuben"
                    },
                    {
                        "name": "Dietmar Fey"
                    }
                ],
                "author_detail": {
                    "name": "Dietmar Fey"
                },
                "author": "Dietmar Fey",
                "arxiv_comment": "9 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3; B.7; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02091v1",
                "updated": "2025-10-02T14:57:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    57,
                    13,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:57:13Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    57,
                    13,
                    3,
                    275,
                    0
                ],
                "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and\n  Reasoning"
                },
                "summary": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models."
                },
                "authors": [
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Keyu Wang"
                    },
                    {
                        "name": "PengXiang Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18262v3",
                "updated": "2025-10-02T14:53:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    53,
                    4,
                    3,
                    275,
                    0
                ],
                "published": "2024-11-27T11:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    59,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "Break the ID-Language Barrier: An Adaption Framework for LLM-based\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Break the ID-Language Barrier: An Adaption Framework for LLM-based\n  Sequential Recommendation"
                },
                "summary": "The recent breakthrough of large language models (LLMs) in natural language\nprocessing has sparked exploration in recommendation systems, however, their\nlimited domain-specific knowledge remains a critical bottleneck. Specifically,\nLLMs lack key pieces of information crucial for sequential recommendations,\nsuch as user behavior patterns. To address this critical gap, we propose\nIDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich\nin domain-specific knowledge, into LLMs to improve recommendation accuracy.\nIDLE-Adapter acts as a bridge, transforming sparse user-item interaction data\ninto dense, LLM-compatible representations through a Pre-trained ID Sequential\nModel, Dimensionality Alignment, Layer-wise Embedding Refinement, and\nLayer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates\nremarkable flexibility by seamlessly integrating ID embeddings from diverse\nID-based sequential models and LLM architectures. Extensive experiments across\nvarious datasets demonstrate the superiority of IDLE-Adapter, achieving over\n10\\% and 20\\% improvements in HitRate@5 and NDCG@5 metrics, respectively,\ncompared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent breakthrough of large language models (LLMs) in natural language\nprocessing has sparked exploration in recommendation systems, however, their\nlimited domain-specific knowledge remains a critical bottleneck. Specifically,\nLLMs lack key pieces of information crucial for sequential recommendations,\nsuch as user behavior patterns. To address this critical gap, we propose\nIDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich\nin domain-specific knowledge, into LLMs to improve recommendation accuracy.\nIDLE-Adapter acts as a bridge, transforming sparse user-item interaction data\ninto dense, LLM-compatible representations through a Pre-trained ID Sequential\nModel, Dimensionality Alignment, Layer-wise Embedding Refinement, and\nLayer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates\nremarkable flexibility by seamlessly integrating ID embeddings from diverse\nID-based sequential models and LLM architectures. Extensive experiments across\nvarious datasets demonstrate the superiority of IDLE-Adapter, achieving over\n10\\% and 20\\% improvements in HitRate@5 and NDCG@5 metrics, respectively,\ncompared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xiaohan Yu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02084v2",
                "updated": "2025-10-03T05:10:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    5,
                    10,
                    2,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-02T14:50:50Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    50,
                    50,
                    3,
                    275,
                    0
                ],
                "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting"
                },
                "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries."
                },
                "authors": [
                    {
                        "name": "Kuiye Ding"
                    },
                    {
                        "name": "Fanda Fan"
                    },
                    {
                        "name": "Zheya Wang"
                    },
                    {
                        "name": "Hongxiao Li"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Chunjie Luo"
                    },
                    {
                        "name": "Jianfeng Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Zhan"
                },
                "author": "Jianfeng Zhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02081v1",
                "updated": "2025-10-02T14:49:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    49,
                    47,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:49:47Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    49,
                    47,
                    3,
                    275,
                    0
                ],
                "title": "Fine-Tuning Flow Matching via Maximum Likelihood Estimation of\n  Reconstructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Flow Matching via Maximum Likelihood Estimation of\n  Reconstructions"
                },
                "summary": "Flow Matching (FM) algorithm achieves remarkable results in generative tasks\nespecially in robotic manipulation. Building upon the foundations of diffusion\nmodels, the simulation-free paradigm of FM enables simple and efficient\ntraining, but inherently introduces a train-inference gap. Specifically, we\ncannot assess the model's output during the training phase. In contrast, other\ngenerative models including Variational Autoencoder (VAE), Normalizing Flow and\nGenerative Adversarial Networks (GANs) directly optimize on the reconstruction\nloss. Such a gap is particularly evident in scenarios that demand high\nprecision, such as robotic manipulation. Moreover, we show that FM's\nover-pursuit of straight predefined paths may introduce some serious problems\nsuch as stiffness into the system. These motivate us to fine-tune FM via\nMaximum Likelihood Estimation of reconstructions - an approach made feasible by\nFM's underlying smooth ODE formulation, in contrast to the stochastic\ndifferential equations (SDEs) used in diffusion models. This paper first\ntheoretically analyzes the relation between training loss and inference error\nin FM. Then we propose a method of fine-tuning FM via Maximum Likelihood\nEstimation of reconstructions, which includes both straightforward fine-tuning\nand residual-based fine-tuning approaches. Furthermore, through specifically\ndesigned architectures, the residual-based fine-tuning can incorporate the\ncontraction property into the model, which is crucial for the model's\nrobustness and interpretability. Experimental results in image generation and\nrobotic manipulation verify that our method reliably improves the inference\nperformance of FM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow Matching (FM) algorithm achieves remarkable results in generative tasks\nespecially in robotic manipulation. Building upon the foundations of diffusion\nmodels, the simulation-free paradigm of FM enables simple and efficient\ntraining, but inherently introduces a train-inference gap. Specifically, we\ncannot assess the model's output during the training phase. In contrast, other\ngenerative models including Variational Autoencoder (VAE), Normalizing Flow and\nGenerative Adversarial Networks (GANs) directly optimize on the reconstruction\nloss. Such a gap is particularly evident in scenarios that demand high\nprecision, such as robotic manipulation. Moreover, we show that FM's\nover-pursuit of straight predefined paths may introduce some serious problems\nsuch as stiffness into the system. These motivate us to fine-tune FM via\nMaximum Likelihood Estimation of reconstructions - an approach made feasible by\nFM's underlying smooth ODE formulation, in contrast to the stochastic\ndifferential equations (SDEs) used in diffusion models. This paper first\ntheoretically analyzes the relation between training loss and inference error\nin FM. Then we propose a method of fine-tuning FM via Maximum Likelihood\nEstimation of reconstructions, which includes both straightforward fine-tuning\nand residual-based fine-tuning approaches. Furthermore, through specifically\ndesigned architectures, the residual-based fine-tuning can incorporate the\ncontraction property into the model, which is crucial for the model's\nrobustness and interpretability. Experimental results in image generation and\nrobotic manipulation verify that our method reliably improves the inference\nperformance of FM."
                },
                "authors": [
                    {
                        "name": "Zhaoyi Li"
                    },
                    {
                        "name": "Jingtao Ding"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Shihua Li"
                    }
                ],
                "author_detail": {
                    "name": "Shihua Li"
                },
                "author": "Shihua Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00320v2",
                "updated": "2025-10-02T14:46:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    46,
                    26,
                    3,
                    275,
                    0
                ],
                "published": "2025-08-30T02:43:50Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    2,
                    43,
                    50,
                    5,
                    242,
                    0
                ],
                "title": "TrimTokenator: Towards Adaptive Visual Token Pruning for Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrimTokenator: Towards Adaptive Visual Token Pruning for Large\n  Multimodal Models"
                },
                "summary": "Large Multimodal Models (LMMs) have achieved significant success across\nvarious tasks. These models usually encode visual inputs into dense token\nsequences, which are then concatenated with textual tokens and jointly\nprocessed by a language model. However, the increased token count substantially\nraises computational and memory costs during inference. Token pruning has\nemerged as a promising approach to address this issue. Existing token pruning\nmethods often rely on costly calibration or suboptimal importance metrics,\nleading to redundant retained tokens. In this paper, we analyze the redundancy\ndifferences between visual and textual tokens and propose pruning exclusively\non visual tokens. Based on this, we propose a visual token pruning strategy\nthat explicitly preserves both cross-modal alignment and intra-modal\ninformational diversity. We introduce a mutual information-based token pruning\nstrategy that removes visual tokens semantically misaligned with textual\ntokens, effectively preserving the alignment between the visual and textual\nmodalities. To further improve the representational quality of the retained\ntokens, we additionally prune redundant visual tokens by maximizing the\nexpected pairwise distances in the embedding space, which is solved efficiently\nwith a greedy algorithm. Extensive experiments demonstrate that our method\nmaintains strong performance while reducing tokens by 88.9% on models such as\nLLaVA-1.5-7B and LLaVA-NEXT-7B, resulting in a 56.7% improvement in inference\nspeed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have achieved significant success across\nvarious tasks. These models usually encode visual inputs into dense token\nsequences, which are then concatenated with textual tokens and jointly\nprocessed by a language model. However, the increased token count substantially\nraises computational and memory costs during inference. Token pruning has\nemerged as a promising approach to address this issue. Existing token pruning\nmethods often rely on costly calibration or suboptimal importance metrics,\nleading to redundant retained tokens. In this paper, we analyze the redundancy\ndifferences between visual and textual tokens and propose pruning exclusively\non visual tokens. Based on this, we propose a visual token pruning strategy\nthat explicitly preserves both cross-modal alignment and intra-modal\ninformational diversity. We introduce a mutual information-based token pruning\nstrategy that removes visual tokens semantically misaligned with textual\ntokens, effectively preserving the alignment between the visual and textual\nmodalities. To further improve the representational quality of the retained\ntokens, we additionally prune redundant visual tokens by maximizing the\nexpected pairwise distances in the embedding space, which is solved efficiently\nwith a greedy algorithm. Extensive experiments demonstrate that our method\nmaintains strong performance while reducing tokens by 88.9% on models such as\nLLaVA-1.5-7B and LLaVA-NEXT-7B, resulting in a 56.7% improvement in inference\nspeed."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Chenrui He"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02073v1",
                "updated": "2025-10-02T14:36:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    36,
                    2,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:36:02Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    36,
                    2,
                    3,
                    275,
                    0
                ],
                "title": "Inferring Optical Tissue Properties from Photoplethysmography using\n  Hybrid Amortized Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Optical Tissue Properties from Photoplethysmography using\n  Hybrid Amortized Inference"
                },
                "summary": "Smart wearables enable continuous tracking of established biomarkers such as\nheart rate, heart rate variability, and blood oxygen saturation via\nphotoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer\nphysiological information, as recent deep learning (DL) studies demonstrate.\nHowever, DL models often rely on features with unclear physiological meaning,\ncreating a tension between predictive power, clinical interpretability, and\nsensor design. We address this gap by introducing PPGen, a biophysical model\nthat relates PPG signals to interpretable physiological and optical parameters.\nBuilding on PPGen, we propose hybrid amortized inference (HAI), enabling fast,\nrobust, and scalable estimation of relevant physiological parameters from PPG\nsignals while correcting for model misspecification. In extensive in-silico\nexperiments, we show that HAI can accurately infer physiological parameters\nunder diverse noise and sensor conditions. Our results illustrate a path toward\nPPG models that retain the fidelity needed for DL-based features while\nsupporting clinical interpretation and informed hardware design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart wearables enable continuous tracking of established biomarkers such as\nheart rate, heart rate variability, and blood oxygen saturation via\nphotoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer\nphysiological information, as recent deep learning (DL) studies demonstrate.\nHowever, DL models often rely on features with unclear physiological meaning,\ncreating a tension between predictive power, clinical interpretability, and\nsensor design. We address this gap by introducing PPGen, a biophysical model\nthat relates PPG signals to interpretable physiological and optical parameters.\nBuilding on PPGen, we propose hybrid amortized inference (HAI), enabling fast,\nrobust, and scalable estimation of relevant physiological parameters from PPG\nsignals while correcting for model misspecification. In extensive in-silico\nexperiments, we show that HAI can accurately infer physiological parameters\nunder diverse noise and sensor conditions. Our results illustrate a path toward\nPPG models that retain the fidelity needed for DL-based features while\nsupporting clinical interpretation and informed hardware design."
                },
                "authors": [
                    {
                        "name": "Jens Behrmann"
                    },
                    {
                        "name": "Maria R. Cervera"
                    },
                    {
                        "name": "Antoine Wehenkel"
                    },
                    {
                        "name": "Andrew C. Miller"
                    },
                    {
                        "name": "Albert Cerussi"
                    },
                    {
                        "name": "Pranay Jain"
                    },
                    {
                        "name": "Vivek Venugopal"
                    },
                    {
                        "name": "Shijie Yan"
                    },
                    {
                        "name": "Guillermo Sapiro"
                    },
                    {
                        "name": "Luca Pegolotti"
                    },
                    {
                        "name": "Jörn-Henrik Jacobsen"
                    }
                ],
                "author_detail": {
                    "name": "Jörn-Henrik Jacobsen"
                },
                "author": "Jörn-Henrik Jacobsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02067v1",
                "updated": "2025-10-02T14:33:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    33,
                    57,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:33:57Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    33,
                    57,
                    3,
                    275,
                    0
                ],
                "title": "Adaptive Kernel Selection for Stein Variational Gradient Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Kernel Selection for Stein Variational Gradient Descent"
                },
                "summary": "A central challenge in Bayesian inference is efficiently approximating\nposterior distributions. Stein Variational Gradient Descent (SVGD) is a popular\nvariational inference method which transports a set of particles to approximate\na target distribution. The SVGD dynamics are governed by a reproducing kernel\nHilbert space (RKHS) and are highly sensitive to the choice of the kernel\nfunction, which directly influences both convergence and approximation quality.\nThe commonly used median heuristic offers a simple approach for setting kernel\nbandwidths but lacks flexibility and often performs poorly, particularly in\nhigh-dimensional settings. In this work, we propose an alternative strategy for\nadaptively choosing kernel parameters over an abstract family of kernels.\nRecent convergence analyses based on the kernelized Stein discrepancy (KSD)\nsuggest that optimizing the kernel parameters by maximizing the KSD can improve\nperformance. Building on this insight, we introduce Adaptive SVGD (Ad-SVGD), a\nmethod that alternates between updating the particles via SVGD and adaptively\ntuning kernel bandwidths through gradient ascent on the KSD. We provide a\nsimplified theoretical analysis that extends existing results on minimizing the\nKSD for fixed kernels to our adaptive setting, showing convergence properties\nfor the maximal KSD over our kernel class. Our empirical results further\nsupport this intuition: Ad-SVGD consistently outperforms standard heuristics in\na variety of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central challenge in Bayesian inference is efficiently approximating\nposterior distributions. Stein Variational Gradient Descent (SVGD) is a popular\nvariational inference method which transports a set of particles to approximate\na target distribution. The SVGD dynamics are governed by a reproducing kernel\nHilbert space (RKHS) and are highly sensitive to the choice of the kernel\nfunction, which directly influences both convergence and approximation quality.\nThe commonly used median heuristic offers a simple approach for setting kernel\nbandwidths but lacks flexibility and often performs poorly, particularly in\nhigh-dimensional settings. In this work, we propose an alternative strategy for\nadaptively choosing kernel parameters over an abstract family of kernels.\nRecent convergence analyses based on the kernelized Stein discrepancy (KSD)\nsuggest that optimizing the kernel parameters by maximizing the KSD can improve\nperformance. Building on this insight, we introduce Adaptive SVGD (Ad-SVGD), a\nmethod that alternates between updating the particles via SVGD and adaptively\ntuning kernel bandwidths through gradient ascent on the KSD. We provide a\nsimplified theoretical analysis that extends existing results on minimizing the\nKSD for fixed kernels to our adaptive setting, showing convergence properties\nfor the maximal KSD over our kernel class. Our empirical results further\nsupport this intuition: Ad-SVGD consistently outperforms standard heuristics in\na variety of tasks."
                },
                "authors": [
                    {
                        "name": "Moritz Melcher"
                    },
                    {
                        "name": "Simon Weissmann"
                    },
                    {
                        "name": "Ashia C. Wilson"
                    },
                    {
                        "name": "Jakob Zech"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Zech"
                },
                "author": "Jakob Zech",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02063v1",
                "updated": "2025-10-02T14:30:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    30,
                    46,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:30:46Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    30,
                    46,
                    3,
                    275,
                    0
                ],
                "title": "MSRepaint: Multiple Sclerosis Repaint with Conditional Denoising\n  Diffusion Implicit Model for Bidirectional Lesion Filling and Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSRepaint: Multiple Sclerosis Repaint with Conditional Denoising\n  Diffusion Implicit Model for Bidirectional Lesion Filling and Synthesis"
                },
                "summary": "In multiple sclerosis, lesions interfere with automated magnetic resonance\nimaging analyses such as brain parcellation and deformable registration, while\nlesion segmentation models are hindered by the limited availability of\nannotated training data. To address both issues, we propose MSRepaint, a\nunified diffusion-based generative model for bidirectional lesion filling and\nsynthesis that restores anatomical continuity for downstream analyses and\naugments segmentation through realistic data generation. MSRepaint conditions\non spatial lesion masks for voxel-level control, incorporates contrast dropout\nto handle missing inputs, integrates a repainting mechanism to preserve\nsurrounding anatomy during lesion filling and synthesis, and employs a\nmulti-view DDIM inversion and fusion pipeline for 3D consistency with fast\ninference. Extensive evaluations demonstrate the effectiveness of MSRepaint\nacross multiple tasks. For lesion filling, we evaluate both the accuracy within\nthe filled regions and the impact on downstream tasks including brain\nparcellation and deformable registration. MSRepaint outperforms the traditional\nlesion filling methods FSL and NiftySeg, and achieves accuracy on par with\nFastSurfer-LIT, a recent diffusion model-based inpainting method, while\noffering over 20 times faster inference. For lesion synthesis, state-of-the-art\nMS lesion segmentation models trained on MSRepaint-synthesized data outperform\nthose trained on CarveMix-synthesized data or real ISBI challenge training data\nacross multiple benchmarks, including the MICCAI 2016 and UMCL datasets.\nAdditionally, we demonstrate that MSRepaint's unified bidirectional filling and\nsynthesis capability, with full spatial control over lesion appearance, enables\nhigh-fidelity simulation of lesion evolution in longitudinal MS progression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multiple sclerosis, lesions interfere with automated magnetic resonance\nimaging analyses such as brain parcellation and deformable registration, while\nlesion segmentation models are hindered by the limited availability of\nannotated training data. To address both issues, we propose MSRepaint, a\nunified diffusion-based generative model for bidirectional lesion filling and\nsynthesis that restores anatomical continuity for downstream analyses and\naugments segmentation through realistic data generation. MSRepaint conditions\non spatial lesion masks for voxel-level control, incorporates contrast dropout\nto handle missing inputs, integrates a repainting mechanism to preserve\nsurrounding anatomy during lesion filling and synthesis, and employs a\nmulti-view DDIM inversion and fusion pipeline for 3D consistency with fast\ninference. Extensive evaluations demonstrate the effectiveness of MSRepaint\nacross multiple tasks. For lesion filling, we evaluate both the accuracy within\nthe filled regions and the impact on downstream tasks including brain\nparcellation and deformable registration. MSRepaint outperforms the traditional\nlesion filling methods FSL and NiftySeg, and achieves accuracy on par with\nFastSurfer-LIT, a recent diffusion model-based inpainting method, while\noffering over 20 times faster inference. For lesion synthesis, state-of-the-art\nMS lesion segmentation models trained on MSRepaint-synthesized data outperform\nthose trained on CarveMix-synthesized data or real ISBI challenge training data\nacross multiple benchmarks, including the MICCAI 2016 and UMCL datasets.\nAdditionally, we demonstrate that MSRepaint's unified bidirectional filling and\nsynthesis capability, with full spatial control over lesion appearance, enables\nhigh-fidelity simulation of lesion evolution in longitudinal MS progression."
                },
                "authors": [
                    {
                        "name": "Jinwei Zhang"
                    },
                    {
                        "name": "Lianrui Zuo"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Samuel W. Remedios"
                    },
                    {
                        "name": "Bennett A. Landman"
                    },
                    {
                        "name": "Peter A. Calabresi"
                    },
                    {
                        "name": "Shiv Saidha"
                    },
                    {
                        "name": "Scott D. Newsome"
                    },
                    {
                        "name": "Dzung L. Pham"
                    },
                    {
                        "name": "Jerry L. Prince"
                    },
                    {
                        "name": "Ellen M. Mowry"
                    },
                    {
                        "name": "Aaron Carass"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Carass"
                },
                "author": "Aaron Carass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02060v1",
                "updated": "2025-10-02T14:28:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    28,
                    45,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:28:45Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    28,
                    45,
                    3,
                    275,
                    0
                ],
                "title": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly\n  Detection"
                },
                "summary": "In tabular anomaly detection (AD), textual semantics often carry critical\nsignals, as the definition of an anomaly is closely tied to domain-specific\ncontext. However, existing benchmarks provide only raw data points without\nsemantic context, overlooking rich textual metadata such as feature\ndescriptions and domain knowledge that experts rely on in practice. This\nlimitation restricts research flexibility and prevents models from fully\nleveraging domain knowledge for detection. ReTabAD addresses this gap by\nrestoring textual semantics to enable context-aware tabular AD research. We\nprovide (1) 20 carefully curated tabular datasets enriched with structured\ntextual metadata, together with implementations of state-of-the-art AD\nalgorithms including classical, deep learning, and LLM-based approaches, and\n(2) a zero-shot LLM framework that leverages semantic context without\ntask-specific training, establishing a strong baseline for future research.\nFurthermore, this work provides insights into the role and utility of textual\nmetadata in AD through experiments and analysis. Results show that semantic\ncontext improves detection performance and enhances interpretability by\nsupporting domain-aware reasoning. These findings establish ReTabAD as a\nbenchmark for systematic exploration of context-aware AD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In tabular anomaly detection (AD), textual semantics often carry critical\nsignals, as the definition of an anomaly is closely tied to domain-specific\ncontext. However, existing benchmarks provide only raw data points without\nsemantic context, overlooking rich textual metadata such as feature\ndescriptions and domain knowledge that experts rely on in practice. This\nlimitation restricts research flexibility and prevents models from fully\nleveraging domain knowledge for detection. ReTabAD addresses this gap by\nrestoring textual semantics to enable context-aware tabular AD research. We\nprovide (1) 20 carefully curated tabular datasets enriched with structured\ntextual metadata, together with implementations of state-of-the-art AD\nalgorithms including classical, deep learning, and LLM-based approaches, and\n(2) a zero-shot LLM framework that leverages semantic context without\ntask-specific training, establishing a strong baseline for future research.\nFurthermore, this work provides insights into the role and utility of textual\nmetadata in AD through experiments and analysis. Results show that semantic\ncontext improves detection performance and enhances interpretability by\nsupporting domain-aware reasoning. These findings establish ReTabAD as a\nbenchmark for systematic exploration of context-aware AD."
                },
                "authors": [
                    {
                        "name": "Sanghyu Yoon"
                    },
                    {
                        "name": "Dongmin Kim"
                    },
                    {
                        "name": "Suhee Yoon"
                    },
                    {
                        "name": "Ye Seul Sim"
                    },
                    {
                        "name": "Seungdong Yoa"
                    },
                    {
                        "name": "Hye-Seung Cho"
                    },
                    {
                        "name": "Soonyoung Lee"
                    },
                    {
                        "name": "Hankook Lee"
                    },
                    {
                        "name": "Woohyung Lim"
                    }
                ],
                "author_detail": {
                    "name": "Woohyung Lim"
                },
                "author": "Woohyung Lim",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02056v1",
                "updated": "2025-10-02T14:25:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    25,
                    29,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:25:29Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    25,
                    29,
                    3,
                    275,
                    0
                ],
                "title": "Adaptive Heterogeneous Mixtures of Normalising Flows for Robust\n  Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Heterogeneous Mixtures of Normalising Flows for Robust\n  Variational Inference"
                },
                "summary": "Normalising-flow variational inference (VI) can approximate complex\nposteriors, yet single-flow models often behave inconsistently across\nqualitatively different distributions. We propose Adaptive Mixture Flow\nVariational Inference (AMF-VI), a heterogeneous mixture of complementary flows\n(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of\nindividual flows, and (ii) adaptive global weight estimation via\nlikelihood-driven updates, without per-sample gating or architectural changes.\nEvaluated on six canonical posterior families of banana, X-shape, two-moons,\nrings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower\nnegative log-likelihood than each single-flow baseline and delivers stable\ngains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),\nindicating improved robustness across shapes and modalities. The procedure is\nefficient and architecture-agnostic, incurring minimal overhead relative to\nstandard flow training, and demonstrates that adaptive mixtures of diverse\nflows provide a reliable route to robust VI across diverse posterior families\nwhilst preserving each expert's inductive bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normalising-flow variational inference (VI) can approximate complex\nposteriors, yet single-flow models often behave inconsistently across\nqualitatively different distributions. We propose Adaptive Mixture Flow\nVariational Inference (AMF-VI), a heterogeneous mixture of complementary flows\n(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of\nindividual flows, and (ii) adaptive global weight estimation via\nlikelihood-driven updates, without per-sample gating or architectural changes.\nEvaluated on six canonical posterior families of banana, X-shape, two-moons,\nrings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower\nnegative log-likelihood than each single-flow baseline and delivers stable\ngains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),\nindicating improved robustness across shapes and modalities. The procedure is\nefficient and architecture-agnostic, incurring minimal overhead relative to\nstandard flow training, and demonstrates that adaptive mixtures of diverse\nflows provide a reliable route to robust VI across diverse posterior families\nwhilst preserving each expert's inductive bias."
                },
                "authors": [
                    {
                        "name": "Benjamin Wiriyapong"
                    },
                    {
                        "name": "Oktay Karakuş"
                    },
                    {
                        "name": "Kirill Sidorov"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Sidorov"
                },
                "author": "Kirill Sidorov",
                "arxiv_comment": "2 Figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02047v1",
                "updated": "2025-10-02T14:22:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    22,
                    20,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:22:20Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    22,
                    20,
                    3,
                    275,
                    0
                ],
                "title": "LLM-Enhanced, Data-Driven Personalized and Equitable Clinician\n  Scheduling: A Predict-then-Optimize Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced, Data-Driven Personalized and Equitable Clinician\n  Scheduling: A Predict-then-Optimize Approach"
                },
                "summary": "Clinician scheduling remains a persistent challenge due to limited clinical\nresources and fluctuating demands. This complexity is especially acute in large\nacademic anesthesiology departments as physicians balance responsibilities\nacross multiple clinical sites with conflicting priorities. Further, scheduling\nmust account for individual clinical and lifestyle preferences to ensure job\nsatisfaction and well-being. Traditional approaches, often based on statistical\nor rule-based optimization models, rely on structured data and explicit domain\nknowledge. However, these methods often overlook unstructured information,\ne.g., free-text notes from routinely administered clinician well-being surveys\nand scheduling platforms. These notes may reveal implicit and underutilized\nclinical resources. Neglecting such information can lead to misaligned\nschedules, increased burnout, overlooked staffing flexibility, and suboptimal\nutilization of available resources. To address this gap, we propose a\npredict-then-optimize framework that integrates classification-based clinician\navailability predictions with a mixed-integer programming schedule optimization\nmodel. Large language models (LLMs) are employed to extract actionable\npreferences and implicit constraints from unstructured schedule notes,\nenhancing the reliability of availability predictions. These predictions then\ninform the schedule optimization considering four objectives: first, ensuring\nclinical full-time equivalent compliance, second, reducing workload imbalances\nby enforcing equitable proportions of shift types, third, maximizing clinician\navailability for assigned shifts, and fourth, schedule consistency. By\ncombining the interpretive power of LLMs with the rigor of mathematical\noptimization, our framework provides a robust, data-driven solution that\nenhances operational efficiency while supporting equity and clinician\nwell-being.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinician scheduling remains a persistent challenge due to limited clinical\nresources and fluctuating demands. This complexity is especially acute in large\nacademic anesthesiology departments as physicians balance responsibilities\nacross multiple clinical sites with conflicting priorities. Further, scheduling\nmust account for individual clinical and lifestyle preferences to ensure job\nsatisfaction and well-being. Traditional approaches, often based on statistical\nor rule-based optimization models, rely on structured data and explicit domain\nknowledge. However, these methods often overlook unstructured information,\ne.g., free-text notes from routinely administered clinician well-being surveys\nand scheduling platforms. These notes may reveal implicit and underutilized\nclinical resources. Neglecting such information can lead to misaligned\nschedules, increased burnout, overlooked staffing flexibility, and suboptimal\nutilization of available resources. To address this gap, we propose a\npredict-then-optimize framework that integrates classification-based clinician\navailability predictions with a mixed-integer programming schedule optimization\nmodel. Large language models (LLMs) are employed to extract actionable\npreferences and implicit constraints from unstructured schedule notes,\nenhancing the reliability of availability predictions. These predictions then\ninform the schedule optimization considering four objectives: first, ensuring\nclinical full-time equivalent compliance, second, reducing workload imbalances\nby enforcing equitable proportions of shift types, third, maximizing clinician\navailability for assigned shifts, and fourth, schedule consistency. By\ncombining the interpretive power of LLMs with the rigor of mathematical\noptimization, our framework provides a robust, data-driven solution that\nenhances operational efficiency while supporting equity and clinician\nwell-being."
                },
                "authors": [
                    {
                        "name": "Anjali Jha"
                    },
                    {
                        "name": "Wanqing Chen"
                    },
                    {
                        "name": "Maxim Eckmann"
                    },
                    {
                        "name": "Ian Stockwell"
                    },
                    {
                        "name": "Jianwu Wang"
                    },
                    {
                        "name": "Kai Sun"
                    }
                ],
                "author_detail": {
                    "name": "Kai Sun"
                },
                "author": "Kai Sun",
                "arxiv_comment": "10 pages, 5 figures, Accepted to IEEE ICDM 2025 Workshops\n  Proceedings; IEEE Computer Society Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02044v1",
                "updated": "2025-10-02T14:18:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    18,
                    20,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:18:20Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    18,
                    20,
                    3,
                    275,
                    0
                ],
                "title": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming\n  Tool Usage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming\n  Tool Usage"
                },
                "summary": "End-to-end speech-in speech-out dialogue systems are emerging as a powerful\nalternative to traditional ASR-LLM-TTS pipelines, generating more natural,\nexpressive responses with significantly lower latency. However, these systems\nremain prone to hallucinations due to limited factual grounding. While\ntext-based dialogue systems address this challenge by integrating tools such as\nweb search and knowledge graph APIs, we introduce the first approach to extend\ntool use directly into speech-in speech-out systems. A key challenge is that\ntool integration substantially increases response latency, disrupting\nconversational flow. To mitigate this, we propose Streaming Retrieval-Augmented\nGeneration (Streaming RAG), a novel framework that reduces user-perceived\nlatency by predicting tool queries in parallel with user speech, even before\nthe user finishes speaking. Specifically, we develop a post-training pipeline\nthat teaches the model when to issue tool calls during ongoing speech and how\nto generate spoken summaries that fuse audio queries with retrieved text\nresults, thereby improving both accuracy and responsiveness. To evaluate our\napproach, we construct AudioCRAG, a benchmark created by converting queries\nfrom the publicly available CRAG dataset into speech form. Experimental results\ndemonstrate that our streaming RAG approach increases QA accuracy by up to 200%\nrelative (from 11.1% to 34.2% absolute) and further enhances user experience by\nreducing tool use latency by 20%. Importantly, our streaming RAG approach is\nmodality-agnostic and can be applied equally to typed input, paving the way for\nmore agentic, real-time AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end speech-in speech-out dialogue systems are emerging as a powerful\nalternative to traditional ASR-LLM-TTS pipelines, generating more natural,\nexpressive responses with significantly lower latency. However, these systems\nremain prone to hallucinations due to limited factual grounding. While\ntext-based dialogue systems address this challenge by integrating tools such as\nweb search and knowledge graph APIs, we introduce the first approach to extend\ntool use directly into speech-in speech-out systems. A key challenge is that\ntool integration substantially increases response latency, disrupting\nconversational flow. To mitigate this, we propose Streaming Retrieval-Augmented\nGeneration (Streaming RAG), a novel framework that reduces user-perceived\nlatency by predicting tool queries in parallel with user speech, even before\nthe user finishes speaking. Specifically, we develop a post-training pipeline\nthat teaches the model when to issue tool calls during ongoing speech and how\nto generate spoken summaries that fuse audio queries with retrieved text\nresults, thereby improving both accuracy and responsiveness. To evaluate our\napproach, we construct AudioCRAG, a benchmark created by converting queries\nfrom the publicly available CRAG dataset into speech form. Experimental results\ndemonstrate that our streaming RAG approach increases QA accuracy by up to 200%\nrelative (from 11.1% to 34.2% absolute) and further enhances user experience by\nreducing tool use latency by 20%. Importantly, our streaming RAG approach is\nmodality-agnostic and can be applied equally to typed input, paving the way for\nmore agentic, real-time AI assistants."
                },
                "authors": [
                    {
                        "name": "Siddhant Arora"
                    },
                    {
                        "name": "Haidar Khan"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Xin Luna Dong"
                    },
                    {
                        "name": "Sajal Choudhary"
                    },
                    {
                        "name": "Seungwhan Moon"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Adithya Sagar"
                    },
                    {
                        "name": "Surya Teja Appini"
                    },
                    {
                        "name": "Kaushik Patnaik"
                    },
                    {
                        "name": "Sanat Sharma"
                    },
                    {
                        "name": "Shinji Watanabe"
                    },
                    {
                        "name": "Anuj Kumar"
                    },
                    {
                        "name": "Ahmed Aly"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Florian Metze"
                    },
                    {
                        "name": "Zhaojiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhaojiang Lin"
                },
                "author": "Zhaojiang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00133v2",
                "updated": "2025-10-02T14:15:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    15,
                    41,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-30T18:11:13Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    18,
                    11,
                    13,
                    1,
                    273,
                    0
                ],
                "title": "Large Language Models Inference Engines based on Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Inference Engines based on Spiking Neural Networks"
                },
                "summary": "Foundational models based on the transformer architecture are currently the\nstate-of-the-art in general language modeling, as well as in scientific areas\nsuch as material science and climate. However, training and deploying these\nmodels is computationally challenging as the time and space complexity has a\nquadratic relation to the input sequence length. Several efforts exploring\nefficient computational paradigms and model architectures to address these\nlimitations have been made. In this work, we explore spiking neural networks\n(SNNs) to design transformer models. A challenge in training large-scale SNNs,\nusing existing surrogate learning methods is inefficient and time-consuming. On\nthe other hand, techniques to convert existing transformer-based models to\ntheir SNN equivalent are not scalable, as achieving optimal performance comes\nat the cost of a large number of spike time-steps, i.e. increased latency. To\naddress this, we propose NeurTransformer, a methodology for designing\ntransformer-based SNN for inference using a supervised fine-tuning approach\nwith existing conversion methods. The proposed methodology works by: (1)\nreplacing the self-attention mechanism with a spike-based self-attention (SSA),\n(2) converting the feed-forward block of the trained transformer model to its\nequivalent SNN, and (3) fine-tuning the SSA block using SNN-based surrogate\nlearning algorithms. We benchmark the proposed methodology and demonstrate its\naccuracy and scalability using three variants of the GPT-2 model of increasing\nmodel size. We observe that the converted GPT-2 small models demonstrate a\n5-12% loss in cosine similarity and a 9.7% reduction in perplexity. Finally, we\ndemonstrate the energy efficiency of the SSA block compared to the ASA block\nand show between 64.71% and 85.28% reductions in estimated energy consumption\nwhen implementing the self-attention mechanism on a digital hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundational models based on the transformer architecture are currently the\nstate-of-the-art in general language modeling, as well as in scientific areas\nsuch as material science and climate. However, training and deploying these\nmodels is computationally challenging as the time and space complexity has a\nquadratic relation to the input sequence length. Several efforts exploring\nefficient computational paradigms and model architectures to address these\nlimitations have been made. In this work, we explore spiking neural networks\n(SNNs) to design transformer models. A challenge in training large-scale SNNs,\nusing existing surrogate learning methods is inefficient and time-consuming. On\nthe other hand, techniques to convert existing transformer-based models to\ntheir SNN equivalent are not scalable, as achieving optimal performance comes\nat the cost of a large number of spike time-steps, i.e. increased latency. To\naddress this, we propose NeurTransformer, a methodology for designing\ntransformer-based SNN for inference using a supervised fine-tuning approach\nwith existing conversion methods. The proposed methodology works by: (1)\nreplacing the self-attention mechanism with a spike-based self-attention (SSA),\n(2) converting the feed-forward block of the trained transformer model to its\nequivalent SNN, and (3) fine-tuning the SSA block using SNN-based surrogate\nlearning algorithms. We benchmark the proposed methodology and demonstrate its\naccuracy and scalability using three variants of the GPT-2 model of increasing\nmodel size. We observe that the converted GPT-2 small models demonstrate a\n5-12% loss in cosine similarity and a 9.7% reduction in perplexity. Finally, we\ndemonstrate the energy efficiency of the SSA block compared to the ASA block\nand show between 64.71% and 85.28% reductions in estimated energy consumption\nwhen implementing the self-attention mechanism on a digital hardware."
                },
                "authors": [
                    {
                        "name": "Adarsha Balaji"
                    },
                    {
                        "name": "Sandeep Madireddy"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep Madireddy"
                },
                "author": "Sandeep Madireddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05735v3",
                "updated": "2025-10-02T14:15:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    15,
                    9,
                    3,
                    275,
                    0
                ],
                "published": "2025-06-06T04:35:19Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    4,
                    35,
                    19,
                    4,
                    157,
                    0
                ],
                "title": "Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation\n  and Confidence Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation\n  and Confidence Awareness"
                },
                "summary": "Machine unlearning techniques aim to mitigate unintended memorization in\nlarge language models (LLMs). However, existing approaches predominantly focus\non the explicit removal of isolated facts, often overlooking latent inferential\ndependencies and the non-deterministic nature of knowledge within LLMs.\nConsequently, facts presumed forgotten may persist implicitly through\ncorrelated information. To address these challenges, we propose a knowledge\nunlearning evaluation framework that more accurately captures the implicit\nstructure of real-world knowledge by representing relevant factual contexts as\nknowledge graphs with associated confidence scores. We further develop an\ninference-based evaluation protocol leveraging powerful LLMs as judges; these\njudges reason over the extracted knowledge subgraph to determine unlearning\nsuccess. Our LLM judges utilize carefully designed prompts and are calibrated\nagainst human evaluations to ensure their trustworthiness and stability.\nExtensive experiments on our newly constructed benchmark demonstrate that our\nframework provides a more realistic and rigorous assessment of unlearning\nperformance. Moreover, our findings reveal that current evaluation strategies\ntend to overestimate unlearning effectiveness. Our code is publicly available\nat https://github.com/Graph-COM/Knowledge_Unlearning.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning techniques aim to mitigate unintended memorization in\nlarge language models (LLMs). However, existing approaches predominantly focus\non the explicit removal of isolated facts, often overlooking latent inferential\ndependencies and the non-deterministic nature of knowledge within LLMs.\nConsequently, facts presumed forgotten may persist implicitly through\ncorrelated information. To address these challenges, we propose a knowledge\nunlearning evaluation framework that more accurately captures the implicit\nstructure of real-world knowledge by representing relevant factual contexts as\nknowledge graphs with associated confidence scores. We further develop an\ninference-based evaluation protocol leveraging powerful LLMs as judges; these\njudges reason over the extracted knowledge subgraph to determine unlearning\nsuccess. Our LLM judges utilize carefully designed prompts and are calibrated\nagainst human evaluations to ensure their trustworthiness and stability.\nExtensive experiments on our newly constructed benchmark demonstrate that our\nframework provides a more realistic and rigorous assessment of unlearning\nperformance. Moreover, our findings reveal that current evaluation strategies\ntend to overestimate unlearning effectiveness. Our code is publicly available\nat https://github.com/Graph-COM/Knowledge_Unlearning.git."
                },
                "authors": [
                    {
                        "name": "Rongzhe Wei"
                    },
                    {
                        "name": "Peizhi Niu"
                    },
                    {
                        "name": "Hans Hao-Hsun Hsu"
                    },
                    {
                        "name": "Ruihan Wu"
                    },
                    {
                        "name": "Haoteng Yin"
                    },
                    {
                        "name": "Mohsen Ghassemi"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Vamsi K. Potluru"
                    },
                    {
                        "name": "Eli Chien"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    },
                    {
                        "name": "Olgica Milenkovic"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17674v2",
                "updated": "2025-10-02T14:13:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    13,
                    37,
                    3,
                    275,
                    0
                ],
                "published": "2024-11-26T18:35:24Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    35,
                    24,
                    1,
                    331,
                    0
                ],
                "title": "Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with\n  Receptive-Field-Aware Attention Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with\n  Receptive-Field-Aware Attention Weighting"
                },
                "summary": "Understanding the emotions in a dialogue usually requires external knowledge\nto accurately understand the contents. As the LLMs become more and more\npowerful, we do not want to settle on the limited ability of the pre-trained\nlanguage model. However, the LLMs either can only process text modality or are\ntoo expensive to process the multimedia information. We aim to utilize both the\npower of LLMs and the supplementary features from the multimedia modalities. In\nthis paper, we present a framework, Lantern, that can improve the performance\nof a certain vanilla model by prompting large language models with\nreceptive-field-aware attention weighting. This framework trained a multi-task\nvanilla model to produce probabilities of emotion classes and dimension scores.\nThese predictions are fed into the LLMs as references to adjust the predicted\nprobabilities of each emotion class with its external knowledge and contextual\nunderstanding. We slice the dialogue into different receptive fields, and each\nsample is included in exactly t receptive fields. Finally, the predictions of\nLLMs are merged with a receptive-field-aware attention-driven weighting module.\nIn the experiments, vanilla models CORECT and SDT are deployed in Lantern with\nGPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way\nsettings demonstrated that the Lantern can significantly improve the\nperformance of current vanilla models by up to 1.23% and 1.80%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the emotions in a dialogue usually requires external knowledge\nto accurately understand the contents. As the LLMs become more and more\npowerful, we do not want to settle on the limited ability of the pre-trained\nlanguage model. However, the LLMs either can only process text modality or are\ntoo expensive to process the multimedia information. We aim to utilize both the\npower of LLMs and the supplementary features from the multimedia modalities. In\nthis paper, we present a framework, Lantern, that can improve the performance\nof a certain vanilla model by prompting large language models with\nreceptive-field-aware attention weighting. This framework trained a multi-task\nvanilla model to produce probabilities of emotion classes and dimension scores.\nThese predictions are fed into the LLMs as references to adjust the predicted\nprobabilities of each emotion class with its external knowledge and contextual\nunderstanding. We slice the dialogue into different receptive fields, and each\nsample is included in exactly t receptive fields. Finally, the predictions of\nLLMs are merged with a receptive-field-aware attention-driven weighting module.\nIn the experiments, vanilla models CORECT and SDT are deployed in Lantern with\nGPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way\nsettings demonstrated that the Lantern can significantly improve the\nperformance of current vanilla models by up to 1.23% and 1.80%."
                },
                "authors": [
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Liyun Zhang"
                    },
                    {
                        "name": "Dian Ding"
                    },
                    {
                        "name": "Dinghua Zhao"
                    },
                    {
                        "name": "Yi-Chao Chen"
                    },
                    {
                        "name": "Ye Wu"
                    },
                    {
                        "name": "Guangtao Xue"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Xue"
                },
                "author": "Guangtao Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12960v2",
                "updated": "2025-10-02T14:09:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    9,
                    44,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-16T11:06:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    6,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Investigating ReLoRA: Effects on the Learning Dynamics of Small Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating ReLoRA: Effects on the Learning Dynamics of Small Language\n  Models"
                },
                "summary": "Parameter-efficient methods like LoRA have revolutionised large language\nmodel (LLM) fine-tuning. ReLoRA extends this idea to pretraining by repeatedly\nmerging and reinitialising low-rank adapters, increasing cumulative rank while\nkeeping updates cheap. This aligns well with observations that high-capacity\nmodels learn through locally low-rank trajectories that expand over time. By\ncontrast, recent work suggests that small language models (SLMs) exhibit rank\ndeficiencies and under-utilise their available dimensionality. This raises a\nnatural question: can ReLoRA's rank-expanding update rule \\textit{steer} SLMs\ntoward healthier learning dynamics, mitigating rank bottlenecks in a\ncapacity-constrained regime? We argue SLMs are an ideal testbed: they train\nquickly, enable controlled ablations, and make rank phenomena more measurable.\nWe present the first systematic study of ReLoRA in SLMs (11M-66M parameters),\nevaluating both performance and learning dynamics. Across loss, Paloma\nperplexity, and BLiMP, we find that ReLoRA underperforms full-rank training,\nwith gaps widening at larger scales. Analysis of proportional effective rank\nand condition numbers shows that ReLoRA amplifies existing rank deficiencies\nand induces ill-conditioned updates early in training. Our results suggest that\nwhile ReLoRA's merge-and-restart strategy can expand ranks in larger models, it\ndoes not straightforwardly translate to capacity-limited SLMs, motivating\nadaptive-rank or hybrid-rank approaches for low-compute pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient methods like LoRA have revolutionised large language\nmodel (LLM) fine-tuning. ReLoRA extends this idea to pretraining by repeatedly\nmerging and reinitialising low-rank adapters, increasing cumulative rank while\nkeeping updates cheap. This aligns well with observations that high-capacity\nmodels learn through locally low-rank trajectories that expand over time. By\ncontrast, recent work suggests that small language models (SLMs) exhibit rank\ndeficiencies and under-utilise their available dimensionality. This raises a\nnatural question: can ReLoRA's rank-expanding update rule \\textit{steer} SLMs\ntoward healthier learning dynamics, mitigating rank bottlenecks in a\ncapacity-constrained regime? We argue SLMs are an ideal testbed: they train\nquickly, enable controlled ablations, and make rank phenomena more measurable.\nWe present the first systematic study of ReLoRA in SLMs (11M-66M parameters),\nevaluating both performance and learning dynamics. Across loss, Paloma\nperplexity, and BLiMP, we find that ReLoRA underperforms full-rank training,\nwith gaps widening at larger scales. Analysis of proportional effective rank\nand condition numbers shows that ReLoRA amplifies existing rank deficiencies\nand induces ill-conditioned updates early in training. Our results suggest that\nwhile ReLoRA's merge-and-restart strategy can expand ranks in larger models, it\ndoes not straightforwardly translate to capacity-limited SLMs, motivating\nadaptive-rank or hybrid-rank approaches for low-compute pretraining."
                },
                "authors": [
                    {
                        "name": "Yuval Weiss"
                    },
                    {
                        "name": "David Demitri Africa"
                    },
                    {
                        "name": "Paula Buttery"
                    },
                    {
                        "name": "Richard Diehl Martinez"
                    }
                ],
                "author_detail": {
                    "name": "Richard Diehl Martinez"
                },
                "author": "Richard Diehl Martinez",
                "arxiv_comment": "12 Pages, 6 Tables, 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v3",
                "updated": "2025-10-02T14:09:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    9,
                    3,
                    3,
                    275,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "arxiv_journal_ref": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10155v2",
                "updated": "2025-10-02T14:05:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    5,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-14T11:10:02Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    10,
                    2,
                    0,
                    195,
                    0
                ],
                "title": "Flexible Feature Distillation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Feature Distillation for Large Language Models"
                },
                "summary": "Knowledge distillation (KD) has become a cornerstone for compressing large\nlanguage models (LLMs). However, existing LLM-KD methods have primarily focused\non logit-based approaches, which achieve good performance but overlook the rich\ninternal representations of LLMs. Feature-level KD could leverage this\nstructure to provide complementary benefits, yet it remains underexplored\nbecause current feature-KD approaches typically assume identical\nteacher-student hidden sizes, a restrictive and unrealistic assumption. A\ncommon workaround is to train a linear projector to align their feature spaces;\nhowever, this introduces additional parameters, distorts teacher embeddings,\nand often degrades downstream performance, especially in generative tasks. We\npropose Flex-KD, a parameter-free framework for task-driven feature\ndistillation for LLMs. Instead of projecting the entire teacher representation,\nFlex-KD uses gradient-based scores to identify the most task-relevant\ndimensions of the teacher's hidden states and distills only this subspace into\nthe student. This ensures that the student's limited capacity is allocated to\ninformative components, while avoiding projector-induced distortion and extra\nparameters. Flex-KD integrates seamlessly with existing KD pipelines and\nsupports differing teacher-student hidden sizes. Extensive experiments across\nboth classification and generative tasks, i.e., instruction-following and\nsummarization, show that Flex-KD consistently boosts student performance,\nachieving up to a 3.75 percent performance gain over the linear projection\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) has become a cornerstone for compressing large\nlanguage models (LLMs). However, existing LLM-KD methods have primarily focused\non logit-based approaches, which achieve good performance but overlook the rich\ninternal representations of LLMs. Feature-level KD could leverage this\nstructure to provide complementary benefits, yet it remains underexplored\nbecause current feature-KD approaches typically assume identical\nteacher-student hidden sizes, a restrictive and unrealistic assumption. A\ncommon workaround is to train a linear projector to align their feature spaces;\nhowever, this introduces additional parameters, distorts teacher embeddings,\nand often degrades downstream performance, especially in generative tasks. We\npropose Flex-KD, a parameter-free framework for task-driven feature\ndistillation for LLMs. Instead of projecting the entire teacher representation,\nFlex-KD uses gradient-based scores to identify the most task-relevant\ndimensions of the teacher's hidden states and distills only this subspace into\nthe student. This ensures that the student's limited capacity is allocated to\ninformative components, while avoiding projector-induced distortion and extra\nparameters. Flex-KD integrates seamlessly with existing KD pipelines and\nsupports differing teacher-student hidden sizes. Extensive experiments across\nboth classification and generative tasks, i.e., instruction-following and\nsummarization, show that Flex-KD consistently boosts student performance,\nachieving up to a 3.75 percent performance gain over the linear projection\nbaseline."
                },
                "authors": [
                    {
                        "name": "Khouloud Saadi"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18536v2",
                "updated": "2025-10-02T14:04:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    4,
                    34,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-24T16:03:29Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    3,
                    29,
                    3,
                    205,
                    0
                ],
                "title": "Is the $3S$-$2D$ mixing strong for the charmonia $ψ(4040)$ and\n  $ψ(4160)$?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is the $3S$-$2D$ mixing strong for the charmonia $ψ(4040)$ and\n  $ψ(4160)$?"
                },
                "summary": "In this work, we revisit the $3S$-$2D$ mixing scheme for the charmonia\n$\\psi(4040)$ and $\\psi(4160)$. We introduce a coupled-channel\nmechanism-distinct from the tensor-force contribution in potential models,\nwhich alone is insufficient to induce significant mixing-to describe the mixing\nbetween these states. Our analysis yields mixing angles of $\\theta_1=7^\\circ$\nand $\\theta_2=10^\\circ$, inconsistent with the larger angle inferred from\nexperimental data, such as the di-lectronic widths of the $\\psi(4040)$ and\n$\\psi(4160)$. We discuss possible origins of this discrepancy and emphasize the\nneed for future experiments to resolve it. Precise measurements of the\nresonance parameters and di-lectronic decay widths, via both inclusive and\nexclusive processes, will be crucial in clarifying this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we revisit the $3S$-$2D$ mixing scheme for the charmonia\n$\\psi(4040)$ and $\\psi(4160)$. We introduce a coupled-channel\nmechanism-distinct from the tensor-force contribution in potential models,\nwhich alone is insufficient to induce significant mixing-to describe the mixing\nbetween these states. Our analysis yields mixing angles of $\\theta_1=7^\\circ$\nand $\\theta_2=10^\\circ$, inconsistent with the larger angle inferred from\nexperimental data, such as the di-lectronic widths of the $\\psi(4040)$ and\n$\\psi(4160)$. We discuss possible origins of this discrepancy and emphasize the\nneed for future experiments to resolve it. Precise measurements of the\nresonance parameters and di-lectronic decay widths, via both inclusive and\nexclusive processes, will be crucial in clarifying this issue."
                },
                "authors": [
                    {
                        "name": "Zi-Long Man"
                    },
                    {
                        "name": "Si-Qiang Luo"
                    },
                    {
                        "name": "Xiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Liu"
                },
                "author": "Xiang Liu",
                "arxiv_comment": "7 pages, 4 table, 1 figure, accepted by Phys. Rev. D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23340v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23340v3",
                "updated": "2025-10-02T14:03:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    3,
                    57,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-27T14:42:48Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    14,
                    42,
                    48,
                    5,
                    270,
                    0
                ],
                "title": "CrediBench: Building Web-Scale Network Datasets for Information\n  Integrity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrediBench: Building Web-Scale Network Datasets for Information\n  Integrity"
                },
                "summary": "Online misinformation poses an escalating threat, amplified by the Internet's\nopen nature and increasingly capable LLMs that generate persuasive yet\ndeceptive content. Existing misinformation detection methods typically focus on\neither textual content or network structure in isolation, failing to leverage\nthe rich, dynamic interplay between website content and hyperlink relationships\nthat characterizes real-world misinformation ecosystems. We introduce\nCrediBench: a large-scale data processing pipeline for constructing temporal\nweb graphs that jointly model textual content and hyperlink structure for\nmisinformation detection. Unlike prior work, our approach captures the dynamic\nevolution of general misinformation domains, including changes in both content\nand inter-site references over time. Our processed one-month snapshot extracted\nfrom the Common Crawl archive in December 2024 contains 45 million nodes and 1\nbillion edges, representing the largest web graph dataset made publicly\navailable for misinformation research to date. From our experiments on this\ngraph snapshot, we demonstrate the strength of both structural and webpage\ncontent signals for learning credibility scores, which measure source\nreliability. The pipeline and experimentation code are all available here, and\nthe dataset is in this folder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online misinformation poses an escalating threat, amplified by the Internet's\nopen nature and increasingly capable LLMs that generate persuasive yet\ndeceptive content. Existing misinformation detection methods typically focus on\neither textual content or network structure in isolation, failing to leverage\nthe rich, dynamic interplay between website content and hyperlink relationships\nthat characterizes real-world misinformation ecosystems. We introduce\nCrediBench: a large-scale data processing pipeline for constructing temporal\nweb graphs that jointly model textual content and hyperlink structure for\nmisinformation detection. Unlike prior work, our approach captures the dynamic\nevolution of general misinformation domains, including changes in both content\nand inter-site references over time. Our processed one-month snapshot extracted\nfrom the Common Crawl archive in December 2024 contains 45 million nodes and 1\nbillion edges, representing the largest web graph dataset made publicly\navailable for misinformation research to date. From our experiments on this\ngraph snapshot, we demonstrate the strength of both structural and webpage\ncontent signals for learning credibility scores, which measure source\nreliability. The pipeline and experimentation code are all available here, and\nthe dataset is in this folder."
                },
                "authors": [
                    {
                        "name": "Emma Kondrup"
                    },
                    {
                        "name": "Sebastian Sabry"
                    },
                    {
                        "name": "Hussein Abdallah"
                    },
                    {
                        "name": "Zachary Yang"
                    },
                    {
                        "name": "James Zhou"
                    },
                    {
                        "name": "Kellin Pelrine"
                    },
                    {
                        "name": "Jean-François Godbout"
                    },
                    {
                        "name": "Michael M. Bronstein"
                    },
                    {
                        "name": "Reihaneh Rabbany"
                    },
                    {
                        "name": "Shenyang Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shenyang Huang"
                },
                "author": "Shenyang Huang",
                "arxiv_comment": "16 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23340v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23340v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21315v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21315v3",
                "updated": "2025-10-02T14:01:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    1,
                    14,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-27T15:13:08Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    13,
                    8,
                    1,
                    147,
                    0
                ],
                "title": "Charting the Landscape of African NLP: Mapping Progress and Shaping the\n  Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the Landscape of African NLP: Mapping Progress and Shaping the\n  Road Ahead"
                },
                "summary": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 884\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 884\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages."
                },
                "authors": [
                    {
                        "name": "Jesujoba O. Alabi"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Dietrich Klakow"
                    }
                ],
                "author_detail": {
                    "name": "Dietrich Klakow"
                },
                "author": "Dietrich Klakow",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21315v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25264v2",
                "updated": "2025-10-02T13:58:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    58,
                    56,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-28T04:50:48Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    4,
                    50,
                    48,
                    6,
                    271,
                    0
                ],
                "title": "GeoSQL-Eval: First Evaluation of LLMs on PostGIS-Based NL2GeoSQL Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoSQL-Eval: First Evaluation of LLMs on PostGIS-Based NL2GeoSQL Queries"
                },
                "summary": "Large language models (LLMs) have shown strong performance in natural\nlanguage to SQL (NL2SQL) tasks within general databases. However, extending to\nGeoSQL introduces additional complexity from spatial data types, function\ninvocation, and coordinate systems, which greatly increases generation and\nexecution difficulty. Existing benchmarks mainly target general SQL, and a\nsystematic evaluation framework for GeoSQL is still lacking. To fill this gap,\nwe present GeoSQL-Eval, the first end-to-end automated evaluation framework for\nPostGIS query generation, together with GeoSQL-Bench, a benchmark for assessing\nLLM performance in NL2GeoSQL tasks. GeoSQL-Bench defines three task\ncategories-conceptual understanding, syntax-level SQL generation, and schema\nretrieval-comprising 14,178 instances, 340 PostGIS functions, and 82 thematic\ndatabases. GeoSQL-Eval is grounded in Webb's Depth of Knowledge (DOK) model,\ncovering four cognitive dimensions, five capability levels, and twenty task\ntypes to establish a comprehensive process from knowledge acquisition and\nsyntax generation to semantic alignment, execution accuracy, and robustness. We\nevaluate 24 representative models across six categories and apply the entropy\nweight method with statistical analyses to uncover performance differences,\ncommon error patterns, and resource usage. Finally, we release a public\nGeoSQL-Eval leaderboard platform for continuous testing and global comparison.\nThis work extends the NL2GeoSQL paradigm and provides a standardized,\ninterpretable, and extensible framework for evaluating LLMs in spatial database\ncontexts, offering valuable references for geospatial information science and\nrelated applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong performance in natural\nlanguage to SQL (NL2SQL) tasks within general databases. However, extending to\nGeoSQL introduces additional complexity from spatial data types, function\ninvocation, and coordinate systems, which greatly increases generation and\nexecution difficulty. Existing benchmarks mainly target general SQL, and a\nsystematic evaluation framework for GeoSQL is still lacking. To fill this gap,\nwe present GeoSQL-Eval, the first end-to-end automated evaluation framework for\nPostGIS query generation, together with GeoSQL-Bench, a benchmark for assessing\nLLM performance in NL2GeoSQL tasks. GeoSQL-Bench defines three task\ncategories-conceptual understanding, syntax-level SQL generation, and schema\nretrieval-comprising 14,178 instances, 340 PostGIS functions, and 82 thematic\ndatabases. GeoSQL-Eval is grounded in Webb's Depth of Knowledge (DOK) model,\ncovering four cognitive dimensions, five capability levels, and twenty task\ntypes to establish a comprehensive process from knowledge acquisition and\nsyntax generation to semantic alignment, execution accuracy, and robustness. We\nevaluate 24 representative models across six categories and apply the entropy\nweight method with statistical analyses to uncover performance differences,\ncommon error patterns, and resource usage. Finally, we release a public\nGeoSQL-Eval leaderboard platform for continuous testing and global comparison.\nThis work extends the NL2GeoSQL paradigm and provides a standardized,\ninterpretable, and extensible framework for evaluating LLMs in spatial database\ncontexts, offering valuable references for geospatial information science and\nrelated applications."
                },
                "authors": [
                    {
                        "name": "Shuyang Hou"
                    },
                    {
                        "name": "Haoyue Jiao"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Lutong Xie"
                    },
                    {
                        "name": "Guanyu Chen"
                    },
                    {
                        "name": "Shaowen Wu"
                    },
                    {
                        "name": "Xuefeng Guan"
                    },
                    {
                        "name": "Huayi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Huayi Wu"
                },
                "author": "Huayi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02025v1",
                "updated": "2025-10-02T13:57:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    57,
                    14,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T13:57:14Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    57,
                    14,
                    3,
                    275,
                    0
                ],
                "title": "Style Over Story: A Process-Oriented Study of Authorial Creativity in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style Over Story: A Process-Oriented Study of Authorial Creativity in\n  Large Language Models"
                },
                "summary": "Evaluations of large language models (LLMs)' creativity have focused\nprimarily on the quality of their outputs rather than the processes that shape\nthem. This study takes a process-oriented approach, drawing on narratology to\nexamine LLMs as computational authors. We introduce constraint-based\ndecision-making as a lens for authorial creativity. Using controlled prompting\nto assign authorial personas, we analyze the creative preferences of the\nmodels. Our findings show that LLMs consistently emphasize Style over other\nelements, including Character, Event, and Setting. By also probing the\nreasoning the models provide for their choices, we show that distinctive\nprofiles emerge across models and argue that our approach provides a novel\nsystematic tool for analyzing AI's authorial creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluations of large language models (LLMs)' creativity have focused\nprimarily on the quality of their outputs rather than the processes that shape\nthem. This study takes a process-oriented approach, drawing on narratology to\nexamine LLMs as computational authors. We introduce constraint-based\ndecision-making as a lens for authorial creativity. Using controlled prompting\nto assign authorial personas, we analyze the creative preferences of the\nmodels. Our findings show that LLMs consistently emphasize Style over other\nelements, including Character, Event, and Setting. By also probing the\nreasoning the models provide for their choices, we show that distinctive\nprofiles emerge across models and argue that our approach provides a novel\nsystematic tool for analyzing AI's authorial creativity."
                },
                "authors": [
                    {
                        "name": "Donghoon Jung"
                    },
                    {
                        "name": "Jiwoo Choi"
                    },
                    {
                        "name": "Songeun Chae"
                    },
                    {
                        "name": "Seohyon Jung"
                    }
                ],
                "author_detail": {
                    "name": "Seohyon Jung"
                },
                "author": "Seohyon Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00015v2",
                "updated": "2025-10-02T13:50:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    50,
                    5,
                    3,
                    275,
                    0
                ],
                "published": "2025-04-23T04:52:26Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    52,
                    26,
                    2,
                    113,
                    0
                ],
                "title": "Design and Application of Multimodal Large Language Model Based System\n  for End to End Automation of Accident Dataset Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Application of Multimodal Large Language Model Based System\n  for End to End Automation of Accident Dataset Generation"
                },
                "summary": "Road traffic accidents remain a major public safety and socio-economic issue\nin developing countries like Bangladesh. Existing accident data collection is\nlargely manual, fragmented, and unreliable, resulting in underreporting and\ninconsistent records. This research proposes a fully automated system using\nLarge Language Models (LLMs) and web scraping techniques to address these\nchallenges. The pipeline consists of four components: automated web scraping\ncode generation, news collection from online sources, accident news\nclassification with structured data extraction, and duplicate removal. The\nsystem uses the multimodal generative LLM Gemini-2.0-Flash for seamless\nautomation. The code generation module classifies webpages into pagination,\ndynamic, or infinite scrolling categories and generates suitable Python scripts\nfor scraping. LLMs also classify and extract key accident information such as\ndate, time, location, fatalities, injuries, road type, vehicle types, and\npedestrian involvement. A deduplication algorithm ensures data integrity by\nremoving duplicate reports. The system scraped 14 major Bangladeshi news sites\nover 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news\narticles and identifying 705 unique accidents. The code generation module\nachieved 91.3% calibration and 80% validation accuracy. Chittagong reported the\nhighest number of accidents (80), fatalities (70), and injuries (115), followed\nby Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning\n(8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also\ndeveloped with usage instructions. This study demonstrates the viability of an\nLLM-powered, scalable system for accurate, low-effort accident data collection,\nproviding a foundation for data-driven road safety policymaking in Bangladesh.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Road traffic accidents remain a major public safety and socio-economic issue\nin developing countries like Bangladesh. Existing accident data collection is\nlargely manual, fragmented, and unreliable, resulting in underreporting and\ninconsistent records. This research proposes a fully automated system using\nLarge Language Models (LLMs) and web scraping techniques to address these\nchallenges. The pipeline consists of four components: automated web scraping\ncode generation, news collection from online sources, accident news\nclassification with structured data extraction, and duplicate removal. The\nsystem uses the multimodal generative LLM Gemini-2.0-Flash for seamless\nautomation. The code generation module classifies webpages into pagination,\ndynamic, or infinite scrolling categories and generates suitable Python scripts\nfor scraping. LLMs also classify and extract key accident information such as\ndate, time, location, fatalities, injuries, road type, vehicle types, and\npedestrian involvement. A deduplication algorithm ensures data integrity by\nremoving duplicate reports. The system scraped 14 major Bangladeshi news sites\nover 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news\narticles and identifying 705 unique accidents. The code generation module\nachieved 91.3% calibration and 80% validation accuracy. Chittagong reported the\nhighest number of accidents (80), fatalities (70), and injuries (115), followed\nby Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning\n(8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also\ndeveloped with usage instructions. This study demonstrates the viability of an\nLLM-powered, scalable system for accurate, low-effort accident data collection,\nproviding a foundation for data-driven road safety policymaking in Bangladesh."
                },
                "authors": [
                    {
                        "name": "MD Thamed Bin Zaman Chowdhury"
                    },
                    {
                        "name": "Moazzem Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Moazzem Hossain"
                },
                "author": "Moazzem Hossain",
                "arxiv_comment": "This paper is accepted for presentation in TRB annual meeting 2026.\n  The version presented here is the preprint version before peer review process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01998v1",
                "updated": "2025-10-02T13:19:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    19,
                    1,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T13:19:01Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    19,
                    1,
                    3,
                    275,
                    0
                ],
                "title": "Erdos-Turan photonic Ising machines with record-high coupling resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Erdos-Turan photonic Ising machines with record-high coupling resolution"
                },
                "summary": "Ising machines have emerged as promising platforms for efficiently tackling a\nwide range of combinatorial optimization problems relevant to resource\nallocation, statistical inference and deep learning, yet their practical\nutility is fundamentally constrained by the coarse resolution of spin-spin\ncouplings (Jij). Current implementations, relying on direct modulation of\nphysical parameters, achieve at most 256 discrete coupling levels, which\nseverely hinder the faithfully modeling of arbitrary real-valued interactions\nin realistic applications. Here we present a novel photonic Ising machine that\nencodes spins in random lattices while programming couplings in the momentum\nspace of light. By introducing the Sidon set-a mathematical structure ensuring\npairwise difference uniqueness - and employing the Erdos-Turan bound, we\nestablish an optical framework in which each spin pair can be assigned a unique\nJij. This approach decouples the resolution limit from hardware modulation to\nthe spatial precision in the momentum space of light. Experimentally, we\ndemonstrate a record-high coupling resolution of 7,038 on a simple photonic\nplatform, surpassing previous Ising machines. Our results highlight the power\nof uniting discrete mathematics with momentum-space photonics, paving the way\ntoward scalable Ising machines capable of faithfully modeling real-world\noptimization problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ising machines have emerged as promising platforms for efficiently tackling a\nwide range of combinatorial optimization problems relevant to resource\nallocation, statistical inference and deep learning, yet their practical\nutility is fundamentally constrained by the coarse resolution of spin-spin\ncouplings (Jij). Current implementations, relying on direct modulation of\nphysical parameters, achieve at most 256 discrete coupling levels, which\nseverely hinder the faithfully modeling of arbitrary real-valued interactions\nin realistic applications. Here we present a novel photonic Ising machine that\nencodes spins in random lattices while programming couplings in the momentum\nspace of light. By introducing the Sidon set-a mathematical structure ensuring\npairwise difference uniqueness - and employing the Erdos-Turan bound, we\nestablish an optical framework in which each spin pair can be assigned a unique\nJij. This approach decouples the resolution limit from hardware modulation to\nthe spatial precision in the momentum space of light. Experimentally, we\ndemonstrate a record-high coupling resolution of 7,038 on a simple photonic\nplatform, surpassing previous Ising machines. Our results highlight the power\nof uniting discrete mathematics with momentum-space photonics, paving the way\ntoward scalable Ising machines capable of faithfully modeling real-world\noptimization problems."
                },
                "authors": [
                    {
                        "name": "Huaqiang Li"
                    },
                    {
                        "name": "Guangfeng Wang"
                    },
                    {
                        "name": "Erez Hasman"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Xianfeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianfeng Chen"
                },
                "author": "Xianfeng Chen",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01995v1",
                "updated": "2025-10-02T13:17:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    17,
                    11,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T13:17:11Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    17,
                    11,
                    3,
                    275,
                    0
                ],
                "title": "LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and\n  Target",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and\n  Target"
                },
                "summary": "Online social media platforms are central to everyday communication and\ninformation seeking. While these platforms serve positive purposes, they also\nprovide fertile ground for the spread of hate speech, offensive language, and\nbullying content targeting individuals, organizations, and communities. Such\ncontent undermines safety, participation, and equity online. Reliable detection\nsystems are therefore needed, especially for low-resource languages where\nmoderation tools are limited. In Bangla, prior work has contributed resources\nand models, but most are single-task (e.g., binary hate/offense) with limited\ncoverage of multi-facet signals (type, severity, target). We address these gaps\nby introducing the first multi-task Bangla hate-speech dataset,\nBanglaMultiHate, one of the largest manually annotated corpus to date. Building\non this resource, we conduct a comprehensive, controlled comparison spanning\nclassical baselines, monolingual pretrained models, and LLMs under zero-shot\nprompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a\nlow-resource setting and reveal a consistent trend: although LoRA-tuned LLMs\nare competitive with BanglaBERT, culturally and linguistically grounded\npretraining remains critical for robust performance. Together, our dataset and\nfindings establish a stronger benchmark for developing culturally aligned\nmoderation tools in low-resource contexts. For reproducibility, we will release\nthe dataset and all related scripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online social media platforms are central to everyday communication and\ninformation seeking. While these platforms serve positive purposes, they also\nprovide fertile ground for the spread of hate speech, offensive language, and\nbullying content targeting individuals, organizations, and communities. Such\ncontent undermines safety, participation, and equity online. Reliable detection\nsystems are therefore needed, especially for low-resource languages where\nmoderation tools are limited. In Bangla, prior work has contributed resources\nand models, but most are single-task (e.g., binary hate/offense) with limited\ncoverage of multi-facet signals (type, severity, target). We address these gaps\nby introducing the first multi-task Bangla hate-speech dataset,\nBanglaMultiHate, one of the largest manually annotated corpus to date. Building\non this resource, we conduct a comprehensive, controlled comparison spanning\nclassical baselines, monolingual pretrained models, and LLMs under zero-shot\nprompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a\nlow-resource setting and reveal a consistent trend: although LoRA-tuned LLMs\nare competitive with BanglaBERT, culturally and linguistically grounded\npretraining remains critical for robust performance. Together, our dataset and\nfindings establish a stronger benchmark for developing culturally aligned\nmoderation tools in low-resource contexts. For reproducibility, we will release\nthe dataset and all related scripts."
                },
                "authors": [
                    {
                        "name": "Md Arid Hasan"
                    },
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Md Fahad Hossain"
                    },
                    {
                        "name": "Usman Naseem"
                    },
                    {
                        "name": "Syed Ishtiaque Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Syed Ishtiaque Ahmed"
                },
                "author": "Syed Ishtiaque Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01994v1",
                "updated": "2025-10-02T13:15:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    15,
                    40,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T13:15:40Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    15,
                    40,
                    3,
                    275,
                    0
                ],
                "title": "Clarifying Semantics of In-Context Examples for Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clarifying Semantics of In-Context Examples for Unit Test Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled promising\nperformance in unit test generation through in-context learning (ICL). However,\nthe quality of in-context examples significantly influences the effectiveness\nof generated tests-poorly structured or semantically unclear test examples\noften lead to suboptimal outputs. In this paper, we propose CLAST, a novel\ntechnique that systematically refines unit tests to improve their semantic\nclarity, thereby enhancing their utility as in-context examples. The approach\ndecomposes complex tests into logically clearer ones and improves semantic\nclarity through a combination of program analysis and LLM-based rewriting. We\nevaluated CLAST on four open-source and three industrial projects. The results\ndemonstrate that CLAST largely outperforms UTgen, the state-of-the-art\nrefinement technique, in both preserving test effectiveness and enhancing\nsemantic clarity. Specifically, CLAST fully retains the original effectiveness\nof unit tests, while UTgen reduces compilation success rate (CSR), pass rate\n(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,\n35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user\nstudy preferred the semantic clarity of CLAST-refined tests. Notably,\nincorporating CLAST-refined tests as examples effectively improves ICL-based\nunit test generation approaches such as RAGGen and TELPA, resulting in an\naverage increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for\ngenerated tests, compared to incorporating UTgen-refined tests. The insights\nfrom the follow-up user study not only reinforce CLAST's potential impact in\nsoftware testing practice but also illuminate avenues for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled promising\nperformance in unit test generation through in-context learning (ICL). However,\nthe quality of in-context examples significantly influences the effectiveness\nof generated tests-poorly structured or semantically unclear test examples\noften lead to suboptimal outputs. In this paper, we propose CLAST, a novel\ntechnique that systematically refines unit tests to improve their semantic\nclarity, thereby enhancing their utility as in-context examples. The approach\ndecomposes complex tests into logically clearer ones and improves semantic\nclarity through a combination of program analysis and LLM-based rewriting. We\nevaluated CLAST on four open-source and three industrial projects. The results\ndemonstrate that CLAST largely outperforms UTgen, the state-of-the-art\nrefinement technique, in both preserving test effectiveness and enhancing\nsemantic clarity. Specifically, CLAST fully retains the original effectiveness\nof unit tests, while UTgen reduces compilation success rate (CSR), pass rate\n(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,\n35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user\nstudy preferred the semantic clarity of CLAST-refined tests. Notably,\nincorporating CLAST-refined tests as examples effectively improves ICL-based\nunit test generation approaches such as RAGGen and TELPA, resulting in an\naverage increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for\ngenerated tests, compared to incorporating UTgen-refined tests. The insights\nfrom the follow-up user study not only reinforce CLAST's potential impact in\nsoftware testing practice but also illuminate avenues for future research."
                },
                "authors": [
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Lin Yang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Jianyi Zhou"
                    },
                    {
                        "name": "Junjie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Chen"
                },
                "author": "Junjie Chen",
                "arxiv_comment": "accepted in the research track of ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23990v2",
                "updated": "2025-10-02T13:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    15,
                    14,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-28T17:32:52Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    32,
                    52,
                    6,
                    271,
                    0
                ],
                "title": "The Hidden Costs of Translation Accuracy: Distillation, Quantization,\n  and Environmental Impact",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Costs of Translation Accuracy: Distillation, Quantization,\n  and Environmental Impact"
                },
                "summary": "The rapid expansion of large language models (LLMs) has heightened concerns\nabout their computational and environmental costs. This study investigates the\ntrade-offs between translation quality and efficiency by comparing full-scale,\ndistilled, and quantized models using machine translation as a case study. We\nevaluated performance on the Flores+ benchmark and through human judgments of\nconversational translations in French, Hindi, and Kannada. Our analysis\nrevealed that the full 3.3B FP32 model, while achieving the highest BLEU\nscores, incurred the largest environmental footprint (~ 0.007-0.008 kg CO2 per\nrun). The distilled 600M FP32 model reduced inference time by 71-78% and carbon\nemissions by 63-65% compared with the full model, with only minimal reductions\nin BLEU scores. Human evaluations further showed that even aggressive\nquantization (INT4) preserved high levels of accuracy and fluency, with\ndifferences between models generally minor. These findings demonstrate that\nmodel compression strategies can substantially reduce computational demands and\nenvironmental impact while maintaining competitive translation quality, though\ntrade-offs are more pronounced in low-resource settings. We argue for\nevaluation frameworks that integrate efficiency and sustainability alongside\naccuracy as central dimensions of progress in NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of large language models (LLMs) has heightened concerns\nabout their computational and environmental costs. This study investigates the\ntrade-offs between translation quality and efficiency by comparing full-scale,\ndistilled, and quantized models using machine translation as a case study. We\nevaluated performance on the Flores+ benchmark and through human judgments of\nconversational translations in French, Hindi, and Kannada. Our analysis\nrevealed that the full 3.3B FP32 model, while achieving the highest BLEU\nscores, incurred the largest environmental footprint (~ 0.007-0.008 kg CO2 per\nrun). The distilled 600M FP32 model reduced inference time by 71-78% and carbon\nemissions by 63-65% compared with the full model, with only minimal reductions\nin BLEU scores. Human evaluations further showed that even aggressive\nquantization (INT4) preserved high levels of accuracy and fluency, with\ndifferences between models generally minor. These findings demonstrate that\nmodel compression strategies can substantially reduce computational demands and\nenvironmental impact while maintaining competitive translation quality, though\ntrade-offs are more pronounced in low-resource settings. We argue for\nevaluation frameworks that integrate efficiency and sustainability alongside\naccuracy as central dimensions of progress in NLP."
                },
                "authors": [
                    {
                        "name": "Dhaathri Vijay"
                    },
                    {
                        "name": "Anandaswarup Vadapalli"
                    }
                ],
                "author_detail": {
                    "name": "Anandaswarup Vadapalli"
                },
                "author": "Anandaswarup Vadapalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01991v1",
                "updated": "2025-10-02T13:13:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    13,
                    19,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T13:13:19Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    13,
                    19,
                    3,
                    275,
                    0
                ],
                "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing"
                },
                "summary": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges\nwith view, temporal, and non-editing region consistency, as well as with\nhandling complex text instructions. To address these issues, we propose\n4DGS-Craft, a consistent and interactive 4DGS editing framework. We first\nintroduce a 4D-aware InstructPix2Pix model to ensure both view and temporal\nconsistency. This model incorporates 4D VGGT geometry features extracted from\nthe initial scene, enabling it to capture underlying 4D geometric structures\nduring editing. We further enhance this model with a multi-view grid module\nthat enforces consistency by iteratively refining multi-view input images while\njointly optimizing the underlying 4D scene. Furthermore, we preserve the\nconsistency of non-edited regions through a novel Gaussian selection mechanism,\nwhich identifies and optimizes only the Gaussians within the edited regions.\nBeyond consistency, facilitating user interaction is also crucial for effective\n4DGS editing. Therefore, we design an LLM-based module for user intent\nunderstanding. This module employs a user instruction template to define atomic\nediting operations and leverages an LLM for reasoning. As a result, our\nframework can interpret user intent and decompose complex instructions into a\nlogical sequence of atomic operations, enabling it to handle intricate user\ncommands and further enhance editing performance. Compared to related works,\nour approach enables more consistent and controllable 4D scene editing. Our\ncode will be made available upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges\nwith view, temporal, and non-editing region consistency, as well as with\nhandling complex text instructions. To address these issues, we propose\n4DGS-Craft, a consistent and interactive 4DGS editing framework. We first\nintroduce a 4D-aware InstructPix2Pix model to ensure both view and temporal\nconsistency. This model incorporates 4D VGGT geometry features extracted from\nthe initial scene, enabling it to capture underlying 4D geometric structures\nduring editing. We further enhance this model with a multi-view grid module\nthat enforces consistency by iteratively refining multi-view input images while\njointly optimizing the underlying 4D scene. Furthermore, we preserve the\nconsistency of non-edited regions through a novel Gaussian selection mechanism,\nwhich identifies and optimizes only the Gaussians within the edited regions.\nBeyond consistency, facilitating user interaction is also crucial for effective\n4DGS editing. Therefore, we design an LLM-based module for user intent\nunderstanding. This module employs a user instruction template to define atomic\nediting operations and leverages an LLM for reasoning. As a result, our\nframework can interpret user intent and decompose complex instructions into a\nlogical sequence of atomic operations, enabling it to handle intricate user\ncommands and further enhance editing performance. Compared to related works,\nour approach enables more consistent and controllable 4D scene editing. Our\ncode will be made available upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Zhenghao Chen"
                    },
                    {
                        "name": "Dong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Xu"
                },
                "author": "Dong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12864v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12864v4",
                "updated": "2025-10-02T13:09:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    9,
                    42,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-19T08:48:12Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    8,
                    48,
                    12,
                    0,
                    139,
                    0
                ],
                "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams"
                },
                "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. To address this, we\nintroduce \\textsc{LEXam}, a novel benchmark derived from 340 law exams spanning\n116 law school courses across a range of subjects and degree levels. The\ndataset comprises 4,886 law exam questions in English and German, including\n2,841 long-form, open-ended questions and 2,045 multiple-choice questions.\nBesides reference answers, the open questions are also accompanied by explicit\nguidance outlining the expected legal reasoning approach such as issue\nspotting, rule recall, or rule application. Our evaluation on both open-ended\nand multiple-choice questions present significant challenges for current LLMs;\nin particular, they notably struggle with open questions that require\nstructured, multi-step legal reasoning. Moreover, our results underscore the\neffectiveness of the dataset in differentiating between models with varying\ncapabilities. Deploying an ensemble LLM-as-a-Judge paradigm with rigorous human\nexpert validation, we demonstrate how model-generated reasoning steps can be\nevaluated consistently and accurately, closely aligning with human expert\nassessments. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. We have open-sourced our code\non \\href{https://github.com/LEXam-Benchmark/LEXam}{GitHub} and released our\ndata on \\href{https://huggingface.co/datasets/LEXam-Benchmark/LEXam}{Hugging\nFace}. Project page: https://lexam-benchmark.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. To address this, we\nintroduce \\textsc{LEXam}, a novel benchmark derived from 340 law exams spanning\n116 law school courses across a range of subjects and degree levels. The\ndataset comprises 4,886 law exam questions in English and German, including\n2,841 long-form, open-ended questions and 2,045 multiple-choice questions.\nBesides reference answers, the open questions are also accompanied by explicit\nguidance outlining the expected legal reasoning approach such as issue\nspotting, rule recall, or rule application. Our evaluation on both open-ended\nand multiple-choice questions present significant challenges for current LLMs;\nin particular, they notably struggle with open questions that require\nstructured, multi-step legal reasoning. Moreover, our results underscore the\neffectiveness of the dataset in differentiating between models with varying\ncapabilities. Deploying an ensemble LLM-as-a-Judge paradigm with rigorous human\nexpert validation, we demonstrate how model-generated reasoning steps can be\nevaluated consistently and accurately, closely aligning with human expert\nassessments. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. We have open-sourced our code\non \\href{https://github.com/LEXam-Benchmark/LEXam}{GitHub} and released our\ndata on \\href{https://huggingface.co/datasets/LEXam-Benchmark/LEXam}{Hugging\nFace}. Project page: https://lexam-benchmark.github.io/"
                },
                "authors": [
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Jakob Merane"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Yoan Hermstrüwer"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Mubashara Akhtar"
                    },
                    {
                        "name": "Etienne Salimbeni"
                    },
                    {
                        "name": "Florian Geering"
                    },
                    {
                        "name": "Oliver Dreyer"
                    },
                    {
                        "name": "Daniel Brunner"
                    },
                    {
                        "name": "Markus Leippold"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Alexander Stremitzer"
                    },
                    {
                        "name": "Christoph Engel"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Joel Niklaus"
                    }
                ],
                "author_detail": {
                    "name": "Joel Niklaus"
                },
                "author": "Joel Niklaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12864v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12864v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00499v2",
                "updated": "2025-10-02T13:05:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    5,
                    41,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-01T04:32:37Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    32,
                    37,
                    2,
                    274,
                    0
                ],
                "title": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance"
                },
                "summary": "Spoken dialogue systems often rely on cascaded pipelines that transcribe,\nprocess, and resynthesize speech. While effective, this design discards\nparalinguistic cues and limits expressivity. Recent end-to-end methods reduce\nlatency and better preserve these cues, yet still rely on text intermediates,\ncreating a fundamental bottleneck. We present MOSS-Speech, a true\nspeech-to-speech large language model that directly understands and generates\nspeech without relying on text guidance. Our approach combines a modality-based\nlayer-splitting architecture with a frozen pre-training strategy, preserving\nthe reasoning and knowledge of pretrained text LLMs while adding native speech\ncapabilities. Experiments show that our model achieves state-of-the-art results\nin spoken question answering and delivers comparable speech-to-speech\nperformance relative to existing text-guided systems, while still maintaining\ncompetitive text performance. By narrowing the gap between text-guided and\ndirect speech generation, our work establishes a new paradigm for expressive\nand efficient end-to-end speech interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken dialogue systems often rely on cascaded pipelines that transcribe,\nprocess, and resynthesize speech. While effective, this design discards\nparalinguistic cues and limits expressivity. Recent end-to-end methods reduce\nlatency and better preserve these cues, yet still rely on text intermediates,\ncreating a fundamental bottleneck. We present MOSS-Speech, a true\nspeech-to-speech large language model that directly understands and generates\nspeech without relying on text guidance. Our approach combines a modality-based\nlayer-splitting architecture with a frozen pre-training strategy, preserving\nthe reasoning and knowledge of pretrained text LLMs while adding native speech\ncapabilities. Experiments show that our model achieves state-of-the-art results\nin spoken question answering and delivers comparable speech-to-speech\nperformance relative to existing text-guided systems, while still maintaining\ncompetitive text performance. By narrowing the gap between text-guided and\ndirect speech generation, our work establishes a new paradigm for expressive\nand efficient end-to-end speech interaction."
                },
                "authors": [
                    {
                        "name": "Xingjian Zhao"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Qinyuan Cheng"
                    },
                    {
                        "name": "Zhaoye Fei"
                    },
                    {
                        "name": "Luozhijie Jin"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Hanfu Chen"
                    },
                    {
                        "name": "Yaozhou Jiang"
                    },
                    {
                        "name": "Qinghui Gao"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Mingshu Chen"
                    },
                    {
                        "name": "Ruiming Wang"
                    },
                    {
                        "name": "Wenbo Zhang"
                    },
                    {
                        "name": "Yiyang Zhang"
                    },
                    {
                        "name": "Donghua Yu"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Xiaogui Yang"
                    },
                    {
                        "name": "Yitian Gong"
                    },
                    {
                        "name": "Yuanfan Xu"
                    },
                    {
                        "name": "Yaqian Zhou"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01622v2",
                "updated": "2025-10-02T13:01:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    1,
                    54,
                    3,
                    275,
                    0
                ],
                "published": "2025-08-03T07:23:02Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    7,
                    23,
                    2,
                    6,
                    215,
                    0
                ],
                "title": "VFP: Variational Flow-Matching Policy for Multi-Modal Robot Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFP: Variational Flow-Matching Policy for Multi-Modal Robot Manipulation"
                },
                "summary": "Flow-matching-based policies have recently emerged as a promising approach\nfor learning-based robot manipulation, offering significant acceleration in\naction sampling compared to diffusion-based policies. However, conventional\nflow-matching methods struggle with multi-modality, often collapsing to\naveraged or ambiguous behaviors in complex manipulation tasks. To address this,\nwe propose the Variational Flow-Matching Policy (VFP), which introduces a\nvariational latent prior for mode-aware action generation and effectively\ncaptures both task-level and trajectory-level multi-modality. VFP further\nincorporates Kantorovich Optimal Transport (K-OT) for distribution-level\nalignment and utilizes a Mixture-of-Experts (MoE) decoder for mode\nspecialization and efficient inference. We comprehensively evaluate VFP on 41\nsimulated tasks and 3 real-robot tasks, demonstrating its effectiveness and\nsampling efficiency in both simulated and real-world settings. Results show\nthat VFP achieves a 49% relative improvement in task success rate over standard\nflow-based baselines in simulation, and further outperforms them on real-robot\ntasks, while still maintaining fast inference and a compact model size. More\ndetails are available on our project page: https://sites.google.com/view/varfp/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-matching-based policies have recently emerged as a promising approach\nfor learning-based robot manipulation, offering significant acceleration in\naction sampling compared to diffusion-based policies. However, conventional\nflow-matching methods struggle with multi-modality, often collapsing to\naveraged or ambiguous behaviors in complex manipulation tasks. To address this,\nwe propose the Variational Flow-Matching Policy (VFP), which introduces a\nvariational latent prior for mode-aware action generation and effectively\ncaptures both task-level and trajectory-level multi-modality. VFP further\nincorporates Kantorovich Optimal Transport (K-OT) for distribution-level\nalignment and utilizes a Mixture-of-Experts (MoE) decoder for mode\nspecialization and efficient inference. We comprehensively evaluate VFP on 41\nsimulated tasks and 3 real-robot tasks, demonstrating its effectiveness and\nsampling efficiency in both simulated and real-world settings. Results show\nthat VFP achieves a 49% relative improvement in task success rate over standard\nflow-based baselines in simulation, and further outperforms them on real-robot\ntasks, while still maintaining fast inference and a compact model size. More\ndetails are available on our project page: https://sites.google.com/view/varfp/"
                },
                "authors": [
                    {
                        "name": "Xuanran Zhai"
                    },
                    {
                        "name": "Qianyou Zhao"
                    },
                    {
                        "name": "Qiaojun Yu"
                    },
                    {
                        "name": "Ce Hao"
                    }
                ],
                "author_detail": {
                    "name": "Ce Hao"
                },
                "author": "Ce Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00866v2",
                "updated": "2025-10-02T12:56:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    12,
                    56,
                    4,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-01T13:15:15Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    15,
                    15,
                    2,
                    274,
                    0
                ],
                "title": "The Data-Quality Illusion: Rethinking Classifier-Based Quality Filtering\n  for LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Data-Quality Illusion: Rethinking Classifier-Based Quality Filtering\n  for LLM Pretraining"
                },
                "summary": "Large-scale models are pretrained on massive web-crawled datasets containing\ndocuments of mixed quality, making data filtering essential. A popular method\nis Classifier-based Quality Filtering (CQF), which trains a binary classifier\nto distinguish between pretraining data and a small, high-quality set. It\nassigns each pretraining document a quality score defined as the classifier's\nscore and retains only the top-scoring ones. We provide an in-depth analysis of\nCQF. We show that while CQF improves downstream task performance, it does not\nnecessarily enhance language modeling on the high-quality dataset. We explain\nthis paradox by the fact that CQF implicitly filters the high-quality dataset\nas well. We further compare the behavior of models trained with CQF to those\ntrained on synthetic data of increasing quality, obtained via random token\npermutations, and find starkly different trends. Our results challenge the view\nthat CQF captures a meaningful notion of data quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale models are pretrained on massive web-crawled datasets containing\ndocuments of mixed quality, making data filtering essential. A popular method\nis Classifier-based Quality Filtering (CQF), which trains a binary classifier\nto distinguish between pretraining data and a small, high-quality set. It\nassigns each pretraining document a quality score defined as the classifier's\nscore and retains only the top-scoring ones. We provide an in-depth analysis of\nCQF. We show that while CQF improves downstream task performance, it does not\nnecessarily enhance language modeling on the high-quality dataset. We explain\nthis paradox by the fact that CQF implicitly filters the high-quality dataset\nas well. We further compare the behavior of models trained with CQF to those\ntrained on synthetic data of increasing quality, obtained via random token\npermutations, and find starkly different trends. Our results challenge the view\nthat CQF captures a meaningful notion of data quality."
                },
                "authors": [
                    {
                        "name": "Thiziri Nait Saada"
                    },
                    {
                        "name": "Louis Bethune"
                    },
                    {
                        "name": "Michal Klein"
                    },
                    {
                        "name": "David Grangier"
                    },
                    {
                        "name": "Marco Cuturi"
                    },
                    {
                        "name": "Pierre Ablin"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Ablin"
                },
                "author": "Pierre Ablin",
                "arxiv_comment": "21 pages, 20 figures, 2 tables, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01975v1",
                "updated": "2025-10-02T12:48:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    12,
                    48,
                    59,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T12:48:59Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    12,
                    48,
                    59,
                    3,
                    275,
                    0
                ],
                "title": "Impact Plasma Amplification of the Ancient Mercury Magnetic Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact Plasma Amplification of the Ancient Mercury Magnetic Field"
                },
                "summary": "Spacecraft measurements of Mercury indicate it has a core dynamo with a\nsurface field of 200-800 nT. These data also indicate that the crust contains\nremanent magnetization likely produced by an ancient magnetic field. The\ninferred magnetization intensity is consistent with a wide range of paleofield\nstrengths (0.2-50 uT), possibly indicating that Mercury once had a dynamo field\nmuch stronger than today. Recent modeling of ancient lunar impacts has\ndemonstrated that plasma generated during basin-formation can transiently\namplify a planetary dynamo field near the surface. Simultaneous impact-induced\npressure waves can then record these fields in the form of crustal shock\nremanent magnetization (SRM). Here, we present impact hydrocode and\nmagnetohydrodynamic simulations of a Caloris-size basin (~1,550 km diameter)\nformation event. Our results demonstrate that the ancient magnetospheric field\n(~0.5-0.9 uT) created by the interaction of the ancient interplanetary magnetic\nfield (IMF) and Mercury's dynamo field can be amplified by the plasma up to ~13\nuT and, via impact pressure waves, be recorded as SRM in the basin antipode.\nSuch magnetization could produce ~5 nT crustal fields at 20-km altitude\nantipodal to Caloris detectable by future spacecraft like BepiColombo.\nFurthermore, impacts in the southern hemisphere that formed ~1,000 km diameter\nbasins (e.g., Andal-Coleridge, Matisse-Repin, Eitkou-Milton, and Sadi-Scopus)\ncould impart crustal magnetization in the northern hemisphere, contributing to\nthe overall remanent field measured by MESSENGER. Overall, the impact plasma\namplification process can contribute to crustal magnetization on airless bodies\nand should be considered when reconstructing dynamo history from crustal\nanomaly measurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spacecraft measurements of Mercury indicate it has a core dynamo with a\nsurface field of 200-800 nT. These data also indicate that the crust contains\nremanent magnetization likely produced by an ancient magnetic field. The\ninferred magnetization intensity is consistent with a wide range of paleofield\nstrengths (0.2-50 uT), possibly indicating that Mercury once had a dynamo field\nmuch stronger than today. Recent modeling of ancient lunar impacts has\ndemonstrated that plasma generated during basin-formation can transiently\namplify a planetary dynamo field near the surface. Simultaneous impact-induced\npressure waves can then record these fields in the form of crustal shock\nremanent magnetization (SRM). Here, we present impact hydrocode and\nmagnetohydrodynamic simulations of a Caloris-size basin (~1,550 km diameter)\nformation event. Our results demonstrate that the ancient magnetospheric field\n(~0.5-0.9 uT) created by the interaction of the ancient interplanetary magnetic\nfield (IMF) and Mercury's dynamo field can be amplified by the plasma up to ~13\nuT and, via impact pressure waves, be recorded as SRM in the basin antipode.\nSuch magnetization could produce ~5 nT crustal fields at 20-km altitude\nantipodal to Caloris detectable by future spacecraft like BepiColombo.\nFurthermore, impacts in the southern hemisphere that formed ~1,000 km diameter\nbasins (e.g., Andal-Coleridge, Matisse-Repin, Eitkou-Milton, and Sadi-Scopus)\ncould impart crustal magnetization in the northern hemisphere, contributing to\nthe overall remanent field measured by MESSENGER. Overall, the impact plasma\namplification process can contribute to crustal magnetization on airless bodies\nand should be considered when reconstructing dynamo history from crustal\nanomaly measurements."
                },
                "authors": [
                    {
                        "name": "Isaac S. Narrett"
                    },
                    {
                        "name": "Rona Oran"
                    },
                    {
                        "name": "Yuxi Chen"
                    },
                    {
                        "name": "Katarina Miljković"
                    },
                    {
                        "name": "Gábor Tóth"
                    },
                    {
                        "name": "Catherine L. Johnson"
                    },
                    {
                        "name": "Benjamin P. Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin P. Weiss"
                },
                "author": "Benjamin P. Weiss",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01249v2",
                "updated": "2025-10-02T12:29:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    12,
                    29,
                    39,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-02T13:17:08Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    17,
                    8,
                    4,
                    122,
                    0
                ],
                "title": "Fusing Foveal Fixations Using Linear Retinal Transformations and\n  Bayesian Experimental Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Foveal Fixations Using Linear Retinal Transformations and\n  Bayesian Experimental Design"
                },
                "summary": "Humans (and many vertebrates) face the problem of fusing together multiple\nfixations of a scene in order to obtain a representation of the whole, where\neach fixation uses a high-resolution fovea and decreasing resolution in the\nperiphery. In this paper we explicitly represent the retinal transformation of\na fixation as a linear downsampling of a high-resolution latent image of the\nscene, exploiting the known geometry. This linear transformation allows us to\ncarry out exact inference for the latent variables in factor analysis (FA) and\nmixtures of FA models of the scene. Further, this allows us to formulate and\nsolve the choice of \"where to look next\" as a Bayesian experimental design\nproblem using the Expected Information Gain criterion. Experiments on the Frey\nfaces and MNIST datasets demonstrate the effectiveness of our models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans (and many vertebrates) face the problem of fusing together multiple\nfixations of a scene in order to obtain a representation of the whole, where\neach fixation uses a high-resolution fovea and decreasing resolution in the\nperiphery. In this paper we explicitly represent the retinal transformation of\na fixation as a linear downsampling of a high-resolution latent image of the\nscene, exploiting the known geometry. This linear transformation allows us to\ncarry out exact inference for the latent variables in factor analysis (FA) and\nmixtures of FA models of the scene. Further, this allows us to formulate and\nsolve the choice of \"where to look next\" as a Bayesian experimental design\nproblem using the Expected Information Gain criterion. Experiments on the Frey\nfaces and MNIST datasets demonstrate the effectiveness of our models."
                },
                "authors": [
                    {
                        "name": "Christopher K. I. Williams"
                    }
                ],
                "author_detail": {
                    "name": "Christopher K. I. Williams"
                },
                "author": "Christopher K. I. Williams",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.02312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02312v1",
                "updated": "2025-10-02T17:59:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation"
                },
                "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference."
                },
                "authors": [
                    {
                        "name": "Anna Kuzina"
                    },
                    {
                        "name": "Maciej Pioro"
                    },
                    {
                        "name": "Paul N. Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02306v1",
                "updated": "2025-10-02T17:59:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    41,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:41Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    41,
                    3,
                    275,
                    0
                ],
                "title": "Drawing Conclusions from Draws: Rethinking Preference Semantics in\n  Arena-Style LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drawing Conclusions from Draws: Rethinking Preference Semantics in\n  Arena-Style LLM Evaluation"
                },
                "summary": "In arena-style evaluation of large language models (LLMs), two LLMs respond\nto a user query, and the user chooses the winning response or deems the\n\"battle\" a draw, resulting in an adjustment to the ratings of both models. The\nprevailing approach for modeling these rating dynamics is to view battles as\ntwo-player game matches, as in chess, and apply the Elo rating system and its\nderivatives. In this paper, we critically examine this paradigm. Specifically,\nwe question whether a draw genuinely means that the two models are equal and\nhence whether their ratings should be equalized. Instead, we conjecture that\ndraws are more indicative of query difficulty: if the query is too easy, then\nboth models are more likely to succeed equally. On three real-world arena\ndatasets, we show that ignoring rating updates for draws yields a 1-3% relative\nincrease in battle outcome prediction accuracy (which includes draws) for all\nfour rating systems studied. Further analyses suggest that draws occur more for\nqueries rated as very easy and those as highly objective, with risk ratios of\n1.37 and 1.35, respectively. We recommend future rating systems to reconsider\nexisting draw semantics and to account for query properties in rating updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In arena-style evaluation of large language models (LLMs), two LLMs respond\nto a user query, and the user chooses the winning response or deems the\n\"battle\" a draw, resulting in an adjustment to the ratings of both models. The\nprevailing approach for modeling these rating dynamics is to view battles as\ntwo-player game matches, as in chess, and apply the Elo rating system and its\nderivatives. In this paper, we critically examine this paradigm. Specifically,\nwe question whether a draw genuinely means that the two models are equal and\nhence whether their ratings should be equalized. Instead, we conjecture that\ndraws are more indicative of query difficulty: if the query is too easy, then\nboth models are more likely to succeed equally. On three real-world arena\ndatasets, we show that ignoring rating updates for draws yields a 1-3% relative\nincrease in battle outcome prediction accuracy (which includes draws) for all\nfour rating systems studied. Further analyses suggest that draws occur more for\nqueries rated as very easy and those as highly objective, with risk ratios of\n1.37 and 1.35, respectively. We recommend future rating systems to reconsider\nexisting draw semantics and to account for query properties in rating updates."
                },
                "authors": [
                    {
                        "name": "Raphael Tang"
                    },
                    {
                        "name": "Crystina Zhang"
                    },
                    {
                        "name": "Wenyan Li"
                    },
                    {
                        "name": "Carmen Lai"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02298v1",
                "updated": "2025-10-02T17:59:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    2,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:02Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    2,
                    3,
                    275,
                    0
                ],
                "title": "ARMADA: Autonomous Online Failure Detection and Human Shared Control\n  Empower Scalable Real-world Deployment and Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARMADA: Autonomous Online Failure Detection and Human Shared Control\n  Empower Scalable Real-world Deployment and Adaptation"
                },
                "summary": "Imitation learning has shown promise in learning from large-scale real-world\ndatasets. However, pretrained policies usually perform poorly without\nsufficient in-domain data. Besides, human-collected demonstrations entail\nsubstantial labour and tend to encompass mixed-quality data and redundant\ninformation. As a workaround, human-in-the-loop systems gather domain-specific\ndata for policy post-training, and exploit closed-loop policy feedback to offer\ninformative guidance, but usually require full-time human surveillance during\npolicy rollout. In this work, we devise ARMADA, a multi-robot deployment and\nadaptation system with human-in-the-loop shared control, featuring an\nautonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA\nenables paralleled policy rollout and requests human intervention only when\nnecessary, significantly reducing reliance on human supervision. Hence, ARMADA\nenables efficient acquisition of in-domain data, and leads to more scalable\ndeployment and faster adaptation to new scenarios. We evaluate the performance\nof ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on\naverage, surpassing prior state-of-the-art failure detection approaches by over\n20%. Besides, ARMADA manifests more than 4$\\times$ increase in success rate and\ngreater than 2$\\times$ reduction in human intervention rate over multiple\nrounds of policy rollout and post-training, compared to previous\nhuman-in-the-loop learning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation learning has shown promise in learning from large-scale real-world\ndatasets. However, pretrained policies usually perform poorly without\nsufficient in-domain data. Besides, human-collected demonstrations entail\nsubstantial labour and tend to encompass mixed-quality data and redundant\ninformation. As a workaround, human-in-the-loop systems gather domain-specific\ndata for policy post-training, and exploit closed-loop policy feedback to offer\ninformative guidance, but usually require full-time human surveillance during\npolicy rollout. In this work, we devise ARMADA, a multi-robot deployment and\nadaptation system with human-in-the-loop shared control, featuring an\nautonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA\nenables paralleled policy rollout and requests human intervention only when\nnecessary, significantly reducing reliance on human supervision. Hence, ARMADA\nenables efficient acquisition of in-domain data, and leads to more scalable\ndeployment and faster adaptation to new scenarios. We evaluate the performance\nof ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on\naverage, surpassing prior state-of-the-art failure detection approaches by over\n20%. Besides, ARMADA manifests more than 4$\\times$ increase in success rate and\ngreater than 2$\\times$ reduction in human intervention rate over multiple\nrounds of policy rollout and post-training, compared to previous\nhuman-in-the-loop learning methods."
                },
                "authors": [
                    {
                        "name": "Wenye Yu"
                    },
                    {
                        "name": "Jun Lv"
                    },
                    {
                        "name": "Zixi Ying"
                    },
                    {
                        "name": "Yang Jin"
                    },
                    {
                        "name": "Chuan Wen"
                    },
                    {
                        "name": "Cewu Lu"
                    }
                ],
                "author_detail": {
                    "name": "Cewu Lu"
                },
                "author": "Cewu Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12021v2",
                "updated": "2025-10-02T17:57:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    57,
                    32,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-15T15:01:03Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    1,
                    3,
                    0,
                    258,
                    0
                ],
                "title": "LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code\n  Analysis"
                },
                "summary": "Large language models (LLMs) have become an essential tool to support\ndevelopers using traditional text-based programming languages, but the\ngraphical notation of the block-based Scratch programming environment inhibits\nthe use of LLMs. To overcome this limitation, we propose the LitterBox+\nframework that extends the Scratch static code analysis tool LitterBox with the\ngenerative abilities of LLMs. By converting block-based code to a textual\nrepresentation suitable for LLMs, LitterBox+ allows users to query LLMs about\ntheir programs, about quality issues reported by LitterBox, and it allows\ngenerating code fixes. Besides offering a programmatic API for these\nfunctionalities, LitterBox+ also extends the Scratch user interface to make\nthese functionalities available directly in the environment familiar to\nlearners. The framework is designed to be easily extensible with other prompts,\nLLM providers, and new features combining the program analysis capabilities of\nLitterBox with the generative features of LLMs. We provide a screencast\ndemonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become an essential tool to support\ndevelopers using traditional text-based programming languages, but the\ngraphical notation of the block-based Scratch programming environment inhibits\nthe use of LLMs. To overcome this limitation, we propose the LitterBox+\nframework that extends the Scratch static code analysis tool LitterBox with the\ngenerative abilities of LLMs. By converting block-based code to a textual\nrepresentation suitable for LLMs, LitterBox+ allows users to query LLMs about\ntheir programs, about quality issues reported by LitterBox, and it allows\ngenerating code fixes. Besides offering a programmatic API for these\nfunctionalities, LitterBox+ also extends the Scratch user interface to make\nthese functionalities available directly in the environment familiar to\nlearners. The framework is designed to be easily extensible with other prompts,\nLLM providers, and new features combining the program analysis capabilities of\nLitterBox with the generative features of LLMs. We provide a screencast\ndemonstrating the tool at https://youtu.be/RZ6E0xgrIgQ."
                },
                "authors": [
                    {
                        "name": "Benedikt Fein"
                    },
                    {
                        "name": "Florian Obermüller"
                    },
                    {
                        "name": "Gordon Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Fraser"
                },
                "author": "Gordon Fraser",
                "arxiv_comment": "ASE 2025 Tool Demonstration Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02286v1",
                "updated": "2025-10-02T17:57:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    57,
                    5,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:57:05Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    57,
                    5,
                    3,
                    275,
                    0
                ],
                "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks"
                },
                "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns."
                },
                "authors": [
                    {
                        "name": "Ruohao Guo"
                    },
                    {
                        "name": "Afshin Oroojlooy"
                    },
                    {
                        "name": "Roshan Sridhar"
                    },
                    {
                        "name": "Miguel Ballesteros"
                    },
                    {
                        "name": "Alan Ritter"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02279v1",
                "updated": "2025-10-02T17:54:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    54,
                    9,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:54:09Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    54,
                    9,
                    3,
                    275,
                    0
                ],
                "title": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods\n  for Natural Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods\n  for Natural Language Generation"
                },
                "summary": "Hallucinations are a common issue that undermine the reliability of large\nlanguage models (LLMs). Recent studies have identified a specific subset of\nhallucinations, known as confabulations, which arise due to predictive\nuncertainty of LLMs. To detect confabulations, various methods for estimating\npredictive uncertainty in natural language generation (NLG) have been\ndeveloped. These methods are typically evaluated by correlating uncertainty\nestimates with the correctness of generated text, with question-answering (QA)\ndatasets serving as the standard benchmark. However, commonly used approximate\ncorrectness functions have substantial disagreement between each other and,\nconsequently, in the ranking of the uncertainty estimation methods. This allows\none to inflate the apparent performance of uncertainty estimation methods. We\npropose using several alternative risk indicators for risk correlation\nexperiments that improve robustness of empirical assessment of UE algorithms\nfor NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge\nvariants leads to reducing the evaluation biases. Furthermore, we explore\nstructured tasks as well as out of distribution and perturbation detection\ntasks which provide robust and controllable risk indicators. Finally, we\npropose to use an Elo rating of uncertainty estimation methods to give an\nobjective summarization over extensive evaluation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations are a common issue that undermine the reliability of large\nlanguage models (LLMs). Recent studies have identified a specific subset of\nhallucinations, known as confabulations, which arise due to predictive\nuncertainty of LLMs. To detect confabulations, various methods for estimating\npredictive uncertainty in natural language generation (NLG) have been\ndeveloped. These methods are typically evaluated by correlating uncertainty\nestimates with the correctness of generated text, with question-answering (QA)\ndatasets serving as the standard benchmark. However, commonly used approximate\ncorrectness functions have substantial disagreement between each other and,\nconsequently, in the ranking of the uncertainty estimation methods. This allows\none to inflate the apparent performance of uncertainty estimation methods. We\npropose using several alternative risk indicators for risk correlation\nexperiments that improve robustness of empirical assessment of UE algorithms\nfor NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge\nvariants leads to reducing the evaluation biases. Furthermore, we explore\nstructured tasks as well as out of distribution and perturbation detection\ntasks which provide robust and controllable risk indicators. Finally, we\npropose to use an Elo rating of uncertainty estimation methods to give an\nobjective summarization over extensive evaluation settings."
                },
                "authors": [
                    {
                        "name": "Mykyta Ielanskyi"
                    },
                    {
                        "name": "Kajetan Schweighofer"
                    },
                    {
                        "name": "Lukas Aichberger"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02274v1",
                "updated": "2025-10-02T17:50:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    50,
                    22,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:50:22Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    50,
                    22,
                    3,
                    275,
                    0
                ],
                "title": "Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps"
                },
                "summary": "Modeling radio frequency (RF) signal propagation is essential for\nunderstanding the environment, as RF signals offer valuable insights beyond the\ncapabilities of RGB cameras, which are limited by the visible-light spectrum,\nlens coverage, and occlusions. It is also useful for supporting wireless\ndiagnosis, deployment, and optimization. However, accurately predicting RF\nsignals in complex environments remains a challenge due to interactions with\nobstacles such as absorption and reflection. We introduce Diffusion^2, a\ndiffusion-based approach that uses 3D point clouds to model the propagation of\nRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.\nTo effectively capture RF-related features from 3D data, we present the RF-3D\nEncoder, which encapsulates the complexities of 3D geometry along with\nsignal-specific details. These features undergo multi-scale embedding to\nsimulate the actual RF signal dissemination process. Our evaluation, based on\nsynthetic and real-world measurements, demonstrates that Diffusion^2 accurately\nestimates the behavior of RF signals in various frequency bands and\nenvironmental conditions, with an error margin of just 1.9 dB and 27x faster\nthan existing methods, marking a significant advancement in the field. Refer to\nhttps://rfvision-project.github.io/ for more information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling radio frequency (RF) signal propagation is essential for\nunderstanding the environment, as RF signals offer valuable insights beyond the\ncapabilities of RGB cameras, which are limited by the visible-light spectrum,\nlens coverage, and occlusions. It is also useful for supporting wireless\ndiagnosis, deployment, and optimization. However, accurately predicting RF\nsignals in complex environments remains a challenge due to interactions with\nobstacles such as absorption and reflection. We introduce Diffusion^2, a\ndiffusion-based approach that uses 3D point clouds to model the propagation of\nRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.\nTo effectively capture RF-related features from 3D data, we present the RF-3D\nEncoder, which encapsulates the complexities of 3D geometry along with\nsignal-specific details. These features undergo multi-scale embedding to\nsimulate the actual RF signal dissemination process. Our evaluation, based on\nsynthetic and real-world measurements, demonstrates that Diffusion^2 accurately\nestimates the behavior of RF signals in various frequency bands and\nenvironmental conditions, with an error margin of just 1.9 dB and 27x faster\nthan existing methods, marking a significant advancement in the field. Refer to\nhttps://rfvision-project.github.io/ for more information."
                },
                "authors": [
                    {
                        "name": "Kyoungjun Park"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Changhan Ge"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Shiqi Jiang"
                },
                "author": "Shiqi Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02271v1",
                "updated": "2025-10-02T17:48:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    48,
                    3,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:48:03Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    48,
                    3,
                    3,
                    275,
                    0
                ],
                "title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in\n  Tool-Augmented Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in\n  Tool-Augmented Agents"
                },
                "summary": "Information seeking is a fundamental requirement for humans. However,\nexisting LLM agents rely heavily on open-web search, which exposes two\nfundamental weaknesses: online content is noisy and unreliable, and many\nreal-world tasks require precise, domain-specific knowledge unavailable from\nthe web. The emergence of the Model Context Protocol (MCP) now allows agents to\ninterface with thousands of specialized tools, seemingly resolving this\nlimitation. Yet it remains unclear whether agents can effectively leverage such\ntools -- and more importantly, whether they can integrate them with\ngeneral-purpose search to solve complex tasks. Therefore, we introduce\nInfoMosaic-Bench, the first benchmark dedicated to multi-source information\nseeking in tool-augmented agents. Covering six representative domains\n(medicine, finance, maps, video, web, and multi-domain integration),\nInfoMosaic-Bench requires agents to combine general-purpose search with\ndomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable\npipeline that grounds task conditions in verified tool outputs, enforces\ncross-source dependencies, and filters out shortcut cases solvable by trivial\nlookup. This design guarantees both reliability and non-triviality. Experiments\nwith 14 state-of-the-art LLM agents reveal three findings: (i) web information\nalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass\nrate; (ii) domain tools provide selective but inconsistent benefits, improving\nsome domains while degrading others; and (iii) 22.4% of failures arise from\nincorrect tool usage or selection, highlighting that current LLMs still\nstruggle with even basic tool handling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information seeking is a fundamental requirement for humans. However,\nexisting LLM agents rely heavily on open-web search, which exposes two\nfundamental weaknesses: online content is noisy and unreliable, and many\nreal-world tasks require precise, domain-specific knowledge unavailable from\nthe web. The emergence of the Model Context Protocol (MCP) now allows agents to\ninterface with thousands of specialized tools, seemingly resolving this\nlimitation. Yet it remains unclear whether agents can effectively leverage such\ntools -- and more importantly, whether they can integrate them with\ngeneral-purpose search to solve complex tasks. Therefore, we introduce\nInfoMosaic-Bench, the first benchmark dedicated to multi-source information\nseeking in tool-augmented agents. Covering six representative domains\n(medicine, finance, maps, video, web, and multi-domain integration),\nInfoMosaic-Bench requires agents to combine general-purpose search with\ndomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable\npipeline that grounds task conditions in verified tool outputs, enforces\ncross-source dependencies, and filters out shortcut cases solvable by trivial\nlookup. This design guarantees both reliability and non-triviality. Experiments\nwith 14 state-of-the-art LLM agents reveal three findings: (i) web information\nalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass\nrate; (ii) domain tools provide selective but inconsistent benefits, improving\nsome domains while degrading others; and (iii) 22.4% of failures arise from\nincorrect tool usage or selection, highlighting that current LLMs still\nstruggle with even basic tool handling."
                },
                "authors": [
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Yuanshuo Zhang"
                    },
                    {
                        "name": "Xiyuan Yang"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Gongyi Zou"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Menglan Chen"
                    },
                    {
                        "name": "Shuo Tang"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02270v1",
                "updated": "2025-10-02T17:47:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    47,
                    39,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:47:39Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    47,
                    39,
                    3,
                    275,
                    0
                ],
                "title": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for\n  Fine-Grained Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for\n  Fine-Grained Image Classification"
                },
                "summary": "Unsupervised adaptation of CLIP-based vision-language models (VLMs) for\nfine-grained image classification requires sensitivity to microscopic local\ncues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse\nglobal features restricts its performance on fine-grained classification tasks.\nPrior efforts inject fine-grained knowledge by aligning large language model\n(LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach\noverlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training\nframework that jointly refines CLIP's visual and textual representations using\nfine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)\nwithin a lightweight TokenFusion module, which builds a saliency-guided\n$\\texttt{[FG]}$ token from patch embeddings and fuses it with the global\n$\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we\nintroduce a two-headed LLM-derived classifier: a frozen classifier that, via\nmulti-view alignment, provides a stable text-based prior for pseudo-labeling,\nand a learnable classifier initialized from LLM descriptions and fine-tuned\nwith TokenFusion. We further develop Dynamic Knowledge Aggregation, which\nconvexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to\niteratively refine pseudo-labels. Together, these components uncover latent\nfine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy\ngain across 13 fine-grained benchmarks while requiring only light adaptation.\nOur code is available at https://github.com/sathiiii/microCLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised adaptation of CLIP-based vision-language models (VLMs) for\nfine-grained image classification requires sensitivity to microscopic local\ncues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse\nglobal features restricts its performance on fine-grained classification tasks.\nPrior efforts inject fine-grained knowledge by aligning large language model\n(LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach\noverlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training\nframework that jointly refines CLIP's visual and textual representations using\nfine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)\nwithin a lightweight TokenFusion module, which builds a saliency-guided\n$\\texttt{[FG]}$ token from patch embeddings and fuses it with the global\n$\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we\nintroduce a two-headed LLM-derived classifier: a frozen classifier that, via\nmulti-view alignment, provides a stable text-based prior for pseudo-labeling,\nand a learnable classifier initialized from LLM descriptions and fine-tuned\nwith TokenFusion. We further develop Dynamic Knowledge Aggregation, which\nconvexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to\niteratively refine pseudo-labels. Together, these components uncover latent\nfine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy\ngain across 13 fine-grained benchmarks while requiring only light adaptation.\nOur code is available at https://github.com/sathiiii/microCLIP."
                },
                "authors": [
                    {
                        "name": "Sathira Silva"
                    },
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Chetan Arora"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02263v1",
                "updated": "2025-10-02T17:44:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    44,
                    23,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:44:23Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    44,
                    23,
                    3,
                    275,
                    0
                ],
                "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems"
                },
                "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration."
                },
                "authors": [
                    {
                        "name": "Yuxiao Qu"
                    },
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Yoonho Lee"
                    },
                    {
                        "name": "Amrith Setlur"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "Chelsea Finn"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02262v1",
                "updated": "2025-10-02T17:43:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    43,
                    1,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:43:01Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    43,
                    1,
                    3,
                    275,
                    0
                ],
                "title": "From Frames to Clips: Efficient Key Clip Selection for Long-Form Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Frames to Clips: Efficient Key Clip Selection for Long-Form Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VLMs) have achieved remarkable results on a\nvariety of vision language tasks, yet their practical use is limited by the\n\"needle in a haystack\" problem: the massive number of visual tokens produced\nfrom raw video frames exhausts the model's context window. Existing solutions\nalleviate this issue by selecting a sparse set of frames, thereby reducing\ntoken count, but such frame-wise selection discards essential temporal\ndynamics, leading to suboptimal reasoning about motion and event continuity. In\nthis work we systematically explore the impact of temporal information and\ndemonstrate that extending selection from isolated key frames to key clips,\nwhich are short, temporally coherent segments, improves video understanding. To\nmaintain a fixed computational budget while accommodating the larger token\nfootprint of clips, we propose an adaptive resolution strategy that dynamically\nbalances spatial resolution and clip length, ensuring a constant token count\nper video. Experiments on three long-form video benchmarks demonstrate that our\ntraining-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and\n10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These\nresults highlight the importance of preserving temporal coherence in frame\nselection and provide a practical pathway for scaling Video LLMs to real world\nvideo understanding applications. Project webpage is available at\nhttps://guangyusun.com/f2c .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VLMs) have achieved remarkable results on a\nvariety of vision language tasks, yet their practical use is limited by the\n\"needle in a haystack\" problem: the massive number of visual tokens produced\nfrom raw video frames exhausts the model's context window. Existing solutions\nalleviate this issue by selecting a sparse set of frames, thereby reducing\ntoken count, but such frame-wise selection discards essential temporal\ndynamics, leading to suboptimal reasoning about motion and event continuity. In\nthis work we systematically explore the impact of temporal information and\ndemonstrate that extending selection from isolated key frames to key clips,\nwhich are short, temporally coherent segments, improves video understanding. To\nmaintain a fixed computational budget while accommodating the larger token\nfootprint of clips, we propose an adaptive resolution strategy that dynamically\nbalances spatial resolution and clip length, ensuring a constant token count\nper video. Experiments on three long-form video benchmarks demonstrate that our\ntraining-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and\n10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These\nresults highlight the importance of preserving temporal coherence in frame\nselection and provide a practical pathway for scaling Video LLMs to real world\nvideo understanding applications. Project webpage is available at\nhttps://guangyusun.com/f2c ."
                },
                "authors": [
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Archit Singhal"
                    },
                    {
                        "name": "Burak Uzkent"
                    },
                    {
                        "name": "Mubarak Shah"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Garin Kessler"
                    }
                ],
                "author_detail": {
                    "name": "Garin Kessler"
                },
                "author": "Garin Kessler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18436v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18436v4",
                "updated": "2025-10-02T17:40:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    40,
                    50,
                    3,
                    275,
                    0
                ],
                "published": "2024-10-24T05:14:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    14,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case\n  Study on English-Korean Code-Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case\n  Study on English-Korean Code-Switching"
                },
                "summary": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can activate, or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) demonstrate multilingual abilities, yet\nthey are English-centric due to dominance of English in training corpora. The\nlimited resource for low-resource languages remains a crucial challenge.\nCode-switching (CS), a phenomenon where multilingual speakers alternate between\nlanguages in a discourse, can convey subtle cultural and linguistic nuances\nthat can be otherwise lost in translation and elicits language-specific\nknowledge in human communications. In light of this, we investigate whether\ncode-switching can activate, or identify and leverage knowledge for reasoning\nwhen LLMs solve low-resource language tasks. To facilitate the research, we\nfirst present EnKoQA, a synthetic English-Korean CS question-answering dataset.\nWe provide comprehensive analysis on a variety of multilingual LLMs by\nsubdividing activation process into knowledge identification and knowledge\nleveraging. Our results demonstrate that compared to English text, CS can\nfaithfully activate knowledge inside LLMs especially on language-specific\ndomains, suggesting the potential of code-switching on low-resource language\ntasks."
                },
                "authors": [
                    {
                        "name": "Seoyeon Kim"
                    },
                    {
                        "name": "Huiseo Kim"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18436v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18436v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02249v1",
                "updated": "2025-10-02T17:36:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    36,
                    50,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:36:50Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    36,
                    50,
                    3,
                    275,
                    0
                ],
                "title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative\n  Entropy Regulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative\n  Entropy Regulation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\non complex problems using long Chain-of-Thought (CoT) reasoning. However, they\noften suffer from overthinking, meaning generating unnecessarily lengthy\nreasoning steps for simpler problems. This issue may degrade the efficiency of\nthe models and make them difficult to adapt the reasoning depth to the\ncomplexity of problems. To address this, we introduce a novel metric Token\nEntropy Cumulative Average (TECA), which measures the extent of exploration\nthroughout the reasoning process. We further propose a novel reasoning paradigm\n-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy\nRegulation (CER) mechanism. This paradigm leverages TECA to help the model\ndynamically determine the optimal point to conclude its thought process and\nprovide a final answer, thus achieving efficient reasoning. Experimental\nresults across diverse mathematical benchmarks show that our approach\nsubstantially mitigates overthinking without sacrificing problem-solving\nability. With our thinking paradigm, the average response length decreases by\nup to 71% on simpler datasets, demonstrating the effectiveness of our method in\ncreating a more efficient and adaptive reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\non complex problems using long Chain-of-Thought (CoT) reasoning. However, they\noften suffer from overthinking, meaning generating unnecessarily lengthy\nreasoning steps for simpler problems. This issue may degrade the efficiency of\nthe models and make them difficult to adapt the reasoning depth to the\ncomplexity of problems. To address this, we introduce a novel metric Token\nEntropy Cumulative Average (TECA), which measures the extent of exploration\nthroughout the reasoning process. We further propose a novel reasoning paradigm\n-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy\nRegulation (CER) mechanism. This paradigm leverages TECA to help the model\ndynamically determine the optimal point to conclude its thought process and\nprovide a final answer, thus achieving efficient reasoning. Experimental\nresults across diverse mathematical benchmarks show that our approach\nsubstantially mitigates overthinking without sacrificing problem-solving\nability. With our thinking paradigm, the average response length decreases by\nup to 71% on simpler datasets, demonstrating the effectiveness of our method in\ncreating a more efficient and adaptive reasoning process."
                },
                "authors": [
                    {
                        "name": "Tianyi Jiang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Yujuan Ding"
                    },
                    {
                        "name": "Kainian Zhu"
                    },
                    {
                        "name": "Fei Ma"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09674v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09674v4",
                "updated": "2025-10-02T17:36:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    36,
                    23,
                    3,
                    275,
                    0
                ],
                "published": "2025-03-12T17:41:25Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    41,
                    25,
                    2,
                    71,
                    0
                ],
                "title": "Probabilistic Reasoning with LLMs for k-anonymity Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Reasoning with LLMs for k-anonymity Estimation"
                },
                "summary": "Probabilistic reasoning is a key aspect of both human and artificial\nintelligence that allows for handling uncertainty and ambiguity in\ndecision-making. In this paper, we introduce a new numerical reasoning task\nunder uncertainty for large language models, focusing on estimating the privacy\nrisk of user-generated documents containing privacy-sensitive information. We\npropose BRANCH, a new LLM methodology that estimates the k-privacy value of a\ntext-the size of the population matching the given information. BRANCH\nfactorizes a joint probability distribution of personal information as random\nvariables. The probability of each factor in a population is estimated\nseparately using a Bayesian network and combined to compute the final k-value.\nOur experiments show that this method successfully estimates the k-value 73% of\nthe time, a 13% increase compared to o3-mini with chain-of-thought reasoning.\nWe also find that LLM uncertainty is a good indicator for accuracy, as\nhigh-variance predictions are 37.47% less accurate on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic reasoning is a key aspect of both human and artificial\nintelligence that allows for handling uncertainty and ambiguity in\ndecision-making. In this paper, we introduce a new numerical reasoning task\nunder uncertainty for large language models, focusing on estimating the privacy\nrisk of user-generated documents containing privacy-sensitive information. We\npropose BRANCH, a new LLM methodology that estimates the k-privacy value of a\ntext-the size of the population matching the given information. BRANCH\nfactorizes a joint probability distribution of personal information as random\nvariables. The probability of each factor in a population is estimated\nseparately using a Bayesian network and combined to compute the final k-value.\nOur experiments show that this method successfully estimates the k-value 73% of\nthe time, a 13% increase compared to o3-mini with chain-of-thought reasoning.\nWe also find that LLM uncertainty is a good indicator for accuracy, as\nhigh-variance predictions are 37.47% less accurate on average."
                },
                "authors": [
                    {
                        "name": "Jonathan Zheng"
                    },
                    {
                        "name": "Sauvik Das"
                    },
                    {
                        "name": "Alan Ritter"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "10 pages, Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09674v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09674v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22872v2",
                "updated": "2025-10-02T17:35:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    35,
                    49,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-26T19:35:10Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    35,
                    10,
                    4,
                    269,
                    0
                ],
                "title": "Anti-Regulatory AI: How \"AI Safety\" is Leveraged Against Regulatory\n  Oversight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anti-Regulatory AI: How \"AI Safety\" is Leveraged Against Regulatory\n  Oversight"
                },
                "summary": "AI companies increasingly develop and deploy privacy-enhancing technologies,\nbias-constraining measures, evaluation frameworks, and alignment techniques --\nframing them as addressing concerns related to data privacy, algorithmic\nfairness, and AI safety. This paper examines the ulterior function of these\ntechnologies as mechanisms of legal influence. First, we examine how\nencryption, federated learning, and synthetic data -- presented as enhancing\nprivacy and reducing bias -- can operate as mechanisms of avoidance with\nexisting regulations in attempts to place data operations outside the scope of\ntraditional regulatory frameworks. Second, we investigate how emerging AI\nsafety practices including open-source model releases, evaluations, and\nalignment techniques can be used as mechanisms of change that direct regulatory\nfocus towards industry-controlled voluntary standards and self-governance. We\nterm this phenomenon \"anti-regulatory AI\" -- the deployment of ostensibly\nprotective technologies that simultaneously shapes the terms of regulatory\noversight. Our analysis additionally reveals how technologies' anti-regulatory\nfunctions are enabled through framing that legitimizes their deployment while\nobscuring their use as regulatory workarounds. This paper closes with a\ndiscussion of policy implications that centers on the consideration of business\nincentives that drive AI development and the role of technical expertise in\nassessing whether these technologies fulfill their purported protections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI companies increasingly develop and deploy privacy-enhancing technologies,\nbias-constraining measures, evaluation frameworks, and alignment techniques --\nframing them as addressing concerns related to data privacy, algorithmic\nfairness, and AI safety. This paper examines the ulterior function of these\ntechnologies as mechanisms of legal influence. First, we examine how\nencryption, federated learning, and synthetic data -- presented as enhancing\nprivacy and reducing bias -- can operate as mechanisms of avoidance with\nexisting regulations in attempts to place data operations outside the scope of\ntraditional regulatory frameworks. Second, we investigate how emerging AI\nsafety practices including open-source model releases, evaluations, and\nalignment techniques can be used as mechanisms of change that direct regulatory\nfocus towards industry-controlled voluntary standards and self-governance. We\nterm this phenomenon \"anti-regulatory AI\" -- the deployment of ostensibly\nprotective technologies that simultaneously shapes the terms of regulatory\noversight. Our analysis additionally reveals how technologies' anti-regulatory\nfunctions are enabled through framing that legitimizes their deployment while\nobscuring their use as regulatory workarounds. This paper closes with a\ndiscussion of policy implications that centers on the consideration of business\nincentives that drive AI development and the role of technical expertise in\nassessing whether these technologies fulfill their purported protections."
                },
                "authors": [
                    {
                        "name": "Rui-Jie Yew"
                    },
                    {
                        "name": "Brian Judge"
                    }
                ],
                "author_detail": {
                    "name": "Brian Judge"
                },
                "author": "Brian Judge",
                "arxiv_comment": "Forthcoming at EAAMO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02243v1",
                "updated": "2025-10-02T17:30:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    30,
                    8,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:30:08Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    30,
                    8,
                    3,
                    275,
                    0
                ],
                "title": "AccurateRAG: A Framework for Building Accurate Retrieval-Augmented\n  Question-Answering Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccurateRAG: A Framework for Building Accurate Retrieval-Augmented\n  Question-Answering Applications"
                },
                "summary": "We introduce AccurateRAG -- a novel framework for constructing\nhigh-performance question-answering applications based on retrieval-augmented\ngeneration (RAG). Our framework offers a pipeline for development efficiency\nwith tools for raw dataset processing, fine-tuning data generation, text\nembedding & LLM fine-tuning, output evaluation, and building RAG systems\nlocally. Experimental results show that our framework outperforms previous\nstrong baselines and obtains new state-of-the-art question-answering\nperformance on benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AccurateRAG -- a novel framework for constructing\nhigh-performance question-answering applications based on retrieval-augmented\ngeneration (RAG). Our framework offers a pipeline for development efficiency\nwith tools for raw dataset processing, fine-tuning data generation, text\nembedding & LLM fine-tuning, output evaluation, and building RAG systems\nlocally. Experimental results show that our framework outperforms previous\nstrong baselines and obtains new state-of-the-art question-answering\nperformance on benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Linh The Nguyen"
                    },
                    {
                        "name": "Chi Tran"
                    },
                    {
                        "name": "Dung Ngoc Nguyen"
                    },
                    {
                        "name": "Van-Cuong Pham"
                    },
                    {
                        "name": "Hoang Ngo"
                    },
                    {
                        "name": "Dat Quoc Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dat Quoc Nguyen"
                },
                "author": "Dat Quoc Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02241v1",
                "updated": "2025-10-02T17:29:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    29,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:29:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    29,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "Study on LLMs for Promptagator-Style Dense Retriever Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study on LLMs for Promptagator-Style Dense Retriever Training"
                },
                "summary": "Promptagator demonstrated that Large Language Models (LLMs) with few-shot\nprompts can be used as task-specific query generators for fine-tuning\ndomain-specialized dense retrieval models. However, the original Promptagator\napproach relied on proprietary and large-scale LLMs which users may not have\naccess to or may be prohibited from using with sensitive data. In this work, we\nstudy the impact of open-source LLMs at accessible scales ($\\leq$14B\nparameters) as an alternative. Our results demonstrate that open-source LLMs as\nsmall as 3B parameters can serve as effective Promptagator-style query\ngenerators. We hope our work will inform practitioners with reliable\nalternatives for synthetic data generation and give insights to maximize\nfine-tuning results for domain-specific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Promptagator demonstrated that Large Language Models (LLMs) with few-shot\nprompts can be used as task-specific query generators for fine-tuning\ndomain-specialized dense retrieval models. However, the original Promptagator\napproach relied on proprietary and large-scale LLMs which users may not have\naccess to or may be prohibited from using with sensitive data. In this work, we\nstudy the impact of open-source LLMs at accessible scales ($\\leq$14B\nparameters) as an alternative. Our results demonstrate that open-source LLMs as\nsmall as 3B parameters can serve as effective Promptagator-style query\ngenerators. We hope our work will inform practitioners with reliable\nalternatives for synthetic data generation and give insights to maximize\nfine-tuning results for domain-specific applications."
                },
                "authors": [
                    {
                        "name": "Daniel Gwon"
                    },
                    {
                        "name": "Nour Jedidi"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "CIKM 2025 short research paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00404v2",
                "updated": "2025-10-02T17:28:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    28,
                    55,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-01T01:29:31Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    29,
                    31,
                    2,
                    274,
                    0
                ],
                "title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features"
                },
                "summary": "Sparse autoencoders (SAEs) have emerged as powerful techniques for\ninterpretability of large language models (LLMs), aiming to decompose hidden\nstates into meaningful semantic features. While several SAE variants have been\nproposed, there remains no principled framework to derive SAEs from the\noriginal dictionary learning formulation. In this work, we introduce such a\nframework by unrolling the proximal gradient method for sparse coding. We show\nthat a single-step update naturally recovers common SAE variants, including\nReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation\nof existing SAEs: their sparsity-inducing regularizers enforce non-negativity,\npreventing a single feature from representing bidirectional concepts (e.g.,\nmale vs. female). This structural constraint fragments semantic axes into\nseparate, redundant features, limiting representational completeness. To\naddress this issue, we propose AbsTopK SAE, a new variant derived from the\n$\\ell_0$ sparsity constraint that applies hard thresholding over the\nlargest-magnitude activations. By preserving both positive and negative\nactivations, AbsTopK uncovers richer, bidirectional conceptual representations.\nComprehensive experiments across four LLMs and seven probing and steering tasks\nshow that AbsTopK improves reconstruction fidelity, enhances interpretability,\nand enables single features to encode contrasting concepts. Remarkably, AbsTopK\nmatches or even surpasses the Difference-in-Mean method, a supervised approach\nthat requires labeled data for each concept and has been shown in prior work to\noutperform SAEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) have emerged as powerful techniques for\ninterpretability of large language models (LLMs), aiming to decompose hidden\nstates into meaningful semantic features. While several SAE variants have been\nproposed, there remains no principled framework to derive SAEs from the\noriginal dictionary learning formulation. In this work, we introduce such a\nframework by unrolling the proximal gradient method for sparse coding. We show\nthat a single-step update naturally recovers common SAE variants, including\nReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation\nof existing SAEs: their sparsity-inducing regularizers enforce non-negativity,\npreventing a single feature from representing bidirectional concepts (e.g.,\nmale vs. female). This structural constraint fragments semantic axes into\nseparate, redundant features, limiting representational completeness. To\naddress this issue, we propose AbsTopK SAE, a new variant derived from the\n$\\ell_0$ sparsity constraint that applies hard thresholding over the\nlargest-magnitude activations. By preserving both positive and negative\nactivations, AbsTopK uncovers richer, bidirectional conceptual representations.\nComprehensive experiments across four LLMs and seven probing and steering tasks\nshow that AbsTopK improves reconstruction fidelity, enhances interpretability,\nand enables single features to encode contrasting concepts. Remarkably, AbsTopK\nmatches or even surpasses the Difference-in-Mean method, a supervised approach\nthat requires labeled data for each concept and has been shown in prior work to\noutperform SAEs."
                },
                "authors": [
                    {
                        "name": "Xudong Zhu"
                    },
                    {
                        "name": "Mohammad Mahdi Khalili"
                    },
                    {
                        "name": "Zhihui Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zhihui Zhu"
                },
                "author": "Zhihui Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10582v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10582v3",
                "updated": "2025-10-02T17:20:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    20,
                    32,
                    3,
                    275,
                    0
                ],
                "published": "2025-01-17T22:20:55Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    22,
                    20,
                    55,
                    4,
                    17,
                    0
                ],
                "title": "Adapting Large Language Models for Character-based Augmentative and\n  Alternative Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models for Character-based Augmentative and\n  Alternative Communication"
                },
                "summary": "Users of Augmentative and Alternative Communication (AAC) may write\nletter-by-letter via an interface that uses a character language model.\nHowever, most state-of-the-art large pretrained language models predict subword\ntokens of variable length. We investigate how to practically use such models to\nmake accurate and efficient character predictions. Our algorithm for producing\ncharacter predictions from a subword large language model (LLM) provides more\naccurate predictions than using a classification layer, a byte-level LLM, or an\nn-gram model. Additionally, we investigate a domain adaptation procedure based\non a large dataset of sentences we curated based on scoring how useful each\nsentence might be for spoken or written AAC communication. We find our\nprocedure further improves model performance on simple, conversational text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Users of Augmentative and Alternative Communication (AAC) may write\nletter-by-letter via an interface that uses a character language model.\nHowever, most state-of-the-art large pretrained language models predict subword\ntokens of variable length. We investigate how to practically use such models to\nmake accurate and efficient character predictions. Our algorithm for producing\ncharacter predictions from a subword large language model (LLM) provides more\naccurate predictions than using a classification layer, a byte-level LLM, or an\nn-gram model. Additionally, we investigate a domain adaptation procedure based\non a large dataset of sentences we curated based on scoring how useful each\nsentence might be for spoken or written AAC communication. We find our\nprocedure further improves model performance on simple, conversational text."
                },
                "authors": [
                    {
                        "name": "Dylan Gaines"
                    },
                    {
                        "name": "Keith Vertanen"
                    }
                ],
                "author_detail": {
                    "name": "Keith Vertanen"
                },
                "author": "Keith Vertanen",
                "arxiv_comment": "To appear in Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10582v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10582v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02230v1",
                "updated": "2025-10-02T17:17:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    17,
                    27,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:17:27Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    17,
                    27,
                    3,
                    275,
                    0
                ],
                "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains\n  Language Models"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference."
                },
                "authors": [
                    {
                        "name": "Phuc Minh Nguyen"
                    },
                    {
                        "name": "Chinh D. La"
                    },
                    {
                        "name": "Duy M. H. Nguyen"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    },
                    {
                        "name": "Binh T. Nguyen"
                    },
                    {
                        "name": "Khoa D. Doan"
                    }
                ],
                "author_detail": {
                    "name": "Khoa D. Doan"
                },
                "author": "Khoa D. Doan",
                "arxiv_comment": "23 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02228v1",
                "updated": "2025-10-02T17:14:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    14,
                    34,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:14:34Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    14,
                    34,
                    3,
                    275,
                    0
                ],
                "title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity"
                },
                "summary": "Scaling laws play a central role in the success of Large Language Models\n(LLMs), enabling the prediction of model performance relative to compute\nbudgets prior to training. While Transformers have been the dominant\narchitecture, recent alternatives such as xLSTM offer linear complexity with\nrespect to context length while remaining competitive in the billion-parameter\nregime. We conduct a comparative investigation on the scaling behavior of\nTransformers and xLSTM along the following lines, providing insights to guide\nfuture model design and deployment. First, we study the scaling behavior for\nxLSTM in compute-optimal and over-training regimes using both IsoFLOP and\nparametric fit approaches on a wide range of model sizes (80M-7B) and number of\ntraining tokens (2B-2T). Second, we examine the dependence of optimal model\nsizes on context length, a pivotal aspect that was largely ignored in previous\nwork. Finally, we analyze inference-time scaling characteristics. Our findings\nreveal that in typical LLM training and inference scenarios, xLSTM scales\nfavorably compared to Transformers. Importantly, xLSTM's advantage widens as\ntraining and inference contexts grow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws play a central role in the success of Large Language Models\n(LLMs), enabling the prediction of model performance relative to compute\nbudgets prior to training. While Transformers have been the dominant\narchitecture, recent alternatives such as xLSTM offer linear complexity with\nrespect to context length while remaining competitive in the billion-parameter\nregime. We conduct a comparative investigation on the scaling behavior of\nTransformers and xLSTM along the following lines, providing insights to guide\nfuture model design and deployment. First, we study the scaling behavior for\nxLSTM in compute-optimal and over-training regimes using both IsoFLOP and\nparametric fit approaches on a wide range of model sizes (80M-7B) and number of\ntraining tokens (2B-2T). Second, we examine the dependence of optimal model\nsizes on context length, a pivotal aspect that was largely ignored in previous\nwork. Finally, we analyze inference-time scaling characteristics. Our findings\nreveal that in typical LLM training and inference scenarios, xLSTM scales\nfavorably compared to Transformers. Importantly, xLSTM's advantage widens as\ntraining and inference contexts grow."
                },
                "authors": [
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Kajetan Schweighofer"
                    },
                    {
                        "name": "Sebastian Böck"
                    },
                    {
                        "name": "Sebastian Lehner"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "arxiv_comment": "Code and data available at\n  https://github.com/NX-AI/xlstm_scaling_laws",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02227v1",
                "updated": "2025-10-02T17:14:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    14,
                    0,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:14:00Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    14,
                    0,
                    3,
                    275,
                    0
                ],
                "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Yuan"
                    },
                    {
                        "name": "Yujuan Ding"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Jinyu Cai"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Hengtao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hengtao Shen"
                },
                "author": "Hengtao Shen",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02219v1",
                "updated": "2025-10-02T17:03:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    3,
                    9,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:03:09Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    3,
                    9,
                    3,
                    275,
                    0
                ],
                "title": "Contrastive Retrieval Heads Improve Attention-Based Re-Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Retrieval Heads Improve Attention-Based Re-Ranking"
                },
                "summary": "The strong zero-shot and long-context capabilities of recent Large Language\nModels (LLMs) have paved the way for highly effective re-ranking systems.\nAttention-based re-rankers leverage attention weights from transformer heads to\nproduce relevance scores, but not all heads are created equally: many\ncontribute noise and redundancy, thus limiting performance. To address this, we\nintroduce CoRe heads, a small set of retrieval heads identified via a\ncontrastive scoring metric that explicitly rewards high attention heads that\ncorrelate with relevant documents, while downplaying nodes with higher\nattention that correlate with irrelevant documents. This relative ranking\ncriterion isolates the most discriminative heads for re-ranking and yields a\nstate-of-the-art list-wise re-ranker. Extensive experiments with three LLMs\nshow that aggregated signals from CoRe heads, constituting less than 1% of all\nheads, substantially improve re-ranking accuracy over strong baselines. We\nfurther find that CoRe heads are concentrated in middle layers, and pruning the\ncomputation of final 50% of model layers preserves accuracy while significantly\nreducing inference time and memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong zero-shot and long-context capabilities of recent Large Language\nModels (LLMs) have paved the way for highly effective re-ranking systems.\nAttention-based re-rankers leverage attention weights from transformer heads to\nproduce relevance scores, but not all heads are created equally: many\ncontribute noise and redundancy, thus limiting performance. To address this, we\nintroduce CoRe heads, a small set of retrieval heads identified via a\ncontrastive scoring metric that explicitly rewards high attention heads that\ncorrelate with relevant documents, while downplaying nodes with higher\nattention that correlate with irrelevant documents. This relative ranking\ncriterion isolates the most discriminative heads for re-ranking and yields a\nstate-of-the-art list-wise re-ranker. Extensive experiments with three LLMs\nshow that aggregated signals from CoRe heads, constituting less than 1% of all\nheads, substantially improve re-ranking accuracy over strong baselines. We\nfurther find that CoRe heads are concentrated in middle layers, and pruning the\ncomputation of final 50% of model layers preserves accuracy while significantly\nreducing inference time and memory usage."
                },
                "authors": [
                    {
                        "name": "Linh Tran"
                    },
                    {
                        "name": "Yulong Li"
                    },
                    {
                        "name": "Radu Florian"
                    },
                    {
                        "name": "Wei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Wei Sun"
                },
                "author": "Wei Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02212v1",
                "updated": "2025-10-02T16:57:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    57,
                    24,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:57:24Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    57,
                    24,
                    3,
                    275,
                    0
                ],
                "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via\n  Reinforcement Learning"
                },
                "summary": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified\nframework for training masked diffusion large language models (dLLMs) to reason\nnot only better (furious), but also faster via reinforcement learning (RL). We\nfirst unify the existing baseline approach such as d1 by proposing to train\nsurrogate policies via off-policy RL, whose likelihood is much more tractable\nas an approximation to the true dLLM policy. This naturally motivates a more\naccurate and informative two-stage likelihood approximation combined with\nimportance sampling correction, which leads to generalized RL algorithms with\nbetter sample efficiency and superior task performance. Second, we propose a\nnew direction of joint training efficient samplers/controllers of dLLMs policy.\nVia RL, we incentivize dLLMs' natural multi-token prediction capabilities by\nletting the model learn to adaptively allocate an inference threshold for each\nprompt. By jointly training the sampler, we yield better accuracies with lower\nnumber of function evaluations (NFEs) compared to training the model only,\nobtaining the best performance in improving the Pareto frontier of the\ninference-time compute of dLLMs. We showcase the effectiveness of our pipeline\nby training open source large diffusion language models over benchmark math and\nplanning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified\nframework for training masked diffusion large language models (dLLMs) to reason\nnot only better (furious), but also faster via reinforcement learning (RL). We\nfirst unify the existing baseline approach such as d1 by proposing to train\nsurrogate policies via off-policy RL, whose likelihood is much more tractable\nas an approximation to the true dLLM policy. This naturally motivates a more\naccurate and informative two-stage likelihood approximation combined with\nimportance sampling correction, which leads to generalized RL algorithms with\nbetter sample efficiency and superior task performance. Second, we propose a\nnew direction of joint training efficient samplers/controllers of dLLMs policy.\nVia RL, we incentivize dLLMs' natural multi-token prediction capabilities by\nletting the model learn to adaptively allocate an inference threshold for each\nprompt. By jointly training the sampler, we yield better accuracies with lower\nnumber of function evaluations (NFEs) compared to training the model only,\nobtaining the best performance in improving the Pareto frontier of the\ninference-time compute of dLLMs. We showcase the effectiveness of our pipeline\nby training open source large diffusion language models over benchmark math and\nplanning tasks."
                },
                "authors": [
                    {
                        "name": "Hanyang Zhao"
                    },
                    {
                        "name": "Dawen Liang"
                    },
                    {
                        "name": "Wenpin Tang"
                    },
                    {
                        "name": "David Yao"
                    },
                    {
                        "name": "Nathan Kallus"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Kallus"
                },
                "author": "Nathan Kallus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00708v2",
                "updated": "2025-10-02T16:56:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    56,
                    36,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-31T20:56:54Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    20,
                    56,
                    54,
                    5,
                    151,
                    0
                ],
                "title": "DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph\n  Completion across General and Biomedical Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph\n  Completion across General and Biomedical Domains"
                },
                "summary": "Knowledge graph completion (KGC) aims to predict missing triples in knowledge\ngraphs (KGs) by leveraging existing triples and textual information. Recently,\ngenerative large language models (LLMs) have been increasingly employed for\ngraph tasks. However, current approaches typically encode graph context in\ntextual form, which fails to fully exploit the potential of LLMs for perceiving\nand reasoning about graph structures. To address this limitation, we propose\nDrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph\nCompletion). DrKGC employs a flexible lightweight model training strategy to\nlearn structural embeddings and logical rules within the KG. It then leverages\na novel bottom-up graph retrieval method to extract a subgraph for each query\nguided by the learned rules. Finally, a graph convolutional network (GCN)\nadapter uses the retrieved subgraph to enhance the structural embeddings, which\nare then integrated into the prompt for effective LLM fine-tuning. Experimental\nresults on two general domain benchmark datasets and two biomedical datasets\ndemonstrate the superior performance of DrKGC. Furthermore, a realistic case\nstudy in the biomedical domain highlights its interpretability and practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graph completion (KGC) aims to predict missing triples in knowledge\ngraphs (KGs) by leveraging existing triples and textual information. Recently,\ngenerative large language models (LLMs) have been increasingly employed for\ngraph tasks. However, current approaches typically encode graph context in\ntextual form, which fails to fully exploit the potential of LLMs for perceiving\nand reasoning about graph structures. To address this limitation, we propose\nDrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph\nCompletion). DrKGC employs a flexible lightweight model training strategy to\nlearn structural embeddings and logical rules within the KG. It then leverages\na novel bottom-up graph retrieval method to extract a subgraph for each query\nguided by the learned rules. Finally, a graph convolutional network (GCN)\nadapter uses the retrieved subgraph to enhance the structural embeddings, which\nare then integrated into the prompt for effective LLM fine-tuning. Experimental\nresults on two general domain benchmark datasets and two biomedical datasets\ndemonstrate the superior performance of DrKGC. Furthermore, a realistic case\nstudy in the biomedical domain highlights its interpretability and practical\nutility."
                },
                "authors": [
                    {
                        "name": "Yongkang Xiao"
                    },
                    {
                        "name": "Sinian Zhang"
                    },
                    {
                        "name": "Yi Dai"
                    },
                    {
                        "name": "Huixue Zhou"
                    },
                    {
                        "name": "Jue Hou"
                    },
                    {
                        "name": "Jie Ding"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_comment": "Accepted at EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02209v1",
                "updated": "2025-10-02T16:54:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    54,
                    57,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:54:57Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    54,
                    57,
                    3,
                    275,
                    0
                ],
                "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?"
                },
                "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain."
                },
                "authors": [
                    {
                        "name": "Yanxu Chen"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Yantao Liu"
                    },
                    {
                        "name": "Jin Ye"
                    },
                    {
                        "name": "Jianing Yu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02208v1",
                "updated": "2025-10-02T16:53:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    53,
                    7,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:53:07Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    53,
                    7,
                    3,
                    275,
                    0
                ],
                "title": "Measurement-Guided Consistency Model Sampling for Inverse Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement-Guided Consistency Model Sampling for Inverse Problems"
                },
                "summary": "Diffusion models have become powerful generative priors for solving inverse\nimaging problems, but their reliance on slow multi-step sampling limits\npractical deployment. Consistency models address this bottleneck by enabling\nhigh-quality generation in a single or only a few steps, yet their direct\nadaptation to inverse problems is underexplored. In this paper, we present a\nmodified consistency sampling approach tailored for inverse problem\nreconstruction: the sampler's stochasticity is guided by a\nmeasurement-consistency mechanism tied to the measurement operator, which\nenforces fidelity to the acquired measurements while retaining the efficiency\nof consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom\ndatasets demonstrate consistent improvements in perceptual and pixel-level\nmetrics, including Fr\\'echet Inception Distance, Kernel Inception Distance,\npeak signal-to-noise ratio, and structural similarity index measure, compared\nto baseline consistency sampling, yielding competitive or superior\nreconstructions with only a handful of steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have become powerful generative priors for solving inverse\nimaging problems, but their reliance on slow multi-step sampling limits\npractical deployment. Consistency models address this bottleneck by enabling\nhigh-quality generation in a single or only a few steps, yet their direct\nadaptation to inverse problems is underexplored. In this paper, we present a\nmodified consistency sampling approach tailored for inverse problem\nreconstruction: the sampler's stochasticity is guided by a\nmeasurement-consistency mechanism tied to the measurement operator, which\nenforces fidelity to the acquired measurements while retaining the efficiency\nof consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom\ndatasets demonstrate consistent improvements in perceptual and pixel-level\nmetrics, including Fr\\'echet Inception Distance, Kernel Inception Distance,\npeak signal-to-noise ratio, and structural similarity index measure, compared\nto baseline consistency sampling, yielding competitive or superior\nreconstructions with only a handful of steps."
                },
                "authors": [
                    {
                        "name": "Amirreza Tanevardi"
                    },
                    {
                        "name": "Pooria Abbas Rad Moghadam"
                    },
                    {
                        "name": "Sajjad Amini"
                    }
                ],
                "author_detail": {
                    "name": "Sajjad Amini"
                },
                "author": "Sajjad Amini",
                "arxiv_comment": "5 pages, 3 figures, submitted to IEEE Signal Processing Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02206v1",
                "updated": "2025-10-02T16:52:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    52,
                    45,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:52:45Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    52,
                    45,
                    3,
                    275,
                    0
                ],
                "title": "Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling"
                },
                "summary": "Sequence-to-sequence models have become central in Artificial Intelligence,\nparticularly following the introduction of the transformer architecture. While\ninitially developed for Natural Language Processing, these models have\ndemonstrated utility across domains, including Computer Vision. Such models\nrequire mechanisms to exchange information along the time dimension, typically\nusing recurrent or self-attention layers. However, self-attention scales\nquadratically with sequence length, limiting its practicality for very long\nsequences.\n  We introduce Poolformer, a sequence-to-sequence model that replaces\nself-attention with recurrent layers and incorporates pooling operations to\nreduce sequence length. Poolformer is defined recursively using SkipBlocks,\nwhich contain residual blocks, a down-pooling layer, a nested SkipBlock, an\nup-pooling layer, and additional residual blocks. We conduct extensive\nexperiments to support our architectural choices.\n  Our results show that pooling greatly accelerates training, improves\nperceptual metrics (FID and IS), and prevents overfitting. Our experiments also\nsuggest that long-range dependencies are handled by deep layers, while shallow\nlayers take care of short-term features.\n  Evaluated on raw audio, which naturally features long sequence lengths,\nPoolformer outperforms state-of-the-art models such as SaShiMi and Mamba.\nFuture directions include applications to text and vision, as well as\nmulti-modal scenarios, where a Poolformer-based LLM could effectively process\ndense representations of images and videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence models have become central in Artificial Intelligence,\nparticularly following the introduction of the transformer architecture. While\ninitially developed for Natural Language Processing, these models have\ndemonstrated utility across domains, including Computer Vision. Such models\nrequire mechanisms to exchange information along the time dimension, typically\nusing recurrent or self-attention layers. However, self-attention scales\nquadratically with sequence length, limiting its practicality for very long\nsequences.\n  We introduce Poolformer, a sequence-to-sequence model that replaces\nself-attention with recurrent layers and incorporates pooling operations to\nreduce sequence length. Poolformer is defined recursively using SkipBlocks,\nwhich contain residual blocks, a down-pooling layer, a nested SkipBlock, an\nup-pooling layer, and additional residual blocks. We conduct extensive\nexperiments to support our architectural choices.\n  Our results show that pooling greatly accelerates training, improves\nperceptual metrics (FID and IS), and prevents overfitting. Our experiments also\nsuggest that long-range dependencies are handled by deep layers, while shallow\nlayers take care of short-term features.\n  Evaluated on raw audio, which naturally features long sequence lengths,\nPoolformer outperforms state-of-the-art models such as SaShiMi and Mamba.\nFuture directions include applications to text and vision, as well as\nmulti-modal scenarios, where a Poolformer-based LLM could effectively process\ndense representations of images and videos."
                },
                "authors": [
                    {
                        "name": "Daniel Gallo Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Gallo Fernández"
                },
                "author": "Daniel Gallo Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02200v1",
                "updated": "2025-10-02T16:49:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    49,
                    27,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:49:27Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    49,
                    27,
                    3,
                    275,
                    0
                ],
                "title": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge\n  Graph Exploration Utilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge\n  Graph Exploration Utilities"
                },
                "summary": "Interacting with knowledge graphs can be a daunting task for people without a\nbackground in computer science since the query language that is used (SPARQL)\nhas a high barrier of entry. Large language models (LLMs) can lower that\nbarrier by providing support in the form of Text2SPARQL translation. In this\npaper we introduce a generalized method based on SPINACH, an LLM backed agent\nthat translates natural language questions to SPARQL queries not in a single\nshot, but as an iterative process of exploration and execution. We describe the\noverall architecture and reasoning behind our design decisions, and also\nconduct a thorough analysis of the agent behavior to gain insights into future\nareas for targeted improvements. This work was motivated by the Text2SPARQL\nchallenge, a challenge that was held to facilitate improvements in the\nText2SPARQL domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interacting with knowledge graphs can be a daunting task for people without a\nbackground in computer science since the query language that is used (SPARQL)\nhas a high barrier of entry. Large language models (LLMs) can lower that\nbarrier by providing support in the form of Text2SPARQL translation. In this\npaper we introduce a generalized method based on SPINACH, an LLM backed agent\nthat translates natural language questions to SPARQL queries not in a single\nshot, but as an iterative process of exploration and execution. We describe the\noverall architecture and reasoning behind our design decisions, and also\nconduct a thorough analysis of the agent behavior to gain insights into future\nareas for targeted improvements. This work was motivated by the Text2SPARQL\nchallenge, a challenge that was held to facilitate improvements in the\nText2SPARQL domain."
                },
                "authors": [
                    {
                        "name": "Felix Brei"
                    },
                    {
                        "name": "Lorenz Bühmann"
                    },
                    {
                        "name": "Johannes Frey"
                    },
                    {
                        "name": "Daniel Gerber"
                    },
                    {
                        "name": "Lars-Peter Meyer"
                    },
                    {
                        "name": "Claus Stadler"
                    },
                    {
                        "name": "Kirill Bulert"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Bulert"
                },
                "author": "Kirill Bulert",
                "arxiv_comment": "peer reviewed publication at Text2SPARQL Workshop @ ESWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02197v1",
                "updated": "2025-10-02T16:45:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    45,
                    43,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:45:43Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    45,
                    43,
                    3,
                    275,
                    0
                ],
                "title": "Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition:\n  A Machine Learning Approach for Small-Scale Farming Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition:\n  A Machine Learning Approach for Small-Scale Farming Applications"
                },
                "summary": "Accurate livestock identification is a cornerstone of modern farming: it\nsupports health monitoring, breeding programs, and productivity tracking.\nHowever, common pig identification methods, such as ear tags and microchips,\nare often unreliable, costly, target pure breeds, and thus impractical for\nsmall-scale farmers. To address this gap, we propose a noninvasive biometric\nidentification approach that leverages uniqueness of the auricular vein\npatterns. To this end, we have collected 800 ear images from 20 mixed-breed\npigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a\nstandard smartphone and simple back lighting. A multistage computer vision\npipeline was developed to enhance vein visibility, extract structural and\nspatial features, and generate biometric signatures. These features were then\nclassified using machine learning models. Support Vector Machines (SVM)\nachieved the highest accuracy: correctly identifying pigs with 98.12% precision\nacross mixed-breed populations. The entire process from image processing to\nclassification was completed in an average of 8.3 seconds, demonstrating\nfeasibility for real-time farm deployment. We believe that by replacing fragile\nphysical identifiers with permanent biological markers, this system provides\nfarmers with a cost-effective and stress-free method of animal identification.\nMore broadly, the findings confirm the practicality of auricular vein\nbiometrics for digitizing livestock management, reinforcing its potential to\nextend the benefits of precision farming to resource-constrained agricultural\ncommunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate livestock identification is a cornerstone of modern farming: it\nsupports health monitoring, breeding programs, and productivity tracking.\nHowever, common pig identification methods, such as ear tags and microchips,\nare often unreliable, costly, target pure breeds, and thus impractical for\nsmall-scale farmers. To address this gap, we propose a noninvasive biometric\nidentification approach that leverages uniqueness of the auricular vein\npatterns. To this end, we have collected 800 ear images from 20 mixed-breed\npigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a\nstandard smartphone and simple back lighting. A multistage computer vision\npipeline was developed to enhance vein visibility, extract structural and\nspatial features, and generate biometric signatures. These features were then\nclassified using machine learning models. Support Vector Machines (SVM)\nachieved the highest accuracy: correctly identifying pigs with 98.12% precision\nacross mixed-breed populations. The entire process from image processing to\nclassification was completed in an average of 8.3 seconds, demonstrating\nfeasibility for real-time farm deployment. We believe that by replacing fragile\nphysical identifiers with permanent biological markers, this system provides\nfarmers with a cost-effective and stress-free method of animal identification.\nMore broadly, the findings confirm the practicality of auricular vein\nbiometrics for digitizing livestock management, reinforcing its potential to\nextend the benefits of precision farming to resource-constrained agricultural\ncommunities."
                },
                "authors": [
                    {
                        "name": "Emmanuel Nsengiyumvaa"
                    },
                    {
                        "name": "Leonard Niyitegekaa"
                    },
                    {
                        "name": "Eric Umuhoza"
                    }
                ],
                "author_detail": {
                    "name": "Eric Umuhoza"
                },
                "author": "Eric Umuhoza",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21499v2",
                "updated": "2025-10-02T16:45:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    45,
                    24,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-25T19:57:36Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    57,
                    36,
                    3,
                    268,
                    0
                ],
                "title": "On Code-Induced Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Code-Induced Reasoning in LLMs"
                },
                "summary": "Code data has been shown to enhance the reasoning capabilities of large\nlanguage models (LLMs), but it remains unclear which aspects of code are most\nresponsible. We investigate this question with a systematic, data-centric\nframework. We construct parallel instruction datasets in ten programming\nlanguages and apply controlled perturbations that selectively disrupt\nstructural or semantic properties of code. We then finetune LLMs from five\nmodel families and eight scales on each variant and evaluate their performance\non natural language, math, and code tasks. Across 3,331 experiments, our\nresults show that LLMs are more vulnerable to structural perturbations than\nsemantic ones, particularly on math and code tasks. Appropriate abstractions\nlike pseudocode and flowcharts can be as effective as code, while encoding the\nsame information with fewer tokens without adhering to original syntax can\noften retain or even improve performance. Remarkably, even corrupted code with\nmisleading signals remains competitive when surface-level regularities persist.\nFinally, syntactic styles also shape task-specific gains with Python favoring\nnatural language reasoning and lower-level languages such as Java and Rust\nfavoring math. Through our systematic framework, we aim to provide insight into\nhow different properties of code influence reasoning and inform the design of\ntraining data for enhancing LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code data has been shown to enhance the reasoning capabilities of large\nlanguage models (LLMs), but it remains unclear which aspects of code are most\nresponsible. We investigate this question with a systematic, data-centric\nframework. We construct parallel instruction datasets in ten programming\nlanguages and apply controlled perturbations that selectively disrupt\nstructural or semantic properties of code. We then finetune LLMs from five\nmodel families and eight scales on each variant and evaluate their performance\non natural language, math, and code tasks. Across 3,331 experiments, our\nresults show that LLMs are more vulnerable to structural perturbations than\nsemantic ones, particularly on math and code tasks. Appropriate abstractions\nlike pseudocode and flowcharts can be as effective as code, while encoding the\nsame information with fewer tokens without adhering to original syntax can\noften retain or even improve performance. Remarkably, even corrupted code with\nmisleading signals remains competitive when surface-level regularities persist.\nFinally, syntactic styles also shape task-specific gains with Python favoring\nnatural language reasoning and lower-level languages such as Java and Rust\nfavoring math. Through our systematic framework, we aim to provide insight into\nhow different properties of code influence reasoning and inform the design of\ntraining data for enhancing LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Abdul Waheed"
                    },
                    {
                        "name": "Zhen Wu"
                    },
                    {
                        "name": "Carolyn Rosé"
                    },
                    {
                        "name": "Daphne Ippolito"
                    }
                ],
                "author_detail": {
                    "name": "Daphne Ippolito"
                },
                "author": "Daphne Ippolito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02194v1",
                "updated": "2025-10-02T16:43:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    43,
                    33,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:43:33Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    43,
                    33,
                    3,
                    275,
                    0
                ],
                "title": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress across a wide\nrange of tasks, but remain vulnerable to safety risks such as harmful content\ngeneration and jailbreak attacks. Existing safety techniques -- including\nexternal guardrails, inference-time guidance, and post-training alignment --\neach face limitations in balancing safety, utility, and controllability. In\nthis work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM\nsafety through safety-aware upcycling. Our approach first identifies\nsafety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)\nstructure, where the router acts as a soft guardrail that selectively activates\noriginal MLPs and added safety experts. We further introduce a two-stage SFT\nstrategy to strengthen safety discrimination while preserving general\ncapabilities. To enable flexible control at inference time, we introduce a\nsafety temperature mechanism, allowing dynamic adjustment of the trade-off\nbetween safety and utility. Experiments across multiple benchmarks, base model,\nand model scales demonstrate that UpSafe$^\\circ$C achieves robust safety\nimprovements against harmful and jailbreak inputs, while maintaining\ncompetitive performance on general tasks. Moreover, analysis shows that safety\ntemperature provides fine-grained inference-time control that achieves the\nPareto-optimal frontier between utility and safety. Our results highlight a new\ndirection for LLM safety: moving from static alignment toward dynamic, modular,\nand inference-aware control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress across a wide\nrange of tasks, but remain vulnerable to safety risks such as harmful content\ngeneration and jailbreak attacks. Existing safety techniques -- including\nexternal guardrails, inference-time guidance, and post-training alignment --\neach face limitations in balancing safety, utility, and controllability. In\nthis work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM\nsafety through safety-aware upcycling. Our approach first identifies\nsafety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)\nstructure, where the router acts as a soft guardrail that selectively activates\noriginal MLPs and added safety experts. We further introduce a two-stage SFT\nstrategy to strengthen safety discrimination while preserving general\ncapabilities. To enable flexible control at inference time, we introduce a\nsafety temperature mechanism, allowing dynamic adjustment of the trade-off\nbetween safety and utility. Experiments across multiple benchmarks, base model,\nand model scales demonstrate that UpSafe$^\\circ$C achieves robust safety\nimprovements against harmful and jailbreak inputs, while maintaining\ncompetitive performance on general tasks. Moreover, analysis shows that safety\ntemperature provides fine-grained inference-time control that achieves the\nPareto-optimal frontier between utility and safety. Our results highlight a new\ndirection for LLM safety: moving from static alignment toward dynamic, modular,\nand inference-aware control."
                },
                "authors": [
                    {
                        "name": "Yuhao Sun"
                    },
                    {
                        "name": "Zhuoer Xu"
                    },
                    {
                        "name": "Shiwen Cui"
                    },
                    {
                        "name": "Kun Yang"
                    },
                    {
                        "name": "Lingyun Yu"
                    },
                    {
                        "name": "Yongdong Zhang"
                    },
                    {
                        "name": "Hongtao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Xie"
                },
                "author": "Hongtao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02185v1",
                "updated": "2025-10-02T16:36:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    36,
                    56,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:36:56Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    36,
                    56,
                    3,
                    275,
                    0
                ],
                "title": "FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen\n  Using Agentic AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen\n  Using Agentic AI"
                },
                "summary": "Fuzz testing has become a cornerstone technique for identifying software bugs\nand security vulnerabilities, with broad adoption in both industry and\nopen-source communities. Directly fuzzing a function requires fuzz drivers,\nwhich translate random fuzzer inputs into valid arguments for the target\nfunction. Given the cost and expertise required to manually develop fuzz\ndrivers, methods exist that leverage program analysis and Large Language Models\nto automatically generate these drivers. However, the generated fuzz drivers\nfrequently lead to false positive crashes, especially in functions highly\nstructured input and complex state requirements. This problem is especially\ncrucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as\nreporting false positive crashes to maintainers impede trust in both the system\nand the team.\n  This paper presents two AI-driven strategies to reduce false positives in\nOSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,\nconstraint-based fuzz driver generation proactively enforces constraints on a\nfunction's inputs and state to guide driver creation. Second, context-based\ncrash validation reactively analyzes function callers to determine whether\nreported crashes are feasible from program entry points. Using 1,500 benchmark\nfunctions from OSS-Fuzz, we show that these strategies reduce spurious crashes\nby up to 8%, cut reported crashes by more than half, and demonstrate that\nfrontier LLMs can serve as reliable program analysis agents. Our results\nhighlight the promise and challenges of integrating AI into large-scale fuzzing\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzz testing has become a cornerstone technique for identifying software bugs\nand security vulnerabilities, with broad adoption in both industry and\nopen-source communities. Directly fuzzing a function requires fuzz drivers,\nwhich translate random fuzzer inputs into valid arguments for the target\nfunction. Given the cost and expertise required to manually develop fuzz\ndrivers, methods exist that leverage program analysis and Large Language Models\nto automatically generate these drivers. However, the generated fuzz drivers\nfrequently lead to false positive crashes, especially in functions highly\nstructured input and complex state requirements. This problem is especially\ncrucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as\nreporting false positive crashes to maintainers impede trust in both the system\nand the team.\n  This paper presents two AI-driven strategies to reduce false positives in\nOSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,\nconstraint-based fuzz driver generation proactively enforces constraints on a\nfunction's inputs and state to guide driver creation. Second, context-based\ncrash validation reactively analyzes function callers to determine whether\nreported crashes are feasible from program entry points. Using 1,500 benchmark\nfunctions from OSS-Fuzz, we show that these strategies reduce spurious crashes\nby up to 8%, cut reported crashes by more than half, and demonstrate that\nfrontier LLMs can serve as reliable program analysis agents. Our results\nhighlight the promise and challenges of integrating AI into large-scale fuzzing\npipelines."
                },
                "authors": [
                    {
                        "name": "Paschal C. Amusuo"
                    },
                    {
                        "name": "Dongge Liu"
                    },
                    {
                        "name": "Ricardo Andres Calvo Mendez"
                    },
                    {
                        "name": "Jonathan Metzman"
                    },
                    {
                        "name": "Oliver Chang"
                    },
                    {
                        "name": "James C. Davis"
                    }
                ],
                "author_detail": {
                    "name": "James C. Davis"
                },
                "author": "James C. Davis",
                "arxiv_comment": "12 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; F.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03206v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03206v3",
                "updated": "2025-10-02T16:36:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    36,
                    8,
                    3,
                    275,
                    0
                ],
                "published": "2025-04-04T06:35:02Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    35,
                    2,
                    4,
                    94,
                    0
                ],
                "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward"
                },
                "summary": "Effective conversational agents like large language models (LLMs) must\npersonalize their interactions to adapt to user preferences, personalities, and\nattributes across diverse domains like education and healthcare. Current\nmethods like Reinforcement Learning from Human Feedback (RLHF), often\nprioritize helpfulness and safety but fall short in fostering truly empathetic,\nadaptive, and personalized dialogues. Existing personalization approaches\ntypically rely on extensive user history, limiting their effectiveness for new\nor context-limited users. To address these limitations, we propose leveraging a\nuser model to incorporate a curiosity-based intrinsic reward into multi-turn\nRLHF. This novel reward mechanism encourages the LLM agent to actively infer\nuser traits by optimizing conversations to improve its user model's accuracy.\nConsequently, the agent delivers more personalized interactions by learning\nmore about the user. We demonstrate our method's effectiveness in two distinct\ndomains: significantly improving personalization performance in a\nconversational recommendation task, and personalizing conversations for\ndifferent learning styles in an educational setting. We show improved\ngeneralization capabilities compared to traditional multi-turn RLHF, all while\nmaintaining conversation quality. Our method offers a promising solution for\ncreating more personalized, adaptive, and engaging conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective conversational agents like large language models (LLMs) must\npersonalize their interactions to adapt to user preferences, personalities, and\nattributes across diverse domains like education and healthcare. Current\nmethods like Reinforcement Learning from Human Feedback (RLHF), often\nprioritize helpfulness and safety but fall short in fostering truly empathetic,\nadaptive, and personalized dialogues. Existing personalization approaches\ntypically rely on extensive user history, limiting their effectiveness for new\nor context-limited users. To address these limitations, we propose leveraging a\nuser model to incorporate a curiosity-based intrinsic reward into multi-turn\nRLHF. This novel reward mechanism encourages the LLM agent to actively infer\nuser traits by optimizing conversations to improve its user model's accuracy.\nConsequently, the agent delivers more personalized interactions by learning\nmore about the user. We demonstrate our method's effectiveness in two distinct\ndomains: significantly improving personalization performance in a\nconversational recommendation task, and personalizing conversations for\ndifferent learning styles in an educational setting. We show improved\ngeneralization capabilities compared to traditional multi-turn RLHF, all while\nmaintaining conversation quality. Our method offers a promising solution for\ncreating more personalized, adaptive, and engaging conversational agents."
                },
                "authors": [
                    {
                        "name": "Yanming Wan"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Marwa Abdulhai"
                    },
                    {
                        "name": "Lior Shani"
                    },
                    {
                        "name": "Natasha Jaques"
                    }
                ],
                "author_detail": {
                    "name": "Natasha Jaques"
                },
                "author": "Natasha Jaques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03206v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03206v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02178v1",
                "updated": "2025-10-02T16:30:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    30,
                    37,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:30:37Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    30,
                    37,
                    3,
                    275,
                    0
                ],
                "title": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical\n  Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical\n  Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis"
                },
                "summary": "3D indoor layout synthesis is crucial for creating virtual environments.\nTraditional methods struggle with generalization due to fixed datasets. While\nrecent LLM and VLM-based approaches offer improved semantic richness, they\noften lack robust and flexible refinement, resulting in suboptimal layouts. We\ndevelop DisCo-Layout, a novel framework that disentangles and coordinates\nphysical and semantic refinement. For independent refinement, our Semantic\nRefinement Tool (SRT) corrects abstract object relationships, while the\nPhysical Refinement Tool (PRT) resolves concrete spatial issues via a\ngrid-matching algorithm. For collaborative refinement, a multi-agent framework\nintelligently orchestrates these tools, featuring a planner for placement\nrules, a designer for initial layouts, and an evaluator for assessment.\nExperiments demonstrate DisCo-Layout's state-of-the-art performance, generating\nrealistic, coherent, and generalizable 3D indoor layouts. Our code will be\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D indoor layout synthesis is crucial for creating virtual environments.\nTraditional methods struggle with generalization due to fixed datasets. While\nrecent LLM and VLM-based approaches offer improved semantic richness, they\noften lack robust and flexible refinement, resulting in suboptimal layouts. We\ndevelop DisCo-Layout, a novel framework that disentangles and coordinates\nphysical and semantic refinement. For independent refinement, our Semantic\nRefinement Tool (SRT) corrects abstract object relationships, while the\nPhysical Refinement Tool (PRT) resolves concrete spatial issues via a\ngrid-matching algorithm. For collaborative refinement, a multi-agent framework\nintelligently orchestrates these tools, featuring a planner for placement\nrules, a designer for initial layouts, and an evaluator for assessment.\nExperiments demonstrate DisCo-Layout's state-of-the-art performance, generating\nrealistic, coherent, and generalizable 3D indoor layouts. Our code will be\npublicly available."
                },
                "authors": [
                    {
                        "name": "Jialin Gao"
                    },
                    {
                        "name": "Donghao Zhou"
                    },
                    {
                        "name": "Mingjian Liang"
                    },
                    {
                        "name": "Lihao Liu"
                    },
                    {
                        "name": "Chi-Wing Fu"
                    },
                    {
                        "name": "Xiaowei Hu"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng-Ann Heng"
                },
                "author": "Pheng-Ann Heng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02173v1",
                "updated": "2025-10-02T16:24:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    24,
                    28,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:24:28Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    24,
                    28,
                    3,
                    275,
                    0
                ],
                "title": "Learning to Reason for Hallucination Span Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason for Hallucination Span Detection"
                },
                "summary": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans."
                },
                "authors": [
                    {
                        "name": "Hsuan Su"
                    },
                    {
                        "name": "Ting-Yao Hu"
                    },
                    {
                        "name": "Hema Swetha Koppula"
                    },
                    {
                        "name": "Kundan Krishna"
                    },
                    {
                        "name": "Hadi Pouransari"
                    },
                    {
                        "name": "Cheng-Yu Hsieh"
                    },
                    {
                        "name": "Cem Koc"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Oncel Tuzel"
                    },
                    {
                        "name": "Raviteja Vemulapalli"
                    }
                ],
                "author_detail": {
                    "name": "Raviteja Vemulapalli"
                },
                "author": "Raviteja Vemulapalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10862v2",
                "updated": "2025-10-02T16:15:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    15,
                    20,
                    3,
                    275,
                    0
                ],
                "published": "2024-10-07T19:53:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    19,
                    53,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "Superficial Safety Alignment Hypothesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superficial Safety Alignment Hypothesis"
                },
                "summary": "As large language models (LLMs) are overwhelmingly more and more integrated\ninto various applications, ensuring they generate safe responses is a pressing\nneed. Previous studies on alignment have largely focused on general\ninstruction-following but have often overlooked the distinct properties of\nsafety alignment, such as the brittleness of safety mechanisms. To bridge the\ngap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which\nposits that safety alignment teaches an otherwise unsafe model to choose the\ncorrect reasoning direction - fulfill or refuse users' requests - interpreted\nas an implicit binary classification task. Through SSAH, we hypothesize that\nonly a few essential components can establish safety guardrails in LLMs. We\nsuccessfully identify four types of attribute-critical components: Safety\nCritical Unit (SCU), Utility Critical Unit (UCU), Complex Unit (CU), and\nRedundant Unit (RU). Our findings show that freezing certain safety-critical\ncomponents during fine-tuning allows the model to retain its safety attributes\nwhile adapting to new tasks. Similarly, we show that leveraging redundant units\nin the pre-trained model as an \"alignment budget\" can effectively minimize the\nalignment tax while achieving the alignment goal. All considered, this paper\nconcludes that the atomic functional unit for safety in LLMs is at the neuron\nlevel and underscores that safety alignment should not be complicated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are overwhelmingly more and more integrated\ninto various applications, ensuring they generate safe responses is a pressing\nneed. Previous studies on alignment have largely focused on general\ninstruction-following but have often overlooked the distinct properties of\nsafety alignment, such as the brittleness of safety mechanisms. To bridge the\ngap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which\nposits that safety alignment teaches an otherwise unsafe model to choose the\ncorrect reasoning direction - fulfill or refuse users' requests - interpreted\nas an implicit binary classification task. Through SSAH, we hypothesize that\nonly a few essential components can establish safety guardrails in LLMs. We\nsuccessfully identify four types of attribute-critical components: Safety\nCritical Unit (SCU), Utility Critical Unit (UCU), Complex Unit (CU), and\nRedundant Unit (RU). Our findings show that freezing certain safety-critical\ncomponents during fine-tuning allows the model to retain its safety attributes\nwhile adapting to new tasks. Similarly, we show that leveraging redundant units\nin the pre-trained model as an \"alignment budget\" can effectively minimize the\nalignment tax while achieving the alignment goal. All considered, this paper\nconcludes that the atomic functional unit for safety in LLMs is at the neuron\nlevel and underscores that safety alignment should not be complicated."
                },
                "authors": [
                    {
                        "name": "Jianwei Li"
                    },
                    {
                        "name": "Jung-Eun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jung-Eun Kim"
                },
                "author": "Jung-Eun Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00750v2",
                "updated": "2025-10-02T16:10:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    10,
                    36,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-31T23:32:01Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    32,
                    1,
                    5,
                    151,
                    0
                ],
                "title": "CodeSense: a Real-World Benchmark and Dataset for Code Semantic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSense: a Real-World Benchmark and Dataset for Code Semantic\n  Reasoning"
                },
                "summary": "Understanding and reasoning about code semantics is essential for enhancing\ncode LLMs' abilities to solve real-world software engineering (SE) tasks.\nAlthough several code reasoning benchmarks exist, most rely on synthetic\ndatasets or educational coding problems and focus on coarse-grained reasoning\ntasks such as input/output prediction, limiting their effectiveness in\nevaluating LLMs in practical SE contexts. To bridge this gap, we propose\nCodeSense, the first benchmark that makes available a spectrum of fine-grained\ncode reasoning tasks concerned with the software engineering of real-world\ncode. We collected Python, C and Java software projects from real-world\nrepositories. We executed tests from these repositories, collected their\nexecution traces, and constructed a ground truth dataset for fine-grained\nsemantic reasoning tasks. We then performed comprehensive evaluations on\nstate-of-the-art LLMs. Our results show a clear performance gap for the models\nto handle fine-grained reasoning tasks. Although prompting techniques such as\nchain-of-thought and in-context learning helped, the lack of code semantics in\nLLMs fundamentally limit models' capabilities of code reasoning. Besides\ndataset, benchmark and evaluation, our work produced an execution tracing\nframework and tool set that make it easy to collect ground truth for\nfine-grained SE reasoning tasks, offering a strong basis for future benchmark\nconstruction and model post training. Our code and data are located at\nhttps://codesense-bench.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and reasoning about code semantics is essential for enhancing\ncode LLMs' abilities to solve real-world software engineering (SE) tasks.\nAlthough several code reasoning benchmarks exist, most rely on synthetic\ndatasets or educational coding problems and focus on coarse-grained reasoning\ntasks such as input/output prediction, limiting their effectiveness in\nevaluating LLMs in practical SE contexts. To bridge this gap, we propose\nCodeSense, the first benchmark that makes available a spectrum of fine-grained\ncode reasoning tasks concerned with the software engineering of real-world\ncode. We collected Python, C and Java software projects from real-world\nrepositories. We executed tests from these repositories, collected their\nexecution traces, and constructed a ground truth dataset for fine-grained\nsemantic reasoning tasks. We then performed comprehensive evaluations on\nstate-of-the-art LLMs. Our results show a clear performance gap for the models\nto handle fine-grained reasoning tasks. Although prompting techniques such as\nchain-of-thought and in-context learning helped, the lack of code semantics in\nLLMs fundamentally limit models' capabilities of code reasoning. Besides\ndataset, benchmark and evaluation, our work produced an execution tracing\nframework and tool set that make it easy to collect ground truth for\nfine-grained SE reasoning tasks, offering a strong basis for future benchmark\nconstruction and model post training. Our code and data are located at\nhttps://codesense-bench.github.io/."
                },
                "authors": [
                    {
                        "name": "Monoshi Kumar Roy"
                    },
                    {
                        "name": "Simin Chen"
                    },
                    {
                        "name": "Benjamin Steenhoek"
                    },
                    {
                        "name": "Jinjun Peng"
                    },
                    {
                        "name": "Gail Kaiser"
                    },
                    {
                        "name": "Baishakhi Ray"
                    },
                    {
                        "name": "Wei Le"
                    }
                ],
                "author_detail": {
                    "name": "Wei Le"
                },
                "author": "Wei Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02157v1",
                "updated": "2025-10-02T16:08:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    8,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:08:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    8,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "Agentic Reasoning and Refinement through Semantic Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Reasoning and Refinement through Semantic Interaction"
                },
                "summary": "Sensemaking report writing often requires multiple refinements in the\niterative process. While Large Language Models (LLMs) have shown promise in\ngenerating initial reports based on human visual workspace representations,\nthey struggle to precisely incorporate sequential semantic interactions during\nthe refinement process. We introduce VIS-ReAct, a framework that reasons about\nnewly-added semantic interactions in visual workspaces to steer the LLM for\nreport refinement.\n  VIS-ReAct is a two-agent framework: a primary LLM analysis agent interprets\nnew semantic interactions to infer user intentions and generate refinement\nplanning, followed by an LLM refinement agent that updates reports accordingly.\nThrough case study, VIS-ReAct outperforms baseline and VIS-ReAct (without LLM\nanalysis) on targeted refinement, semantic fidelity, and transparent inference.\nResults demonstrate that VIS-ReAct better handles various interaction types and\ngranularities while enhancing the transparency of human-LLM collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensemaking report writing often requires multiple refinements in the\niterative process. While Large Language Models (LLMs) have shown promise in\ngenerating initial reports based on human visual workspace representations,\nthey struggle to precisely incorporate sequential semantic interactions during\nthe refinement process. We introduce VIS-ReAct, a framework that reasons about\nnewly-added semantic interactions in visual workspaces to steer the LLM for\nreport refinement.\n  VIS-ReAct is a two-agent framework: a primary LLM analysis agent interprets\nnew semantic interactions to infer user intentions and generate refinement\nplanning, followed by an LLM refinement agent that updates reports accordingly.\nThrough case study, VIS-ReAct outperforms baseline and VIS-ReAct (without LLM\nanalysis) on targeted refinement, semantic fidelity, and transparent inference.\nResults demonstrate that VIS-ReAct better handles various interaction types and\ngranularities while enhancing the transparency of human-LLM collaboration."
                },
                "authors": [
                    {
                        "name": "Xuxin Tang"
                    },
                    {
                        "name": "Rehema Abulikemu"
                    },
                    {
                        "name": "Eric Krokos"
                    },
                    {
                        "name": "Kirsten Whitley"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Chris North"
                    }
                ],
                "author_detail": {
                    "name": "Chris North"
                },
                "author": "Chris North",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15153v2",
                "updated": "2025-10-02T15:55:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    55,
                    21,
                    3,
                    275,
                    0
                ],
                "published": "2025-02-21T02:24:43Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    2,
                    24,
                    43,
                    4,
                    52,
                    0
                ],
                "title": "When Disagreements Elicit Robustness: Investigating Self-Repair\n  Capabilities under LLM Multi-Agent Disagreements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Disagreements Elicit Robustness: Investigating Self-Repair\n  Capabilities under LLM Multi-Agent Disagreements"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have upgraded them from\nsophisticated text generators to autonomous agents capable of cooperation and\ntool use in multi-agent systems (MAS). However, it remains unclear how\ndisagreements shape collective decision-making. In this paper, we revisit the\nrole of disagreement and argue that general, partially overlapping\ndisagreements prevent premature consensus and expand the explored solution\nspace, while disagreements on task-critical steps can derail collaboration\ndepending on the topology of solution paths. We investigate two collaborative\nsettings with distinct path structures: collaborative reasoning (CounterFact,\nMQuAKE-cf), which typically follows a single evidential chain, whereas\ncollaborative programming (HumanEval, GAIA) often adopts multiple valid\nimplementations. Disagreements are instantiated as general heterogeneity among\nagents and as task-critical counterfactual knowledge edits injected into\ncontext or parameters. Experiments reveal that general disagreements\nconsistently improve success by encouraging complementary exploration. By\ncontrast, task-critical disagreements substantially reduce success on\nsingle-path reasoning, yet have a limited impact on programming, where agents\ncan choose alternative solutions. Trace analyses show that MAS frequently\nbypasses the edited facts in programming but rarely does so in reasoning,\nrevealing an emergent self-repair capability that depends on solution-path\nrather than scale alone. Our code is available at\nhttps://github.com/wbw625/MultiAgentRobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have upgraded them from\nsophisticated text generators to autonomous agents capable of cooperation and\ntool use in multi-agent systems (MAS). However, it remains unclear how\ndisagreements shape collective decision-making. In this paper, we revisit the\nrole of disagreement and argue that general, partially overlapping\ndisagreements prevent premature consensus and expand the explored solution\nspace, while disagreements on task-critical steps can derail collaboration\ndepending on the topology of solution paths. We investigate two collaborative\nsettings with distinct path structures: collaborative reasoning (CounterFact,\nMQuAKE-cf), which typically follows a single evidential chain, whereas\ncollaborative programming (HumanEval, GAIA) often adopts multiple valid\nimplementations. Disagreements are instantiated as general heterogeneity among\nagents and as task-critical counterfactual knowledge edits injected into\ncontext or parameters. Experiments reveal that general disagreements\nconsistently improve success by encouraging complementary exploration. By\ncontrast, task-critical disagreements substantially reduce success on\nsingle-path reasoning, yet have a limited impact on programming, where agents\ncan choose alternative solutions. Trace analyses show that MAS frequently\nbypasses the edited facts in programming but rarely does so in reasoning,\nrevealing an emergent self-repair capability that depends on solution-path\nrather than scale alone. Our code is available at\nhttps://github.com/wbw625/MultiAgentRobustness."
                },
                "authors": [
                    {
                        "name": "Tianjie Ju"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Mong-Li Lee"
                    },
                    {
                        "name": "Wynne Hsu"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Qianren Wang"
                    },
                    {
                        "name": "Pengzhou Cheng"
                    },
                    {
                        "name": "Zongru Wu"
                    },
                    {
                        "name": "Haodong Zhao"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Gongshen Liu"
                },
                "author": "Gongshen Liu",
                "arxiv_comment": "Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14746v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14746v3",
                "updated": "2025-10-02T15:55:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    55,
                    20,
                    3,
                    275,
                    0
                ],
                "published": "2025-08-20T14:43:04Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    43,
                    4,
                    2,
                    232,
                    0
                ],
                "title": "MissionHD: Hyperdimensional Refinement of Distribution-Deficient\n  Reasoning Graphs for Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MissionHD: Hyperdimensional Refinement of Distribution-Deficient\n  Reasoning Graphs for Video Anomaly Detection"
                },
                "summary": "LLM-generated reasoning graphs, referred to as mission-specific graphs\n(MSGs), are increasingly used for video anomaly detection (VAD) and recognition\n(VAR). These MSGs are novel artifacts: they often exhibit skewed connectivity\nand lack large-scale datasets for pre-training, which makes existing graph\nstructure refinement (GSR) methods ineffective. To address this challenge, we\npropose HDC-constrained Graph Structure Refinement (HDC-GSR), a paradigm that\nleverages hyperdimensional computing (HDC) to optimize decodable graph\nrepresentations without relying on structural-distribution learning. Building\non this paradigm, we introduce MissionHD, an HDC framework that encodes graphs\nwith constrained graph-neural operations, aligns them directly with downstream\ntask loss, and decodes refined structures. Experiments on VAD/VAR benchmarks\ndemonstrate that MissionHD-refined graphs consistently improve performance,\nestablishing HDC-GSR as an effective pre-processing step for structured\nreasoning in video anomaly tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-generated reasoning graphs, referred to as mission-specific graphs\n(MSGs), are increasingly used for video anomaly detection (VAD) and recognition\n(VAR). These MSGs are novel artifacts: they often exhibit skewed connectivity\nand lack large-scale datasets for pre-training, which makes existing graph\nstructure refinement (GSR) methods ineffective. To address this challenge, we\npropose HDC-constrained Graph Structure Refinement (HDC-GSR), a paradigm that\nleverages hyperdimensional computing (HDC) to optimize decodable graph\nrepresentations without relying on structural-distribution learning. Building\non this paradigm, we introduce MissionHD, an HDC framework that encodes graphs\nwith constrained graph-neural operations, aligns them directly with downstream\ntask loss, and decodes refined structures. Experiments on VAD/VAR benchmarks\ndemonstrate that MissionHD-refined graphs consistently improve performance,\nestablishing HDC-GSR as an effective pre-processing step for structured\nreasoning in video anomaly tasks."
                },
                "authors": [
                    {
                        "name": "Sanggeon Yun"
                    },
                    {
                        "name": "Raheeb Hassan"
                    },
                    {
                        "name": "Ryozo Masukawa"
                    },
                    {
                        "name": "Nathaniel D. Bastian"
                    },
                    {
                        "name": "Mohsen Imani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Imani"
                },
                "author": "Mohsen Imani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14746v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14746v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02133v1",
                "updated": "2025-10-02T15:42:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    42,
                    35,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T15:42:35Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    42,
                    35,
                    3,
                    275,
                    0
                ],
                "title": "FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic\n  Documents for Training Document Understanding Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic\n  Documents for Training Document Understanding Models"
                },
                "summary": "Developing document understanding models at enterprise scale requires large,\ndiverse, and well-annotated datasets spanning a wide range of document types.\nHowever, collecting such data is prohibitively expensive due to privacy\nconstraints, legal restrictions, and the sheer volume of manual annotation\nneeded - costs that can scale into millions of dollars. We introduce FlexDoc, a\nscalable synthetic data generation framework that combines Stochastic Schemas\nand Parameterized Sampling to produce realistic, multilingual semi-structured\ndocuments with rich annotations. By probabilistically modeling layout patterns,\nvisual structure, and content variability, FlexDoc enables the controlled\ngeneration of diverse document variants at scale. Experiments on Key\nInformation Extraction (KIE) tasks demonstrate that FlexDoc-generated data\nimproves the absolute F1 Score by up to 11% when used to augment real datasets,\nwhile reducing annotation effort by over 90% compared to traditional\nhard-template methods. The solution is in active deployment, where it has\naccelerated the development of enterprise-grade document understanding models\nwhile significantly reducing data acquisition and annotation costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing document understanding models at enterprise scale requires large,\ndiverse, and well-annotated datasets spanning a wide range of document types.\nHowever, collecting such data is prohibitively expensive due to privacy\nconstraints, legal restrictions, and the sheer volume of manual annotation\nneeded - costs that can scale into millions of dollars. We introduce FlexDoc, a\nscalable synthetic data generation framework that combines Stochastic Schemas\nand Parameterized Sampling to produce realistic, multilingual semi-structured\ndocuments with rich annotations. By probabilistically modeling layout patterns,\nvisual structure, and content variability, FlexDoc enables the controlled\ngeneration of diverse document variants at scale. Experiments on Key\nInformation Extraction (KIE) tasks demonstrate that FlexDoc-generated data\nimproves the absolute F1 Score by up to 11% when used to augment real datasets,\nwhile reducing annotation effort by over 90% compared to traditional\nhard-template methods. The solution is in active deployment, where it has\naccelerated the development of enterprise-grade document understanding models\nwhile significantly reducing data acquisition and annotation costs."
                },
                "authors": [
                    {
                        "name": "Karan Dua"
                    },
                    {
                        "name": "Hitesh Laxmichand Patel"
                    },
                    {
                        "name": "Puneet Mittal"
                    },
                    {
                        "name": "Ranjeet Gupta"
                    },
                    {
                        "name": "Amit Agarwal"
                    },
                    {
                        "name": "Praneet Pabolu"
                    },
                    {
                        "name": "Srikant Panda"
                    },
                    {
                        "name": "Hansa Meghwani"
                    },
                    {
                        "name": "Graham Horwood"
                    },
                    {
                        "name": "Fahad Shah"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shah"
                },
                "author": "Fahad Shah",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10; I.4.8; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23500v2",
                "updated": "2025-10-02T15:34:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    34,
                    43,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-27T21:15:22Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    21,
                    15,
                    22,
                    5,
                    270,
                    0
                ],
                "title": "Beyond Outliers: A Study of Optimizers Under Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Outliers: A Study of Optimizers Under Quantization"
                },
                "summary": "As new optimizers gain traction and model quantization becomes standard for\nefficient deployment, a key question arises: how does the choice of optimizer\naffect model performance in the presence of quantization? Despite progress in\nboth areas, systematic evidence on optimizer-quantization interactions remains\nlimited. To fill this gap, we study the impact of optimizer choice on model\nrobustness under quantization, considering both post-training quantization\n(PTQ), and quantization-aware training (QAT). We first train full-precision\nmodels, ranging from 50M to 1.5B parameters, with six optimizers, to explore\nthe hyperparameter landscape, and establish well-tuned baselines. We then apply\nPTQ to evaluate how model performance degrades when trained with different\noptimizers. We find that outlier-related metrics, such as the max-to-mean ratio\n(MMR) and Kurtosis, fail to predict the PTQ performance across different\noptimizers. We show analytically that this is due to the MMR capturing only\nisolated layer errors, while ignoring how quantization errors accumulate and\npropagate through the network. To study the QAT degradation, we train quantized\nmodels from scratch and compare them to our original-precision baselines. We\nfind that optimizers performing well in the original pretraining setup may not\nremain optimal under QAT, and that models trained with Shampoo show the lowest\naccuracy degradation. Finally, we derive scaling laws for quantization-aware\ntraining under different optimizers, showing that Shampoo achieves the highest\nparameter efficiency of all tested optimizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As new optimizers gain traction and model quantization becomes standard for\nefficient deployment, a key question arises: how does the choice of optimizer\naffect model performance in the presence of quantization? Despite progress in\nboth areas, systematic evidence on optimizer-quantization interactions remains\nlimited. To fill this gap, we study the impact of optimizer choice on model\nrobustness under quantization, considering both post-training quantization\n(PTQ), and quantization-aware training (QAT). We first train full-precision\nmodels, ranging from 50M to 1.5B parameters, with six optimizers, to explore\nthe hyperparameter landscape, and establish well-tuned baselines. We then apply\nPTQ to evaluate how model performance degrades when trained with different\noptimizers. We find that outlier-related metrics, such as the max-to-mean ratio\n(MMR) and Kurtosis, fail to predict the PTQ performance across different\noptimizers. We show analytically that this is due to the MMR capturing only\nisolated layer errors, while ignoring how quantization errors accumulate and\npropagate through the network. To study the QAT degradation, we train quantized\nmodels from scratch and compare them to our original-precision baselines. We\nfind that optimizers performing well in the original pretraining setup may not\nremain optimal under QAT, and that models trained with Shampoo show the lowest\naccuracy degradation. Finally, we derive scaling laws for quantization-aware\ntraining under different optimizers, showing that Shampoo achieves the highest\nparameter efficiency of all tested optimizers."
                },
                "authors": [
                    {
                        "name": "Georgios Vlassis"
                    },
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Alexandra Volkova"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18379v2",
                "updated": "2025-10-02T15:20:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    20,
                    2,
                    3,
                    275,
                    0
                ],
                "published": "2025-08-25T18:13:50Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    18,
                    13,
                    50,
                    0,
                    237,
                    0
                ],
                "title": "REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking"
                },
                "summary": "Large Language Models (LLMs) have shown strong capabilities in document\nre-ranking, a key component in modern Information Retrieval (IR) systems.\nHowever, existing LLM-based approaches face notable limitations, including\nranking uncertainty, unstable top-k recovery, and high token cost due to\ntoken-intensive prompting. To effectively address these limitations, we propose\nREALM, an uncertainty-aware re-ranking framework that models LLM-derived\nrelevance as Gaussian distributions and refines them through recursive Bayesian\nupdates. By explicitly capturing uncertainty and minimizing redundant queries,\nREALM achieves better rankings more efficiently. Experimental results\ndemonstrate that our REALM surpasses state-of-the-art re-rankers while\nsignificantly reducing token usage and latency, improving NDCG@10 by 0.7-11.9\nand simultaneously reducing the number of LLM inferences by 23.4-84.4%,\npromoting it as the next-generation re-ranker for modern IR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong capabilities in document\nre-ranking, a key component in modern Information Retrieval (IR) systems.\nHowever, existing LLM-based approaches face notable limitations, including\nranking uncertainty, unstable top-k recovery, and high token cost due to\ntoken-intensive prompting. To effectively address these limitations, we propose\nREALM, an uncertainty-aware re-ranking framework that models LLM-derived\nrelevance as Gaussian distributions and refines them through recursive Bayesian\nupdates. By explicitly capturing uncertainty and minimizing redundant queries,\nREALM achieves better rankings more efficiently. Experimental results\ndemonstrate that our REALM surpasses state-of-the-art re-rankers while\nsignificantly reducing token usage and latency, improving NDCG@10 by 0.7-11.9\nand simultaneously reducing the number of LLM inferences by 23.4-84.4%,\npromoting it as the next-generation re-ranker for modern IR systems."
                },
                "authors": [
                    {
                        "name": "Pinhuan Wang"
                    },
                    {
                        "name": "Zhiqiu Xia"
                    },
                    {
                        "name": "Chunhua Liao"
                    },
                    {
                        "name": "Feiyi Wang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_comment": "EMNLP 2025 (Main Conference, Oral). 15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09404v2",
                "updated": "2025-10-02T15:18:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    18,
                    2,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-12T21:16:08Z",
                "published_parsed": [
                    2025,
                    7,
                    12,
                    21,
                    16,
                    8,
                    5,
                    193,
                    0
                ],
                "title": "Scaling Laws for Optimal Data Mixtures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Optimal Data Mixtures"
                },
                "summary": "Large foundation models are typically trained on data from multiple domains,\nwith the data mixture--the proportion of each domain used--playing a critical\nrole in model performance. The standard approach to selecting this mixture\nrelies on trial and error, which becomes impractical for large-scale\npretraining. We propose a systematic method to determine the optimal data\nmixture for any target domain using scaling laws. Our approach accurately\npredicts the loss of a model of size $N$ trained with $D$ tokens and a specific\ndomain weight vector $h$. We validate the universality of these scaling laws by\ndemonstrating their predictive power in three distinct and large-scale\nsettings: large language model (LLM), native multimodal model (NMM), and large\nvision models (LVM) pretraining. We further show that these scaling laws can\nextrapolate to new data mixtures and across scales: their parameters can be\naccurately estimated using a few small-scale training runs, and used to\nestimate the performance at larger scales and unseen domain weights. The\nscaling laws allow to derive the optimal domain weights for any target domain\nunder a given training budget ($N$,$D$), providing a principled alternative to\ncostly trial-and-error methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large foundation models are typically trained on data from multiple domains,\nwith the data mixture--the proportion of each domain used--playing a critical\nrole in model performance. The standard approach to selecting this mixture\nrelies on trial and error, which becomes impractical for large-scale\npretraining. We propose a systematic method to determine the optimal data\nmixture for any target domain using scaling laws. Our approach accurately\npredicts the loss of a model of size $N$ trained with $D$ tokens and a specific\ndomain weight vector $h$. We validate the universality of these scaling laws by\ndemonstrating their predictive power in three distinct and large-scale\nsettings: large language model (LLM), native multimodal model (NMM), and large\nvision models (LVM) pretraining. We further show that these scaling laws can\nextrapolate to new data mixtures and across scales: their parameters can be\naccurately estimated using a few small-scale training runs, and used to\nestimate the performance at larger scales and unseen domain weights. The\nscaling laws allow to derive the optimal domain weights for any target domain\nunder a given training budget ($N$,$D$), providing a principled alternative to\ncostly trial-and-error methods."
                },
                "authors": [
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Louis Bethune"
                    },
                    {
                        "name": "Dan Busbridge"
                    },
                    {
                        "name": "David Grangier"
                    },
                    {
                        "name": "Enrico Fini"
                    },
                    {
                        "name": "Alaaeldin El-Nouby"
                    },
                    {
                        "name": "Pierre Ablin"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Ablin"
                },
                "author": "Pierre Ablin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00553v2",
                "updated": "2025-10-02T15:16:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    16,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-01T06:13:50Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    13,
                    50,
                    2,
                    274,
                    0
                ],
                "title": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models"
                },
                "summary": "Recent advances in reasoning capabilities of large language models (LLMs) are\nlargely driven by reinforcement learning (RL), yet the underlying parameter\ndynamics during RL training remain poorly understood. This work identifies two\nfundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1\nDominance, where the top singular subspace of the parameter update matrix\nnearly fully determines reasoning improvements, recovering over 99\\% of\nperformance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace\nevolves linearly throughout training, enabling accurate prediction from early\ncheckpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the\ngeneralizability of these properties. More importantly, based on these\nfindings, we propose AlphaRL, a plug-in acceleration framework that\nextrapolates the final parameter update using a short early training window,\nachieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning\nperformance without extra modules or hyperparameter tuning. This positions our\nfinding as a versatile and practical tool for large-scale RL, opening a path\ntoward principled, interpretable, and efficient training paradigm for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reasoning capabilities of large language models (LLMs) are\nlargely driven by reinforcement learning (RL), yet the underlying parameter\ndynamics during RL training remain poorly understood. This work identifies two\nfundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1\nDominance, where the top singular subspace of the parameter update matrix\nnearly fully determines reasoning improvements, recovering over 99\\% of\nperformance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace\nevolves linearly throughout training, enabling accurate prediction from early\ncheckpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the\ngeneralizability of these properties. More importantly, based on these\nfindings, we propose AlphaRL, a plug-in acceleration framework that\nextrapolates the final parameter update using a short early training window,\nachieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning\nperformance without extra modules or hyperparameter tuning. This positions our\nfinding as a versatile and practical tool for large-scale RL, opening a path\ntoward principled, interpretable, and efficient training paradigm for LLMs."
                },
                "authors": [
                    {
                        "name": "Yuchen Cai"
                    },
                    {
                        "name": "Ding Cao"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Yuqing Huang"
                    },
                    {
                        "name": "Zhenyu Tan"
                    },
                    {
                        "name": "Benyi Zhang"
                    },
                    {
                        "name": "Guiquan Liu"
                    },
                    {
                        "name": "Junfeng Fang"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Fang"
                },
                "author": "Junfeng Fang",
                "arxiv_comment": "43 pages, 28 figures; 43",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02109v1",
                "updated": "2025-10-02T15:16:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    16,
                    20,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T15:16:20Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    16,
                    20,
                    3,
                    275,
                    0
                ],
                "title": "SpurBreast: A Curated Dataset for Investigating Spurious Correlations in\n  Real-world Breast MRI Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpurBreast: A Curated Dataset for Investigating Spurious Correlations in\n  Real-world Breast MRI Classification"
                },
                "summary": "Deep neural networks (DNNs) have demonstrated remarkable success in medical\nimaging, yet their real-world deployment remains challenging due to spurious\ncorrelations, where models can learn non-clinical features instead of\nmeaningful medical patterns. Existing medical imaging datasets are not designed\nto systematically study this issue, largely due to restrictive licensing and\nlimited supplementary patient data. To address this gap, we introduce\nSpurBreast, a curated breast MRI dataset that intentionally incorporates\nspurious correlations to evaluate their impact on model performance. Analyzing\nover 100 features involving patient, device, and imaging protocol, we identify\ntwo dominant spurious signals: magnetic field strength (a global feature\ninfluencing the entire image) and image orientation (a local feature affecting\nspatial alignment). Through controlled dataset splits, we demonstrate that DNNs\ncan exploit these non-clinical signals, achieving high validation accuracy\nwhile failing to generalize to unbiased test data. Alongside these two datasets\ncontaining spurious correlations, we also provide benchmark datasets without\nspurious correlations, allowing researchers to systematically investigate\nclinically relevant and irrelevant features, uncertainty estimation,\nadversarial robustness, and generalization strategies. Models and datasets are\navailable at https://github.com/utkuozbulak/spurbreast.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) have demonstrated remarkable success in medical\nimaging, yet their real-world deployment remains challenging due to spurious\ncorrelations, where models can learn non-clinical features instead of\nmeaningful medical patterns. Existing medical imaging datasets are not designed\nto systematically study this issue, largely due to restrictive licensing and\nlimited supplementary patient data. To address this gap, we introduce\nSpurBreast, a curated breast MRI dataset that intentionally incorporates\nspurious correlations to evaluate their impact on model performance. Analyzing\nover 100 features involving patient, device, and imaging protocol, we identify\ntwo dominant spurious signals: magnetic field strength (a global feature\ninfluencing the entire image) and image orientation (a local feature affecting\nspatial alignment). Through controlled dataset splits, we demonstrate that DNNs\ncan exploit these non-clinical signals, achieving high validation accuracy\nwhile failing to generalize to unbiased test data. Alongside these two datasets\ncontaining spurious correlations, we also provide benchmark datasets without\nspurious correlations, allowing researchers to systematically investigate\nclinically relevant and irrelevant features, uncertainty estimation,\nadversarial robustness, and generalization strategies. Models and datasets are\navailable at https://github.com/utkuozbulak/spurbreast."
                },
                "authors": [
                    {
                        "name": "Jong Bum Won"
                    },
                    {
                        "name": "Wesley De Neve"
                    },
                    {
                        "name": "Joris Vankerschaver"
                    },
                    {
                        "name": "Utku Ozbulak"
                    }
                ],
                "author_detail": {
                    "name": "Utku Ozbulak"
                },
                "author": "Utku Ozbulak",
                "arxiv_comment": "Accepted for publication in the 28th International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02104v1",
                "updated": "2025-10-02T15:10:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    10,
                    26,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T15:10:26Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    10,
                    26,
                    3,
                    275,
                    0
                ],
                "title": "LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot\n  Grasping with Ambiguous Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot\n  Grasping with Ambiguous Instructions"
                },
                "summary": "The existing language-driven grasping methods struggle to fully handle\nambiguous instructions containing implicit intents. To tackle this challenge,\nwe propose LangGrasp, a novel language-interactive robotic grasping framework.\nThe framework integrates fine-tuned large language models (LLMs) to leverage\ntheir robust commonsense understanding and environmental perception\ncapabilities, thereby deducing implicit intents from linguistic instructions\nand clarifying task requirements along with target manipulation objects.\nFurthermore, our designed point cloud localization module, guided by 2D part\nsegmentation, enables partial point cloud localization in scenes, thereby\nextending grasping operations from coarse-grained object-level to fine-grained\npart-level manipulation. Experimental results show that the LangGrasp framework\naccurately resolves implicit intents in ambiguous instructions, identifying\ncritical operations and target information that are unstated yet essential for\ntask completion. Additionally, it dynamically selects optimal grasping poses by\nintegrating environmental information. This enables high-precision grasping\nfrom object-level to part-level manipulation, significantly enhancing the\nadaptability and task execution efficiency of robots in unstructured\nenvironments. More information and code are available here:\nhttps://github.com/wu467/LangGrasp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The existing language-driven grasping methods struggle to fully handle\nambiguous instructions containing implicit intents. To tackle this challenge,\nwe propose LangGrasp, a novel language-interactive robotic grasping framework.\nThe framework integrates fine-tuned large language models (LLMs) to leverage\ntheir robust commonsense understanding and environmental perception\ncapabilities, thereby deducing implicit intents from linguistic instructions\nand clarifying task requirements along with target manipulation objects.\nFurthermore, our designed point cloud localization module, guided by 2D part\nsegmentation, enables partial point cloud localization in scenes, thereby\nextending grasping operations from coarse-grained object-level to fine-grained\npart-level manipulation. Experimental results show that the LangGrasp framework\naccurately resolves implicit intents in ambiguous instructions, identifying\ncritical operations and target information that are unstated yet essential for\ntask completion. Additionally, it dynamically selects optimal grasping poses by\nintegrating environmental information. This enables high-precision grasping\nfrom object-level to part-level manipulation, significantly enhancing the\nadaptability and task execution efficiency of robots in unstructured\nenvironments. More information and code are available here:\nhttps://github.com/wu467/LangGrasp."
                },
                "authors": [
                    {
                        "name": "Yunhan Lin"
                    },
                    {
                        "name": "Wenqi Wu"
                    },
                    {
                        "name": "Zhijie Zhang"
                    },
                    {
                        "name": "Huasong Min"
                    }
                ],
                "author_detail": {
                    "name": "Huasong Min"
                },
                "author": "Huasong Min",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02103v1",
                "updated": "2025-10-02T15:09:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    9,
                    10,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T15:09:10Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    9,
                    10,
                    3,
                    275,
                    0
                ],
                "title": "Sensing-Secure ISAC: Ambiguity Function Engineering for Impairing\n  Unauthorized Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing-Secure ISAC: Ambiguity Function Engineering for Impairing\n  Unauthorized Sensing"
                },
                "summary": "The deployment of integrated sensing and communication (ISAC) brings along\nunprecedented vulnerabilities to authorized sensing, necessitating the\ndevelopment of secure solutions. Sensing parameters are embedded within the\ntarget-reflected signal leaked to unauthorized passive radar sensing\neavesdroppers (Eve), implying that they can silently extract sensory\ninformation without prior knowledge of the information data. To overcome this\nlimitation, we propose a sensing-secure ISAC framework that ensures secure\ntarget detection and estimation for the legitimate system, while obfuscating\nunauthorized sensing without requiring any prior knowledge of Eve. By\nintroducing artificial imperfections into the ambiguity function (AF) of ISAC\nsignals, we introduce artificial targets into Eve's range profile which\nincrease its range estimation ambiguity. In contrast, the legitimate sensing\nreceiver (Alice) can suppress these AF artifacts using mismatched filtering,\nalbeit at the expense of signal-to-noise ratio (SNR) loss. Employing an OFDM\nsignal, a structured subcarrier power allocation scheme is designed to shape\nthe secure autocorrelation function (ACF), inserting periodic peaks to mislead\nEve's range estimation and degrade target detection performance. To quantify\nthe sensing security, we introduce peak sidelobe level (PSL) and integrated\nsidelobe level (ISL) as key performance metrics. Then, we analyze the three-way\ntrade-offs between communication, legitimate sensing, and sensing security,\nhighlighting the impact of the proposed sensing-secure ISAC signaling on system\nperformance. We formulate a convex optimization problem to maximize ISAC\nperformance while guaranteeing a certain sensing security level. Numerical\nresults validate the effectiveness of the proposed sensing-secure ISAC\nsignaling, demonstrating its ability to degrade Eve's target estimation while\npreserving Alice's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of integrated sensing and communication (ISAC) brings along\nunprecedented vulnerabilities to authorized sensing, necessitating the\ndevelopment of secure solutions. Sensing parameters are embedded within the\ntarget-reflected signal leaked to unauthorized passive radar sensing\neavesdroppers (Eve), implying that they can silently extract sensory\ninformation without prior knowledge of the information data. To overcome this\nlimitation, we propose a sensing-secure ISAC framework that ensures secure\ntarget detection and estimation for the legitimate system, while obfuscating\nunauthorized sensing without requiring any prior knowledge of Eve. By\nintroducing artificial imperfections into the ambiguity function (AF) of ISAC\nsignals, we introduce artificial targets into Eve's range profile which\nincrease its range estimation ambiguity. In contrast, the legitimate sensing\nreceiver (Alice) can suppress these AF artifacts using mismatched filtering,\nalbeit at the expense of signal-to-noise ratio (SNR) loss. Employing an OFDM\nsignal, a structured subcarrier power allocation scheme is designed to shape\nthe secure autocorrelation function (ACF), inserting periodic peaks to mislead\nEve's range estimation and degrade target detection performance. To quantify\nthe sensing security, we introduce peak sidelobe level (PSL) and integrated\nsidelobe level (ISL) as key performance metrics. Then, we analyze the three-way\ntrade-offs between communication, legitimate sensing, and sensing security,\nhighlighting the impact of the proposed sensing-secure ISAC signaling on system\nperformance. We formulate a convex optimization problem to maximize ISAC\nperformance while guaranteeing a certain sensing security level. Numerical\nresults validate the effectiveness of the proposed sensing-secure ISAC\nsignaling, demonstrating its ability to degrade Eve's target estimation while\npreserving Alice's performance."
                },
                "authors": [
                    {
                        "name": "Kawon Han"
                    },
                    {
                        "name": "Kaitao Meng"
                    },
                    {
                        "name": "Christos Masouros"
                    }
                ],
                "author_detail": {
                    "name": "Christos Masouros"
                },
                "author": "Christos Masouros",
                "arxiv_comment": "15 pages, 12 figures, accepted to IEEE Transactions on Wireless\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02091v1",
                "updated": "2025-10-02T14:57:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    57,
                    13,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:57:13Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    57,
                    13,
                    3,
                    275,
                    0
                ],
                "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and\n  Reasoning"
                },
                "summary": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models."
                },
                "authors": [
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Keyu Wang"
                    },
                    {
                        "name": "PengXiang Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18262v3",
                "updated": "2025-10-02T14:53:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    53,
                    4,
                    3,
                    275,
                    0
                ],
                "published": "2024-11-27T11:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    59,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "Break the ID-Language Barrier: An Adaption Framework for LLM-based\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Break the ID-Language Barrier: An Adaption Framework for LLM-based\n  Sequential Recommendation"
                },
                "summary": "The recent breakthrough of large language models (LLMs) in natural language\nprocessing has sparked exploration in recommendation systems, however, their\nlimited domain-specific knowledge remains a critical bottleneck. Specifically,\nLLMs lack key pieces of information crucial for sequential recommendations,\nsuch as user behavior patterns. To address this critical gap, we propose\nIDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich\nin domain-specific knowledge, into LLMs to improve recommendation accuracy.\nIDLE-Adapter acts as a bridge, transforming sparse user-item interaction data\ninto dense, LLM-compatible representations through a Pre-trained ID Sequential\nModel, Dimensionality Alignment, Layer-wise Embedding Refinement, and\nLayer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates\nremarkable flexibility by seamlessly integrating ID embeddings from diverse\nID-based sequential models and LLM architectures. Extensive experiments across\nvarious datasets demonstrate the superiority of IDLE-Adapter, achieving over\n10\\% and 20\\% improvements in HitRate@5 and NDCG@5 metrics, respectively,\ncompared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent breakthrough of large language models (LLMs) in natural language\nprocessing has sparked exploration in recommendation systems, however, their\nlimited domain-specific knowledge remains a critical bottleneck. Specifically,\nLLMs lack key pieces of information crucial for sequential recommendations,\nsuch as user behavior patterns. To address this critical gap, we propose\nIDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich\nin domain-specific knowledge, into LLMs to improve recommendation accuracy.\nIDLE-Adapter acts as a bridge, transforming sparse user-item interaction data\ninto dense, LLM-compatible representations through a Pre-trained ID Sequential\nModel, Dimensionality Alignment, Layer-wise Embedding Refinement, and\nLayer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates\nremarkable flexibility by seamlessly integrating ID embeddings from diverse\nID-based sequential models and LLM architectures. Extensive experiments across\nvarious datasets demonstrate the superiority of IDLE-Adapter, achieving over\n10\\% and 20\\% improvements in HitRate@5 and NDCG@5 metrics, respectively,\ncompared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xiaohan Yu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02060v1",
                "updated": "2025-10-02T14:28:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    28,
                    45,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:28:45Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    28,
                    45,
                    3,
                    275,
                    0
                ],
                "title": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly\n  Detection"
                },
                "summary": "In tabular anomaly detection (AD), textual semantics often carry critical\nsignals, as the definition of an anomaly is closely tied to domain-specific\ncontext. However, existing benchmarks provide only raw data points without\nsemantic context, overlooking rich textual metadata such as feature\ndescriptions and domain knowledge that experts rely on in practice. This\nlimitation restricts research flexibility and prevents models from fully\nleveraging domain knowledge for detection. ReTabAD addresses this gap by\nrestoring textual semantics to enable context-aware tabular AD research. We\nprovide (1) 20 carefully curated tabular datasets enriched with structured\ntextual metadata, together with implementations of state-of-the-art AD\nalgorithms including classical, deep learning, and LLM-based approaches, and\n(2) a zero-shot LLM framework that leverages semantic context without\ntask-specific training, establishing a strong baseline for future research.\nFurthermore, this work provides insights into the role and utility of textual\nmetadata in AD through experiments and analysis. Results show that semantic\ncontext improves detection performance and enhances interpretability by\nsupporting domain-aware reasoning. These findings establish ReTabAD as a\nbenchmark for systematic exploration of context-aware AD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In tabular anomaly detection (AD), textual semantics often carry critical\nsignals, as the definition of an anomaly is closely tied to domain-specific\ncontext. However, existing benchmarks provide only raw data points without\nsemantic context, overlooking rich textual metadata such as feature\ndescriptions and domain knowledge that experts rely on in practice. This\nlimitation restricts research flexibility and prevents models from fully\nleveraging domain knowledge for detection. ReTabAD addresses this gap by\nrestoring textual semantics to enable context-aware tabular AD research. We\nprovide (1) 20 carefully curated tabular datasets enriched with structured\ntextual metadata, together with implementations of state-of-the-art AD\nalgorithms including classical, deep learning, and LLM-based approaches, and\n(2) a zero-shot LLM framework that leverages semantic context without\ntask-specific training, establishing a strong baseline for future research.\nFurthermore, this work provides insights into the role and utility of textual\nmetadata in AD through experiments and analysis. Results show that semantic\ncontext improves detection performance and enhances interpretability by\nsupporting domain-aware reasoning. These findings establish ReTabAD as a\nbenchmark for systematic exploration of context-aware AD."
                },
                "authors": [
                    {
                        "name": "Sanghyu Yoon"
                    },
                    {
                        "name": "Dongmin Kim"
                    },
                    {
                        "name": "Suhee Yoon"
                    },
                    {
                        "name": "Ye Seul Sim"
                    },
                    {
                        "name": "Seungdong Yoa"
                    },
                    {
                        "name": "Hye-Seung Cho"
                    },
                    {
                        "name": "Soonyoung Lee"
                    },
                    {
                        "name": "Hankook Lee"
                    },
                    {
                        "name": "Woohyung Lim"
                    }
                ],
                "author_detail": {
                    "name": "Woohyung Lim"
                },
                "author": "Woohyung Lim",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02047v1",
                "updated": "2025-10-02T14:22:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    22,
                    20,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:22:20Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    22,
                    20,
                    3,
                    275,
                    0
                ],
                "title": "LLM-Enhanced, Data-Driven Personalized and Equitable Clinician\n  Scheduling: A Predict-then-Optimize Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced, Data-Driven Personalized and Equitable Clinician\n  Scheduling: A Predict-then-Optimize Approach"
                },
                "summary": "Clinician scheduling remains a persistent challenge due to limited clinical\nresources and fluctuating demands. This complexity is especially acute in large\nacademic anesthesiology departments as physicians balance responsibilities\nacross multiple clinical sites with conflicting priorities. Further, scheduling\nmust account for individual clinical and lifestyle preferences to ensure job\nsatisfaction and well-being. Traditional approaches, often based on statistical\nor rule-based optimization models, rely on structured data and explicit domain\nknowledge. However, these methods often overlook unstructured information,\ne.g., free-text notes from routinely administered clinician well-being surveys\nand scheduling platforms. These notes may reveal implicit and underutilized\nclinical resources. Neglecting such information can lead to misaligned\nschedules, increased burnout, overlooked staffing flexibility, and suboptimal\nutilization of available resources. To address this gap, we propose a\npredict-then-optimize framework that integrates classification-based clinician\navailability predictions with a mixed-integer programming schedule optimization\nmodel. Large language models (LLMs) are employed to extract actionable\npreferences and implicit constraints from unstructured schedule notes,\nenhancing the reliability of availability predictions. These predictions then\ninform the schedule optimization considering four objectives: first, ensuring\nclinical full-time equivalent compliance, second, reducing workload imbalances\nby enforcing equitable proportions of shift types, third, maximizing clinician\navailability for assigned shifts, and fourth, schedule consistency. By\ncombining the interpretive power of LLMs with the rigor of mathematical\noptimization, our framework provides a robust, data-driven solution that\nenhances operational efficiency while supporting equity and clinician\nwell-being.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinician scheduling remains a persistent challenge due to limited clinical\nresources and fluctuating demands. This complexity is especially acute in large\nacademic anesthesiology departments as physicians balance responsibilities\nacross multiple clinical sites with conflicting priorities. Further, scheduling\nmust account for individual clinical and lifestyle preferences to ensure job\nsatisfaction and well-being. Traditional approaches, often based on statistical\nor rule-based optimization models, rely on structured data and explicit domain\nknowledge. However, these methods often overlook unstructured information,\ne.g., free-text notes from routinely administered clinician well-being surveys\nand scheduling platforms. These notes may reveal implicit and underutilized\nclinical resources. Neglecting such information can lead to misaligned\nschedules, increased burnout, overlooked staffing flexibility, and suboptimal\nutilization of available resources. To address this gap, we propose a\npredict-then-optimize framework that integrates classification-based clinician\navailability predictions with a mixed-integer programming schedule optimization\nmodel. Large language models (LLMs) are employed to extract actionable\npreferences and implicit constraints from unstructured schedule notes,\nenhancing the reliability of availability predictions. These predictions then\ninform the schedule optimization considering four objectives: first, ensuring\nclinical full-time equivalent compliance, second, reducing workload imbalances\nby enforcing equitable proportions of shift types, third, maximizing clinician\navailability for assigned shifts, and fourth, schedule consistency. By\ncombining the interpretive power of LLMs with the rigor of mathematical\noptimization, our framework provides a robust, data-driven solution that\nenhances operational efficiency while supporting equity and clinician\nwell-being."
                },
                "authors": [
                    {
                        "name": "Anjali Jha"
                    },
                    {
                        "name": "Wanqing Chen"
                    },
                    {
                        "name": "Maxim Eckmann"
                    },
                    {
                        "name": "Ian Stockwell"
                    },
                    {
                        "name": "Jianwu Wang"
                    },
                    {
                        "name": "Kai Sun"
                    }
                ],
                "author_detail": {
                    "name": "Kai Sun"
                },
                "author": "Kai Sun",
                "arxiv_comment": "10 pages, 5 figures, Accepted to IEEE ICDM 2025 Workshops\n  Proceedings; IEEE Computer Society Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02044v1",
                "updated": "2025-10-02T14:18:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    18,
                    20,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T14:18:20Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    18,
                    20,
                    3,
                    275,
                    0
                ],
                "title": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming\n  Tool Usage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming\n  Tool Usage"
                },
                "summary": "End-to-end speech-in speech-out dialogue systems are emerging as a powerful\nalternative to traditional ASR-LLM-TTS pipelines, generating more natural,\nexpressive responses with significantly lower latency. However, these systems\nremain prone to hallucinations due to limited factual grounding. While\ntext-based dialogue systems address this challenge by integrating tools such as\nweb search and knowledge graph APIs, we introduce the first approach to extend\ntool use directly into speech-in speech-out systems. A key challenge is that\ntool integration substantially increases response latency, disrupting\nconversational flow. To mitigate this, we propose Streaming Retrieval-Augmented\nGeneration (Streaming RAG), a novel framework that reduces user-perceived\nlatency by predicting tool queries in parallel with user speech, even before\nthe user finishes speaking. Specifically, we develop a post-training pipeline\nthat teaches the model when to issue tool calls during ongoing speech and how\nto generate spoken summaries that fuse audio queries with retrieved text\nresults, thereby improving both accuracy and responsiveness. To evaluate our\napproach, we construct AudioCRAG, a benchmark created by converting queries\nfrom the publicly available CRAG dataset into speech form. Experimental results\ndemonstrate that our streaming RAG approach increases QA accuracy by up to 200%\nrelative (from 11.1% to 34.2% absolute) and further enhances user experience by\nreducing tool use latency by 20%. Importantly, our streaming RAG approach is\nmodality-agnostic and can be applied equally to typed input, paving the way for\nmore agentic, real-time AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end speech-in speech-out dialogue systems are emerging as a powerful\nalternative to traditional ASR-LLM-TTS pipelines, generating more natural,\nexpressive responses with significantly lower latency. However, these systems\nremain prone to hallucinations due to limited factual grounding. While\ntext-based dialogue systems address this challenge by integrating tools such as\nweb search and knowledge graph APIs, we introduce the first approach to extend\ntool use directly into speech-in speech-out systems. A key challenge is that\ntool integration substantially increases response latency, disrupting\nconversational flow. To mitigate this, we propose Streaming Retrieval-Augmented\nGeneration (Streaming RAG), a novel framework that reduces user-perceived\nlatency by predicting tool queries in parallel with user speech, even before\nthe user finishes speaking. Specifically, we develop a post-training pipeline\nthat teaches the model when to issue tool calls during ongoing speech and how\nto generate spoken summaries that fuse audio queries with retrieved text\nresults, thereby improving both accuracy and responsiveness. To evaluate our\napproach, we construct AudioCRAG, a benchmark created by converting queries\nfrom the publicly available CRAG dataset into speech form. Experimental results\ndemonstrate that our streaming RAG approach increases QA accuracy by up to 200%\nrelative (from 11.1% to 34.2% absolute) and further enhances user experience by\nreducing tool use latency by 20%. Importantly, our streaming RAG approach is\nmodality-agnostic and can be applied equally to typed input, paving the way for\nmore agentic, real-time AI assistants."
                },
                "authors": [
                    {
                        "name": "Siddhant Arora"
                    },
                    {
                        "name": "Haidar Khan"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Xin Luna Dong"
                    },
                    {
                        "name": "Sajal Choudhary"
                    },
                    {
                        "name": "Seungwhan Moon"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Adithya Sagar"
                    },
                    {
                        "name": "Surya Teja Appini"
                    },
                    {
                        "name": "Kaushik Patnaik"
                    },
                    {
                        "name": "Sanat Sharma"
                    },
                    {
                        "name": "Shinji Watanabe"
                    },
                    {
                        "name": "Anuj Kumar"
                    },
                    {
                        "name": "Ahmed Aly"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Florian Metze"
                    },
                    {
                        "name": "Zhaojiang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhaojiang Lin"
                },
                "author": "Zhaojiang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05735v3",
                "updated": "2025-10-02T14:15:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    15,
                    9,
                    3,
                    275,
                    0
                ],
                "published": "2025-06-06T04:35:19Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    4,
                    35,
                    19,
                    4,
                    157,
                    0
                ],
                "title": "Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation\n  and Confidence Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation\n  and Confidence Awareness"
                },
                "summary": "Machine unlearning techniques aim to mitigate unintended memorization in\nlarge language models (LLMs). However, existing approaches predominantly focus\non the explicit removal of isolated facts, often overlooking latent inferential\ndependencies and the non-deterministic nature of knowledge within LLMs.\nConsequently, facts presumed forgotten may persist implicitly through\ncorrelated information. To address these challenges, we propose a knowledge\nunlearning evaluation framework that more accurately captures the implicit\nstructure of real-world knowledge by representing relevant factual contexts as\nknowledge graphs with associated confidence scores. We further develop an\ninference-based evaluation protocol leveraging powerful LLMs as judges; these\njudges reason over the extracted knowledge subgraph to determine unlearning\nsuccess. Our LLM judges utilize carefully designed prompts and are calibrated\nagainst human evaluations to ensure their trustworthiness and stability.\nExtensive experiments on our newly constructed benchmark demonstrate that our\nframework provides a more realistic and rigorous assessment of unlearning\nperformance. Moreover, our findings reveal that current evaluation strategies\ntend to overestimate unlearning effectiveness. Our code is publicly available\nat https://github.com/Graph-COM/Knowledge_Unlearning.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning techniques aim to mitigate unintended memorization in\nlarge language models (LLMs). However, existing approaches predominantly focus\non the explicit removal of isolated facts, often overlooking latent inferential\ndependencies and the non-deterministic nature of knowledge within LLMs.\nConsequently, facts presumed forgotten may persist implicitly through\ncorrelated information. To address these challenges, we propose a knowledge\nunlearning evaluation framework that more accurately captures the implicit\nstructure of real-world knowledge by representing relevant factual contexts as\nknowledge graphs with associated confidence scores. We further develop an\ninference-based evaluation protocol leveraging powerful LLMs as judges; these\njudges reason over the extracted knowledge subgraph to determine unlearning\nsuccess. Our LLM judges utilize carefully designed prompts and are calibrated\nagainst human evaluations to ensure their trustworthiness and stability.\nExtensive experiments on our newly constructed benchmark demonstrate that our\nframework provides a more realistic and rigorous assessment of unlearning\nperformance. Moreover, our findings reveal that current evaluation strategies\ntend to overestimate unlearning effectiveness. Our code is publicly available\nat https://github.com/Graph-COM/Knowledge_Unlearning.git."
                },
                "authors": [
                    {
                        "name": "Rongzhe Wei"
                    },
                    {
                        "name": "Peizhi Niu"
                    },
                    {
                        "name": "Hans Hao-Hsun Hsu"
                    },
                    {
                        "name": "Ruihan Wu"
                    },
                    {
                        "name": "Haoteng Yin"
                    },
                    {
                        "name": "Mohsen Ghassemi"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Vamsi K. Potluru"
                    },
                    {
                        "name": "Eli Chien"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    },
                    {
                        "name": "Olgica Milenkovic"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17674v2",
                "updated": "2025-10-02T14:13:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    13,
                    37,
                    3,
                    275,
                    0
                ],
                "published": "2024-11-26T18:35:24Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    35,
                    24,
                    1,
                    331,
                    0
                ],
                "title": "Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with\n  Receptive-Field-Aware Attention Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with\n  Receptive-Field-Aware Attention Weighting"
                },
                "summary": "Understanding the emotions in a dialogue usually requires external knowledge\nto accurately understand the contents. As the LLMs become more and more\npowerful, we do not want to settle on the limited ability of the pre-trained\nlanguage model. However, the LLMs either can only process text modality or are\ntoo expensive to process the multimedia information. We aim to utilize both the\npower of LLMs and the supplementary features from the multimedia modalities. In\nthis paper, we present a framework, Lantern, that can improve the performance\nof a certain vanilla model by prompting large language models with\nreceptive-field-aware attention weighting. This framework trained a multi-task\nvanilla model to produce probabilities of emotion classes and dimension scores.\nThese predictions are fed into the LLMs as references to adjust the predicted\nprobabilities of each emotion class with its external knowledge and contextual\nunderstanding. We slice the dialogue into different receptive fields, and each\nsample is included in exactly t receptive fields. Finally, the predictions of\nLLMs are merged with a receptive-field-aware attention-driven weighting module.\nIn the experiments, vanilla models CORECT and SDT are deployed in Lantern with\nGPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way\nsettings demonstrated that the Lantern can significantly improve the\nperformance of current vanilla models by up to 1.23% and 1.80%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the emotions in a dialogue usually requires external knowledge\nto accurately understand the contents. As the LLMs become more and more\npowerful, we do not want to settle on the limited ability of the pre-trained\nlanguage model. However, the LLMs either can only process text modality or are\ntoo expensive to process the multimedia information. We aim to utilize both the\npower of LLMs and the supplementary features from the multimedia modalities. In\nthis paper, we present a framework, Lantern, that can improve the performance\nof a certain vanilla model by prompting large language models with\nreceptive-field-aware attention weighting. This framework trained a multi-task\nvanilla model to produce probabilities of emotion classes and dimension scores.\nThese predictions are fed into the LLMs as references to adjust the predicted\nprobabilities of each emotion class with its external knowledge and contextual\nunderstanding. We slice the dialogue into different receptive fields, and each\nsample is included in exactly t receptive fields. Finally, the predictions of\nLLMs are merged with a receptive-field-aware attention-driven weighting module.\nIn the experiments, vanilla models CORECT and SDT are deployed in Lantern with\nGPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way\nsettings demonstrated that the Lantern can significantly improve the\nperformance of current vanilla models by up to 1.23% and 1.80%."
                },
                "authors": [
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Liyun Zhang"
                    },
                    {
                        "name": "Dian Ding"
                    },
                    {
                        "name": "Dinghua Zhao"
                    },
                    {
                        "name": "Yi-Chao Chen"
                    },
                    {
                        "name": "Ye Wu"
                    },
                    {
                        "name": "Guangtao Xue"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Xue"
                },
                "author": "Guangtao Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12960v2",
                "updated": "2025-10-02T14:09:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    9,
                    44,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-16T11:06:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    11,
                    6,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Investigating ReLoRA: Effects on the Learning Dynamics of Small Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating ReLoRA: Effects on the Learning Dynamics of Small Language\n  Models"
                },
                "summary": "Parameter-efficient methods like LoRA have revolutionised large language\nmodel (LLM) fine-tuning. ReLoRA extends this idea to pretraining by repeatedly\nmerging and reinitialising low-rank adapters, increasing cumulative rank while\nkeeping updates cheap. This aligns well with observations that high-capacity\nmodels learn through locally low-rank trajectories that expand over time. By\ncontrast, recent work suggests that small language models (SLMs) exhibit rank\ndeficiencies and under-utilise their available dimensionality. This raises a\nnatural question: can ReLoRA's rank-expanding update rule \\textit{steer} SLMs\ntoward healthier learning dynamics, mitigating rank bottlenecks in a\ncapacity-constrained regime? We argue SLMs are an ideal testbed: they train\nquickly, enable controlled ablations, and make rank phenomena more measurable.\nWe present the first systematic study of ReLoRA in SLMs (11M-66M parameters),\nevaluating both performance and learning dynamics. Across loss, Paloma\nperplexity, and BLiMP, we find that ReLoRA underperforms full-rank training,\nwith gaps widening at larger scales. Analysis of proportional effective rank\nand condition numbers shows that ReLoRA amplifies existing rank deficiencies\nand induces ill-conditioned updates early in training. Our results suggest that\nwhile ReLoRA's merge-and-restart strategy can expand ranks in larger models, it\ndoes not straightforwardly translate to capacity-limited SLMs, motivating\nadaptive-rank or hybrid-rank approaches for low-compute pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient methods like LoRA have revolutionised large language\nmodel (LLM) fine-tuning. ReLoRA extends this idea to pretraining by repeatedly\nmerging and reinitialising low-rank adapters, increasing cumulative rank while\nkeeping updates cheap. This aligns well with observations that high-capacity\nmodels learn through locally low-rank trajectories that expand over time. By\ncontrast, recent work suggests that small language models (SLMs) exhibit rank\ndeficiencies and under-utilise their available dimensionality. This raises a\nnatural question: can ReLoRA's rank-expanding update rule \\textit{steer} SLMs\ntoward healthier learning dynamics, mitigating rank bottlenecks in a\ncapacity-constrained regime? We argue SLMs are an ideal testbed: they train\nquickly, enable controlled ablations, and make rank phenomena more measurable.\nWe present the first systematic study of ReLoRA in SLMs (11M-66M parameters),\nevaluating both performance and learning dynamics. Across loss, Paloma\nperplexity, and BLiMP, we find that ReLoRA underperforms full-rank training,\nwith gaps widening at larger scales. Analysis of proportional effective rank\nand condition numbers shows that ReLoRA amplifies existing rank deficiencies\nand induces ill-conditioned updates early in training. Our results suggest that\nwhile ReLoRA's merge-and-restart strategy can expand ranks in larger models, it\ndoes not straightforwardly translate to capacity-limited SLMs, motivating\nadaptive-rank or hybrid-rank approaches for low-compute pretraining."
                },
                "authors": [
                    {
                        "name": "Yuval Weiss"
                    },
                    {
                        "name": "David Demitri Africa"
                    },
                    {
                        "name": "Paula Buttery"
                    },
                    {
                        "name": "Richard Diehl Martinez"
                    }
                ],
                "author_detail": {
                    "name": "Richard Diehl Martinez"
                },
                "author": "Richard Diehl Martinez",
                "arxiv_comment": "12 Pages, 6 Tables, 8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v3",
                "updated": "2025-10-02T14:09:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    9,
                    3,
                    3,
                    275,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "arxiv_journal_ref": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10155v2",
                "updated": "2025-10-02T14:05:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    5,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-14T11:10:02Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    10,
                    2,
                    0,
                    195,
                    0
                ],
                "title": "Flexible Feature Distillation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Feature Distillation for Large Language Models"
                },
                "summary": "Knowledge distillation (KD) has become a cornerstone for compressing large\nlanguage models (LLMs). However, existing LLM-KD methods have primarily focused\non logit-based approaches, which achieve good performance but overlook the rich\ninternal representations of LLMs. Feature-level KD could leverage this\nstructure to provide complementary benefits, yet it remains underexplored\nbecause current feature-KD approaches typically assume identical\nteacher-student hidden sizes, a restrictive and unrealistic assumption. A\ncommon workaround is to train a linear projector to align their feature spaces;\nhowever, this introduces additional parameters, distorts teacher embeddings,\nand often degrades downstream performance, especially in generative tasks. We\npropose Flex-KD, a parameter-free framework for task-driven feature\ndistillation for LLMs. Instead of projecting the entire teacher representation,\nFlex-KD uses gradient-based scores to identify the most task-relevant\ndimensions of the teacher's hidden states and distills only this subspace into\nthe student. This ensures that the student's limited capacity is allocated to\ninformative components, while avoiding projector-induced distortion and extra\nparameters. Flex-KD integrates seamlessly with existing KD pipelines and\nsupports differing teacher-student hidden sizes. Extensive experiments across\nboth classification and generative tasks, i.e., instruction-following and\nsummarization, show that Flex-KD consistently boosts student performance,\nachieving up to a 3.75 percent performance gain over the linear projection\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) has become a cornerstone for compressing large\nlanguage models (LLMs). However, existing LLM-KD methods have primarily focused\non logit-based approaches, which achieve good performance but overlook the rich\ninternal representations of LLMs. Feature-level KD could leverage this\nstructure to provide complementary benefits, yet it remains underexplored\nbecause current feature-KD approaches typically assume identical\nteacher-student hidden sizes, a restrictive and unrealistic assumption. A\ncommon workaround is to train a linear projector to align their feature spaces;\nhowever, this introduces additional parameters, distorts teacher embeddings,\nand often degrades downstream performance, especially in generative tasks. We\npropose Flex-KD, a parameter-free framework for task-driven feature\ndistillation for LLMs. Instead of projecting the entire teacher representation,\nFlex-KD uses gradient-based scores to identify the most task-relevant\ndimensions of the teacher's hidden states and distills only this subspace into\nthe student. This ensures that the student's limited capacity is allocated to\ninformative components, while avoiding projector-induced distortion and extra\nparameters. Flex-KD integrates seamlessly with existing KD pipelines and\nsupports differing teacher-student hidden sizes. Extensive experiments across\nboth classification and generative tasks, i.e., instruction-following and\nsummarization, show that Flex-KD consistently boosts student performance,\nachieving up to a 3.75 percent performance gain over the linear projection\nbaseline."
                },
                "authors": [
                    {
                        "name": "Khouloud Saadi"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23340v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23340v3",
                "updated": "2025-10-02T14:03:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    3,
                    57,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-27T14:42:48Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    14,
                    42,
                    48,
                    5,
                    270,
                    0
                ],
                "title": "CrediBench: Building Web-Scale Network Datasets for Information\n  Integrity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrediBench: Building Web-Scale Network Datasets for Information\n  Integrity"
                },
                "summary": "Online misinformation poses an escalating threat, amplified by the Internet's\nopen nature and increasingly capable LLMs that generate persuasive yet\ndeceptive content. Existing misinformation detection methods typically focus on\neither textual content or network structure in isolation, failing to leverage\nthe rich, dynamic interplay between website content and hyperlink relationships\nthat characterizes real-world misinformation ecosystems. We introduce\nCrediBench: a large-scale data processing pipeline for constructing temporal\nweb graphs that jointly model textual content and hyperlink structure for\nmisinformation detection. Unlike prior work, our approach captures the dynamic\nevolution of general misinformation domains, including changes in both content\nand inter-site references over time. Our processed one-month snapshot extracted\nfrom the Common Crawl archive in December 2024 contains 45 million nodes and 1\nbillion edges, representing the largest web graph dataset made publicly\navailable for misinformation research to date. From our experiments on this\ngraph snapshot, we demonstrate the strength of both structural and webpage\ncontent signals for learning credibility scores, which measure source\nreliability. The pipeline and experimentation code are all available here, and\nthe dataset is in this folder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online misinformation poses an escalating threat, amplified by the Internet's\nopen nature and increasingly capable LLMs that generate persuasive yet\ndeceptive content. Existing misinformation detection methods typically focus on\neither textual content or network structure in isolation, failing to leverage\nthe rich, dynamic interplay between website content and hyperlink relationships\nthat characterizes real-world misinformation ecosystems. We introduce\nCrediBench: a large-scale data processing pipeline for constructing temporal\nweb graphs that jointly model textual content and hyperlink structure for\nmisinformation detection. Unlike prior work, our approach captures the dynamic\nevolution of general misinformation domains, including changes in both content\nand inter-site references over time. Our processed one-month snapshot extracted\nfrom the Common Crawl archive in December 2024 contains 45 million nodes and 1\nbillion edges, representing the largest web graph dataset made publicly\navailable for misinformation research to date. From our experiments on this\ngraph snapshot, we demonstrate the strength of both structural and webpage\ncontent signals for learning credibility scores, which measure source\nreliability. The pipeline and experimentation code are all available here, and\nthe dataset is in this folder."
                },
                "authors": [
                    {
                        "name": "Emma Kondrup"
                    },
                    {
                        "name": "Sebastian Sabry"
                    },
                    {
                        "name": "Hussein Abdallah"
                    },
                    {
                        "name": "Zachary Yang"
                    },
                    {
                        "name": "James Zhou"
                    },
                    {
                        "name": "Kellin Pelrine"
                    },
                    {
                        "name": "Jean-François Godbout"
                    },
                    {
                        "name": "Michael M. Bronstein"
                    },
                    {
                        "name": "Reihaneh Rabbany"
                    },
                    {
                        "name": "Shenyang Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shenyang Huang"
                },
                "author": "Shenyang Huang",
                "arxiv_comment": "16 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23340v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23340v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21315v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21315v3",
                "updated": "2025-10-02T14:01:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    1,
                    14,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-27T15:13:08Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    13,
                    8,
                    1,
                    147,
                    0
                ],
                "title": "Charting the Landscape of African NLP: Mapping Progress and Shaping the\n  Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the Landscape of African NLP: Mapping Progress and Shaping the\n  Road Ahead"
                },
                "summary": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 884\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With over 2,000 languages and potentially millions of speakers, Africa\nrepresents one of the richest linguistic regions in the world. Yet, this\ndiversity is scarcely reflected in state-of-the-art natural language processing\n(NLP) systems and large language models (LLMs), which predominantly support a\nnarrow set of high-resource languages. This exclusion not only limits the reach\nand utility of modern NLP technologies but also risks widening the digital\ndivide across linguistic communities. Nevertheless, NLP research on African\nlanguages is active and growing. In recent years, there has been a surge of\ninterest in this area, driven by several factors-including the creation of\nmultilingual language resources, the rise of community-led initiatives, and\nincreased support through funding programs. In this survey, we analyze 884\nresearch papers on NLP for African languages published over the past five\nyears, offering a comprehensive overview of recent progress across core tasks.\nWe identify key trends shaping the field and conclude by outlining promising\ndirections to foster more inclusive and sustainable NLP research for African\nlanguages."
                },
                "authors": [
                    {
                        "name": "Jesujoba O. Alabi"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Dietrich Klakow"
                    }
                ],
                "author_detail": {
                    "name": "Dietrich Klakow"
                },
                "author": "Dietrich Klakow",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21315v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25264v2",
                "updated": "2025-10-02T13:58:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    58,
                    56,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-28T04:50:48Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    4,
                    50,
                    48,
                    6,
                    271,
                    0
                ],
                "title": "GeoSQL-Eval: First Evaluation of LLMs on PostGIS-Based NL2GeoSQL Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoSQL-Eval: First Evaluation of LLMs on PostGIS-Based NL2GeoSQL Queries"
                },
                "summary": "Large language models (LLMs) have shown strong performance in natural\nlanguage to SQL (NL2SQL) tasks within general databases. However, extending to\nGeoSQL introduces additional complexity from spatial data types, function\ninvocation, and coordinate systems, which greatly increases generation and\nexecution difficulty. Existing benchmarks mainly target general SQL, and a\nsystematic evaluation framework for GeoSQL is still lacking. To fill this gap,\nwe present GeoSQL-Eval, the first end-to-end automated evaluation framework for\nPostGIS query generation, together with GeoSQL-Bench, a benchmark for assessing\nLLM performance in NL2GeoSQL tasks. GeoSQL-Bench defines three task\ncategories-conceptual understanding, syntax-level SQL generation, and schema\nretrieval-comprising 14,178 instances, 340 PostGIS functions, and 82 thematic\ndatabases. GeoSQL-Eval is grounded in Webb's Depth of Knowledge (DOK) model,\ncovering four cognitive dimensions, five capability levels, and twenty task\ntypes to establish a comprehensive process from knowledge acquisition and\nsyntax generation to semantic alignment, execution accuracy, and robustness. We\nevaluate 24 representative models across six categories and apply the entropy\nweight method with statistical analyses to uncover performance differences,\ncommon error patterns, and resource usage. Finally, we release a public\nGeoSQL-Eval leaderboard platform for continuous testing and global comparison.\nThis work extends the NL2GeoSQL paradigm and provides a standardized,\ninterpretable, and extensible framework for evaluating LLMs in spatial database\ncontexts, offering valuable references for geospatial information science and\nrelated applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown strong performance in natural\nlanguage to SQL (NL2SQL) tasks within general databases. However, extending to\nGeoSQL introduces additional complexity from spatial data types, function\ninvocation, and coordinate systems, which greatly increases generation and\nexecution difficulty. Existing benchmarks mainly target general SQL, and a\nsystematic evaluation framework for GeoSQL is still lacking. To fill this gap,\nwe present GeoSQL-Eval, the first end-to-end automated evaluation framework for\nPostGIS query generation, together with GeoSQL-Bench, a benchmark for assessing\nLLM performance in NL2GeoSQL tasks. GeoSQL-Bench defines three task\ncategories-conceptual understanding, syntax-level SQL generation, and schema\nretrieval-comprising 14,178 instances, 340 PostGIS functions, and 82 thematic\ndatabases. GeoSQL-Eval is grounded in Webb's Depth of Knowledge (DOK) model,\ncovering four cognitive dimensions, five capability levels, and twenty task\ntypes to establish a comprehensive process from knowledge acquisition and\nsyntax generation to semantic alignment, execution accuracy, and robustness. We\nevaluate 24 representative models across six categories and apply the entropy\nweight method with statistical analyses to uncover performance differences,\ncommon error patterns, and resource usage. Finally, we release a public\nGeoSQL-Eval leaderboard platform for continuous testing and global comparison.\nThis work extends the NL2GeoSQL paradigm and provides a standardized,\ninterpretable, and extensible framework for evaluating LLMs in spatial database\ncontexts, offering valuable references for geospatial information science and\nrelated applications."
                },
                "authors": [
                    {
                        "name": "Shuyang Hou"
                    },
                    {
                        "name": "Haoyue Jiao"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Lutong Xie"
                    },
                    {
                        "name": "Guanyu Chen"
                    },
                    {
                        "name": "Shaowen Wu"
                    },
                    {
                        "name": "Xuefeng Guan"
                    },
                    {
                        "name": "Huayi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Huayi Wu"
                },
                "author": "Huayi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02025v1",
                "updated": "2025-10-02T13:57:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    57,
                    14,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T13:57:14Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    57,
                    14,
                    3,
                    275,
                    0
                ],
                "title": "Style Over Story: A Process-Oriented Study of Authorial Creativity in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style Over Story: A Process-Oriented Study of Authorial Creativity in\n  Large Language Models"
                },
                "summary": "Evaluations of large language models (LLMs)' creativity have focused\nprimarily on the quality of their outputs rather than the processes that shape\nthem. This study takes a process-oriented approach, drawing on narratology to\nexamine LLMs as computational authors. We introduce constraint-based\ndecision-making as a lens for authorial creativity. Using controlled prompting\nto assign authorial personas, we analyze the creative preferences of the\nmodels. Our findings show that LLMs consistently emphasize Style over other\nelements, including Character, Event, and Setting. By also probing the\nreasoning the models provide for their choices, we show that distinctive\nprofiles emerge across models and argue that our approach provides a novel\nsystematic tool for analyzing AI's authorial creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluations of large language models (LLMs)' creativity have focused\nprimarily on the quality of their outputs rather than the processes that shape\nthem. This study takes a process-oriented approach, drawing on narratology to\nexamine LLMs as computational authors. We introduce constraint-based\ndecision-making as a lens for authorial creativity. Using controlled prompting\nto assign authorial personas, we analyze the creative preferences of the\nmodels. Our findings show that LLMs consistently emphasize Style over other\nelements, including Character, Event, and Setting. By also probing the\nreasoning the models provide for their choices, we show that distinctive\nprofiles emerge across models and argue that our approach provides a novel\nsystematic tool for analyzing AI's authorial creativity."
                },
                "authors": [
                    {
                        "name": "Donghoon Jung"
                    },
                    {
                        "name": "Jiwoo Choi"
                    },
                    {
                        "name": "Songeun Chae"
                    },
                    {
                        "name": "Seohyon Jung"
                    }
                ],
                "author_detail": {
                    "name": "Seohyon Jung"
                },
                "author": "Seohyon Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00015v2",
                "updated": "2025-10-02T13:50:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    50,
                    5,
                    3,
                    275,
                    0
                ],
                "published": "2025-04-23T04:52:26Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    52,
                    26,
                    2,
                    113,
                    0
                ],
                "title": "Design and Application of Multimodal Large Language Model Based System\n  for End to End Automation of Accident Dataset Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Application of Multimodal Large Language Model Based System\n  for End to End Automation of Accident Dataset Generation"
                },
                "summary": "Road traffic accidents remain a major public safety and socio-economic issue\nin developing countries like Bangladesh. Existing accident data collection is\nlargely manual, fragmented, and unreliable, resulting in underreporting and\ninconsistent records. This research proposes a fully automated system using\nLarge Language Models (LLMs) and web scraping techniques to address these\nchallenges. The pipeline consists of four components: automated web scraping\ncode generation, news collection from online sources, accident news\nclassification with structured data extraction, and duplicate removal. The\nsystem uses the multimodal generative LLM Gemini-2.0-Flash for seamless\nautomation. The code generation module classifies webpages into pagination,\ndynamic, or infinite scrolling categories and generates suitable Python scripts\nfor scraping. LLMs also classify and extract key accident information such as\ndate, time, location, fatalities, injuries, road type, vehicle types, and\npedestrian involvement. A deduplication algorithm ensures data integrity by\nremoving duplicate reports. The system scraped 14 major Bangladeshi news sites\nover 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news\narticles and identifying 705 unique accidents. The code generation module\nachieved 91.3% calibration and 80% validation accuracy. Chittagong reported the\nhighest number of accidents (80), fatalities (70), and injuries (115), followed\nby Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning\n(8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also\ndeveloped with usage instructions. This study demonstrates the viability of an\nLLM-powered, scalable system for accurate, low-effort accident data collection,\nproviding a foundation for data-driven road safety policymaking in Bangladesh.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Road traffic accidents remain a major public safety and socio-economic issue\nin developing countries like Bangladesh. Existing accident data collection is\nlargely manual, fragmented, and unreliable, resulting in underreporting and\ninconsistent records. This research proposes a fully automated system using\nLarge Language Models (LLMs) and web scraping techniques to address these\nchallenges. The pipeline consists of four components: automated web scraping\ncode generation, news collection from online sources, accident news\nclassification with structured data extraction, and duplicate removal. The\nsystem uses the multimodal generative LLM Gemini-2.0-Flash for seamless\nautomation. The code generation module classifies webpages into pagination,\ndynamic, or infinite scrolling categories and generates suitable Python scripts\nfor scraping. LLMs also classify and extract key accident information such as\ndate, time, location, fatalities, injuries, road type, vehicle types, and\npedestrian involvement. A deduplication algorithm ensures data integrity by\nremoving duplicate reports. The system scraped 14 major Bangladeshi news sites\nover 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news\narticles and identifying 705 unique accidents. The code generation module\nachieved 91.3% calibration and 80% validation accuracy. Chittagong reported the\nhighest number of accidents (80), fatalities (70), and injuries (115), followed\nby Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning\n(8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also\ndeveloped with usage instructions. This study demonstrates the viability of an\nLLM-powered, scalable system for accurate, low-effort accident data collection,\nproviding a foundation for data-driven road safety policymaking in Bangladesh."
                },
                "authors": [
                    {
                        "name": "MD Thamed Bin Zaman Chowdhury"
                    },
                    {
                        "name": "Moazzem Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Moazzem Hossain"
                },
                "author": "Moazzem Hossain",
                "arxiv_comment": "This paper is accepted for presentation in TRB annual meeting 2026.\n  The version presented here is the preprint version before peer review process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11212v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11212v6",
                "updated": "2025-10-02T13:35:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    35,
                    0,
                    3,
                    275,
                    0
                ],
                "published": "2024-01-20T11:37:44Z",
                "published_parsed": [
                    2024,
                    1,
                    20,
                    11,
                    37,
                    44,
                    5,
                    20,
                    0
                ],
                "title": "Programming Distributed Collective Processes in the eXchange Calculus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Distributed Collective Processes in the eXchange Calculus"
                },
                "summary": "Recent trends like the Internet of Things (IoT) suggest a vision of dense and\nmulti-scale deployments of computing devices in nearly all kinds of\nenvironments. A prominent engineering challenge revolves around programming the\ncollective adaptive behaviour of such computational ecosystems. This requires\nabstractions able to capture concepts like ensembles (dynamic groups of\ncooperating devices) and collective tasks (joint activities carried out by\nensembles). In this work, we consider collections of devices interacting with\nneighbours and that execute in nearly-synchronised sense-compute-interact\nrounds, where the computation is given by a single program mapping sensing\nvalues and incoming messages to output and outcoming messages. To support\nprogramming whole computational collectives, we propose the abstraction of a\ndistributed collective process, which can be used to define at once the\nensemble formation logic and its collective task. We formalise the abstraction\nin the eXchange Calculus (XC), a core functional language based on neighbouring\nvalues (maps from neighbours to values) where state and interaction is handled\nthrough a single primitive, exchange, and provide a corresponding\nimplementation in the FCPP language. Then, we exercise distributed collective\nprocesses using two case studies: multi-hop message propagation and distributed\nmonitoring of spatial properties. Finally, we discuss the features of the\nabstraction and its suitability for different kinds of distributed computing\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent trends like the Internet of Things (IoT) suggest a vision of dense and\nmulti-scale deployments of computing devices in nearly all kinds of\nenvironments. A prominent engineering challenge revolves around programming the\ncollective adaptive behaviour of such computational ecosystems. This requires\nabstractions able to capture concepts like ensembles (dynamic groups of\ncooperating devices) and collective tasks (joint activities carried out by\nensembles). In this work, we consider collections of devices interacting with\nneighbours and that execute in nearly-synchronised sense-compute-interact\nrounds, where the computation is given by a single program mapping sensing\nvalues and incoming messages to output and outcoming messages. To support\nprogramming whole computational collectives, we propose the abstraction of a\ndistributed collective process, which can be used to define at once the\nensemble formation logic and its collective task. We formalise the abstraction\nin the eXchange Calculus (XC), a core functional language based on neighbouring\nvalues (maps from neighbours to values) where state and interaction is handled\nthrough a single primitive, exchange, and provide a corresponding\nimplementation in the FCPP language. Then, we exercise distributed collective\nprocesses using two case studies: multi-hop message propagation and distributed\nmonitoring of spatial properties. Finally, we discuss the features of the\nabstraction and its suitability for different kinds of distributed computing\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Audrito"
                    },
                    {
                        "name": "Roberto Casadei"
                    },
                    {
                        "name": "Ferruccio Damiani"
                    },
                    {
                        "name": "Gianluca Torta"
                    },
                    {
                        "name": "Mirko Viroli"
                    }
                ],
                "author_detail": {
                    "name": "Mirko Viroli"
                },
                "author": "Mirko Viroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11212v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11212v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01997v1",
                "updated": "2025-10-02T13:18:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    18,
                    43,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T13:18:43Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    18,
                    43,
                    3,
                    275,
                    0
                ],
                "title": "Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing\n  Routing in Lightweight Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing\n  Routing in Lightweight Image Super-Resolution"
                },
                "summary": "Image Super-Resolution (SR) aims to reconstruct high-resolution images from\nlow-resolution counterparts, but the computational complexity of deep\nlearning-based methods often hinders practical deployment. CAMixer is the\npioneering work to integrate the advantages of existing lightweight SR methods\nand proposes a content-aware mixer to route token mixers of varied complexities\naccording to the difficulty of content recovery. However, several limitations\nremain, such as poor adaptability, coarse-grained masking and spatial\ninflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking\nmechanism that identifies pure pixels and exempts them from expensive\ncomputations. PP utilizes fixed color center points to classify pixels into\ndistinct categories, enabling fine-grained, spatially flexible masking while\nmaintaining adaptive flexibility. Integrated into the state-of-the-art\nATD-light model, PP-ATD-light achieves superior SR performance with minimal\noverhead, outperforming CAMixer-ATD-light in reconstruction quality and\nparameter efficiency when saving a similar amount of computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image Super-Resolution (SR) aims to reconstruct high-resolution images from\nlow-resolution counterparts, but the computational complexity of deep\nlearning-based methods often hinders practical deployment. CAMixer is the\npioneering work to integrate the advantages of existing lightweight SR methods\nand proposes a content-aware mixer to route token mixers of varied complexities\naccording to the difficulty of content recovery. However, several limitations\nremain, such as poor adaptability, coarse-grained masking and spatial\ninflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking\nmechanism that identifies pure pixels and exempts them from expensive\ncomputations. PP utilizes fixed color center points to classify pixels into\ndistinct categories, enabling fine-grained, spatially flexible masking while\nmaintaining adaptive flexibility. Integrated into the state-of-the-art\nATD-light model, PP-ATD-light achieves superior SR performance with minimal\noverhead, outperforming CAMixer-ATD-light in reconstruction quality and\nparameter efficiency when saving a similar amount of computation."
                },
                "authors": [
                    {
                        "name": "Junyu Wu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Gangshan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Gangshan Wu"
                },
                "author": "Gangshan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01995v1",
                "updated": "2025-10-02T13:17:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    17,
                    11,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T13:17:11Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    17,
                    11,
                    3,
                    275,
                    0
                ],
                "title": "LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and\n  Target",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and\n  Target"
                },
                "summary": "Online social media platforms are central to everyday communication and\ninformation seeking. While these platforms serve positive purposes, they also\nprovide fertile ground for the spread of hate speech, offensive language, and\nbullying content targeting individuals, organizations, and communities. Such\ncontent undermines safety, participation, and equity online. Reliable detection\nsystems are therefore needed, especially for low-resource languages where\nmoderation tools are limited. In Bangla, prior work has contributed resources\nand models, but most are single-task (e.g., binary hate/offense) with limited\ncoverage of multi-facet signals (type, severity, target). We address these gaps\nby introducing the first multi-task Bangla hate-speech dataset,\nBanglaMultiHate, one of the largest manually annotated corpus to date. Building\non this resource, we conduct a comprehensive, controlled comparison spanning\nclassical baselines, monolingual pretrained models, and LLMs under zero-shot\nprompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a\nlow-resource setting and reveal a consistent trend: although LoRA-tuned LLMs\nare competitive with BanglaBERT, culturally and linguistically grounded\npretraining remains critical for robust performance. Together, our dataset and\nfindings establish a stronger benchmark for developing culturally aligned\nmoderation tools in low-resource contexts. For reproducibility, we will release\nthe dataset and all related scripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online social media platforms are central to everyday communication and\ninformation seeking. While these platforms serve positive purposes, they also\nprovide fertile ground for the spread of hate speech, offensive language, and\nbullying content targeting individuals, organizations, and communities. Such\ncontent undermines safety, participation, and equity online. Reliable detection\nsystems are therefore needed, especially for low-resource languages where\nmoderation tools are limited. In Bangla, prior work has contributed resources\nand models, but most are single-task (e.g., binary hate/offense) with limited\ncoverage of multi-facet signals (type, severity, target). We address these gaps\nby introducing the first multi-task Bangla hate-speech dataset,\nBanglaMultiHate, one of the largest manually annotated corpus to date. Building\non this resource, we conduct a comprehensive, controlled comparison spanning\nclassical baselines, monolingual pretrained models, and LLMs under zero-shot\nprompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a\nlow-resource setting and reveal a consistent trend: although LoRA-tuned LLMs\nare competitive with BanglaBERT, culturally and linguistically grounded\npretraining remains critical for robust performance. Together, our dataset and\nfindings establish a stronger benchmark for developing culturally aligned\nmoderation tools in low-resource contexts. For reproducibility, we will release\nthe dataset and all related scripts."
                },
                "authors": [
                    {
                        "name": "Md Arid Hasan"
                    },
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Md Fahad Hossain"
                    },
                    {
                        "name": "Usman Naseem"
                    },
                    {
                        "name": "Syed Ishtiaque Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Syed Ishtiaque Ahmed"
                },
                "author": "Syed Ishtiaque Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01994v1",
                "updated": "2025-10-02T13:15:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    15,
                    40,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T13:15:40Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    15,
                    40,
                    3,
                    275,
                    0
                ],
                "title": "Clarifying Semantics of In-Context Examples for Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clarifying Semantics of In-Context Examples for Unit Test Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled promising\nperformance in unit test generation through in-context learning (ICL). However,\nthe quality of in-context examples significantly influences the effectiveness\nof generated tests-poorly structured or semantically unclear test examples\noften lead to suboptimal outputs. In this paper, we propose CLAST, a novel\ntechnique that systematically refines unit tests to improve their semantic\nclarity, thereby enhancing their utility as in-context examples. The approach\ndecomposes complex tests into logically clearer ones and improves semantic\nclarity through a combination of program analysis and LLM-based rewriting. We\nevaluated CLAST on four open-source and three industrial projects. The results\ndemonstrate that CLAST largely outperforms UTgen, the state-of-the-art\nrefinement technique, in both preserving test effectiveness and enhancing\nsemantic clarity. Specifically, CLAST fully retains the original effectiveness\nof unit tests, while UTgen reduces compilation success rate (CSR), pass rate\n(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,\n35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user\nstudy preferred the semantic clarity of CLAST-refined tests. Notably,\nincorporating CLAST-refined tests as examples effectively improves ICL-based\nunit test generation approaches such as RAGGen and TELPA, resulting in an\naverage increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for\ngenerated tests, compared to incorporating UTgen-refined tests. The insights\nfrom the follow-up user study not only reinforce CLAST's potential impact in\nsoftware testing practice but also illuminate avenues for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled promising\nperformance in unit test generation through in-context learning (ICL). However,\nthe quality of in-context examples significantly influences the effectiveness\nof generated tests-poorly structured or semantically unclear test examples\noften lead to suboptimal outputs. In this paper, we propose CLAST, a novel\ntechnique that systematically refines unit tests to improve their semantic\nclarity, thereby enhancing their utility as in-context examples. The approach\ndecomposes complex tests into logically clearer ones and improves semantic\nclarity through a combination of program analysis and LLM-based rewriting. We\nevaluated CLAST on four open-source and three industrial projects. The results\ndemonstrate that CLAST largely outperforms UTgen, the state-of-the-art\nrefinement technique, in both preserving test effectiveness and enhancing\nsemantic clarity. Specifically, CLAST fully retains the original effectiveness\nof unit tests, while UTgen reduces compilation success rate (CSR), pass rate\n(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,\n35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user\nstudy preferred the semantic clarity of CLAST-refined tests. Notably,\nincorporating CLAST-refined tests as examples effectively improves ICL-based\nunit test generation approaches such as RAGGen and TELPA, resulting in an\naverage increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for\ngenerated tests, compared to incorporating UTgen-refined tests. The insights\nfrom the follow-up user study not only reinforce CLAST's potential impact in\nsoftware testing practice but also illuminate avenues for future research."
                },
                "authors": [
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Lin Yang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Jianyi Zhou"
                    },
                    {
                        "name": "Junjie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Chen"
                },
                "author": "Junjie Chen",
                "arxiv_comment": "accepted in the research track of ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23990v2",
                "updated": "2025-10-02T13:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    15,
                    14,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-28T17:32:52Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    32,
                    52,
                    6,
                    271,
                    0
                ],
                "title": "The Hidden Costs of Translation Accuracy: Distillation, Quantization,\n  and Environmental Impact",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Costs of Translation Accuracy: Distillation, Quantization,\n  and Environmental Impact"
                },
                "summary": "The rapid expansion of large language models (LLMs) has heightened concerns\nabout their computational and environmental costs. This study investigates the\ntrade-offs between translation quality and efficiency by comparing full-scale,\ndistilled, and quantized models using machine translation as a case study. We\nevaluated performance on the Flores+ benchmark and through human judgments of\nconversational translations in French, Hindi, and Kannada. Our analysis\nrevealed that the full 3.3B FP32 model, while achieving the highest BLEU\nscores, incurred the largest environmental footprint (~ 0.007-0.008 kg CO2 per\nrun). The distilled 600M FP32 model reduced inference time by 71-78% and carbon\nemissions by 63-65% compared with the full model, with only minimal reductions\nin BLEU scores. Human evaluations further showed that even aggressive\nquantization (INT4) preserved high levels of accuracy and fluency, with\ndifferences between models generally minor. These findings demonstrate that\nmodel compression strategies can substantially reduce computational demands and\nenvironmental impact while maintaining competitive translation quality, though\ntrade-offs are more pronounced in low-resource settings. We argue for\nevaluation frameworks that integrate efficiency and sustainability alongside\naccuracy as central dimensions of progress in NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of large language models (LLMs) has heightened concerns\nabout their computational and environmental costs. This study investigates the\ntrade-offs between translation quality and efficiency by comparing full-scale,\ndistilled, and quantized models using machine translation as a case study. We\nevaluated performance on the Flores+ benchmark and through human judgments of\nconversational translations in French, Hindi, and Kannada. Our analysis\nrevealed that the full 3.3B FP32 model, while achieving the highest BLEU\nscores, incurred the largest environmental footprint (~ 0.007-0.008 kg CO2 per\nrun). The distilled 600M FP32 model reduced inference time by 71-78% and carbon\nemissions by 63-65% compared with the full model, with only minimal reductions\nin BLEU scores. Human evaluations further showed that even aggressive\nquantization (INT4) preserved high levels of accuracy and fluency, with\ndifferences between models generally minor. These findings demonstrate that\nmodel compression strategies can substantially reduce computational demands and\nenvironmental impact while maintaining competitive translation quality, though\ntrade-offs are more pronounced in low-resource settings. We argue for\nevaluation frameworks that integrate efficiency and sustainability alongside\naccuracy as central dimensions of progress in NLP."
                },
                "authors": [
                    {
                        "name": "Dhaathri Vijay"
                    },
                    {
                        "name": "Anandaswarup Vadapalli"
                    }
                ],
                "author_detail": {
                    "name": "Anandaswarup Vadapalli"
                },
                "author": "Anandaswarup Vadapalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01991v1",
                "updated": "2025-10-02T13:13:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    13,
                    19,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T13:13:19Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    13,
                    19,
                    3,
                    275,
                    0
                ],
                "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing"
                },
                "summary": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges\nwith view, temporal, and non-editing region consistency, as well as with\nhandling complex text instructions. To address these issues, we propose\n4DGS-Craft, a consistent and interactive 4DGS editing framework. We first\nintroduce a 4D-aware InstructPix2Pix model to ensure both view and temporal\nconsistency. This model incorporates 4D VGGT geometry features extracted from\nthe initial scene, enabling it to capture underlying 4D geometric structures\nduring editing. We further enhance this model with a multi-view grid module\nthat enforces consistency by iteratively refining multi-view input images while\njointly optimizing the underlying 4D scene. Furthermore, we preserve the\nconsistency of non-edited regions through a novel Gaussian selection mechanism,\nwhich identifies and optimizes only the Gaussians within the edited regions.\nBeyond consistency, facilitating user interaction is also crucial for effective\n4DGS editing. Therefore, we design an LLM-based module for user intent\nunderstanding. This module employs a user instruction template to define atomic\nediting operations and leverages an LLM for reasoning. As a result, our\nframework can interpret user intent and decompose complex instructions into a\nlogical sequence of atomic operations, enabling it to handle intricate user\ncommands and further enhance editing performance. Compared to related works,\nour approach enables more consistent and controllable 4D scene editing. Our\ncode will be made available upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges\nwith view, temporal, and non-editing region consistency, as well as with\nhandling complex text instructions. To address these issues, we propose\n4DGS-Craft, a consistent and interactive 4DGS editing framework. We first\nintroduce a 4D-aware InstructPix2Pix model to ensure both view and temporal\nconsistency. This model incorporates 4D VGGT geometry features extracted from\nthe initial scene, enabling it to capture underlying 4D geometric structures\nduring editing. We further enhance this model with a multi-view grid module\nthat enforces consistency by iteratively refining multi-view input images while\njointly optimizing the underlying 4D scene. Furthermore, we preserve the\nconsistency of non-edited regions through a novel Gaussian selection mechanism,\nwhich identifies and optimizes only the Gaussians within the edited regions.\nBeyond consistency, facilitating user interaction is also crucial for effective\n4DGS editing. Therefore, we design an LLM-based module for user intent\nunderstanding. This module employs a user instruction template to define atomic\nediting operations and leverages an LLM for reasoning. As a result, our\nframework can interpret user intent and decompose complex instructions into a\nlogical sequence of atomic operations, enabling it to handle intricate user\ncommands and further enhance editing performance. Compared to related works,\nour approach enables more consistent and controllable 4D scene editing. Our\ncode will be made available upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Zhenghao Chen"
                    },
                    {
                        "name": "Dong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Xu"
                },
                "author": "Dong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12864v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12864v4",
                "updated": "2025-10-02T13:09:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    9,
                    42,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-19T08:48:12Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    8,
                    48,
                    12,
                    0,
                    139,
                    0
                ],
                "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams"
                },
                "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. To address this, we\nintroduce \\textsc{LEXam}, a novel benchmark derived from 340 law exams spanning\n116 law school courses across a range of subjects and degree levels. The\ndataset comprises 4,886 law exam questions in English and German, including\n2,841 long-form, open-ended questions and 2,045 multiple-choice questions.\nBesides reference answers, the open questions are also accompanied by explicit\nguidance outlining the expected legal reasoning approach such as issue\nspotting, rule recall, or rule application. Our evaluation on both open-ended\nand multiple-choice questions present significant challenges for current LLMs;\nin particular, they notably struggle with open questions that require\nstructured, multi-step legal reasoning. Moreover, our results underscore the\neffectiveness of the dataset in differentiating between models with varying\ncapabilities. Deploying an ensemble LLM-as-a-Judge paradigm with rigorous human\nexpert validation, we demonstrate how model-generated reasoning steps can be\nevaluated consistently and accurately, closely aligning with human expert\nassessments. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. We have open-sourced our code\non \\href{https://github.com/LEXam-Benchmark/LEXam}{GitHub} and released our\ndata on \\href{https://huggingface.co/datasets/LEXam-Benchmark/LEXam}{Hugging\nFace}. Project page: https://lexam-benchmark.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. To address this, we\nintroduce \\textsc{LEXam}, a novel benchmark derived from 340 law exams spanning\n116 law school courses across a range of subjects and degree levels. The\ndataset comprises 4,886 law exam questions in English and German, including\n2,841 long-form, open-ended questions and 2,045 multiple-choice questions.\nBesides reference answers, the open questions are also accompanied by explicit\nguidance outlining the expected legal reasoning approach such as issue\nspotting, rule recall, or rule application. Our evaluation on both open-ended\nand multiple-choice questions present significant challenges for current LLMs;\nin particular, they notably struggle with open questions that require\nstructured, multi-step legal reasoning. Moreover, our results underscore the\neffectiveness of the dataset in differentiating between models with varying\ncapabilities. Deploying an ensemble LLM-as-a-Judge paradigm with rigorous human\nexpert validation, we demonstrate how model-generated reasoning steps can be\nevaluated consistently and accurately, closely aligning with human expert\nassessments. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. We have open-sourced our code\non \\href{https://github.com/LEXam-Benchmark/LEXam}{GitHub} and released our\ndata on \\href{https://huggingface.co/datasets/LEXam-Benchmark/LEXam}{Hugging\nFace}. Project page: https://lexam-benchmark.github.io/"
                },
                "authors": [
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Jakob Merane"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Yoan Hermstrüwer"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Mubashara Akhtar"
                    },
                    {
                        "name": "Etienne Salimbeni"
                    },
                    {
                        "name": "Florian Geering"
                    },
                    {
                        "name": "Oliver Dreyer"
                    },
                    {
                        "name": "Daniel Brunner"
                    },
                    {
                        "name": "Markus Leippold"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Alexander Stremitzer"
                    },
                    {
                        "name": "Christoph Engel"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Joel Niklaus"
                    }
                ],
                "author_detail": {
                    "name": "Joel Niklaus"
                },
                "author": "Joel Niklaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12864v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12864v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00499v2",
                "updated": "2025-10-02T13:05:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    13,
                    5,
                    41,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-01T04:32:37Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    32,
                    37,
                    2,
                    274,
                    0
                ],
                "title": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance"
                },
                "summary": "Spoken dialogue systems often rely on cascaded pipelines that transcribe,\nprocess, and resynthesize speech. While effective, this design discards\nparalinguistic cues and limits expressivity. Recent end-to-end methods reduce\nlatency and better preserve these cues, yet still rely on text intermediates,\ncreating a fundamental bottleneck. We present MOSS-Speech, a true\nspeech-to-speech large language model that directly understands and generates\nspeech without relying on text guidance. Our approach combines a modality-based\nlayer-splitting architecture with a frozen pre-training strategy, preserving\nthe reasoning and knowledge of pretrained text LLMs while adding native speech\ncapabilities. Experiments show that our model achieves state-of-the-art results\nin spoken question answering and delivers comparable speech-to-speech\nperformance relative to existing text-guided systems, while still maintaining\ncompetitive text performance. By narrowing the gap between text-guided and\ndirect speech generation, our work establishes a new paradigm for expressive\nand efficient end-to-end speech interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken dialogue systems often rely on cascaded pipelines that transcribe,\nprocess, and resynthesize speech. While effective, this design discards\nparalinguistic cues and limits expressivity. Recent end-to-end methods reduce\nlatency and better preserve these cues, yet still rely on text intermediates,\ncreating a fundamental bottleneck. We present MOSS-Speech, a true\nspeech-to-speech large language model that directly understands and generates\nspeech without relying on text guidance. Our approach combines a modality-based\nlayer-splitting architecture with a frozen pre-training strategy, preserving\nthe reasoning and knowledge of pretrained text LLMs while adding native speech\ncapabilities. Experiments show that our model achieves state-of-the-art results\nin spoken question answering and delivers comparable speech-to-speech\nperformance relative to existing text-guided systems, while still maintaining\ncompetitive text performance. By narrowing the gap between text-guided and\ndirect speech generation, our work establishes a new paradigm for expressive\nand efficient end-to-end speech interaction."
                },
                "authors": [
                    {
                        "name": "Xingjian Zhao"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Qinyuan Cheng"
                    },
                    {
                        "name": "Zhaoye Fei"
                    },
                    {
                        "name": "Luozhijie Jin"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Hanfu Chen"
                    },
                    {
                        "name": "Yaozhou Jiang"
                    },
                    {
                        "name": "Qinghui Gao"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Mingshu Chen"
                    },
                    {
                        "name": "Ruiming Wang"
                    },
                    {
                        "name": "Wenbo Zhang"
                    },
                    {
                        "name": "Yiyang Zhang"
                    },
                    {
                        "name": "Donghua Yu"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Xiaogui Yang"
                    },
                    {
                        "name": "Yitian Gong"
                    },
                    {
                        "name": "Yuanfan Xu"
                    },
                    {
                        "name": "Yaqian Zhou"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00866v2",
                "updated": "2025-10-02T12:56:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    12,
                    56,
                    4,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-01T13:15:15Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    15,
                    15,
                    2,
                    274,
                    0
                ],
                "title": "The Data-Quality Illusion: Rethinking Classifier-Based Quality Filtering\n  for LLM Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Data-Quality Illusion: Rethinking Classifier-Based Quality Filtering\n  for LLM Pretraining"
                },
                "summary": "Large-scale models are pretrained on massive web-crawled datasets containing\ndocuments of mixed quality, making data filtering essential. A popular method\nis Classifier-based Quality Filtering (CQF), which trains a binary classifier\nto distinguish between pretraining data and a small, high-quality set. It\nassigns each pretraining document a quality score defined as the classifier's\nscore and retains only the top-scoring ones. We provide an in-depth analysis of\nCQF. We show that while CQF improves downstream task performance, it does not\nnecessarily enhance language modeling on the high-quality dataset. We explain\nthis paradox by the fact that CQF implicitly filters the high-quality dataset\nas well. We further compare the behavior of models trained with CQF to those\ntrained on synthetic data of increasing quality, obtained via random token\npermutations, and find starkly different trends. Our results challenge the view\nthat CQF captures a meaningful notion of data quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale models are pretrained on massive web-crawled datasets containing\ndocuments of mixed quality, making data filtering essential. A popular method\nis Classifier-based Quality Filtering (CQF), which trains a binary classifier\nto distinguish between pretraining data and a small, high-quality set. It\nassigns each pretraining document a quality score defined as the classifier's\nscore and retains only the top-scoring ones. We provide an in-depth analysis of\nCQF. We show that while CQF improves downstream task performance, it does not\nnecessarily enhance language modeling on the high-quality dataset. We explain\nthis paradox by the fact that CQF implicitly filters the high-quality dataset\nas well. We further compare the behavior of models trained with CQF to those\ntrained on synthetic data of increasing quality, obtained via random token\npermutations, and find starkly different trends. Our results challenge the view\nthat CQF captures a meaningful notion of data quality."
                },
                "authors": [
                    {
                        "name": "Thiziri Nait Saada"
                    },
                    {
                        "name": "Louis Bethune"
                    },
                    {
                        "name": "Michal Klein"
                    },
                    {
                        "name": "David Grangier"
                    },
                    {
                        "name": "Marco Cuturi"
                    },
                    {
                        "name": "Pierre Ablin"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Ablin"
                },
                "author": "Pierre Ablin",
                "arxiv_comment": "21 pages, 20 figures, 2 tables, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01967v1",
                "updated": "2025-10-02T12:39:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    12,
                    39,
                    57,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T12:39:57Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    12,
                    39,
                    57,
                    3,
                    275,
                    0
                ],
                "title": "ZK-WAGON: Imperceptible Watermark for Image Generation Models using\n  ZK-SNARKs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZK-WAGON: Imperceptible Watermark for Image Generation Models using\n  ZK-SNARKs"
                },
                "summary": "As image generation models grow increasingly powerful and accessible,\nconcerns around authenticity, ownership, and misuse of synthetic media have\nbecome critical. The ability to generate lifelike images indistinguishable from\nreal ones introduces risks such as misinformation, deepfakes, and intellectual\nproperty violations. Traditional watermarking methods either degrade image\nquality, are easily removed, or require access to confidential model internals\n- making them unsuitable for secure and scalable deployment. We are the first\nto introduce ZK-WAGON, a novel system for watermarking image generation models\nusing the Zero-Knowledge Succinct Non Interactive Argument of Knowledge\n(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing\nmodel weights, generation prompts, or any sensitive internal information. We\npropose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively\nconvert key layers of an image generation model into a circuit, reducing proof\ngeneration time significantly. Generated ZK-SNARK proofs are imperceptibly\nembedded into a generated image via Least Significant Bit (LSB) steganography.\nWe demonstrate this system on both GAN and Diffusion models, providing a\nsecure, model-agnostic pipeline for trustworthy AI image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As image generation models grow increasingly powerful and accessible,\nconcerns around authenticity, ownership, and misuse of synthetic media have\nbecome critical. The ability to generate lifelike images indistinguishable from\nreal ones introduces risks such as misinformation, deepfakes, and intellectual\nproperty violations. Traditional watermarking methods either degrade image\nquality, are easily removed, or require access to confidential model internals\n- making them unsuitable for secure and scalable deployment. We are the first\nto introduce ZK-WAGON, a novel system for watermarking image generation models\nusing the Zero-Knowledge Succinct Non Interactive Argument of Knowledge\n(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing\nmodel weights, generation prompts, or any sensitive internal information. We\npropose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively\nconvert key layers of an image generation model into a circuit, reducing proof\ngeneration time significantly. Generated ZK-SNARK proofs are imperceptibly\nembedded into a generated image via Least Significant Bit (LSB) steganography.\nWe demonstrate this system on both GAN and Diffusion models, providing a\nsecure, model-agnostic pipeline for trustworthy AI image generation."
                },
                "authors": [
                    {
                        "name": "Aadarsh Anantha Ramakrishnan"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Selvanayagam S"
                    },
                    {
                        "name": "Kunwar Singh"
                    }
                ],
                "author_detail": {
                    "name": "Kunwar Singh"
                },
                "author": "Kunwar Singh",
                "arxiv_comment": "Accepted at AI-ML Systems 2025, Bangalore, India,\n  https://www.aimlsystems.org/2025/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25848v2",
                "updated": "2025-10-02T12:24:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    12,
                    24,
                    56,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-30T06:37:47Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    6,
                    37,
                    47,
                    1,
                    273,
                    0
                ],
                "title": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in\n  Vision-Language Models"
                },
                "summary": "Reasoning has emerged as a pivotal capability in Large Language Models\n(LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy\nOptimization (GRPO), these models are able to solve complex tasks such as\nmathematics and code generation. Building on these advances, recent research\nhas sought to extend reasoning to Vision-Language Models (VLMs), yielding\npromising results across diverse visual tasks. Despite this progress, our study\nuncovers the dual nature of multimodal reasoning: while it substantially\nenhances logical inference and facilitates performance on challenging problems,\nit may gradually impair perceptual grounding, leading to recognition failures\non otherwise basic visual questions. Through further analysis, we attribute\nthis phenomenon to visual forgetting, wherein prolonged reasoning causes the\nmodel to increasingly disregard visual input. To address this, we propose\nVision-Anchored Policy Optimization (VAPO), a simple yet effective method that\nexplicitly steers the reasoning process toward visually grounded trajectories.\nOur result model, VAPO-Thinker-7B, significantly strengthens the model's\nreliance on visual information and achieves new state-of-the-art results on a\nwide range of established benchmarks. Project page:\nhttps://xytian1008.github.io/VAPO/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning has emerged as a pivotal capability in Large Language Models\n(LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy\nOptimization (GRPO), these models are able to solve complex tasks such as\nmathematics and code generation. Building on these advances, recent research\nhas sought to extend reasoning to Vision-Language Models (VLMs), yielding\npromising results across diverse visual tasks. Despite this progress, our study\nuncovers the dual nature of multimodal reasoning: while it substantially\nenhances logical inference and facilitates performance on challenging problems,\nit may gradually impair perceptual grounding, leading to recognition failures\non otherwise basic visual questions. Through further analysis, we attribute\nthis phenomenon to visual forgetting, wherein prolonged reasoning causes the\nmodel to increasingly disregard visual input. To address this, we propose\nVision-Anchored Policy Optimization (VAPO), a simple yet effective method that\nexplicitly steers the reasoning process toward visually grounded trajectories.\nOur result model, VAPO-Thinker-7B, significantly strengthens the model's\nreliance on visual information and achieves new state-of-the-art results on a\nwide range of established benchmarks. Project page:\nhttps://xytian1008.github.io/VAPO/"
                },
                "authors": [
                    {
                        "name": "Xinyu Tian"
                    },
                    {
                        "name": "Shu Zou"
                    },
                    {
                        "name": "Zhaoyuan Yang"
                    },
                    {
                        "name": "Mengqi He"
                    },
                    {
                        "name": "Fabian Waschkowski"
                    },
                    {
                        "name": "Lukas Wesemann"
                    },
                    {
                        "name": "Peter Tu"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04630v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04630v2",
                "updated": "2025-10-02T12:24:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    12,
                    24,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-04-06T21:42:02Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    21,
                    42,
                    2,
                    6,
                    96,
                    0
                ],
                "title": "Foundation Models for Software Engineering of Cyber-Physical Systems:\n  the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Models for Software Engineering of Cyber-Physical Systems:\n  the Road Ahead"
                },
                "summary": "FMs, particularly LLMs, are increasingly used to support various software\nengineering activities (e.g., coding and testing). Their applications in the\nsoftware engineering of CPSs are also growing. However, research in this area\nremains limited. Moreover, existing studies have primarily focused on LLMs-only\none type of FM-leaving ample opportunities to explore others, such as\nvision-language models. We argue that, in addition to LLMs, other FMs utilizing\ndifferent data modalities (e.g., images, audio) and multimodal models (which\nintegrate multiple modalities) hold great potential for supporting CPS software\nengineering, given that these systems process diverse data types. To address\nthis, we present a research roadmap for integrating FMs into various phases of\nCPS software engineering, highlighting key research opportunities and\nchallenges for the software engineering community. Moreover, we discuss the\ncommon challenges associated with applying FMs in this context, including the\ncorrectness of FM-generated artifacts, as well as the inherent uncertainty and\nhallucination associated with FMs. This roadmap is intended for researchers and\npractitioners in CPS software engineering, providing future research directions\nusing FMs in this domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMs, particularly LLMs, are increasingly used to support various software\nengineering activities (e.g., coding and testing). Their applications in the\nsoftware engineering of CPSs are also growing. However, research in this area\nremains limited. Moreover, existing studies have primarily focused on LLMs-only\none type of FM-leaving ample opportunities to explore others, such as\nvision-language models. We argue that, in addition to LLMs, other FMs utilizing\ndifferent data modalities (e.g., images, audio) and multimodal models (which\nintegrate multiple modalities) hold great potential for supporting CPS software\nengineering, given that these systems process diverse data types. To address\nthis, we present a research roadmap for integrating FMs into various phases of\nCPS software engineering, highlighting key research opportunities and\nchallenges for the software engineering community. Moreover, we discuss the\ncommon challenges associated with applying FMs in this context, including the\ncorrectness of FM-generated artifacts, as well as the inherent uncertainty and\nhallucination associated with FMs. This roadmap is intended for researchers and\npractitioners in CPS software engineering, providing future research directions\nusing FMs in this domain."
                },
                "authors": [
                    {
                        "name": "Chengjie Lu"
                    },
                    {
                        "name": "Pablo Valle"
                    },
                    {
                        "name": "Jiahui Wu"
                    },
                    {
                        "name": "Erblin Isaku"
                    },
                    {
                        "name": "Hassan Sartaj"
                    },
                    {
                        "name": "Aitor Arrieta"
                    },
                    {
                        "name": "Shaukat Ali"
                    }
                ],
                "author_detail": {
                    "name": "Shaukat Ali"
                },
                "author": "Shaukat Ali",
                "arxiv_comment": "2 figures, 37 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04630v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04630v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01954v1",
                "updated": "2025-10-02T12:23:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    12,
                    23,
                    57,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T12:23:57Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    12,
                    23,
                    57,
                    3,
                    275,
                    0
                ],
                "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in\n  MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in\n  MLLMs"
                },
                "summary": "Multimodal large language models (MLLMs) have advanced rapidly in recent\nyears. However, existing approaches for vision tasks often rely on indirect\nrepresentations, such as generating coordinates as text for detection, which\nlimits performance and prevents dense prediction tasks like segmentation. To\novercome these challenges, we introduce Patch-as-Decodable Token (PaDT), a\nunified paradigm that enables MLLMs to directly generate both textual and\ndiverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),\nderived from visual patch embeddings of query images and interleaved seamlessly\nwith LLM's output textual tokens. A lightweight decoder then transforms LLM's\noutputs into detection, segmentation, and grounding predictions. Unlike prior\nmethods, PaDT processes VRTs independently at each forward pass and dynamically\nexpands the embedding table, thus improving localization and differentiation\namong similar objects. We further tailor a training strategy for PaDT by\nrandomly selecting VRTs for supervised fine-tuning and introducing a robust\nper-token cross-entropy loss. Our empirical studies across four visual\nperception and understanding tasks suggest PaDT consistently achieving\nstate-of-the-art performance, even compared with significantly larger MLLM\nmodels. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have advanced rapidly in recent\nyears. However, existing approaches for vision tasks often rely on indirect\nrepresentations, such as generating coordinates as text for detection, which\nlimits performance and prevents dense prediction tasks like segmentation. To\novercome these challenges, we introduce Patch-as-Decodable Token (PaDT), a\nunified paradigm that enables MLLMs to directly generate both textual and\ndiverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),\nderived from visual patch embeddings of query images and interleaved seamlessly\nwith LLM's output textual tokens. A lightweight decoder then transforms LLM's\noutputs into detection, segmentation, and grounding predictions. Unlike prior\nmethods, PaDT processes VRTs independently at each forward pass and dynamically\nexpands the embedding table, thus improving localization and differentiation\namong similar objects. We further tailor a training strategy for PaDT by\nrandomly selecting VRTs for supervised fine-tuning and introducing a robust\nper-token cross-entropy loss. Our empirical studies across four visual\nperception and understanding tasks suggest PaDT consistently achieving\nstate-of-the-art performance, even compared with significantly larger MLLM\nmodels. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT."
                },
                "authors": [
                    {
                        "name": "Yongyi Su"
                    },
                    {
                        "name": "Haojie Zhang"
                    },
                    {
                        "name": "Shijie Li"
                    },
                    {
                        "name": "Nanqing Liu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Junyi Pan"
                    },
                    {
                        "name": "Yuan Liu"
                    },
                    {
                        "name": "Xiaofen Xing"
                    },
                    {
                        "name": "Chong Sun"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Xulei Yang"
                    },
                    {
                        "name": "Xun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xun Xu"
                },
                "author": "Xun Xu",
                "arxiv_comment": "24 pages, 12 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11997v2",
                "updated": "2025-10-02T11:55:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    55,
                    50,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-16T07:50:43Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    50,
                    43,
                    2,
                    197,
                    0
                ],
                "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection"
                },
                "summary": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods."
                },
                "authors": [
                    {
                        "name": "Tairan Huang"
                    },
                    {
                        "name": "Yili Wang"
                    },
                    {
                        "name": "Qiutong Li"
                    },
                    {
                        "name": "Changlong He"
                    },
                    {
                        "name": "Jianliang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianliang Gao"
                },
                "author": "Jianliang Gao",
                "arxiv_doi": "10.1145/3746027.3755245",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755245",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.11997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM MM 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19629v2",
                "updated": "2025-10-02T11:52:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    52,
                    44,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-26T07:47:50Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    7,
                    47,
                    50,
                    0,
                    146,
                    0
                ],
                "title": "Software Engineering for Self-Adaptive Robotics: A Research Agenda",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Engineering for Self-Adaptive Robotics: A Research Agenda"
                },
                "summary": "Self-adaptive robotic systems operate autonomously in dynamic and uncertain\nenvironments, requiring robust real-time monitoring and adaptive behaviour.\nUnlike traditional robotic software with predefined logic, self-adaptive robots\nexploit artificial intelligence (AI), machine learning, and model-driven\nengineering to adapt continuously to changing conditions, thereby ensuring\nreliability, safety, and optimal performance. This paper presents a research\nagenda for software engineering in self-adaptive robotics, structured along two\ndimensions. The first concerns the software engineering lifecycle,\nrequirements, design, development, testing, and operations, tailored to the\nchallenges of self-adaptive robotics. The second focuses on enabling\ntechnologies such as digital twins, AI-driven adaptation, and quantum\ncomputing, which support runtime monitoring, fault detection, and automated\ndecision-making. We identify open challenges, including verifying adaptive\nbehaviours under uncertainty, balancing trade-offs between adaptability,\nperformance, and safety, and integrating self-adaptation frameworks like\nMAPE-K/MAPLE-K. By consolidating these challenges into a roadmap toward 2030,\nthis work contributes to the foundations of trustworthy and efficient\nself-adaptive robotic systems capable of meeting the complexities of real-world\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-adaptive robotic systems operate autonomously in dynamic and uncertain\nenvironments, requiring robust real-time monitoring and adaptive behaviour.\nUnlike traditional robotic software with predefined logic, self-adaptive robots\nexploit artificial intelligence (AI), machine learning, and model-driven\nengineering to adapt continuously to changing conditions, thereby ensuring\nreliability, safety, and optimal performance. This paper presents a research\nagenda for software engineering in self-adaptive robotics, structured along two\ndimensions. The first concerns the software engineering lifecycle,\nrequirements, design, development, testing, and operations, tailored to the\nchallenges of self-adaptive robotics. The second focuses on enabling\ntechnologies such as digital twins, AI-driven adaptation, and quantum\ncomputing, which support runtime monitoring, fault detection, and automated\ndecision-making. We identify open challenges, including verifying adaptive\nbehaviours under uncertainty, balancing trade-offs between adaptability,\nperformance, and safety, and integrating self-adaptation frameworks like\nMAPE-K/MAPLE-K. By consolidating these challenges into a roadmap toward 2030,\nthis work contributes to the foundations of trustworthy and efficient\nself-adaptive robotic systems capable of meeting the complexities of real-world\ndeployment."
                },
                "authors": [
                    {
                        "name": "Hassan Sartaj"
                    },
                    {
                        "name": "Shaukat Ali"
                    },
                    {
                        "name": "Ana Cavalcanti"
                    },
                    {
                        "name": "Lukas Esterle"
                    },
                    {
                        "name": "Cláudio Gomes"
                    },
                    {
                        "name": "Peter Gorm Larsen"
                    },
                    {
                        "name": "Anastasios Tefas"
                    },
                    {
                        "name": "Jim Woodcock"
                    },
                    {
                        "name": "Houxiang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Houxiang Zhang"
                },
                "author": "Houxiang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01932v1",
                "updated": "2025-10-02T11:49:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    49,
                    48,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T11:49:48Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    49,
                    48,
                    3,
                    275,
                    0
                ],
                "title": "Veri-R1: Toward Precise and Faithful Claim Verification via Online\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Veri-R1: Toward Precise and Faithful Claim Verification via Online\n  Reinforcement Learning"
                },
                "summary": "Claim verification with large language models (LLMs) has recently attracted\nconsiderable attention, owing to their superior reasoning capabilities and\ntransparent verification pathways compared to traditional answer-only\njudgments. Online claim verification requires iterative evidence retrieval and\nreasoning, yet existing approaches mainly rely on prompt engineering or\npredesigned reasoning workflows without offering a unified training paradigm to\nimprove necessary skills. Therefore, we introduce Veri-R1, an online\nreinforcement learning (RL) framework that enables an LLM to interact with a\nsearch engine and to receive reward signals that explicitly shape its planning,\nretrieval, and reasoning behaviors. The dynamic interaction between models and\nretrieval systems more accurately reflects real-world verification scenarios\nand fosters comprehensive verification skills. Empirical results show that\nVeri-R1 improves joint accuracy by up to 30% and doubles evidence score, often\nsurpassing larger-scale counterparts. Ablation studies further reveal the\nimpact of reward components and the link between output logits and label\naccuracy. Our results highlight the effectiveness of online RL for precise and\nfaithful claim verification and provide a foundation for future research. We\nrelease our code to support community progress in LLM empowered claim\nverification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Claim verification with large language models (LLMs) has recently attracted\nconsiderable attention, owing to their superior reasoning capabilities and\ntransparent verification pathways compared to traditional answer-only\njudgments. Online claim verification requires iterative evidence retrieval and\nreasoning, yet existing approaches mainly rely on prompt engineering or\npredesigned reasoning workflows without offering a unified training paradigm to\nimprove necessary skills. Therefore, we introduce Veri-R1, an online\nreinforcement learning (RL) framework that enables an LLM to interact with a\nsearch engine and to receive reward signals that explicitly shape its planning,\nretrieval, and reasoning behaviors. The dynamic interaction between models and\nretrieval systems more accurately reflects real-world verification scenarios\nand fosters comprehensive verification skills. Empirical results show that\nVeri-R1 improves joint accuracy by up to 30% and doubles evidence score, often\nsurpassing larger-scale counterparts. Ablation studies further reveal the\nimpact of reward components and the link between output logits and label\naccuracy. Our results highlight the effectiveness of online RL for precise and\nfaithful claim verification and provide a foundation for future research. We\nrelease our code to support community progress in LLM empowered claim\nverification."
                },
                "authors": [
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Yi R."
                    },
                    {
                        "name": "Fung"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "arxiv_affiliation": "May",
                "author": "Heng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01929v1",
                "updated": "2025-10-02T11:47:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    47,
                    18,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T11:47:18Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    47,
                    18,
                    3,
                    275,
                    0
                ],
                "title": "Inverse Language Modeling towards Robust and Grounded LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Language Modeling towards Robust and Grounded LLMs"
                },
                "summary": "The current landscape of defensive mechanisms for LLMs is fragmented and\nunderdeveloped, unlike prior work on classifiers. To further promote\nadversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), a\nunified framework that simultaneously 1) improves the robustness of LLMs to\ninput perturbations, and, at the same time, 2) enables native grounding by\ninverting model outputs to identify potentially toxic or unsafe input triggers.\nILM transforms LLMs from static generators into analyzable and robust systems,\npotentially helping RED teaming. ILM can lay the foundation for next-generation\nLLMs that are not only robust and grounded but also fundamentally more\ncontrollable and trustworthy. The code is publicly available at\ngithub.com/davegabe/pag-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current landscape of defensive mechanisms for LLMs is fragmented and\nunderdeveloped, unlike prior work on classifiers. To further promote\nadversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), a\nunified framework that simultaneously 1) improves the robustness of LLMs to\ninput perturbations, and, at the same time, 2) enables native grounding by\ninverting model outputs to identify potentially toxic or unsafe input triggers.\nILM transforms LLMs from static generators into analyzable and robust systems,\npotentially helping RED teaming. ILM can lay the foundation for next-generation\nLLMs that are not only robust and grounded but also fundamentally more\ncontrollable and trustworthy. The code is publicly available at\ngithub.com/davegabe/pag-llm."
                },
                "authors": [
                    {
                        "name": "Davide Gabrielli"
                    },
                    {
                        "name": "Simone Sestito"
                    },
                    {
                        "name": "Iacopo Masi"
                    }
                ],
                "author_detail": {
                    "name": "Iacopo Masi"
                },
                "author": "Iacopo Masi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01925v2",
                "updated": "2025-10-03T10:55:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    55,
                    18,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-02T11:42:17Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    42,
                    17,
                    3,
                    275,
                    0
                ],
                "title": "Enhancing Large Language Model Reasoning with Reward Models: An\n  Analytical Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Model Reasoning with Reward Models: An\n  Analytical Survey"
                },
                "summary": "Reward models (RMs) play a critical role in enhancing the reasoning\nperformance of LLMs. For example, they can provide training signals to finetune\nLLMs during reinforcement learning (RL) and help select the best answer from\nmultiple candidates during inference. In this paper, we provide a systematic\nintroduction to RMs, along with a comprehensive survey of their applications in\nLLM reasoning. We first review fundamental concepts of RMs, including their\narchitectures, training methodologies, and evaluation techniques. Then, we\nexplore their key applications: (1) guiding generation and selecting optimal\noutputs during LLM inference, (2) facilitating data synthesis and iterative\nself-improvement for LLMs, and (3) providing training signals in RL-based\nfinetuning. Finally, we discuss critical open questions regarding the\nselection, generalization, evaluation, and enhancement of RMs, based on\nexisting research and our own empirical findings. Our analysis aims to provide\nactionable insights for the effective deployment and advancement of RMs for LLM\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) play a critical role in enhancing the reasoning\nperformance of LLMs. For example, they can provide training signals to finetune\nLLMs during reinforcement learning (RL) and help select the best answer from\nmultiple candidates during inference. In this paper, we provide a systematic\nintroduction to RMs, along with a comprehensive survey of their applications in\nLLM reasoning. We first review fundamental concepts of RMs, including their\narchitectures, training methodologies, and evaluation techniques. Then, we\nexplore their key applications: (1) guiding generation and selecting optimal\noutputs during LLM inference, (2) facilitating data synthesis and iterative\nself-improvement for LLMs, and (3) providing training signals in RL-based\nfinetuning. Finally, we discuss critical open questions regarding the\nselection, generalization, evaluation, and enhancement of RMs, based on\nexisting research and our own empirical findings. Our analysis aims to provide\nactionable insights for the effective deployment and advancement of RMs for LLM\nreasoning."
                },
                "authors": [
                    {
                        "name": "Qiyuan Liu"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Xuhong Chen"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yee Whye Teh"
                    },
                    {
                        "name": "Ning Miao"
                    }
                ],
                "author_detail": {
                    "name": "Ning Miao"
                },
                "author": "Ning Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01924v1",
                "updated": "2025-10-02T11:41:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    41,
                    30,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T11:41:30Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    41,
                    30,
                    3,
                    275,
                    0
                ],
                "title": "To Mask or to Mirror: Human-AI Alignment in Collective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Mask or to Mirror: Human-AI Alignment in Collective Reasoning"
                },
                "summary": "As large language models (LLMs) are increasingly used to model and augment\ncollective decision-making, it is critical to examine their alignment with\nhuman social reasoning. We present an empirical framework for assessing\ncollective alignment, in contrast to prior work on the individual level. Using\nthe Lost at Sea social psychology task, we conduct a large-scale online\nexperiment (N=748), randomly assigning groups to leader elections with either\nvisible demographic attributes (e.g. name, gender) or pseudonymous aliases. We\nthen simulate matched LLM groups conditioned on the human data, benchmarking\nGemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some\nmirror human biases; others mask these biases and attempt to compensate for\nthem. We empirically demonstrate that human-AI alignment in collective\nreasoning depends on context, cues, and model-specific inductive biases.\nUnderstanding how LLMs align with collective human behavior is critical to\nadvancing socially-aligned AI, and demands dynamic benchmarks that capture the\ncomplexities of collective reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used to model and augment\ncollective decision-making, it is critical to examine their alignment with\nhuman social reasoning. We present an empirical framework for assessing\ncollective alignment, in contrast to prior work on the individual level. Using\nthe Lost at Sea social psychology task, we conduct a large-scale online\nexperiment (N=748), randomly assigning groups to leader elections with either\nvisible demographic attributes (e.g. name, gender) or pseudonymous aliases. We\nthen simulate matched LLM groups conditioned on the human data, benchmarking\nGemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some\nmirror human biases; others mask these biases and attempt to compensate for\nthem. We empirically demonstrate that human-AI alignment in collective\nreasoning depends on context, cues, and model-specific inductive biases.\nUnderstanding how LLMs align with collective human behavior is critical to\nadvancing socially-aligned AI, and demands dynamic benchmarks that capture the\ncomplexities of collective reasoning."
                },
                "authors": [
                    {
                        "name": "Crystal Qian"
                    },
                    {
                        "name": "Aaron Parisi"
                    },
                    {
                        "name": "Clémentine Bouleau"
                    },
                    {
                        "name": "Vivian Tsai"
                    },
                    {
                        "name": "Maël Lebreton"
                    },
                    {
                        "name": "Lucas Dixon"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Dixon"
                },
                "author": "Lucas Dixon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22255v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22255v3",
                "updated": "2025-10-02T11:37:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    37,
                    39,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-26T12:19:22Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    19,
                    22,
                    4,
                    269,
                    0
                ],
                "title": "Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase\n  Heuristics for 2D Bin-Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase\n  Heuristics for 2D Bin-Packing"
                },
                "summary": "This paper presents an evaluation framework for assessing Large Language\nModels' (LLMs) capabilities in combinatorial optimization, specifically\naddressing the 2D bin-packing problem. We introduce a systematic methodology\nthat combines LLMs with evolutionary algorithms to generate and refine\nheuristic solutions iteratively. Through comprehensive experiments comparing\nLLM generated heuristics against traditional approaches (Finite First-Fit and\nHybrid First-Fit), we demonstrate that LLMs can produce more efficient\nsolutions while requiring fewer computational resources. Our evaluation reveals\nthat GPT-4o achieves optimal solutions within two iterations, reducing average\nbin usage from 16 to 15 bins while improving space utilization from 0.76-0.78\nto 0.83. This work contributes to understanding LLM evaluation in specialized\ndomains and establishes benchmarks for assessing LLM performance in\ncombinatorial optimization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an evaluation framework for assessing Large Language\nModels' (LLMs) capabilities in combinatorial optimization, specifically\naddressing the 2D bin-packing problem. We introduce a systematic methodology\nthat combines LLMs with evolutionary algorithms to generate and refine\nheuristic solutions iteratively. Through comprehensive experiments comparing\nLLM generated heuristics against traditional approaches (Finite First-Fit and\nHybrid First-Fit), we demonstrate that LLMs can produce more efficient\nsolutions while requiring fewer computational resources. Our evaluation reveals\nthat GPT-4o achieves optimal solutions within two iterations, reducing average\nbin usage from 16 to 15 bins while improving space utilization from 0.76-0.78\nto 0.83. This work contributes to understanding LLM evaluation in specialized\ndomains and establishes benchmarks for assessing LLM performance in\ncombinatorial optimization tasks."
                },
                "authors": [
                    {
                        "name": "Syed Mahbubul Huq"
                    },
                    {
                        "name": "Daniel Brito"
                    },
                    {
                        "name": "Daniel Sikar"
                    },
                    {
                        "name": "Chris Child"
                    },
                    {
                        "name": "Tillman Weyde"
                    },
                    {
                        "name": "Rajesh Mojumder"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Mojumder"
                },
                "author": "Rajesh Mojumder",
                "arxiv_comment": "1 table, 6 figures. 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Accepted for the Workshop: Evaluating the Evolving LLM\n  Lifecycle Benchmarks, Emergent Abilities, and Scaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22255v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22255v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14245v2",
                "updated": "2025-10-02T11:31:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    31,
                    47,
                    3,
                    275,
                    0
                ],
                "published": "2025-06-17T07:06:56Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    7,
                    6,
                    56,
                    1,
                    168,
                    0
                ],
                "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs"
                },
                "summary": "Recent advancements in long chain-of-thought (CoT) reasoning, particularly\nthrough the Group Relative Policy Optimization algorithm used by DeepSeek-R1,\nhave led to significant interest in the potential of Reinforcement Learning\nwith Verifiable Rewards (RLVR) for Large Language Models (LLMs). While RLVR\npromises to improve reasoning by allowing models to learn from free\nexploration, there remains debate over whether it truly enhances reasoning\nabilities or simply boosts sampling efficiency. This paper systematically\ninvestigates the impact of RLVR on LLM reasoning. We revisit Pass@K experiments\nand demonstrate that RLVR can extend the reasoning boundary for both\nmathematical and coding tasks. This is supported by our introduction of a novel\nevaluation metric, CoT-Pass@K, which captures reasoning success by accounting\nfor both the final answer and intermediate reasoning steps. Furthermore, we\npresent a theoretical framework explaining RLVR's incentive mechanism,\ndemonstrating how it can encourage correct reasoning even when rewards are\nbased solely on answer correctness. Our analysis of RLVR's training dynamics\nreveals that it incentivizes correct reasoning early in the process, with\nsubstantial improvements in reasoning quality confirmed through extensive\nevaluations. These findings provide strong evidence of RLVR's potential to\nenhance LLM reasoning, offering valuable insights into its mechanisms and\nperformance improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in long chain-of-thought (CoT) reasoning, particularly\nthrough the Group Relative Policy Optimization algorithm used by DeepSeek-R1,\nhave led to significant interest in the potential of Reinforcement Learning\nwith Verifiable Rewards (RLVR) for Large Language Models (LLMs). While RLVR\npromises to improve reasoning by allowing models to learn from free\nexploration, there remains debate over whether it truly enhances reasoning\nabilities or simply boosts sampling efficiency. This paper systematically\ninvestigates the impact of RLVR on LLM reasoning. We revisit Pass@K experiments\nand demonstrate that RLVR can extend the reasoning boundary for both\nmathematical and coding tasks. This is supported by our introduction of a novel\nevaluation metric, CoT-Pass@K, which captures reasoning success by accounting\nfor both the final answer and intermediate reasoning steps. Furthermore, we\npresent a theoretical framework explaining RLVR's incentive mechanism,\ndemonstrating how it can encourage correct reasoning even when rewards are\nbased solely on answer correctness. Our analysis of RLVR's training dynamics\nreveals that it incentivizes correct reasoning early in the process, with\nsubstantial improvements in reasoning quality confirmed through extensive\nevaluations. These findings provide strong evidence of RLVR's potential to\nenhance LLM reasoning, offering valuable insights into its mechanisms and\nperformance improvements."
                },
                "authors": [
                    {
                        "name": "Xumeng Wen"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Shun Zheng"
                    },
                    {
                        "name": "Shengyu Ye"
                    },
                    {
                        "name": "Zhirong Wu"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Zhijian Xu"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Ziming Miao"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "Update with more experiments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01910v1",
                "updated": "2025-10-02T11:30:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    30,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T11:30:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    30,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under\n  Deficiencies with Iterative Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under\n  Deficiencies with Iterative Refinement"
                },
                "summary": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications,\nserving as a core technique for learning from graph-structured data, such as\ntext-attributed graphs. Yet in real-world scenarios, such graphs exhibit\ndeficiencies that substantially undermine GNN performance. While prior\nGNN-based augmentation studies have explored robustness against individual\nimperfections, a systematic understanding of how graph-native and Large\nLanguage Models (LLMs) enhanced methods behave under compound deficiencies is\nstill missing. Specifically, there has been no comprehensive investigation\ncomparing conventional approaches and recent LLM-on-graph frameworks, leaving\ntheir merits unclear. To fill this gap, we conduct the first empirical study\nthat benchmarks these two lines of methods across diverse graph deficiencies,\nrevealing overlooked vulnerabilities and challenging the assumption that LLM\naugmentation is consistently superior. Building on empirical findings, we\npropose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement\n(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is\nthe first iterative paradigm that leverages Retrieval-Augmented Generation\n(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,\ndiverse augmentations and enforcing discriminative representations through\niterative graph contrastive learning. It transforms LLM augmentation for graphs\nfrom static signal injection into dynamic refinement. Extensive experiments\ndemonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced\nbaselines, achieving up to 82.43% average improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications,\nserving as a core technique for learning from graph-structured data, such as\ntext-attributed graphs. Yet in real-world scenarios, such graphs exhibit\ndeficiencies that substantially undermine GNN performance. While prior\nGNN-based augmentation studies have explored robustness against individual\nimperfections, a systematic understanding of how graph-native and Large\nLanguage Models (LLMs) enhanced methods behave under compound deficiencies is\nstill missing. Specifically, there has been no comprehensive investigation\ncomparing conventional approaches and recent LLM-on-graph frameworks, leaving\ntheir merits unclear. To fill this gap, we conduct the first empirical study\nthat benchmarks these two lines of methods across diverse graph deficiencies,\nrevealing overlooked vulnerabilities and challenging the assumption that LLM\naugmentation is consistently superior. Building on empirical findings, we\npropose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement\n(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is\nthe first iterative paradigm that leverages Retrieval-Augmented Generation\n(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,\ndiverse augmentations and enforcing discriminative representations through\niterative graph contrastive learning. It transforms LLM augmentation for graphs\nfrom static signal injection into dynamic refinement. Extensive experiments\ndemonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced\nbaselines, achieving up to 82.43% average improvement."
                },
                "authors": [
                    {
                        "name": "Zhaoyan Wang"
                    },
                    {
                        "name": "Zheng Gao"
                    },
                    {
                        "name": "Arogya Kharel"
                    },
                    {
                        "name": "In-Young Ko"
                    }
                ],
                "author_detail": {
                    "name": "In-Young Ko"
                },
                "author": "In-Young Ko",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23281v2",
                "updated": "2025-10-02T11:27:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    27,
                    40,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-29T09:28:06Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    28,
                    6,
                    3,
                    149,
                    0
                ],
                "title": "MathArena: Evaluating LLMs on Uncontaminated Math Competitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathArena: Evaluating LLMs on Uncontaminated Math Competitions"
                },
                "summary": "The rapid advancement of reasoning capabilities in large language models\n(LLMs) has led to notable improvements on mathematical benchmarks. However,\nmany of the most commonly used evaluation datasets (e.g., AIME 2024) are widely\navailable online, making it difficult to disentangle genuine reasoning from\npotential memorization. Furthermore, these benchmarks do not evaluate\nproof-writing capabilities, which are crucial for many mathematical tasks. To\naddress this, we introduce MathArena, a new benchmark based on the following\nkey insight: recurring math competitions provide a stream of high-quality,\nchallenging problems that can be used for real-time evaluation of LLMs. By\nevaluating models as soon as new problems are released, we effectively\neliminate the risk of contamination. Using this framework, we find strong signs\nof contamination in AIME 2024. Nonetheless, evaluations on harder competitions,\nsuch as CMIMC 2025, demonstrate impressive reasoning capabilities in\ntop-performing models. MathArena is also the first benchmark for proof-writing\ncapabilities. On IMO 2025, top models achieve slightly less than 40%,\ndemonstrating both notable progress and significant room for improvement. So\nfar, we have evaluated over $50$ models across seven competitions, totaling\n$162$ problems. As an evolving benchmark, MathArena will continue to track the\nprogress of LLMs on newly released competitions, ensuring rigorous and\nup-to-date evaluation of mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of reasoning capabilities in large language models\n(LLMs) has led to notable improvements on mathematical benchmarks. However,\nmany of the most commonly used evaluation datasets (e.g., AIME 2024) are widely\navailable online, making it difficult to disentangle genuine reasoning from\npotential memorization. Furthermore, these benchmarks do not evaluate\nproof-writing capabilities, which are crucial for many mathematical tasks. To\naddress this, we introduce MathArena, a new benchmark based on the following\nkey insight: recurring math competitions provide a stream of high-quality,\nchallenging problems that can be used for real-time evaluation of LLMs. By\nevaluating models as soon as new problems are released, we effectively\neliminate the risk of contamination. Using this framework, we find strong signs\nof contamination in AIME 2024. Nonetheless, evaluations on harder competitions,\nsuch as CMIMC 2025, demonstrate impressive reasoning capabilities in\ntop-performing models. MathArena is also the first benchmark for proof-writing\ncapabilities. On IMO 2025, top models achieve slightly less than 40%,\ndemonstrating both notable progress and significant room for improvement. So\nfar, we have evaluated over $50$ models across seven competitions, totaling\n$162$ problems. As an evolving benchmark, MathArena will continue to track the\nprogress of LLMs on newly released competitions, ensuring rigorous and\nup-to-date evaluation of mathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Jasper Dekoninck"
                    },
                    {
                        "name": "Ivo Petrov"
                    },
                    {
                        "name": "Nikola Jovanović"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11031v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11031v3",
                "updated": "2025-10-02T11:25:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    25,
                    50,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-16T09:26:06Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    9,
                    26,
                    6,
                    4,
                    136,
                    0
                ],
                "title": "OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic\n  Ontological Understanding, Reasoning and Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic\n  Ontological Understanding, Reasoning and Learning"
                },
                "summary": "Large language models have demonstrated remarkable capabilities across a wide\nrange of tasks, yet their ability to process structured symbolic knowledge\nremains underexplored. To address this gap, we propose a taxonomy of\nontological capabilities and introduce OntoURL, the first comprehensive\nbenchmark designed to systematically evaluate LLMs' capabilities in handling\nontologies -- formal and symbolic representations of domain knowledge. Based on\nthe proposed taxonomy, OntoURL systematically assesses three dimensions:\nunderstanding, reasoning, and learning through 15 distinct tasks comprising\n57,303 questions derived from 40 ontologies across 8 domains. Experiments with\n20 open-source LLMs reveal significant performance differences across models,\ntasks, and domains, with current LLMs showing capabilities in understanding\nontological knowledge but weaknesses in reasoning and learning tasks. Further\nexperiments with few-shot and chain-of-thought prompting illustrate how\ndifferent prompting strategies affect model performance. Additionally, a human\nevaluation reveals that LLMs outperform humans in understanding and reasoning\ntasks but fall short in most learning tasks. These findings highlight both the\npotential and limitations of LLMs in processing symbolic knowledge and\nestablish OntoURL as a critical benchmark for advancing the integration of LLMs\nwith formal knowledge representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable capabilities across a wide\nrange of tasks, yet their ability to process structured symbolic knowledge\nremains underexplored. To address this gap, we propose a taxonomy of\nontological capabilities and introduce OntoURL, the first comprehensive\nbenchmark designed to systematically evaluate LLMs' capabilities in handling\nontologies -- formal and symbolic representations of domain knowledge. Based on\nthe proposed taxonomy, OntoURL systematically assesses three dimensions:\nunderstanding, reasoning, and learning through 15 distinct tasks comprising\n57,303 questions derived from 40 ontologies across 8 domains. Experiments with\n20 open-source LLMs reveal significant performance differences across models,\ntasks, and domains, with current LLMs showing capabilities in understanding\nontological knowledge but weaknesses in reasoning and learning tasks. Further\nexperiments with few-shot and chain-of-thought prompting illustrate how\ndifferent prompting strategies affect model performance. Additionally, a human\nevaluation reveals that LLMs outperform humans in understanding and reasoning\ntasks but fall short in most learning tasks. These findings highlight both the\npotential and limitations of LLMs in processing symbolic knowledge and\nestablish OntoURL as a critical benchmark for advancing the integration of LLMs\nwith formal knowledge representations."
                },
                "authors": [
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Huiyuan Lai"
                    },
                    {
                        "name": "Qianru Meng"
                    },
                    {
                        "name": "Johan Bos"
                    }
                ],
                "author_detail": {
                    "name": "Johan Bos"
                },
                "author": "Johan Bos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11031v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11031v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04512v2",
                "updated": "2025-10-02T11:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    15,
                    14,
                    3,
                    275,
                    0
                ],
                "published": "2025-06-04T23:25:16Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    23,
                    25,
                    16,
                    2,
                    155,
                    0
                ],
                "title": "Schema Generation for Large Knowledge Graphs Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema Generation for Large Knowledge Graphs Using Large Language Models"
                },
                "summary": "Schemas play a vital role in ensuring data quality and supporting usability\nin the Semantic Web and natural language processing. Traditionally, their\ncreation demands substantial involvement from knowledge engineers and domain\nexperts. Leveraging the impressive capabilities of large language models (LLMs)\nin tasks like ontology engineering, we explore schema generation using LLMs. To\nbridge the resource gap, we introduce two datasets: YAGO Schema and Wikidata\nEntitySchema, along with novel evaluation metrics. The LLM-based pipelines\nutilize local and global information from knowledge graphs (KGs) to generate\nschemas in Shape Expressions (ShEx). Experiments demonstrate LLMs' strong\npotential in producing high-quality ShEx schemas, paving the way for scalable,\nautomated schema generation for large KGs. Furthermore, our benchmark\nintroduces a new challenge for structured generation, pushing the limits of\nLLMs on syntactically rich formalisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schemas play a vital role in ensuring data quality and supporting usability\nin the Semantic Web and natural language processing. Traditionally, their\ncreation demands substantial involvement from knowledge engineers and domain\nexperts. Leveraging the impressive capabilities of large language models (LLMs)\nin tasks like ontology engineering, we explore schema generation using LLMs. To\nbridge the resource gap, we introduce two datasets: YAGO Schema and Wikidata\nEntitySchema, along with novel evaluation metrics. The LLM-based pipelines\nutilize local and global information from knowledge graphs (KGs) to generate\nschemas in Shape Expressions (ShEx). Experiments demonstrate LLMs' strong\npotential in producing high-quality ShEx schemas, paving the way for scalable,\nautomated schema generation for large KGs. Furthermore, our benchmark\nintroduces a new challenge for structured generation, pushing the limits of\nLLMs on syntactically rich formalisms."
                },
                "authors": [
                    {
                        "name": "Bohui Zhang"
                    },
                    {
                        "name": "Yuan He"
                    },
                    {
                        "name": "Lydia Pintscher"
                    },
                    {
                        "name": "Albert Meroño Peñuela"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26592v2",
                "updated": "2025-10-02T11:06:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    11,
                    6,
                    30,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-30T17:46:08Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    46,
                    8,
                    1,
                    273,
                    0
                ],
                "title": "Generating Difficult-to-Translate Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Difficult-to-Translate Texts"
                },
                "summary": "Machine translation benchmarks sourced from the real world are quickly\nobsoleted, due to most examples being easy for state-of-the-art translation\nmodels. This limits the benchmark's ability to distinguish which model is\nbetter or to reveal models' weaknesses. Current methods for creating difficult\ntest cases, such as subsampling or from-scratch synthesis, either fall short of\nidentifying difficult examples or suffer from a lack of diversity and\nnaturalness. Inspired by the iterative process of human experts probing for\nmodel failures, we propose MT-breaker, a method where a large language model\niteratively refines a source text to increase its translation difficulty. The\nLLM iteratively queries a target machine translation model to guide its\ngeneration of difficult examples. Our approach generates examples that are more\nchallenging for the target MT model while preserving the diversity of natural\ntexts. While the examples are tailored to a particular machine translation\nmodel during the generation, the difficulty also transfers to other models and\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine translation benchmarks sourced from the real world are quickly\nobsoleted, due to most examples being easy for state-of-the-art translation\nmodels. This limits the benchmark's ability to distinguish which model is\nbetter or to reveal models' weaknesses. Current methods for creating difficult\ntest cases, such as subsampling or from-scratch synthesis, either fall short of\nidentifying difficult examples or suffer from a lack of diversity and\nnaturalness. Inspired by the iterative process of human experts probing for\nmodel failures, we propose MT-breaker, a method where a large language model\niteratively refines a source text to increase its translation difficulty. The\nLLM iteratively queries a target machine translation model to guide its\ngeneration of difficult examples. Our approach generates examples that are more\nchallenging for the target MT model while preserving the diversity of natural\ntexts. While the examples are tailored to a particular machine translation\nmodel during the generation, the difficulty also transfers to other models and\nlanguages."
                },
                "authors": [
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Wenda Xu"
                    },
                    {
                        "name": "Parker Riley"
                    },
                    {
                        "name": "Juraj Juraska"
                    },
                    {
                        "name": "Mara Finkelstein"
                    },
                    {
                        "name": "Markus Freitag"
                    },
                    {
                        "name": "Daniel Deutsch"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Deutsch"
                },
                "author": "Daniel Deutsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07451v2",
                "updated": "2025-10-02T10:51:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    51,
                    9,
                    3,
                    275,
                    0
                ],
                "published": "2025-01-13T16:24:49Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    24,
                    49,
                    0,
                    13,
                    0
                ],
                "title": "A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal\n  Sensor Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal\n  Sensor Fusion"
                },
                "summary": "Model compression is essential in the deployment of large Computer Vision\nmodels on embedded devices. However, static optimization techniques (e.g.\npruning, quantization, etc.) neglect the fact that different inputs have\ndifferent complexities, thus requiring different amount of computations.\nDynamic Neural Networks allow to condition the number of computations to the\nspecific input. The current literature on the topic is very extensive and\nfragmented. We present a comprehensive survey that synthesizes and unifies\nexisting Dynamic Neural Networks research in the context of Computer Vision.\nAdditionally, we provide a logical taxonomy based on which component of the\nnetwork is adaptive: the output, the computation graph or the input.\nFurthermore, we argue that Dynamic Neural Networks are particularly beneficial\nin the context of Sensor Fusion for better adaptivity, noise reduction and\ninformation prioritization. We present preliminary works in this direction. We\ncomplement this survey with a curated repository listing all the surveyed\npapers, each with a brief summary of the solution and the code base when\navailable: https://github.com/DTU-PAS/awesome-dynn-for-cv .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model compression is essential in the deployment of large Computer Vision\nmodels on embedded devices. However, static optimization techniques (e.g.\npruning, quantization, etc.) neglect the fact that different inputs have\ndifferent complexities, thus requiring different amount of computations.\nDynamic Neural Networks allow to condition the number of computations to the\nspecific input. The current literature on the topic is very extensive and\nfragmented. We present a comprehensive survey that synthesizes and unifies\nexisting Dynamic Neural Networks research in the context of Computer Vision.\nAdditionally, we provide a logical taxonomy based on which component of the\nnetwork is adaptive: the output, the computation graph or the input.\nFurthermore, we argue that Dynamic Neural Networks are particularly beneficial\nin the context of Sensor Fusion for better adaptivity, noise reduction and\ninformation prioritization. We present preliminary works in this direction. We\ncomplement this survey with a curated repository listing all the surveyed\npapers, each with a brief summary of the solution and the code base when\navailable: https://github.com/DTU-PAS/awesome-dynn-for-cv ."
                },
                "authors": [
                    {
                        "name": "Fabio Montello"
                    },
                    {
                        "name": "Ronja Güldenring"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Lazaros Nalpantidis"
                    }
                ],
                "author_detail": {
                    "name": "Lazaros Nalpantidis"
                },
                "author": "Lazaros Nalpantidis",
                "arxiv_comment": "Under review at Image and Vision Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.0; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01879v1",
                "updated": "2025-10-02T10:35:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    35,
                    39,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T10:35:39Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    35,
                    39,
                    3,
                    275,
                    0
                ],
                "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and\n  Reintegration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPAIR: Robust Editing via Progressive Adaptive Intervention and\n  Reintegration"
                },
                "summary": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training for large language models (LLMs) is constrained by the high\ncost of acquiring new knowledge or correcting errors and by the unintended side\neffects that frequently arise from retraining. To address these issues, we\nintroduce REPAIR (Robust Editing via Progressive Adaptive Intervention and\nReintegration), a lifelong editing framework designed to support precise and\nlow-cost model updates while preserving non-target knowledge. REPAIR mitigates\nthe instability and conflicts of large-scale sequential edits through a\nclosed-loop feedback mechanism coupled with dynamic memory management.\nFurthermore, by incorporating frequent knowledge fusion and enforcing strong\nlocality guards, REPAIR effectively addresses the shortcomings of traditional\ndistribution-agnostic approaches that often overlook unintended ripple effects.\nOur experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%\nacross multiple model families and significantly reduces knowledge forgetting.\nThis work introduces a robust framework for developing reliable, scalable, and\ncontinually evolving LLMs."
                },
                "authors": [
                    {
                        "name": "Yisu Wang"
                    },
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Haoyuan Song"
                    },
                    {
                        "name": "Wenjie Huang"
                    },
                    {
                        "name": "Chaozheng Wang"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuming Ran"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Ran"
                },
                "author": "Xuming Ran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01878v1",
                "updated": "2025-10-02T10:35:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    35,
                    38,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T10:35:38Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    35,
                    38,
                    3,
                    275,
                    0
                ],
                "title": "Randomized Gradient Subspaces for Efficient Large Language Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized Gradient Subspaces for Efficient Large Language Model\n  Training"
                },
                "summary": "Training large language models (LLMs) is often bottlenecked by extreme memory\ndemands, with optimizer states dominating the footprint. Recent works mitigates\nthis cost by projecting gradients into low-dimensional subspaces using\nsophisticated update strategies. In this paper, we analyze the dynamics of\ngradient space and its underlying subspaces. We find that while a small\nsubspace captures most gradient energy, a significant portion still resides in\nthe residual bulk; moreover, the influence of the core subspace diminishes over\ntime and in deeper layers. We also observe that the gradient space exhibits\nnear-flat curvature, calling for algorithms that explicitly account for this\ngeometry. Motivated by these insights, we introduce a suite of randomized\nalgorithms, GrassWalk and GrassJump, which exploit subspace and achieve\nstate-of-the-art memory savings while improving performance on LLaMA-1B and\nLLaMA-7B pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) is often bottlenecked by extreme memory\ndemands, with optimizer states dominating the footprint. Recent works mitigates\nthis cost by projecting gradients into low-dimensional subspaces using\nsophisticated update strategies. In this paper, we analyze the dynamics of\ngradient space and its underlying subspaces. We find that while a small\nsubspace captures most gradient energy, a significant portion still resides in\nthe residual bulk; moreover, the influence of the core subspace diminishes over\ntime and in deeper layers. We also observe that the gradient space exhibits\nnear-flat curvature, calling for algorithms that explicitly account for this\ngeometry. Motivated by these insights, we introduce a suite of randomized\nalgorithms, GrassWalk and GrassJump, which exploit subspace and achieve\nstate-of-the-art memory savings while improving performance on LLaMA-1B and\nLLaMA-7B pretraining."
                },
                "authors": [
                    {
                        "name": "Sahar Rajabi"
                    },
                    {
                        "name": "Nayeema Nonta"
                    },
                    {
                        "name": "Samanvay Vajpayee"
                    },
                    {
                        "name": "Sirisha Rambhatla"
                    }
                ],
                "author_detail": {
                    "name": "Sirisha Rambhatla"
                },
                "author": "Sirisha Rambhatla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01869v1",
                "updated": "2025-10-02T10:21:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    21,
                    35,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T10:21:35Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    21,
                    35,
                    3,
                    275,
                    0
                ],
                "title": "TACOS: Task Agnostic COordinator of a multi-drone System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACOS: Task Agnostic COordinator of a multi-drone System"
                },
                "summary": "When a single pilot is responsible for managing a multi-drone system, the\ntask demands varying levels of autonomy, from direct control of individual\nUAVs, to group-level coordination, to fully autonomous swarm behaviors for\naccomplishing high-level tasks. Enabling such flexible interaction requires a\nframework that supports multiple modes of shared autonomy. As language models\ncontinue to improve in reasoning and planning, they provide a natural\nfoundation for such systems, reducing pilot workload by enabling high-level\ntask delegation through intuitive, language-based interfaces. In this paper we\npresent TACOS (Task-Agnostic COordinator of a multi-drone System), a unified\nframework that enables high-level natural language control of multi-UAV systems\nthrough Large Language Models (LLMs). TACOS integrates three key capabilities\ninto a single architecture: a one-to-many natural language interface for\nintuitive user interaction, an intelligent coordinator for translating user\nintent into structured task plans, and an autonomous agent that executes plans\ninteracting with the real-world. TACOS allows a LLM to interact with a library\nof executable APIs, bridging semantic reasoning with real-time multi-robot\ncoordination. We demonstrate the system in real-world multi-drone system and\nconduct an ablation study to assess the contribution of each module.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a single pilot is responsible for managing a multi-drone system, the\ntask demands varying levels of autonomy, from direct control of individual\nUAVs, to group-level coordination, to fully autonomous swarm behaviors for\naccomplishing high-level tasks. Enabling such flexible interaction requires a\nframework that supports multiple modes of shared autonomy. As language models\ncontinue to improve in reasoning and planning, they provide a natural\nfoundation for such systems, reducing pilot workload by enabling high-level\ntask delegation through intuitive, language-based interfaces. In this paper we\npresent TACOS (Task-Agnostic COordinator of a multi-drone System), a unified\nframework that enables high-level natural language control of multi-UAV systems\nthrough Large Language Models (LLMs). TACOS integrates three key capabilities\ninto a single architecture: a one-to-many natural language interface for\nintuitive user interaction, an intelligent coordinator for translating user\nintent into structured task plans, and an autonomous agent that executes plans\ninteracting with the real-world. TACOS allows a LLM to interact with a library\nof executable APIs, bridging semantic reasoning with real-time multi-robot\ncoordination. We demonstrate the system in real-world multi-drone system and\nconduct an ablation study to assess the contribution of each module."
                },
                "authors": [
                    {
                        "name": "Alessandro Nazzari"
                    },
                    {
                        "name": "Roberto Rubinacci"
                    },
                    {
                        "name": "Marco Lovera"
                    }
                ],
                "author_detail": {
                    "name": "Marco Lovera"
                },
                "author": "Marco Lovera",
                "arxiv_comment": "6 pages, 6 figures, accepted as poster at 2025 IEEE International\n  Symposium on Multi-Robot & Multi-Agent Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17098v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17098v2",
                "updated": "2025-10-02T10:10:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    10,
                    56,
                    3,
                    275,
                    0
                ],
                "published": "2024-12-22T17:17:28Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    17,
                    17,
                    28,
                    6,
                    357,
                    0
                ],
                "title": "DreamOmni: Unified Image Generation and Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamOmni: Unified Image Generation and Editing"
                },
                "summary": "Currently, the success of large language models (LLMs) illustrates that a\nunified multitasking approach can significantly enhance model usability,\nstreamline deployment, and foster synergistic benefits across different tasks.\nHowever, in computer vision, while text-to-image (T2I) models have\nsignificantly improved generation quality through scaling up, their framework\ndesign did not initially consider how to unify with downstream tasks, such as\nvarious types of editing. To address this, we introduce DreamOmni, a unified\nmodel for image generation and editing. We begin by analyzing existing\nframeworks and the requirements of downstream tasks, proposing a unified\nframework that integrates both T2I models and various editing tasks.\nFurthermore, another key challenge is the efficient creation of high-quality\nediting data, particularly for instruction-based and drag-based editing. To\nthis end, we develop a synthetic data pipeline using sticker-like elements to\nsynthesize accurate, high-quality datasets efficiently, which enables editing\ndata scaling up for unified model training. For training, DreamOmni jointly\ntrains T2I generation and downstream tasks. T2I training enhances the model's\nunderstanding of specific concepts and improves generation quality, while\nediting training helps the model grasp the nuances of the editing task. This\ncollaboration significantly boosts editing performance. Extensive experiments\nconfirm the effectiveness of DreamOmni. The code and model will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, the success of large language models (LLMs) illustrates that a\nunified multitasking approach can significantly enhance model usability,\nstreamline deployment, and foster synergistic benefits across different tasks.\nHowever, in computer vision, while text-to-image (T2I) models have\nsignificantly improved generation quality through scaling up, their framework\ndesign did not initially consider how to unify with downstream tasks, such as\nvarious types of editing. To address this, we introduce DreamOmni, a unified\nmodel for image generation and editing. We begin by analyzing existing\nframeworks and the requirements of downstream tasks, proposing a unified\nframework that integrates both T2I models and various editing tasks.\nFurthermore, another key challenge is the efficient creation of high-quality\nediting data, particularly for instruction-based and drag-based editing. To\nthis end, we develop a synthetic data pipeline using sticker-like elements to\nsynthesize accurate, high-quality datasets efficiently, which enables editing\ndata scaling up for unified model training. For training, DreamOmni jointly\ntrains T2I generation and downstream tasks. T2I training enhances the model's\nunderstanding of specific concepts and improves generation quality, while\nediting training helps the model grasp the nuances of the editing task. This\ncollaboration significantly boosts editing performance. Extensive experiments\nconfirm the effectiveness of DreamOmni. The code and model will be released."
                },
                "authors": [
                    {
                        "name": "Bin Xia"
                    },
                    {
                        "name": "Yuechen Zhang"
                    },
                    {
                        "name": "Jingyao Li"
                    },
                    {
                        "name": "Chengyao Wang"
                    },
                    {
                        "name": "Yitong Wang"
                    },
                    {
                        "name": "Xinglong Wu"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17098v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17098v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01863v1",
                "updated": "2025-10-02T10:08:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    8,
                    59,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T10:08:59Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    8,
                    59,
                    3,
                    275,
                    0
                ],
                "title": "Microscaling Floating Point Formats for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microscaling Floating Point Formats for Large Language Models"
                },
                "summary": "The increasing computational and memory demands of large language models\n(LLMs) necessitate innovative approaches to optimize resource usage without\ncompromising performance. This paper leverages microscaling floating-point\nformats, a novel technique designed to address these challenges by reducing the\nstorage and computational overhead associated with numerical representations in\nLLMs. Unlike traditional floating-point representations that allocate a\ndedicated scale for each value, microscaling employs a shared scale across a\nblock of values, enabling compact one-byte floating-point representations while\nmaintaining an extended dynamic range. We explore the application of\nmicroscaling in the context of 8-bit floating-point formats to significantly\nreduce memory footprint and computational costs. We tested several\nconfigurations of microscaling floats within the GPT-2 LLM architecture,\ndemonstrating that microscaling data formats can achieve competitive accuracy\nduring training and inference, proving its efficacy as a resource-efficient\nalternative for deploying LLMs at scale. The source code is publicly available\nat: https://github.com/unipi-dii-compressedarith/llm.c-sve",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing computational and memory demands of large language models\n(LLMs) necessitate innovative approaches to optimize resource usage without\ncompromising performance. This paper leverages microscaling floating-point\nformats, a novel technique designed to address these challenges by reducing the\nstorage and computational overhead associated with numerical representations in\nLLMs. Unlike traditional floating-point representations that allocate a\ndedicated scale for each value, microscaling employs a shared scale across a\nblock of values, enabling compact one-byte floating-point representations while\nmaintaining an extended dynamic range. We explore the application of\nmicroscaling in the context of 8-bit floating-point formats to significantly\nreduce memory footprint and computational costs. We tested several\nconfigurations of microscaling floats within the GPT-2 LLM architecture,\ndemonstrating that microscaling data formats can achieve competitive accuracy\nduring training and inference, proving its efficacy as a resource-efficient\nalternative for deploying LLMs at scale. The source code is publicly available\nat: https://github.com/unipi-dii-compressedarith/llm.c-sve"
                },
                "authors": [
                    {
                        "name": "Marco Cococcioni"
                    },
                    {
                        "name": "Dario Pagani"
                    },
                    {
                        "name": "Federico Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Federico Rossi"
                },
                "author": "Federico Rossi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00919v2",
                "updated": "2025-10-02T09:55:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    9,
                    55,
                    14,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-01T13:57:53Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    57,
                    53,
                    2,
                    274,
                    0
                ],
                "title": "Benchmarking Foundation Models with Retrieval-Augmented Generation in\n  Olympic-Level Physics Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Foundation Models with Retrieval-Augmented Generation in\n  Olympic-Level Physics Problem Solving"
                },
                "summary": "Retrieval-augmented generation (RAG) with foundation models has achieved\nstrong performance across diverse tasks, but their capacity for expert-level\nreasoning-such as solving Olympiad-level physics problems-remains largely\nunexplored. Inspired by the way students prepare for competitions by reviewing\npast problems, we investigate the potential of RAG to enhance physics reasoning\nin foundation models. We introduce PhoPile, a high-quality multimodal dataset\nspecifically designed for Olympiad-level physics, enabling systematic study of\nretrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,\ncapturing the inherently multimodal nature of physics problem solving. Using\nPhoPile, we benchmark RAG-augmented foundation models, covering both large\nlanguage models (LLMs) and large multimodal models (LMMs) with multiple\nretrievers. Our results demonstrate that integrating retrieval with physics\ncorpora can improve model performance, while also highlighting challenges that\nmotivate further research in retrieval-augmented physics reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) with foundation models has achieved\nstrong performance across diverse tasks, but their capacity for expert-level\nreasoning-such as solving Olympiad-level physics problems-remains largely\nunexplored. Inspired by the way students prepare for competitions by reviewing\npast problems, we investigate the potential of RAG to enhance physics reasoning\nin foundation models. We introduce PhoPile, a high-quality multimodal dataset\nspecifically designed for Olympiad-level physics, enabling systematic study of\nretrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,\ncapturing the inherently multimodal nature of physics problem solving. Using\nPhoPile, we benchmark RAG-augmented foundation models, covering both large\nlanguage models (LLMs) and large multimodal models (LMMs) with multiple\nretrievers. Our results demonstrate that integrating retrieval with physics\ncorpora can improve model performance, while also highlighting challenges that\nmotivate further research in retrieval-augmented physics reasoning."
                },
                "authors": [
                    {
                        "name": "Shunfeng Zheng"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Zihan Zhang"
                    },
                    {
                        "name": "Zhitan Wu"
                    },
                    {
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "arxiv_comment": "Accepted to EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12051v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12051v5",
                "updated": "2025-10-02T09:49:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    9,
                    49,
                    7,
                    3,
                    275,
                    0
                ],
                "published": "2025-03-15T08:54:25Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    8,
                    54,
                    25,
                    5,
                    74,
                    0
                ],
                "title": "TLUE: A Tibetan Language Understanding Evaluation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TLUE: A Tibetan Language Understanding Evaluation Benchmark"
                },
                "summary": "Large language models have made tremendous progress in recent years, but\nlow-resource languages, like Tibetan, remain significantly underrepresented in\ntheir evaluation. Despite Tibetan being spoken by over seven million people, it\nhas largely been neglected in the development and assessment of large language\nmodels. To address this gap, we present a \\textbf{T}ibetan \\textbf{L}anguage\n\\textbf{U}nderstanding \\textbf{E}valuation Benchmark, \\textbf{TLUE}, the first\nlarge-scale benchmark for measuring the proficiency of LLMs in the Tibetan\nlanguage. \\textbf{TLUE} comprises two major components: a comprehensive\nmulti-task understanding benchmark spanning 5 domains and 67 subdomains, and a\nsafety benchmark encompassing 7 subdomains. Then, we evaluate a diverse set of\nstate-of-the-art large language models. Experimental results demonstrate that\nmost large language models perform below the random baseline, highlighting the\nconsiderable challenges they face in Tibetan language processing. \\textbf{TLUE}\nprovides a crucial foundation for advancing future research in Tibetan language\nunderstanding and highlights the importance of promoting greater inclusivity in\nthe development of large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have made tremendous progress in recent years, but\nlow-resource languages, like Tibetan, remain significantly underrepresented in\ntheir evaluation. Despite Tibetan being spoken by over seven million people, it\nhas largely been neglected in the development and assessment of large language\nmodels. To address this gap, we present a \\textbf{T}ibetan \\textbf{L}anguage\n\\textbf{U}nderstanding \\textbf{E}valuation Benchmark, \\textbf{TLUE}, the first\nlarge-scale benchmark for measuring the proficiency of LLMs in the Tibetan\nlanguage. \\textbf{TLUE} comprises two major components: a comprehensive\nmulti-task understanding benchmark spanning 5 domains and 67 subdomains, and a\nsafety benchmark encompassing 7 subdomains. Then, we evaluate a diverse set of\nstate-of-the-art large language models. Experimental results demonstrate that\nmost large language models perform below the random baseline, highlighting the\nconsiderable challenges they face in Tibetan language processing. \\textbf{TLUE}\nprovides a crucial foundation for advancing future research in Tibetan language\nunderstanding and highlights the importance of promoting greater inclusivity in\nthe development of large language models."
                },
                "authors": [
                    {
                        "name": "Fan Gao"
                    },
                    {
                        "name": "Cheng Huang"
                    },
                    {
                        "name": "Nyima Tashi"
                    },
                    {
                        "name": "Xiangxiang Wang"
                    },
                    {
                        "name": "Thupten Tsering"
                    },
                    {
                        "name": "Ban Ma-bao"
                    },
                    {
                        "name": "Renzeg Duojie"
                    },
                    {
                        "name": "Gadeng Luosang"
                    },
                    {
                        "name": "Rinchen Dongrub"
                    },
                    {
                        "name": "Dorje Tashi"
                    },
                    {
                        "name": "Hao Wang Xiao Feng"
                    },
                    {
                        "name": "Yongbin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Yu"
                },
                "author": "Yongbin Yu",
                "arxiv_comment": "Accepted by EMNLP Main Conference (Poster)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12051v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12051v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01842v1",
                "updated": "2025-10-02T09:37:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    9,
                    37,
                    12,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T09:37:12Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    9,
                    37,
                    12,
                    3,
                    275,
                    0
                ],
                "title": "Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model\n  Selection and Benchmarking for Tabular datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model\n  Selection and Benchmarking for Tabular datasets"
                },
                "summary": "The field of AutoML has made remarkable progress in post-hoc model selection,\nwith libraries capable of automatically identifying the most performing models\nfor a given dataset. Nevertheless, these methods often rely on exhaustive\nhyperparameter searches, where methods automatically train and test different\ntypes of models on the target dataset. Contrastingly, pre-hoc prediction\nemerges as a promising alternative, capable of bypassing exhaustive search\nthrough intelligent pre-selection of models. Despite its potential, pre-hoc\nprediction remains under-explored in the literature. This paper explores the\nintersection of AutoML and pre-hoc model selection by leveraging traditional\nmodels and Large Language Model (LLM) agents to reduce the search space of\nAutoML libraries. By relying on dataset descriptions and statistical\ninformation, we reduce the AutoML search space. Our methodology is applied to\nthe AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark\ncontaining 175 tabular classification datasets available on OpenML. The\nproposed approach offers a shift in AutoML workflows, significantly reducing\ncomputational overhead, while still selecting the best model for the given\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of AutoML has made remarkable progress in post-hoc model selection,\nwith libraries capable of automatically identifying the most performing models\nfor a given dataset. Nevertheless, these methods often rely on exhaustive\nhyperparameter searches, where methods automatically train and test different\ntypes of models on the target dataset. Contrastingly, pre-hoc prediction\nemerges as a promising alternative, capable of bypassing exhaustive search\nthrough intelligent pre-selection of models. Despite its potential, pre-hoc\nprediction remains under-explored in the literature. This paper explores the\nintersection of AutoML and pre-hoc model selection by leveraging traditional\nmodels and Large Language Model (LLM) agents to reduce the search space of\nAutoML libraries. By relying on dataset descriptions and statistical\ninformation, we reduce the AutoML search space. Our methodology is applied to\nthe AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark\ncontaining 175 tabular classification datasets available on OpenML. The\nproposed approach offers a shift in AutoML workflows, significantly reducing\ncomputational overhead, while still selecting the best model for the given\ndataset."
                },
                "authors": [
                    {
                        "name": "Yannis Belkhiter"
                    },
                    {
                        "name": "Seshu Tirupathi"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Sachin Sharma"
                    },
                    {
                        "name": "John D. Kelleher"
                    }
                ],
                "author_detail": {
                    "name": "John D. Kelleher"
                },
                "author": "John D. Kelleher",
                "arxiv_comment": "Oral Presentations ADAPT Annual Scientific Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17506v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17506v3",
                "updated": "2025-10-02T09:34:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    9,
                    34,
                    54,
                    3,
                    275,
                    0
                ],
                "published": "2024-05-26T14:27:26Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    27,
                    26,
                    6,
                    147,
                    0
                ],
                "title": "Subspace Node Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subspace Node Pruning"
                },
                "summary": "Improving the efficiency of neural network inference is undeniably important\nin a time where commercial use of AI models increases daily. Node pruning is\nthe art of removing computational units such as neurons, filters, attention\nheads, or even entire layers to significantly reduce inference time while\nretaining network performance. In this work, we propose the projection of unit\nactivations to an orthogonal subspace in which there is no redundant activity\nand within which we may prune nodes while simultaneously recovering the impact\nof lost units via linear least squares. We furthermore show that the order in\nwhich units are orthogonalized can be optimized to maximally rank units by\ntheir redundancy. Finally, we leverage these orthogonal subspaces to\nautomatically determine layer-wise pruning ratios based upon the relative scale\nof node activations in our subspace, equivalent to cumulative variance. Our\nmethod matches or exceeds state-of-the-art pruning results on ImageNet-trained\nVGG-16, ResNet-50 and DeiT models while simultaneously having up to 24x lower\ncomputational cost than alternative methods. We also demonstrate that this\nmethod can be applied in a one-shot manner to OPT LLM models, again\noutperforming competing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the efficiency of neural network inference is undeniably important\nin a time where commercial use of AI models increases daily. Node pruning is\nthe art of removing computational units such as neurons, filters, attention\nheads, or even entire layers to significantly reduce inference time while\nretaining network performance. In this work, we propose the projection of unit\nactivations to an orthogonal subspace in which there is no redundant activity\nand within which we may prune nodes while simultaneously recovering the impact\nof lost units via linear least squares. We furthermore show that the order in\nwhich units are orthogonalized can be optimized to maximally rank units by\ntheir redundancy. Finally, we leverage these orthogonal subspaces to\nautomatically determine layer-wise pruning ratios based upon the relative scale\nof node activations in our subspace, equivalent to cumulative variance. Our\nmethod matches or exceeds state-of-the-art pruning results on ImageNet-trained\nVGG-16, ResNet-50 and DeiT models while simultaneously having up to 24x lower\ncomputational cost than alternative methods. We also demonstrate that this\nmethod can be applied in a one-shot manner to OPT LLM models, again\noutperforming competing methods."
                },
                "authors": [
                    {
                        "name": "Joshua Offergeld"
                    },
                    {
                        "name": "Marcel van Gerven"
                    },
                    {
                        "name": "Nasir Ahmad"
                    }
                ],
                "author_detail": {
                    "name": "Nasir Ahmad"
                },
                "author": "Nasir Ahmad",
                "arxiv_comment": "18 pages, 10 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17506v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17506v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]